{
    "abstractText": "Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Advait Gadhikar"
        },
        {
            "affiliations": [],
            "name": "Rebekka Burkholz"
        }
    ],
    "id": "SP:86273eccbdcf703733b7c93c0fd572ea1326688a",
    "references": [
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Elad Hazan"
            ],
            "title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Noah Golowich",
                "Wei Hu"
            ],
            "title": "A convergence analysis of gradient descent for deep linear neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Daniel Hsu",
                "Siyuan Ma",
                "Soumik Mandal"
            ],
            "title": "Reconciling modern machinelearning practice and the classical bias\u2013variance trade-off",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Etienne Boursier",
                "Loucas Pillaud-Vivien",
                "Nicolas Flammarion"
            ],
            "title": "Gradient flow dynamics of shallow reLU networks for square loss and orthogonal inputs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Rebekka Burkolz"
            ],
            "title": "Most activation functions can win the lottery without excessive depth",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyu Chang",
                "Yingcong Li",
                "Samet Oymak",
                "Christos Thrampoulidis"
            ],
            "title": "Provable benefits of overparameterization in model compression: From double descent to pruning neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Arthur da Cunha",
                "Emanuele Natale",
                "Laurent Viennot"
            ],
            "title": "Proving the lottery ticket hypothesis for convolutional neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ilias Diakonikolas",
                "Surbhi Goel",
                "Sushrut Karmalkar",
                "Adam R Klivans",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Approximation schemes for relu regression",
            "venue": "In Conference on learning theory,",
            "year": 2020
        },
        {
            "authors": [
                "Simon S Du",
                "Wei Hu",
                "Jason D Lee"
            ],
            "title": "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Utku Evci",
                "Yani Ioannou",
                "Cem Keskin",
                "Yann Dauphin"
            ],
            "title": "Gradient flow in sparse neural networks and how lottery tickets win",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Damien Ferbach",
                "Christos Tsirigotis",
                "Gauthier Gidel",
                "Joey Bose"
            ],
            "title": "A general framework for proving the equivariant strong lottery ticket hypothesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jonas Fischer",
                "Advait Gadhikar",
                "Rebekka Burkholz"
            ],
            "title": "Lottery tickets with nonzero biases",
            "venue": "arXiv preprint arXiv:2110.11150,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin"
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "David J Schwab",
                "Ari S Morcos"
            ],
            "title": "The early phase of neural network training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "David J. Schwab",
                "Ari S. Morcos"
            ],
            "title": "Training batchnorm and only batchnorm: On the expressive power of random features in {cnn}s",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Spencer Frei",
                "Yuan Cao",
                "Quanquan Gu"
            ],
            "title": "Agnostic learning of a single neuron with gradient descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Advait Harshal Gadhikar",
                "Sohom Mukherjee",
                "Rebekka Burkholz"
            ],
            "title": "Why random pruning is all we need to start sparse",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Anna Golubeva",
                "Guy Gur-Ari",
                "Behnam Neyshabur"
            ],
            "title": "Are wider nets better given the same number of parameters",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Seyed Mohammadreza Mousavi Kalan",
                "Mahdi Soltanolkotabi",
                "A Salman Avestimehr"
            ],
            "title": "Fitting relus via sgd and quantized sgd",
            "venue": "IEEE international symposium on information theory (ISIT),",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Aditya Kusupati",
                "Vivek Ramanujan",
                "Raghav Somani",
                "Mitchell Wortsman",
                "Prateek Jain",
                "Sham Kakade",
                "Ali Farhadi"
            ],
            "title": "Soft threshold weight reparameterization for learnable sparsity",
            "venue": "In Proceedings of the International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Namhoon Lee",
                "Thalaiyasingam Ajanthan",
                "Philip H.S. Torr"
            ],
            "title": "Snip: single-shot network pruning based on connection sensitivity",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Sangmin Lee",
                "Byeongsu Sim",
                "Jong Chul Ye"
            ],
            "title": "Support vectors and gradient dynamics for implicit bias in relu networks",
            "venue": "arXiv preprint arXiv:2202.05510,",
            "year": 2022
        },
        {
            "authors": [
                "Sangmin Lee",
                "Byeongsu Sim",
                "Jong Chul Ye"
            ],
            "title": "Magnitude and angle dynamics in training single relu neurons",
            "venue": "arXiv preprint arXiv:2209.13394,",
            "year": 2022
        },
        {
            "authors": [
                "Ning Liu",
                "Geng Yuan",
                "Zhengping Che",
                "Xuan Shen",
                "Xiaolong Ma",
                "Qing Jin",
                "Jian Ren",
                "Jian Tang",
                "Sijia Liu",
                "Yanzhi Wang"
            ],
            "title": "Lottery ticket preserves weight correlation: Is it desirable or not",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shiwei Liu",
                "Tianlong Chen",
                "Xiaohan Chen",
                "Li Shen",
                "Decebal Constantin Mocanu",
                "Zhangyang Wang",
                "Mykola Pechenizkiy"
            ],
            "title": "The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaolong Ma",
                "Geng Yuan",
                "Xuan Shen",
                "Tianlong Chen",
                "Xuxi Chen",
                "Xiaohan Chen",
                "Ning Liu",
                "Minghai Qin",
                "Sijia Liu",
                "Zhangyang Wang",
                "Yanzhi Wang"
            ],
            "title": "Sanity checks for lottery tickets: Does your winning ticket really win the jackpot",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Eran Malach",
                "Gilad Yehudai",
                "Shai Shalev-Schwartz",
                "Ohad Shamir"
            ],
            "title": "Proving the lottery ticket hypothesis: Pruning is all you need",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Zhiyuan Li",
                "Srinadh Bhojanapalli",
                "Yann LeCun",
                "Nathan Srebro"
            ],
            "title": "The role of over-parametrization in generalization of neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Laurent Orseau",
                "Marcus Hutter",
                "Omar Rivasplata"
            ],
            "title": "Logarithmic pruning is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Samet Oymak",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Overparameterized nonlinear learning: Gradient descent takes the shortest path",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Mansheej Paul",
                "Brett Larsen",
                "Surya Ganguli",
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Lottery tickets on a data diet: Finding initializations with sparse trainable networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mansheej Paul",
                "Feng Chen",
                "Brett W. Larsen",
                "Jonathan Frankle",
                "Surya Ganguli",
                "Gintare Karolina Dziugaite"
            ],
            "title": "Unmasking the lottery ticket hypothesis: What\u2019s encoded in a winning ticket\u2019s mask",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ankit Pensia",
                "Shashank Rajput",
                "Alliot Nagle",
                "Harit Vishwakarma",
                "Dimitris Papailiopoulos"
            ],
            "title": "Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Vivek Ramanujan",
                "Mitchell Wortsman",
                "Aniruddha Kembhavi",
                "Ali Farhadi",
                "Mohammad Rastegari"
            ],
            "title": "What\u2019s hidden in a randomly weighted neural network",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Vivek Ramanujan",
                "Mitchell Wortsman",
                "Aniruddha Kembhavi",
                "Ali Farhadi",
                "Mohammad Rastegari"
            ],
            "title": "What\u2019s hidden in a randomly weighted neural network",
            "venue": "In Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Renda",
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "Comparing rewinding and fine-tuning in neural network pruning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "K Simonyan",
                "A Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In 3rd International Conference on Learning Representations (ICLR",
            "year": 2015
        },
        {
            "authors": [
                "Mahdi Soltanolkotabi"
            ],
            "title": "Learning relus via gradient descent",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jingtong Su",
                "Yihang Chen",
                "Tianle Cai",
                "Tianhao Wu",
                "Ruiqi Gao",
                "Liwei Wang",
                "Jason D Lee"
            ],
            "title": "Sanity-checking pruning methods: Random tickets can win the jackpot",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yan Shuo Tan",
                "Roman Vershynin"
            ],
            "title": "Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval",
            "year": 1910
        },
        {
            "authors": [
                "Hidenori Tanaka",
                "Daniel Kunin",
                "Daniel L. Yamins",
                "Surya Ganguli"
            ],
            "title": "Pruning neural networks without any data by iteratively conserving synaptic flow",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gal Vardi",
                "Gilad Yehudai",
                "Ohad Shamir"
            ],
            "title": "Learning a single neuron with bias using gradient descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Qingyang Wang",
                "Michael Alan Powell",
                "Eric W Bridgeford",
                "Ali Geisa",
                "Joshua T Vogelstein"
            ],
            "title": "Polarity is all you need to learn and transfer",
            "year": 2023
        },
        {
            "authors": [
                "Gilad Yehudai",
                "Shamir Ohad"
            ],
            "title": "Learning a single neuron with gradient methods",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Shuai Zhang",
                "Meng Wang",
                "Sijia Liu",
                "Pin-Yu Chen",
                "Jinjun Xiong"
            ],
            "title": "Why lottery ticket wins? a theoretical perspective of sample complexity on sparse neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hattie Zhou",
                "Janice Lan",
                "Rosanne Liu",
                "Jason Yosinski"
            ],
            "title": "Deconstructing lottery tickets: Zeros, signs, and the supermask",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Overparametrization has been key to the huge success of deep learning (Bubeck et al., 2023; Neyshabur et al., 2019; Belkin et al., 2019). Adding more trainable parameters to models has shown to consistently improve performance of deep neural networks over multiple tasks. While it has been shown that there often exist sparser neural network representations that can achieve competitive performance, they are usually not well trainable by standard neural network optimization approaches (Evci et al., 2022), which is a major challenge for learning small scale (sparse) neural networks from scratch to save computational resources.\nThe Lottery Ticket Hypothesis (LTH) by Frankle & Carbin (2019) is based on an empirical existence proof that the optimization of at least some sparse neural network architectures is feasible with the right initialization. According to the LTH, dense, randomly initialized neural networks contain subnetworks that can be trained in isolation with the same training algorithm that is successful for the dense networks. A strong version of this hypothesis Ramanujan et al. (2020a); Zhou et al. (2019), which has also been proven theoretically (Malach et al., 2020; Pensia et al., 2020; Orseau et al., 2020; Fischer et al., 2021; Burkholz et al.; Burkolz, 2022; da Cunha et al., 2022; Gadhikar et al., 2023; Ferbach et al., 2023), suggests that the identified initial parameters might be strongly tied to the identified sparse structure. Related experimental studies and theoretical investigations support this conjecture (Evci et al., 2022; Paul et al., 2023).\nIn line with these findings, contemporary pruning algorithms currently address the dual challenge of structure and parameter learning only jointly. Iterative Magnitude Pruning (IMP) (Frankle & Carbin, 2019) and successive methods derived from it, like Weight Rewinding (WR) (Frankle et al., 2020a) and Learning Rate Rewinding (LRR) (Renda et al., 2020; Liu et al., 2021a) follow an iterative pruning \u2013 training procedure that removes a fraction of parameters in every pruning iteration until a target sparsity is reached. This achieves state-of-the-art neural network sparsification (Paul et al., 2023), albeit at substantial computational cost.\nWhile this cost can be reduced by starting the pruning procedure from a sparser, randomly pruned network (Gadhikar et al., 2023), the question remains whether the identification of small sparse neural network models necessitates training an overparameterized model first. Multiple works attest that overparameterization aids pruning (Zhang et al., 2021; Chang et al., 2021; Golubeva et al., 2020).\nThis suggests that overparameterized optimization obtains information that should be valuable for the performance of a sparsified model. Conforming with this reasoning, IMP was found less effective for complex architectures than Weight Rewinding (WR) (Renda et al., 2020), which rewinds parameters to values that have been obtained by training the dense, overparameterized model for a few epochs (instead of rewinding to their initial value like IMP). LRR (Renda et al., 2020) completely gets rid of the weight rewinding step and continues to train a pruned model from its current state while repeating the same learning rate schedule in every iteration. Eliminating the parameter rewinding step has enabled LRR to achieve consistent accuracy gains and improve the movement of parameters away from their initial values (Liu et al., 2021a).\nComplimentary to Paul et al. (2023); Liu et al. (2021a), we identify a mechanism that provides LRR with (provable) optimization advantages that are facilitated by pruning a trained overparameterized model. First, we gain provable insights into LRR and IMP for a minimal example, i.e., learning a single hidden ReLU neuron. Our exact solutions to the gradient flow dynamics for high-dimensional inputs could be of independent interest. The initial overparameterization of the hidden neuron enables learning provably and facilitates the identification of the correct ground truth mask by pruning. LRR benefits from the robustness of the overparameterized neuron to different parameter initializations, as it is capable of switching initially problematic parameter sign configurations that would result in the failure of IMP.\nWe verify in extensive experiments on standard benchmark data that our theoretical insights capture a practically relevant phenomenon and that our intuition regarding parameter sign switches also applies to more complex architectures and tasks. We find that while LRR is able to perform more sign flips, these happen in early training \u2013 pruning iterations, when a higher degree of overparameterization is available to facilitate them. In this regime, LRR is also more robust to sign perturbations.\nThis observation suggests that LRR could define a more reliable parameter training algorithm than IMP for general masks. However in iterative pruning schemes like IMP and LRR, the mask identification step is closely coupled with parameter optimization. Changing either of these aspects could affect the overall performance considerably. For example, learning only the mask (strong lottery tickets (Ramanujan et al., 2020b)) or learning only the parameters with a random mask (Liu et al., 2021b; Gadhikar et al., 2023) are unable to achieve the same performance as IMP at high sparsities. Yet, we carefully disentangle the optimization of parameters and mask learning aspect to show that LRR achieves more reliable training results for different masks. In addition, it can also identify a better mask that can sometimes achieve a higher performance than the IMP mask, even when both are optimized even with IMP.\nContributions. Our main contributions are as follows:\n\u2022 To analyze the advantages of LRR for parameter optimization and mask identification, we conduct experiments that disentangle these two aspects and find that the benefits of LRR are two-fold. (a) LRR often finds a better sparse mask during training and (b) LRR is more effective in optimizing parameters of a diverse masks (eg: a random mask).\n\u2022 We experimentally verify that, in comparison with IMP, LRR is more flexible in switching parameter signs during early pruning iterations, when the network is still overparameterized. It also recovers more reliably from sign perturbations.\n\u2022 For a univariate single hidden neuron network, we derive closed form solutions of its gradient flow dynamics and compare them with training and pruning an overparameterized neuron. LRR is provably more likely to converge to a ground truth target while IMP is more susceptible to failure due to its inability to switch initial problematic weight signs."
        },
        {
            "heading": "1.1 RELATED WORK",
            "text": "Insights into IMP. Paul et al. (2023) attribute the success of IMP to iteratively pruning a small fraction of parameters in every step which allows consecutively pruned networks to be linearly mode connected (Frankle et al., 2020a; Paul et al., 2022). This can be achieved by WR if the dense network is trained for sufficently many epochs. They argue that as long as consecutive networks are sufficiently close, IMP finds sparse networks that belong to the same linearly mode connected region of the loss landscape. Evci et al. (2022) similarly claim that IMP finds an initialization that is close to the pruning solution and within the same basin of attraction. Liu et al. (2021a) similarly show\nthat initial and final weights are correlated for IMP. In our experiments we study the WR variant of IMP, where the dense network has been trained for sufficiently many epochs to obtain the initial parameters for IMP, but we still find that, in comparison, LRR switches more signs and can achieve better performance.\nThe role of sign switches. While Wang et al. (2023) have recently verified the importance of suitable parameter signs for better training of neural networks in general, they have not analyzed their impact on neural network sparsification. Zhou et al. (2019) study the weight distributions for IMP and find that rewinding only parameter signs can be sufficient. Large scale problems, however, rely on learning signs in early epochs and require a good combination with respective parameter magnitudes, as discussed by Frankle et al. (2020b) for IMP. These results are still focused on the IMP learning mechanism and its coupling to the mask learning. In contrast, we show that identifying good signs (and magnitudes) early enables LRR to not only find a better mask but to also learn more effectively if the mask identification is independent from the parameter optimization.\nMask optimization. Random sparse masks also qualify as trainable lottery tickets Su et al. (2020); Ma et al. (2021); Liu et al. (2021b) which suggests that the mask identification can be separated from parameter optimization upto certain sparsities (Gadhikar et al., 2023). Our experiments isolate the advantages of LRR on both these aspects.\nTraining dynamics of overparametrized networks. The training dynamics of overparametrized networks have been theoretically investigated in multiple works, which frequently employ a balanced initialization (Du et al., 2018) and a related conservation law under gradient flow in their analysis. Arora et al. (2018; 2019) study deep linear networks in this context, while Du et al. theoretically characterizes the gradient flow dynamics of two layer ReLU networks. While they require a high degree of overparameterization, Boursier et al. (2022) obtains more detailed statements on the dynamics with a more flexible paramterization but assume orthogonal data input.\nSingle hidden neuron setting. These results do not directly transfer to the single hidden neuron case, which has been subject of active research Yehudai & Ohad (2020); Lee et al. (2022a); Vardi et al. (2021); Oymak & Soltanolkotabi (2019); Soltanolkotabi (2017); Kalan et al. (2019); Frei et al. (2020); Diakonikolas et al. (2020); Tan & Vershynin (2019); Du et al.. Most works assume that the outer weight a is fixed, while only the inner weight vector w is learned and mostly study noise free data. We extend similar results to trainable outer weight and characterize the precise training dynamics of an univariate (masked) neuron in closed form. Lee et al. (2022b) study a similar univariate case but do not consider label noise in their analysis.\nMost importantly, similar results have not been deduced and studied under the premise of network pruning. They enable us to derive a mechanism that gives LRR a provable benefit over IMP, which is inherited from overparameterized training."
        },
        {
            "heading": "2 THEORETICAL INSIGHTS FOR A SINGLE HIDDEN NEURON NETWORK",
            "text": "Intuition behind LRR versus IMP. The advantage of IMP is that it was designed to identify lottery tickets and thus successfully initialize sparse masks (i.e. sparse neural network structures). However, in order to find such an initialization, we show that the information obtained in earlier pruning iterations with the aid of overparameterization is valuable in learning better models. Notably, we find that each pruning iteration transfers key information about parameter signs to the next iteration. Forgetting this information (due to weight rewinding) means that IMP is challenged to learn the appropriate parameter signs from scratch in each iteration.\nTo establish provable insights of this form, we face the theoretical challenge to describe the learning dynamics of the parameters in response to different initializations. We therefore focus on an example of minimum complexity that still enables us to isolate a mechanism by which LRR has a higher chance to succeed in solving a learning task. In doing so, we study a single hidden neuron, 2-layer neural network under gradient flow dynamics, as visualized in Fig. 1 (a).\nFor our purpose, we focus on two main aspects: (i) The trainability of the masked neural network (i.e., a single hidden neuron with d = 1 input), once the sparse mask is identified. (ii) The ability of LRR to leverage the initial overparameterization (i.e., a single hidden neuron with d > 1 inputs) in the model to learn appropriate parameter signs.\nRegarding (i), we have to distinguish four different initialization scenarios. Only one scenario (yellow quadrant in Fig. 1 (a)) leads to accurate learning. LRR is able to inherit this setup from the trained overparameterized network and succeed (see Fig. 1 (c)) in a case when IMP fails (Fig. 1 (b)) because it rewinds its parameters to an initial problematic setting. To explain these results in detail, we have to formalize the set-up.\nLRR. We focus on comparing Iterative Magnitude Pruning (IMP) and Learning Rate Rewinding (LRR). Both cases comprise iterative pruning-training cycles. The i-th pruning cycle identifies a binary mask M (i) \u2208 {0, 1}N , which is established by pruning a fraction of neural network parameters \u03b8(i\u22121) with the smallest magnitude. A training cycle relearns the remaining parameters \u03b8(i) of the masked neural network f(x | M(i)\u03b8(i)). The only difference between LRR and IMP is induced by how each training cycle is initialized (See Fig. 1(b)). In case of LRR, the parameters of the previous training cycle that were not pruned away are used as initial values of the new training cycle so that \u03b8(i)(0) = \u03b8(i\u22121)(tend). Thus, training continues (with a learning rate that is reset to its initial value).\nIMP. In case of IMP, each pruning iteration starts from the same initial parameters \u03b8(i)(0) = \u03b8(0)(0) and parameters learnt in the previous iteration are forgotten. While our theoretical analysis focuses on IMP, Frankle et al. (2021); Su et al. (2020); Ma et al. (2021) have shown that IMP does not scale well to larger architectures. Hence, we employ the more successful variant Weight Rewinding (WR) in our experiments. Here, the parameters are not rewound to their initial values but to the parameters of the dense network which was trained for a few warm-up epochs \u03b8(i)(0) = \u03b8(0)(k) Frankle et al. (2020a); Renda et al. (2020). Our theory also applies to this case but we will mostly discuss rewinding to the initial values for simplicity. From now on we use IMP to refer to IMP in our theory and WR in our experiments.\nProblem set-up. Consider a single hidden neuron network with input x \u2208 Rd, given as f(x) := a\u03d5(wx) with the ReLU activation \u03d5(x) = max{x, 0} (see Fig. 1). Note that one of the weights could assume the role of a bias if one of the inputs is constant in all samples, e.g., xi = 1. The task is to learn a scalar target t(x) = \u03d5(x1) only dependent on the first coordinate of x, from which n noisy training data points Y = t(X1) + \u03b6 are generated (upper case denotes random variables.) For simplicity, we assume that all input components are independently and identically (iid) distributed and follow a normal distribution Xi \u223c N (0, I/d), while the noise follows an independent normal distribution \u03b6 \u223c N (0, \u03c32). The precise assumptions on the data distributions are not crucial for our results but clarify our later experimental setting. Based on a training set (xi, yi) for i \u2208 [n] = {1, 2.., n}, learning implies minimizing the mean squared error under gradient flow\nL = 1 2n n\u2211 i=1 (f(xi)\u2212 yi)2 , dL dt = \u2212\u2202L \u2202a ; dwi dt = \u2212 \u2202L \u2202wi (\u2200i \u2208 [1, d]), (1)\nwhich resembles the dynamics induced by minimizing L with gradient descent for sufficiently small learning rates. Note that also higher learning rates and more advanced optimizers like LBFGS converge to the same values that we derive based on gradient flow for this exemplary problem. Stochastic Gradient (SGD) would introduce additional batch noise and exaggerate the issue that we\nwill discuss for small sample sizes. As gradient flow is sufficient to highlight the mechanism that we are interested in, we focus our analysis on this case.\nTo simplify our exposition and to establish closed form solutions, we assume that the parameters are initialized in a balanced state such that a(0)2 = \u2211d i=1 w 2 i (0), which is preserved through training\n(Arora et al., 2018; Du et al.; 2018) so that a(t)2 = \u2211d\ni=1 w 2 i (t).\n2.1 TRAINING DYNAMICS FOR ONE-DIMENSIONAL INPUT (d = 1)\nLet us start with the case, in which we have identified the correct mask by pruning away the remaining inputs and we know the ground truth structure of the problem. Studying this one dimensional case will help us identify typical failure conditions in the learning dynamics and how these failure conditions are more likely to occur in IMP than LRR. Knowing the correct mask, our model is reduced to the one-dimensional input case (d = 1) after pruning, so that f(x) = a\u03d5(wx), while the target labels are drawn from y \u223c \u03d5(x) + \u03b6. Since the ReLU neuron is active only when wx > 0, we have to distinguish all possible initial sign combinations of w and a to analyze the learning dynamics. The following theorem states our main result, which is also visualized in Fig. 1 (a).\nTheorem 2.1. Let a target t(x) = \u03d5(x) and network f(x) = a\u03d5(wx) be given such that a and w follow the gradient flow dynamics (1) with a random balanced parameter initialization and sufficiently many samples. If a(0) > 0 and w(0) > 0, f(x) can learn the correct target. In all other cases (a(0) > 0, w(0) < 0), (a(0) < 0, w(0) > 0) and (a(0) < 0, w(0) < 0) learning fails.\nThe proof in Appendix A.1 derives the closed form solutions of the learning dynamics of f(x) under gradient flow for each combination of initial signs. It establishes that training a single neuron f(x) = a\u03d5(wx) from scratch to learn the noisy target \u03d5(x) + \u03b6 can be expected to fail at least with probability 3/4 if we choose a standard balanced parameter initialization scheme where either signs are equally likely to occur for a(0), w(0).\nWhy should this imply a disadvantage for IMP over LRR? As we will argue next, overparameterization in form of additional independent input dimensions x \u2208 Rd can improve substantially the learning success as the set of samples activated by ReLU becomes less dependent on the initialization of the first element w1(0) of w. Thus training an overparameterized neuron first, enables LRR and IMP to identify the correct mask. Yet, after reinitialization, IMP is reduced to the failure case described above with probability 3/4, considering the combination of initial signs of a(0) and w1(0). In contrast, LRR continues training from the learned parameters. It thus inherits a potential sign switch from w1(0) < 0 to w1(0) > 0 if a(0) > 0 during training (and pruning) the overparameterized model. Thus, the probability that LRR fails due to a bad initial sign after identifying the correct mask is reduced to 1/2, as also explained in Fig. 1.\n2.2 LEARNING AN OVERPARAMETRIZED NEURON (d > 1)\nAs we have established the failure cases of the single input case in the previous section, we now focus on how overparameterization (to d > 1) can help avoid one case and thus aid LRR, while IMP is unable to benefit from the same.\nMultiple works have derived that convergence of the overparameterized model (d > 1) happens under mild assumptions and with high probability in case of zero noise and Gaussian input data, suggesting that overparameterization critically aids our original learning problem. For instance, (Yehudai & Ohad, 2020) have shown that convergence to a target vector v is exponentially fast \u2225w(t) \u2212 v\u2225 \u2264 \u2225w(0) \u2212 v\u2225 exp(\u2212\u03bbt), where the convergence rate \u03bb > 0 depends on the angle between w(0) and w(t) assuming that a(0) = a(t) = 1 is not trainable.\nInsight: For our purpose, it is sufficient that the learning dynamics can change the sign of w1(0) < 0 to w1(\u221e) > 0 if d \u2265 2. This would correspond to the first training round of LRR and IMP. Furthermore, training the neuron with multiple inputs enables the pruning step to identify the correct ground truth mask under zero noise, as wk(\u221e) \u2248 0 for k \u0338= 1. Yet, while IMP would restart training from w1(0) < 0 and fail to learn a parameterization that corresponds to the ground truth, LRR succeeds, as it starts from w1(\u221e) > 0.\nThese results assume, however, that a(0) = 1 is fixed and not trainable. In the previous section, we have also identified major training failure points if a(0) < 0. As it turns out, training a single multivariate neuron does not enable recovery from such a problematic initialization in general. Lemma 2.2. Assume that a and w follow the gradient flow dynamics induced by Eq. (7) with Gaussian iid input data, zero noise, and that initially 0 < |a|\u2225w(0)\u2225 \u2264 2 and a(0)2 = \u2225w(0)\u22252. Then a cannot switch its sign during gradient flow.\nThis excludes another relevant event that could have given IMP an advantage over LRR. Note that IMP could succeed while LRR fails, if we start from a promising initialization w1(0) > 0 and a(0) > 0 but the parameters converge during the first training round to values w1(0) < 0 and a(0) < 0 that would hamper successful training after pruning. This option is prevented, however, by the fact that a cannot switch its sign in case of zero noise. We therefore conclude our theoretical analysis with our main insight. Theorem 2.3. Assume that a and w follow the gradient flow dynamics induced by Eq. (7) with Gaussian iid input data, zero noise, and that initially 0 < |a|\u2225w(0)\u2225 \u2264 2 and a(0)2 = \u2225w(0)\u22252. If w1(0) < 0 and a(0) > 0, LRR attains a lower objective (1) than IMP. In all other cases, LRR performs at least as well as IMP."
        },
        {
            "heading": "2.3 VERIFYING THEORETICAL INSIGHTS BASED ON SINGLE HIDDEN NEURON NETWORK",
            "text": "Figure 2 (a) empirically validates our theoretical insights for d > 1 and compares LRR and IMP for each combination of initial signs of a(0), w1(0). A single hidden neuron network with input dimension d = 10 and random balanced Gaussian initialization is trained with LBFGS to minimize the objective function 1 for a noisy target (\u03c32 = 0.01). Averages and 0.95 confidence intervals over 10 runs for each case are shown. In each run, we prune and train over 3 levels for 1000 epochs each, while removing the same fraction of parameters in each level to achieve a target sparsity of 90% so that only one single input remains. In line with the theory, we find that IMP is only successful in the case a(0) > 0 and w1(0) > 0, while LRR succeeds as long as a(0) > 0."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "Our first objective is to analyze whether our theoretical intuition that LRR is more flexible in learning advantageous sign configurations transfers to more complex tasks related to standard benchmarks. Different from the simplified one hidden neuron setting, LRR and IMP also identify different masks. Thus, our second objective is to disentangle the impact of the different learning mechanisms and potential sign flips on both, the mask learning and the parameter optimization given a fixed mask.\nTo this end, we perform experiments on CIFAR10, CIFAR100 (Krizhevsky, 2009) and Tiny ImageNet (Le & Yang, 2015) with ResNet18 or ResNet50 with IMP and LRR that start from the same initializations. Table 1 in the appendix describes the details of the setup. To strengthen the IMP baseline, we in fact study WR and thus rewind the parameters to values that we have obtained after a sufficiently high number of training epochs of the dense model, which is in line with successfully obtaining matching networks as found by Paul et al. (2023).\nLRR modifications. Different from our theoretical investigations, we have to take more complex factors into account that influence the training process like learning rate schedules and batch normalization (BN). We found that the originally proposed training schedule of LRR can suffer from diminishing BN weights that impair training stability on larger scale problems like CIFAR100 and Tiny ImageNet (see Fig. 4 and Fig. 11 in the appendix). To avoid this issue, we propose to rewind BN parameters when the mask is decoupled from parameter optimization. In all our experiments, we introduce warmup after each pruning iteration, which increases the flexibility of LRR to optimize different masks as well as improves baseline performance (see Fig. 8 in appendix). Fig. 4 (c, d) provides an example where these modifications make LRR competitive with IMP on the IMP mask.\nWe start our investigations with observations regarding the performance of LRR and IMP in different learning scenarios before we isolate potential mechanisms that govern these observations like sign flips and network overparameterization. Our experiments establish and confirm that LRR outperforms IMP on all our benchmarks. Does this performance boost result from an improved mask identification or stronger parameter optimization?\nLRR identifies a better mask. Even though the mask identification of IMP is coupled to its training procedure, Fig. 3 (a, b) show that the mask that has been identified by LRR also achieves a higher performance than the IMP mask on CIFAR10 when its parameters are optimized with IMP. Similar improvements are observed on CIFAR100 (Fig. 3 (c, d)) except at high sparsities (> 95%) where the coupling of the mask and parameter optimization is more relevant.\nLRR is more flexible in optimizing different masks. According to Fig. 4(a, b), training LRR with the IMP mask (blue curve) is able to improve over IMP for CIFAR10. While the original LRR is less competitive for learning with the IMP mask on CIFAR100, LRR with BN parameter rewinding after each pruning iteration outperforms IMP both on CIFAR10 and CIFAR100 even at high sparsities. Similar results for Tiny ImageNet are presented in Fig. 2(d). Yet, are IMP and LRR masks sufficiently diverse? Since IMP and LRR masks are identified based on a similar magnitude based pruning criterion, the other mask and parameter initialization might still carry relevant information for the respective optimization task. In order to completely decouple the sparse mask from the parameter optimization and the initialization, we also study the LRR and IMP parameter optimization on a random mask.\nRandom Masks. For the same randomly pruned mask with balanced sparsity ratios (Gadhikar et al., 2023) and identical initialization, we compare training from initial values (IMP-rand) or training from the values obtained by the previous training\u2013pruning iteration (LRR-rand) (see Fig. 2(b, c)). Rewinding the BN parameters assists gradual random pruning and improves optimization, thus, LRR-rand (rewind BN) outperforms IMP-rand. This confirms that LRR seems to employ a more flexible parameter optimization approach irrespective of task specific masks.\nOur theoretical insights align with the observation that LRR learns network parameters more reliably than IMP. The main mechanism that strengthens LRR in our toy model is the fact that it inherits parameter signs that are identified by training an overparameterized model that is sufficiently flexible to correct initially problematic weight signs. To investigate whether a similar mechanism supports LRR also in a more complex setting, we study the sign flip dynamics.\nLRR enables early and stable sign switches. Fig. 4 confirms that LRR corrects initial signs primarily in earlier iterations when the mask is denser and the model more overparameterized. Moreover, the signs also stabilize early and remain largely constant for the subsequent pruning iterations (Fig. 5). Learnt parameters at consecutive sparsity levels in LRR tend to share the same sign in later iterations, but IMP must align initial signs in each pruning iteration, leading to unstable, back and forth flipping of learnt signs across sparsity levels. Overall, LRR changes more signs than IMP at lower sparsities on CIFAR10, yet, the effect is more pronounced in larger networks for CIFAR100 and Tiny ImageNet, where IMP fails to identify stable sign configurations even at high sparsities (see also Fig. 13 in appendix). These results apply to settings where the mask and parameter learning is coupled. Constraining both IMP and LRR to the same mask, LRR also appears to be more flexible and is able to improve performance by learning a larger fraction of parameter signs earlier than IMP (see Fig. 5(b)). For random masks, generally more unstable sign flips occur due to the fact that the mask and parameter values are not aligned well. Yet, LRR appears to be more stable and is able to flip more signs overall (Fig. 14 (a, b) in appendix). Even with the improved LRR mask, IMP seems unable to perform effective sign switches (Fig. 14 (c, d) in appendix).\nYet, maybe the LRR optimization can simply tolerate more sign switches? Furthermore, is LRR only able to switch signs in early training rounds due to the higher overparameterization of the networks? To answer these questions and learn more about the causal connection between sign switches and learning, next we study the effect of sign perturbations.\nLRR recovers from random sign perturbations. In order to characterize the effect of correct parameter signs on mask identification, we randomly perturb signs at different levels of sparsity\nfor both LRR and IMP. Sign perturbation at a low sparsity has little effect on CIFAR10 and both LRR and IMP are able to recover achieving baseline accuracy (Fig. 6(a, b)). For the more complex CIFAR100 dataset, signs have a stronger influence on masks and neither LRR nor IMP can fully recover to baseline performance. However, LRR is still able to achieve a higher performance than the IMP baseline, but IMP struggles after perturbing initial signs, as the mask does not fit to its initialization (Fig. 6(c, d)).\nFig. 7(a,b) shows results for perturbing a larger fraction of signs at much higher sparsity, i.e., 83%. LRR is able to recover over IMP at later sparsities on CIFAR10. Interestingly, on CIFAR100, LRR suffers more than IMP from the sign perturbation potentially due to a lack of overparameterization at high sparsity. LRR recovers slowly but still achieves baseline performance beyond 95% sparsity. The performance of subsequent masks obtained after perturbing signs reaffirms that parameter signs strongly influence the quality of the mask identification and LRR is capable of rearranging signs in order to find a better mask and optimize the corresponding parameters effectively. Yet, LRR requires training time and initial overparameterization to be effective.\nThe interplay of magnitude and signs. Recent analyses of IMP (Frankle et al., 2020b; Zhou et al., 2019) have found that signs that are learnt at later iterations are more informative and initializing with them improves IMP. In line with this insight, Fig. 7(c) highlights that rewinding only weight amplitude while maintaining the learnt signs improves over IMP. Yet, according to Frankle et al. (2020b) the combination with learned weight magnitudes can further strengthen the approach. Our next results imply that the magnitudes might be more relevant for the actual mask learning than the parameter optimization. We find that the learnt signs and the LRR mask contain most of the relevant information. Fig. 7(c) confirms that if we initialize IMP with the LRR signs and restrict it to the LRR mask, we can match the performance of LRR despite rewinding the weight magnitudes in every iteration. These results imply that a major drawback of IMP as a parameter optimization procedure could be that it forgets crucial sign information during weight rewinding."
        },
        {
            "heading": "4 CONCLUSIONS",
            "text": "Learning Rate Rewinding (LRR), Iterative Magnitude Pruning (IMP) and Weight Rewinding (WR) present cornerstones in our efforts to identify lottery tickets and sparsify neural networks, but the reasons for their successes and limitations are not well understood. To deepen our insights into their inner workings, we have highlighted a mechanism that gives LRR a competitive edge in structure learning and parameter optimization.\nIn a simplified single hidden neuron model, LRR provably recovers from initially problematic sign configurations by inheriting the signs from a trained overparameterized model, which is more robust to different initializations. This main theoretical insight also applies to more complex learning settings, as we show in experiments on standard benchmark data. Accordingly, LRR is more flexible in switching signs during early pruning\u2013training iterations by utilizing the still available overparameterization. As a consequence, LRR identifies not only highly performant masks. More importantly, it can also optimize parameters effectively given diverse sets of masks. In future, we envision that insights into the underlying mechanisms like ours could inspire the development of more efficient sparse training algorithms that can optimize sparse networks from scratch."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 PROOFS: ONE DIMENSIONAL INPUT\nTheorem A.1. Let a target function t(x) = \u03d5(x) and network f(x) = a\u03d5(wx) be given such that a and w follow the gradient flow dynamics induced by Eq. (1) with a random balanced parameter initialization and sufficiently many samples. If a(0) > 0 and w(0) > 0, f(x) can learn the correct target. In all other cases (a(0) > 0, w(0) < 0), (a(0) < 0, w(0) > 0) and (a(0) < 0, w(0) < 0) learning fails.\nProof: The derivatives of the parameters (a,w) with respect to the loss are given by\n\u2202L \u2202a = \u2212 1 n n\u2211 i=1 (yi \u2212 a\u03d5(wxi))\u03d5(wxi); \u2202L \u2202w = \u2212 1 n n\u2211 i=1 (yi \u2212 a\u03d5(wxi)) axi1(wxi>0), (2)\nwhich induce the following ODEs under gradient flow:\na\u0307 = \u2212aw2C1 + wC2, w\u0307 = \u2212wa2C1 + aC2, (3) where C1 = 1n \u2211 i\u2208I x 2 i and C2 = 1 n \u2211 i\u2208I(xi\u03d5(xi) + \u03b6ixi). Note that C1 and C2 can change over time, as they depend on the potentially changing set I(t) that comprises all samples on which the neuron is switched on: I(t) := {i|wi(t)xi > 0}. To solve this set of ODEs, we have to dinstinguish multiple cases. For all of them, we have a2(t) = w2(t). Thus, both parameters can pass through zero only together and potentially switch their sign only in this case. If w(t) does not switch its sign during training, C1 and C2 remain constant, we know that a(t) = sign(a(0))|w(t)| and we can focus on solving the dynamic equation for w at least until a potential sign switch. Replacing a(t) = sign(a(0))|w(t)| in the ODE for w leads to w\u0307 = \u2212w3C1 +wC\u03032 with C\u03032 = C2 sign(a(0)) sign(w(0)). As this ODE is of Bernoulli form, it has a closed form solution. Note that generally C1 \u2265 0. If C1 = 0, nothing happens and our function remains a constant 0. Otherwise, we have\nw(t) = \u221a C\u03032w(0) exp(2C\u03032t)\u221a\nC\u03032 \u2212 C1w(0)2 \u221a C1w(0)2 exp(4C\u03032t)\nC\u03032\u2212C1w(0)2 + 1\n, if C\u03032 > 0, (4)\nw(t) = sign(w(0))\n\u221a \u2212C\u03032\nexp ( \u22122C\u03032t )( C1 \u2212 C\u03032/w2(0) ) \u2212 C1 , if C\u03032 < 0, and (5)\nw(t) = w(0)\u221a\n2C1w2(0)t+ 1 if C\u03032 = 0. (6)\nwhich can be easily verified by differentiation. Note that these equations only hold until w(t) passes through 0. If that happens at t0, C1 and C2 actually change and we have to modify the above equations. Technically, if w(t0) = 0, the gradient flow dynamics halt because C1(t0) = 0 and C2(t0) = 0. Yet, in a noisy learning setting with discrete step sizes, it is not impossible that parameters switch their sign. In this case, a new dynamics start from the switching point t0 on a time scale t\u0303 = t\u2212 t0\u2212 \u03f5 that continues from the previously found parameters w(t\u0303 = 0) = w(t = t0+ \u03f5). We now develop an intuition of what this means for our learning problem, by differentiating the problem into different cases based on initial parameter signs.\nCorrect initial sign. If we start with the correct signs w(0) > 0 and a(0) > 0 as the target, then our neuron has no problem to learn the right parameters given enough samples. w(0) > 0 implies that I = {i|xi > 0} so that C2 = 1/n \u2211 i\u2208I(xi\u03d5(xi) + \u03b6ixi) = 1/n \u2211 i\u2208I x 2 i + 1/n \u2211 i\u2208I \u03b6ixi. According to the law of large numbers, limn\u2192\u221e C2 = E(\u03d5(X)21) + E(\u03d5(X)1\u03b6) = 1/(2d) > 0 and limn\u2192\u221e C1 = E(\u03d5(X)21) = 1/(2d) almost surely. Also for finite samples, we likely have C2 > 0, as we will make more precise later. Thus, C\u03032 = sign(w(0)a(0))C2 = C2 > 0 and w converges\nto w(\u221e) = \u221a C\u03032/C1 = 1 without passing through 0. Also a(\u221e) = sign(a(0))|w(\u221e)| = 1\ncorresponds to the correct target value.\nCase w(0) > 0 but a(0) < 0. Since I is defined as above, our initial constants C1 and C2 are identical. Yet, C\u03032 = \u2212C2 < 0 changes its sign, which has a considerable impact on the learning dynamics. As a(0) has started with the wrong sign, the gradient dynamics try to rectify it and send w(t) to 0 in the process, as a(t) would need to pass through 0 to switch its sign. Thus, learning fails. In case of finite samples, C\u03032 is subject to noise. If C\u03032 < 0, a and w still converge to 0. However,\nif C\u03032 > 0 because of substantial noise, w would converge to a positive value w(\u221e) = \u221a C\u03032/C1,\nwhile a(\u221e) = \u2212 \u221a C\u03032/C1 < 0, which would not align at all with the target \u03d5(x).\nCase w(0) < 0. This case is bound to fail regardless of the sign of a(0), if the noise is not helping with a sign switch. The reason is that the neuron is initially switched on only on the negative samples I = {i|xi < 0}, for which the true labels are zero. In consequence, C2 = 1/n \u2211 i\u2208I(xi\u03d5(xi) +\n\u03b6ixi) = 1/n \u2211\ni\u2208I \u03b6ixi and limn\u2192\u221e C2 = E(\u2212\u03d5(\u2212X)1\u03b6) = 0 almost surely. Thus, the training data provides an incentive for a and w to converge to 0 without passing through 0 in between. Also for finite samples, we have C\u03032 = \u2212a(0)/n \u2211 i\u2208I \u03b6ixi > 0 with probability 1/2. In this case, w will\nconverge to a negative value sign(w0) \u221a C\u03032/C1. If C\u03032 < 0, then both w and a converge to 0 without\nthe opportunity to change the sign with a too large discrete gradient step.\nFinite sample considerations. Interestingly, the number of samples and the noise level do not really influence the (failed) learning outcome in case of a negative initial weight w(0) < 0. The case w(0) > 0, a(0) < 0 is not able to learn a model that can come close to the ground truth target. Thus, only the potential success case w(0) > 0 and a(0) > 0 depends meaningfully on the data and its signal to noise ratio, as our set-up reduces to an overparameterized linear regression problem with outcome a(\u221e) = w(\u221e) = \u221a C2/C1 if C2 > 0. (Note that C2 < 0 would imply such high noise that the learning could also not be regarded successful, as a(\u221e) = w(\u221e) = 0.) The sample complexity of learning the parameters is the only part that depends on distribution assumptions regarding the input and noise. The effective regression parameter a(\u221e)w(\u221e) = C2/C1 = 1 + ( \u2211 i\u2208I \u03b6ixi)/( \u2211 i\u2208I x 2 i ) depends in the usual way on the noise, but requires double the number of samples as a normal regression problem, as in approximately half of the cases, the neuron is switched off.\nA.2 PROOFS: OVERPARAMETERIZED INPUT (d > 1)\nIn the following section, we prove our main theorem that allows us to conclude that LRR has a higher chance to succeed in learning a single univariate neuron than IMP.\nLearning an overparameterized multivariate neuron f(x) = a\u03d5(wTx) for x \u2208 Rd corresponds to a more complex set of coupled gradient flow ODEs, if d > 1.\nw\u0307 = \u2212a2C1w + aC2, a\u0307 = \u2212awTC1w +C2Tw,\nwith C1 = 1\nn \u2211 i\u2208I xixi T , C2 = 1 n \u2211 i\u2208I yixi, (7)\nwhere the dynamic set I is again defined as the set of samples on which the neuron is activated so that I = {i | wTi xi > 0}. The main difference to the previous one-dimensional case is that this set is initially not determined by w1(0). Even in case of a problematic initialization w1(0) < 0, the neuron can learn a better model because of c2,1 > 0.\nWe cannot expect to derive the gradient flow dynamics for this problem in closed form, as C1 and C2 depend on w in complicated nonlinear ways. However, the structure of a solution is apparent, as the problem corresponds to an overparameterized linear regression problem given I. Lee et al. (2022a) have discussed the solutions to this general problem in case of positive input and fixed, non-trainable a. Assuming balancedness a2 = \u2225w\u22252, our solution must also be of the form w = \u03b2/ \u221a \u2225\u03b2\u2225 and\na = sign(a) \u221a\n\u2225\u03b2\u2225, where \u03b2 = X+y is the mean squared error minimizer and X = (xi,k)ik \u2208 R|I|\u00d7d corresponds to the data matrix on the active samples. Under conditions that enable successful optimization, we obtain \u03b2 \u2248 v = (1, 0, ...)T . Yet, there are still several issues that can arise during training as the set I changes with w. Solving the set of ODEs is generally a hard problem, even though several variants have been well studied\nYehudai & Ohad (2020); Lee et al. (2022a); Vardi et al. (2021;?); Oymak & Soltanolkotabi (2019); Soltanolkotabi (2017); Kalan et al. (2019); Frei et al. (2020); Diakonikolas et al. (2020); Tan & Vershynin (2019); Du et al.. Most works assume that the outer weight is fixed a(0) = a(t) = 1 and only the inner weight vector w is learned Yehudai & Ohad (2020); Lee et al. (2022a); Vardi et al. (2021). Only (Boursier et al., 2022) considers trainable a but excludes the single hidden neuron case and is restricted to orthogonal inputs. Furthermore, the noise is usually considered to be 0. In the large sample setting, this assumption would be well justified, as the noise contribution to C2 approaches 0. However, to guarantee convergence, additional assumptions on the data generating process are still required, as Yehudai & Ohad (2020) have pointed out with a counter example that for a given parameter initialization method (in form of a product distribution) there exist a data distribution for which gradient flow (or SGD or GD) does not converge with probability at least 1 \u2212 exp(\u2212d/4). Vardi et al. (2021) have furthermore shown that if we also want to learn biases (which would be the case if one of our data input components is constant 1), a uniform initial weight distribution could lead to failed learning results with probability close to 1/2.\nRecall that three scenarios prevent IMP from succeeding in learning our target \u03d5(x1): a) a(0) < 0, w1(0) > 0, b) a(0) < 0, w1(0) < 0, and (c) a(0) > 0 and w1(0) < 0. LRR cannot succeed in case of (a) and (b) as well, because a cannot switch its sign to a(\u221e) > 0 during training, as the following lemma states.\nStatement (Restated Lemma 2.2). Assume that a and w follow the gradient flow dynamics induced by Eq. (7) with Gaussian iid input data, zero noise, and that initially 0 < |a|\u2225w(0)\u2225 \u2264 2 and a(0)2 = \u2225w(0)\u22252. Then a cannot switch its sign during gradient flow.\nProof. This statement follows immediately from the balancedness property. To switch its sign, a(t) would need to pass through zero. Thus, let us assume there exists a time point t0 > 0 so that a(t0) = 0. Since a(t)2 = \u2225w(t)\u22252 for the complete dynamics, this implies that \u2225w(t0)\u22252 = 0. As this switches of the neuron, C1(t0) = C2(t0) = 0 so that a\u0307(t0) = 0 and w\u0307k(t0) = 0. It follows that a(t) = 0 and \u2225w(t)\u22252 = 0 for all t \u2265 t0 so that no sign switch occurs.\nNote that for finite, relatively high learning rates, it could be possible that a neuron switches its sign because it never switches off completely and instead overshoots 0 with a large enough gradient step. In most cases, this would provide LRR with an advantage. Because if a problematic initialization with a(0) < 0 could be mitigated by training so that a(\u221e) > 0, LRR would benefit but not IMP. The only scenario in favor for IMP would be the case that the initialization is advantageous so that a(0) > 0 and w1(0) > 0 but becomes problematic during training so that a(\u221e) < 0. This, however, would require such high noise that also training an univariate neuron from scratch could not result in a good model. It is therefore an irrelevant (and unlikely) case and does not impact our main conclusions, which are restated below for convenience.\nStatement (Restated Theorem 2.3). Assume that a and w follow the gradient flow dynamics induced by Eq. (7) with Gaussian iid input data, zero noise, and that initially 0 < |a|\u2225w(0)\u2225 \u2264 2 and a(0)2 = \u2225w(0)\u22252. If w1(0) < 0 and a(0) > 0, LRR attains a lower objective (1) than IMP. In all other cases, LRR performs at least as well as IMP.\nProof. According to Lemma 2.2, a(0) \u2264 0 implies a(\u221e) \u2264 0 so that neither IMP nor LRR can succeed to learn the correct univariate target neuron after pruning. We can therefore focus our analysis on the case a(\u221e) > 0. Note that this implies that a(t) = \u2225w(t)\u2225 because a does not switch its sign.\nBoth IMP and LRR rely on the first overparameterized training cycle to result in a successful mask identification, which requires |w1| >> |wi| for any i \u0338= 1. Otherwise, both approaches (IMP and LRR) would fail. Hypothetically, it could be possible that |w1(\u221e)| >> |wi(\u221e)| while w1(\u221e) < 0, which would not correspond to a successful training round, since w(\u221e) \u0338= (1, 0, 0, ...) but would result in a correct mask identification. This case would be interesting, as it would allow IMP to succeed if w1(0) > 0 while LRR could not, as it would start training an univariate neuron from w1(\u221e) < 0. However, note that the derivative of w would be nonzero in this case. Thus, there exists no stationary point with the property w1(\u221e) < 0. In consequence, only cases of successful training offer interesting instances to compare IMP and LRR. For our argument, it would be sufficient to show that learning a multivariate neuron is suc-\ncessful with nonzero probability and argue that LRR succeeds while IMP fails in some of these cases. Yet, we can derive a much stronger statement by using and adapting Theorem 6.4 by Yehudai and Shamir (Yehudai & Ohad, 2020) and prove that learning is generally successful for reasonable initializations.\nLearning a single neuron. We define z(t) = a(t)w(t). Note that z has therefore norm \u2225z\u2225 = \u2225w\u22252 and its direction coincides with the one of w, as z/\u2225z\u2225 = w/\u2225w\u2225. In case of successful training, we expect z(t) \u2192 v for t \u2192 \u221e, where v is a general target vector. In our case, we assume v = (1, 0, 0, ...). Our main goal is to bound the time derivative \u03b4/\u03b4t\u2225z(t) \u2212 v\u22252 = 2 \u27e8z\u0307(t), z(t)\u2212 v\u27e9 \u2264 \u2212\u03bb\u2225z(t) \u2212 v\u22252 for a \u03bb > 0. Gro\u0308nwall\u2019s inequality would then imply that \u2225z(t)\u2212v\u22252 \u2264 \u2225z(0)\u2212v(0)\u22252 exp(\u2212\u03bbt) and hence z(t) \u2192 v exponentially fast.\nIn contrast to (Yehudai & Ohad, 2020), the derivative z\u0307(t) = aw\u0307 + a\u0307w consists of two parts that we have to control separately.\nIt is easy to see based on Eq. (7) that the time derivative of w(t) fulfills:\n\u27e8aw\u0307, z \u2212 v\u27e9 =\u2212 a2 1 n \u2211 i\u2208I,vT xi>0 \u27e8xi, z \u2212 v\u27e92 (8)\n\u2212 a2 1 n \u2211 i\u2208I,vT xi\u22640 \u27e8xi, z \u2212 v\u27e9 \u27e8xi, z\u27e9 (9)\n=\u2212 a2\u2225z \u2212 v\u22252 1 n \u2211 i\u2208I,vT xi>0 \u2225xi\u22252 cos (\u03c6(xi, z \u2212 v))2 (10)\n\u2212 a2 1 n \u2211 i\u2208I,vT xi\u22640 \u27e8xi, z\u27e92 \u2212 a2 1 n \u2211 i\u2208I,vT xi\u22640 \u27e8xi,\u2212v\u27e9 (11)\n\u2264 \u2212a2\u2225z \u2212 v\u22252\u03bb1 = \u2212\u03bb1\u2225z\u2225\u2225z \u2212 v\u22252 \u2265 \u2212\u03bb0\u2225z \u2212 v\u22252, (12)\nwhere we dropped the term (11) because it is negative. (Note that all involved factors are positive because \u2212vTxi \u2265 0.) Furthermore, we have \u03bb1 = 1/n \u2211 i\u2208I,vT xi>0 \u2225xi\u2225\n2 cos (\u03c6(xi, z \u2212 v))2 > 0 with high probability with respect to the data distribution according to Lemma B1 by Yehudai and Shamir (Yehudai & Ohad, 2020). Similarly, the proof of Thm. 5.3 by (Yehudai & Ohad, 2020) argues why a2 = \u2225z\u2225 > 0 is bounded from below, which allows us to integrate its lower bound into the constant \u03bb0.\nThe second term of z\u0307 is not considered by (Yehudai & Ohad, 2020), as they assume that a is not trainable. We get:\n\u27e8a\u0307w, z \u2212 v\u27e9 =a\u0307 \u27e8w, z \u2212 v\u27e9 \u2264 0. (13)\nThe last inequality can be deduced by distinguishing two cases. If \u27e8w, z \u2212 v\u27e9 > 0, then a\u0307 < 0. If \u27e8w, z \u2212 v\u27e9 < 0, then a\u0307 > 0. This follows from the fact that\na\u0307 = \u2212\u2225w\u22253 1 n \u2211 i\u2208I \u2329 w \u2225w\u2225 , xi \u232a + 1 n \u2211 i\u2208I,vT xi>0 \u27e8v, xi\u27e9 \u27e8w, xi\u27e9 (14)\nand that \u27e8w, z \u2212 v\u27e9 = \u2225w\u22253 \u2212 \u27e8w,v\u27e9 = \u2225w\u22253 \u2212 w1. On average with respect to the data, we thus receive Eq. (13). Note that the normal case is that \u27e8w, z \u2212 v\u27e9 > 0, as this holds initially with high probability and it remains intact during training.\nCombining Eq. (12) and Eq. (13) completes our argument, since\n\u03b4\u2225z(t)\u2212 v\u22252\n\u03b4t = 2 \u27e8z\u0307(t), z(t)\u2212 v\u27e9 \u2264 \u2212\u03bb\u2225z(t)\u2212 v\u22252 (15)\nfor a \u03bb > 0. Gro\u0308nwall\u2019s inequality leads to \u2225z(t) \u2212 v\u22252 \u2264 \u2225z(0) \u2212 v(0)\u22252 exp(\u2212\u03bbt) and hence z(t) \u2192 v exponentially fast. LRR outperforms IMP. If training the overparameterized neuron is successful with a(0) > 0 and a(\u221e) > 0 as discussed previously, then w(1)1 (\u221e) = 1 > 0. After pruning, LRR has to train an\nunivariate neuron with the initial condition w(2)1 (0) = w (1) 1 (\u221e) > 0, which converges to the correct ground truth model. However, if w(1)1 (0) < 0, IMP has to start training from w (2) 1 (0) = w (1) 1 (0) < 0, which leads to a wrong model estimate, as discussed in Section 2.1.\nA.3 EXPERIMENTAL DETAILS\nDataset CIFAR10 CIFAR100 Tiny ImageNet ImageNet Model ResNet18 ResNet50 ResNet50 ResNet50 Epochs 150 150 150 90\nLR 0.1 0.1 0.1 0.1 Scheduler cosine-warmup step-warmup step-warmup step-warmup Batch Size 256 256 256 256\nWarmup Epochs 50 10 50 10 Optimizer SGD SGD SGD SGD\nWeight Decay 1e-4 1e-3 1e-3 1e-4 Momentum 0.9 0.9 0.9 0.9\nInit Kaiming Normal Kaiming Normal Kaiming Normal Kaiming Normal\nTable 1: Experimental Setup\nImage Classification Tasks. Table 1 details our experimental setup. In each pruning iteration, we keep 80% of the currently remaining parameters of highest magnitude (Frankle & Carbin, 2019).\nWarmup Epochs. Each training run of a dense network starts with warmup epochs with a linearly increasing learning rate from upto 0.1. Weights are rewound to their values after warmup in case of IMP (i.e. similar to WR). We ensure that each run of IMP and LRR has an identical rewind point (after warmup epochs).\nThe learning rate schedules we found to achieve the best performance in our experiments as confirmed in Figure 8 and 9:\n(i) cosine-warmup: Learning rate is increased linearly to 0.1 for the first 10 epochs, followed by a cosine lr schedule for the remaining 140 epochs.\n(ii) step-warmup: Learning rate is increased linearly to 0.1 for the first 10 epochs, followed by a step lr schedule for the remaining 140 epochs with learning multiplied by a factor of 0.1 at epochs 60 and 120.\n(iii) ImageNet: For ImageNet, we use a constant learning rate of 0.01 during the warmup phase. The learning rate is reduced by a factor of 10 every 30 epochs after the warmup phase, starting from 0.1.\nIn case of random pruning we randomly remove parameters in each layer in order to maintain a balanced layerwise sparsity ratio (Gadhikar et al., 2023) i.e. every layer has an equal number of\nnonzero parameters. Each run is repeated thrice and we report the mean and 95% confidence interval of these runs. All experiments are performed on a Nvidia A100 GPU. Our code is built on the work of (Kusupati et al., 2020).\nA.4 ADDITIONAL RESULTS\nBN parameter distributions: We plot the layer wise distributions of the learnable scaling parameter \u03b3 for our experiments. The distributions are similar on CIFAR10 (Figure 10) with most values being positive in every layer for IMP, LRR and LRR with IMP mask. Hence, rewinding BN parameters also finds similar distributions.\nHowever, the rewinding BN improves performance of LRR with IMP mask on CIFAR100 (See Fig. 4). This can be attributed to the reducing the number of neurons (channels) where \u03b3 = 0 (eg: Layer 22, 23 in CIFAR100 Fig. 11). We find that this is is necessary in deeper layers to aid signal propagation and hence we propose rewinding BN parameters to aid LRR when the mask is decoupled from the optimization."
        },
        {
            "heading": "BN params for CIFAR10 ResNet18 at Sparsity: 96",
            "text": "Interplay of magnitude and signs on CIFAR100. On CIFAR100 too, the signs and mask learnt by LRR contain majority of the information required while training. When using the mask and signs"
        },
        {
            "heading": "BN params for CIFAR100 ResNet50 at Sparsity: 96",
            "text": "learnt by LRR and only rewinding weight magnitudes, we match the performance of LRR (see Fig. 12)."
        },
        {
            "heading": "80 85 90 95 100",
            "text": "Sparsity\n78\n79\n80\n81\nTe st\nAc c\nOverall sign flips for LRR vs IMP. As verified experimentally, LRR enables more sign flips early in training. This can also be confirmed by measuring the total number of sign flips from initial signs to learnt signs at each sparsity level for both LRR and IMP. We plot the difference between the number of sign flips enabled by LRR and IMP at each pruning iteration in Figure 13. Early sparsities show a large positive difference between the number of signs flipped by LRR and IMP, showing the ability of LRR to enable more sign flips.\nLRR enables early sign switches for parameter optimization. Figure 14(a, b) shows for a randomized mask, LRR-rand enables a larger fraction of signs to switch earlier in training than IMPrand in spite of unstable sign flips due to a randomized mask which does not align with parameter values. On the other hand, the ability to switch signs still lacks in IMP in spite of training with an improved LRR mask as shown in Figure 14(c, d) highlighting that the weight rewinding step leads to loss of sign information.\nLRR enables early sign switches on ImageNet. We report results on ImageNet in Figure 15 (a) for a ResNet50. We find that our insights translate to the large scale setting as well. To support our hypothesis of the importance of learnt signs, we show that using the initial weight magnitudes of\nIMP with the mask learnt by LRR and the weight signs learnt by LRR, we are still able to match the performance of LRR (blue curve). Figure 15 (b) confirms that LRR enables early sign switches by the third prune-train iteration while IMP struggles to perform sign switches.\nImpact of pruning criteria. In order to highlight that different pruning criteria can benefit from initial overparameterization with continued training in comparison with weight rewinding, we report results for pruning iteratively with Synflow (Tanaka et al., 2020) and SNIP (Lee et al., 2019). Although, these criteria have been proposed for pruning at initialization, we use them iteratively following the same prune-train procedure as LRR and IMP. Although LRR and IMP are used in the context of magnitude pruning, we use the same terms followed by the pruning criterion to differen-\ntiate between training with learning rate rewinding (LRR (synflow/snip)) and training with weight rewinding (IMP (synflow/snip)). For eg: IMP (synflow) indicates that each prune - train iteration prunes weights based on the Synflow criterion and rewinds the nonzero weights back to their initial values.\nFigure 16 supports our hypothesis that for different pruning criteria, like Synflow and SNIP, continued training benefits from initial overparameterization by enabling better sign switchyoes. The blue curve confirms that as long as the mask and sign learnt by LRR (synflow/snip) is used, we can match the performance of LRR (synflow/snip) denoted by the purple curve for any weight magnitude ((IMP init + LRR mask + LRR sign (snip/synflow)).\nCIFAR10 on VGG16. We also report results for a VGG16 model with Batch Normalization (Simonyan & Zisserman, 2015) on CIFAR10. Figure 17(a) shows that LRR improves over IMP and that its signs and mask contain sufficient information to match the performance of LRR (see blue curve). Figure 17(b) further confirms that LRR enables early sign switches compared to IMP, supporting our hypothesis that LRR gains performance due to its ability to effectively switch signs in early iterations.\nImpact of overparameterization on single hidden neuron network. We show that increasing overparameterization in the input dimension d for the single hidden neuron network defined in Section 2 aids LRR. We follow the same experimental setup as Section 2.3. In the case when d = 1, LRR and IMP are equally likely to succeed for the case where a(0) > 0, w1(0) > 0 (see Figure 18(a)). If we increase d > 1, we find that LRR is now able to leverage the overparameterized model\nin the early pruning iterations to flip initial bad signs and succeed more often than IMP (see Figure 18(b), (c), (d)) as long as a(0) > 0."
        }
    ],
    "title": "MASKS, SIGNS, AND LEARNING RATE REWINDING",
    "year": 2024
}