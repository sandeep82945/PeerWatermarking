{
    "abstractText": "This paper rigorously shows how over-parameterization dramatically changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from nearisotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where M\u2217 \u2208 Rn\u00d7n is a positive semi-definite unknown matrix of rank r \u226a n, and one uses a symmetric parameterization XX\u22a4 to learn M\u2217. Here, X \u2208 Rn\u00d7k with k > r is the factor matrix. We give a novel \u03a9 ( 1/T 2 ) lower bound of randomly initialized GD for the over-parameterized case (k > r) where T is the number of iterations. This is in stark contrast to the exact-parameterization scenario (k = r) where the convergence rate is exp (\u2212\u03a9 (T )). Next, we study asymmetric setting where M\u2217 \u2208 Rn1\u00d7n2 is the unknown matrix of rank r \u226a min{n1, n2}, and one uses an asymmetric parameterization FG\u22a4 to learn M\u2217 where F \u2208 Rn1\u00d7k and G \u2208 Rn2\u00d7k. Building on prior work, we give a global exact convergence result of randomly initialized GD for the exact-parameterization case (k = r) with an exp (\u2212\u03a9 (T )) rate. Furthermore, we give the first global exact convergence result for the over-parameterization case (k > r) with an exp ( \u2212\u03a9 ( \u03b1T )) rate where \u03b1 is the initialization scale. This linear convergence result in the over-parameterization case is especially significant because one can apply the asymmetric parameterization to the symmetric setting to speed up from \u03a9 ( 1/T 2 ) to linear convergence. Therefore, we identify a surprising phenomenon: asymmetric parameterization can exponentially speed up convergence. Equally surprising is our analysis that highlights the importance of imbalance between F and G. This is in sharp contrast to prior works which emphasize balance. We further give an example showing the dependency on \u03b1 in the convergence rate is unavoidable in the worst case. On the other hand, we propose a novel method that only modifies one step of GD and obtains a convergence rate independent of \u03b1, recovering the rate in the exact-parameterization case. We provide empirical studies to verify our theoretical findings.",
    "authors": [],
    "id": "SP:e4f0ebcb9fc8a0e525881e49d5afe2e22d2c3a07",
    "references": [
        {
            "authors": [
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Yingyu Liang"
            ],
            "title": "Learning and generalization in overparameterized neural networks, going beyond two layers",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Wei Hu",
                "Yuping Luo"
            ],
            "title": "Implicit regularization in deep matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Srinadh Bhojanapalli",
                "Anastasios Kyrillidis",
                "Sujay Sanghavi"
            ],
            "title": "Dropping convexity for faster semi-definite optimization",
            "venue": "In Conference on Learning Theory,",
            "year": 2016
        },
        {
            "authors": [
                "Yingjie Bi",
                "Haixiang Zhang",
                "Javad Lavaei"
            ],
            "title": "Local and global linear convergence of general lowrank matrix recovery problems",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Emmanuel Candes",
                "Benjamin Recht"
            ],
            "title": "Exact matrix completion via convex optimization",
            "venue": "Communications of the ACM,",
            "year": 2012
        },
        {
            "authors": [
                "Emmanuel J Candes",
                "Yaniv Plan"
            ],
            "title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2011
        },
        {
            "authors": [
                "Chih-Fan Chen",
                "Chia-Po Wei",
                "Yu-Chiang Frank Wang"
            ],
            "title": "Low-rank matrix recovery with structural incoherence for robust face recognition",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Edouard Oyallon",
                "Francis Bach"
            ],
            "title": "On lazy training in differentiable programming",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mark A Davenport",
                "Justin Romberg"
            ],
            "title": "An overview of low-rank matrix recovery from incomplete observations",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Simon S Du",
                "Chi Jin",
                "Jason D Lee",
                "Michael I Jordan",
                "Aarti Singh",
                "Barnabas Poczos"
            ],
            "title": "Gradient descent can take exponential time to escape saddle points",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Simon S Du",
                "Wei Hu",
                "Jason D Lee"
            ],
            "title": "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Simon S Du",
                "Xiyu Zhai",
                "Barnabas Poczos",
                "Aarti Singh"
            ],
            "title": "Gradient descent provably optimizes over-parameterized neural networks",
            "venue": "arXiv preprint arXiv:1810.02054,",
            "year": 2018
        },
        {
            "authors": [
                "Raaz Dwivedi",
                "Nhat Ho",
                "Koulik Khamaru",
                "Martin J Wainwright",
                "Michael I Jordan",
                "Bin Yu"
            ],
            "title": "Singularity, misspecification and the convergence rate of em",
            "year": 2020
        },
        {
            "authors": [
                "Cong Fang",
                "Jason Lee",
                "Pengkun Yang",
                "Tong Zhang"
            ],
            "title": "Modeling from features: a mean-field framework for over-parameterized deep neural networks",
            "venue": "In Conference on learning theory,",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ge",
                "Chi Jin",
                "Yi Zheng"
            ],
            "title": "No spurious local minima in nonconvex low rank problems: A unified geometric analysis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Liwei Jiang",
                "Yudong Chen",
                "Lijun Ding"
            ],
            "title": "Algorithmic regularization in model-free overparametrized asymmetric matrix factorization",
            "venue": "arXiv preprint arXiv:2203.02839,",
            "year": 2022
        },
        {
            "authors": [
                "Chi Jin",
                "Rong Ge",
                "Praneeth Netrapalli",
                "Sham M Kakade",
                "Michael I Jordan"
            ],
            "title": "How to escape saddle points efficiently",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Jikai Jin",
                "Zhiyuan Li",
                "Kaifeng Lyu",
                "Simon Shaolei Du",
                "Jason D Lee"
            ],
            "title": "Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Xingguo Li",
                "Junwei Lu",
                "Raman Arora",
                "Jarvis Haupt",
                "Han Liu",
                "Zhaoran Wang",
                "Tuo Zhao"
            ],
            "title": "Symmetry, saddle points, and global optimization landscape of nonconvex matrix factorization",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Tengyu Ma",
                "Hongyang Zhang"
            ],
            "title": "Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Yuping Luo",
                "Kaifeng Lyu"
            ],
            "title": "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning",
            "venue": "arXiv preprint arXiv:2012.09839,",
            "year": 2020
        },
        {
            "authors": [
                "Guangcan Liu",
                "Zhouchen Lin",
                "Shuicheng Yan",
                "Ju Sun",
                "Yong Yu",
                "Yi Ma"
            ],
            "title": "Robust recovery of subspace structures by low-rank representation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Yiping Lu",
                "Chao Ma",
                "Yulong Lu",
                "Jianfeng Lu",
                "Lexing Ying"
            ],
            "title": "A mean field analysis of deep resnet and beyond: Towards provably optimization via overparameterization from depth",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Cong Ma",
                "Yuanxin Li",
                "Yuejie Chi"
            ],
            "title": "Beyond procrustes: Balancing-free gradient descent for asymmetric low-rank matrix sensing",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jianhao Ma",
                "Salar Fattahi"
            ],
            "title": "Sign-rip: A robust restricted isometry property for low-rank matrix recovery",
            "venue": "arXiv preprint arXiv:2102.02969,",
            "year": 2021
        },
        {
            "authors": [
                "Jianhao Ma",
                "Salar Fattahi"
            ],
            "title": "Global convergence of sub-gradient method for robust matrix recovery: Small initialization, noisy measurements, and over-parameterization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Phan-Minh Nguyen",
                "Huy Tuan Pham"
            ],
            "title": "A rigorous framework for the mean field limit of multilayer neural networks",
            "venue": "arXiv preprint arXiv:2001.11443,",
            "year": 2020
        },
        {
            "authors": [
                "Yigang Peng",
                "Jinli Suo",
                "Qionghai Dai",
                "Wenli Xu"
            ],
            "title": "Reweighted low-rank matrix recovery and its application in image restoration",
            "venue": "IEEE transactions on cybernetics,",
            "year": 2014
        },
        {
            "authors": [
                "Benjamin Recht",
                "Maryam Fazel",
                "Pablo A Parrilo"
            ],
            "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
            "venue": "SIAM review,",
            "year": 2010
        },
        {
            "authors": [
                "Frederieke Richert",
                "Roman Worschech",
                "Bernd Rosenow"
            ],
            "title": "Soft mode in the dynamics of overrealizable online learning for soft committee machines",
            "venue": "Physical Review E,",
            "year": 2022
        },
        {
            "authors": [
                "Itay Safran",
                "Ohad Shamir"
            ],
            "title": "Spurious local minima are common in two-layer relu neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Mahdi Soltanolkotabi",
                "Dominik St\u00f6ger",
                "Changzhi Xie"
            ],
            "title": "Implicit balancing and regularization: Generalization and convergence guarantees for overparameterized asymmetric matrix sensing",
            "venue": "arXiv preprint arXiv:2303.14244,",
            "year": 2023
        },
        {
            "authors": [
                "Dominik St\u00f6ger",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Tong",
                "Cong Ma",
                "Yuejie Chi"
            ],
            "title": "Accelerating ill-conditioned low-rank matrix estimation via scaled gradient descent",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Tu",
                "Ross Boczar",
                "Max Simchowitz",
                "Mahdi Soltanolkotabi",
                "Ben Recht"
            ],
            "title": "Low-rank solutions of linear matrix equations via procrustes flow",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Lingxiao Wang",
                "Xiao Zhang",
                "Quanquan Gu"
            ],
            "title": "A unified computational and statistical framework for nonconvex low-rank matrix estimation",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Colin Wei",
                "Jason D Lee",
                "Qiang Liu",
                "Tengyu Ma"
            ],
            "title": "Regularization matters: Generalization and optimization of neural nets vs their induced kernel",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyuan Weng",
                "Xin Wang"
            ],
            "title": "Low-rank matrix completion for array signal processing",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2012
        },
        {
            "authors": [
                "Fan Wu",
                "Patrick Rebeschini"
            ],
            "title": "Implicit regularization in matrix sensing via mirror descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yihong Wu",
                "Harrison H Zhou"
            ],
            "title": "Randomly initialized em algorithm for two-component gaussian mixture achieves near optimality in o(n) iterations",
            "venue": "Mathematical Statistics and Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Weihang Xu",
                "Simon Du"
            ],
            "title": "Over-parameterization exponentially slows down gradient descent for learning a single neuron",
            "venue": "In The Thirty Sixth Annual Conference on Learning Theory,",
            "year": 2023
        },
        {
            "authors": [
                "Xingyu Xu",
                "Yandi Shen",
                "Yuejie Chi",
                "Cong Ma"
            ],
            "title": "The power of preconditioning in overparameterized low-rank matrix sensing",
            "venue": "arXiv preprint arXiv:2302.01186,",
            "year": 2023
        },
        {
            "authors": [
                "Tian Ye",
                "Simon S Du"
            ],
            "title": "Global convergence of gradient descent for asymmetric low-rank matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Gavin Zhang",
                "Salar Fattahi",
                "Richard Y Zhang"
            ],
            "title": "Preconditioned gradient descent for overparameterized nonconvex burer-monteiro factorization with global optimality certification",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2023
        },
        {
            "authors": [
                "Haixiang Zhang",
                "Yingjie Bi",
                "Javad Lavaei"
            ],
            "title": "General low-rank matrix optimization: Geometric analysis and sharper bounds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jialun Zhang",
                "Salar Fattahi",
                "Richard Y Zhang"
            ],
            "title": "Preconditioned gradient descent for overparameterized nonconvex matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Zhang",
                "Lingxiao Wang",
                "Quanquan Gu"
            ],
            "title": "A unified framework for nonconvex low-rank plus sparse matrix recovery",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Xiao Zhang",
                "Lingxiao Wang",
                "Yaodong Yu",
                "Quanquan Gu"
            ],
            "title": "A primal-dual analysis of global optimality in nonconvex low-rank matrix recovery",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Bo Zhao",
                "Justin P Haldar",
                "Cornelius Brinegar",
                "Zhi-Pei Liang"
            ],
            "title": "Low rank matrix recovery for real-time cardiac mri",
            "venue": "In 2010 ieee international symposium on biomedical imaging: From nano to macro,",
            "year": 2010
        },
        {
            "authors": [
                "Tuo Zhao",
                "Zhaoran Wang",
                "Han Liu"
            ],
            "title": "A nonconvex optimization framework for low rank matrix estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Qinqing Zheng",
                "John Lafferty"
            ],
            "title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Zhihui Zhu",
                "Qiuwei Li",
                "Gongguo Tang",
                "Michael B Wakin"
            ],
            "title": "Global optimality in low-rank matrix optimization",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Zhihui Zhu",
                "Qiuwei Li",
                "Gongguo Tang",
                "Michael B Wakin"
            ],
            "title": "The global optimization geometry of low-rank matrix optimization",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Jiacheng Zhuo",
                "Jeongyeol Kwon",
                "Nhat Ho",
                "Constantine Caramanis"
            ],
            "title": "On the computational and statistical complexity of over-parameterized matrix sensing",
            "venue": "arXiv preprint arXiv:2102.02756,",
            "year": 2021
        },
        {
            "authors": [
                "Difan Zou",
                "Yuan Cao",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Gradient descent optimizes overparameterized deep relu networks",
            "venue": "Machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "2023) also provides a fine-grained analysis of the GD dynamics. More recently, Zhang et al. (2021b; 2023) empirically observe that in practice, in the over-parameterization case, GD converges with a sublinear rate, which is exponentially slower than the rate in the exact-parameterization",
            "year": 2023
        },
        {
            "authors": [
                "Ma",
                "Fattahi"
            ],
            "title": "2023) study the performance of the subgradient method with L1 loss under a different set of assumptions on A and showed a linear convergence rate up to some error related to the initialization scale",
            "year": 2023
        },
        {
            "authors": [
                "Tu"
            ],
            "title": "2016) adds a balancing regularization term",
            "venue": "For the asymmetric matrix setting,",
            "year": 2021
        },
        {
            "authors": [
                "Jiang"
            ],
            "title": "2022) considers the asymmetric matrix factorization setting, and proves that starting with a small initialization, the vanilla gradient descent sequentially recovers the principled component of the ground-truth matrix. Soltanolkotabi et al. (2023) proves the convergence of gradient descent in the asymmetric matrix sensing setting",
            "year": 2023
        },
        {
            "authors": [
                "Zhu"
            ],
            "title": "Landscape Analysis of Non-convex Low-rank Problems. The aforementioned works mainly focus on studying the dynamics of GD. There is also a complementary line of works that studies the landscape of the loss functions, and shows the loss functions enjoy benign landscape properties such as (1) all local minima are global, and (2) all saddle points are strict Ge et al",
            "year": 2023
        },
        {
            "authors": [
                "GD Jin"
            ],
            "title": "2017), to obtain a convergence result. There are some works establishing the general landscape analysis for the non-convex lowrank problems. Zhang et al. (2021a) obtains less conservative conditions for guaranteeing the nonexistence of spurious second-order critical points and the strict saddle property, for both symmetric and asymmetric low-rank minimization problems",
            "year": 2022
        },
        {
            "authors": [
                "Soltanolkotabi"
            ],
            "title": "2023), the initialization is F0 = \u03b1 \u00b7 F\u03030 and G0 = \u03b1 \u00b7 G\u03030, while Lemma G.3 uses an imbalance initialization",
            "year": 2023
        },
        {
            "authors": [
                "Soltanolkotabi"
            ],
            "title": "There exist parameters \u03b60",
            "year": 2023
        },
        {
            "authors": [
                "Soltanolkotabi"
            ],
            "title": "2023), the initialization is F0 = \u03b1 \u00b7 F\u03030 and G0 = \u03b1 \u00b7 G\u03030, while Lemma G.3 uses a slightly imbalance initialization",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": ") lower bound of randomly initialized GD for the over-parameterized\ncase (k > r) where T is the number of iterations. This is in stark contrast to the exact-parameterization scenario (k = r) where the convergence rate is exp (\u2212\u2126 (T )). Next, we study asymmetric setting where M\u2217 \u2208 Rn1\u00d7n2 is the unknown matrix of rank r \u226a min{n1, n2}, and one uses an asymmetric parameterization FG\u22a4 to learn M\u2217 where F \u2208 Rn1\u00d7k and G \u2208 Rn2\u00d7k. Building on prior work, we give a global exact convergence result of randomly initialized GD for the exact-parameterization case (k = r) with an exp (\u2212\u2126 (T )) rate. Furthermore, we give the first global exact convergence result for the over-parameterization case (k > r) with an exp ( \u2212\u2126 ( \u03b12T )) rate where \u03b1 is the initialization scale. This linear convergence result in the over-parameterization case is especially significant because one can apply the asymmetric parameterization to the symmetric setting to speed up from \u2126 ( 1/T 2 ) to linear convergence. Therefore, we identify a surprising phenomenon: asymmetric parameterization can exponentially speed up convergence. Equally surprising is our analysis that highlights the importance of imbalance between F and G. This is in sharp contrast to prior works which emphasize balance. We further give an example showing the dependency on \u03b1 in the convergence rate is unavoidable in the worst case. On the other hand, we propose a novel method that only modifies one step of GD and obtains a convergence rate independent of \u03b1, recovering the rate in the exact-parameterization case. We provide empirical studies to verify our theoretical findings."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "A line of recent work showed over-parameterization plays a key role in optimization, especially for neural networks (Allen-Zhu et al., 2019; Du et al., 2018b; Jacot et al., 2018; Safran & Shamir, 2018; Chizat et al., 2019; Wei et al., 2019; Nguyen & Pham, 2020; Fang et al., 2021; Lu et al., 2020; Zou et al., 2020). However, our understanding of the impact of over-parameterization on optimization is far from complete. In this paper, we focus on the canonical matrix sensing problem and show that over-parameterization qualitatively changes the convergence behaviors of gradient descent (GD).\nMatrix sensing aims to recover a low-rank unknown matrix M\u22c6 from m linear measurements,\nyi = Ai(M\u22c6) = \u27e8Ai,M\u22c6\u27e9 = tr(A\u22a4i M\u22c6), for i = 1, . . . ,m, (1.1) where Ai is a linear measurement operator and Ai is the measurement matrix of the same size as M\u22c6. This is a classical problem with numerous real-world applications, including signal processing (Weng & Wang, 2012) and face recognition (Chen et al., 2012), image reconstruction (Zhao et al., 2010; Peng et al., 2014). Moreover, this problem can serve as a test-bed of convergence behaviors in\ndeep learning theory since it is non-convex and retains many key phenomena (Soltanolkotabi et al., 2023; Jin et al., 2023; Li et al., 2018; 2020; Arora et al., 2019). We primarily focus on the overparameterized case where we use a model with rank larger than that of M\u22c6 in the learning process. This case is particularly relevant because rank(M\u22c6) is usually unknown in practice."
        },
        {
            "heading": "1.1 SETTING 1: SYMMETRIC MATRIX SENSING WITH SYMMETRIC PARAMETERIZATION",
            "text": "We first consider the symmetric matrix sensing setting, where M\u22c6 \u2208 Rn\u00d7n is a positive semidefinite matrix of rank r \u226a n. A standard approach is to use a factored form XX\u22a4 to learn M\u22c6 where X \u2208 Rn\u00d7k. We call this symmetric parameterization because XX\u22a4 is always symmetric and positive semi-definite. We will also introduce the asymmetric parameterization soon. We call the case when k = r the exact-parameterization because the rank of XX\u22a4 matches that of M\u22c6. However, in practice, r is often unknown, so one may choose some large enough k > r to ensure the expressiveness of XX\u22a4, and we call this case over-parameterization.\nWe consider using gradient descent to minimize the standard L2 loss for training: Ltr(X) = 1 2m \u2211m i=1 ( yi \u2212 \u27e8Ai, XX\u22a4\u27e9 )2 . We use the Frobneius norm of the reconstruction error as the performance metric:\nL(X) = 1\n2 \u2225XX\u22a4 \u2212M\u22c6\u22252F . (1.2)\nWe note that L(X) is also the matrix factorization loss and can be viewed as a special case of Ltr when {Ai}mi=1 are random Gaussian matrices and the number of linear measurements go to infinity. For the exact-parameterization case, one can combine the results by Sto\u0308ger & Soltanolkotabi (2021) and Tu et al. (2016) to show that randomly initialized gradient descent enjoys an exp (\u2212\u2126 (T )) convergence rate where T is the number of iterations. For the over-parameterization case, one can combine the results by Sto\u0308ger & Soltanolkotabi (2021) and Zhuo et al. (2021) to show an O ( 1/T 2\n) convergence rate upper bound, which is exponentially worse. This behavior has been empirically observed (Zhang et al., 2021b; 2023; Zhuo et al., 2021) without a rigorous proof. See Figure 1.\nContribution 1: \u2126(1/T 2) Lower Bound for Symmetric Over-Parameterization. Our first contribution is a rigorous exponential separation between the exact-parameterization and overparameterization by proving an \u2126(1/T 2) convergence rate lower bound for the symmetric setting with the symmetric over-parameterization.\nTheorem 1.1 (Informal). Suppose we initialize X with a Gaussian distribution with small enough variance that scales with \u03b12, and use gradient descent with a small enough constant step size to optimize the matrix factorization loss (1.2). Let Xt denote the factor matrix at the t-th iteration.\nThen with high probability over the initialization, there exists T (0) > 0 such that we have1 2 \u2225XtX\u22a4t \u2212M\u22c6\u22252F \u2265 ( \u03b12\nt\n)2 ,\u2200t \u2265 T (0). (1.3)\nTechnical Insight: We find the root cause of the slow convergence is from the redundant space in XX\u22a4, which converges to 0 at much slower rate compared to the signal space which converges to M\u22c6 with a linear rate. To derive the lower bound, we construct a potential function and use some novel analyses of the updating rule to show that the potential function decreases slowly after a few rounds. See the precise theorem and more technical discussions in Section 3."
        },
        {
            "heading": "1.2 SETTING 2: SYMMETRIC AND ASYMMETRIC MATRIX SENSING WITH ASYMMETRIC PARAMETERIZATION",
            "text": "Next, we consider the more general asymmetric matrix sensing problem where the ground-truth M\u22c6 \u2208 Rn1\u00d7n2 is asymmetric matrix of rank r. For this setting, we must use the asymmetric parameterization. Specifically, we use FG\u22a4 to learn M\u22c6 where F \u2208 Rn1\u00d7k and G \u2208 Rn2\u00d7k. Same as in the symmetric case, exact-parameterization means k = r and over-parameterization means k > r. We still use gradient descent to optimize the L2 loss for training:\nLtr(F,G) = 1\n2m m\u2211 i=1 ( yi \u2212 \u27e8Ai, FG\u22a4\u27e9 )2 , (1.4)\nand the performance metric is still: L(F,G) = 12\u2225FG \u22a4\u2212M\u22c6\u22252F . To enable the analysis, we assume throughout the paper that the matrices {Ai}mi=1 satisfies the Restricted Isometry Property (RIP) of order 2k + 1 with parameter \u03b4 \u2264 O\u0303( 1\u221a\nkr ). (See Definition 2.1 for the detailed definition).\nAlso note that even for the symmetric matrix sensing problem where M\u22c6 is positive semi-definite, one can still use asymmetric parameterization. Although doing so seems unnecessary at the first glance, we will soon see using asymmetric parameterization enjoys an exponential gain.\nContribution 2: Global Exact Convergence of Gradient Descent for Asymmetric ExactParameterization with a Linear Convergence Rate. Our second major contribution is a global exact convergence result for randomly initilized gradient descent, and we show it enjoys a linear convergence rate.3\nTheorem 1.2 (Informal). In the exact-parameterization setting (k = r), suppose we initialize F and G using a Gaussian distribution with small enough variance \u03b12 and use gradient descent with a small enough constant step size to optimize the asymmetric matrix sensing loss (1.4). Let Ft and Gt denote the factor matrices at the t-the iteration. Then with high probability over the random initialization, there exists T (1) > 0 such that we have\n\u2225FtG\u22a4t \u2212M\u22c6\u22252F = exp (\u2212\u2126 (t)) ,\u2200t \u2265 T (1). (1.5)\nCompared to our results, prior results either require initialization to be close to optimal (Ma et al., 2021), or can only guarantee to find a point with an error of similar scale as the initialization (Soltanolkotabi et al., 2023). In contrast, our result only relies on random initialization and guarantees the error goes to 0 as t goes to infinity. Notably, this convergence rate is independent of \u03b1. See Figure 2(a). Naturally, such a result is expected and can be derived (with considerable effort) by the works (Ma et al., 2021, Theorem 1) and (Soltanolkotabi et al., 2023). Our proof strategy is very different from (Ma et al., 2021) as we further decompose the factors F and G, and we only need (Soltanolkotabi et al., 2023, Theroem 3.3) to deal with the initial phase.\nContribution 3: Global Exact Convergence of Gradient Descent for Asymetric OverParameterization with an Initialization-Dependent Linear Convergence Rate. Our next contribution is analogue theorem for the over-parameterization case with caveat that the initialization scale \u03b1 also appears in the convergence rate.\n1For clarity, in our informal theorems in Section 1, we only display the dependency on \u03b1 and T , and ignore parameters such as dimension, condition number, and step size.\n2T (0) here and T (1), T (2), T (3) in theorems below represent the burn-in time to get to a neighborhood of an optimum, which can depend on initilization scale \u03b1, condition number, dimension, and step size.\n3By exact convergence we mean the error goes to 0 as t goes to infinity in contrast to prior works which only gurantee to converge to a point with the error proportional to the initialization scale \u03b1.\nTheorem 1.3 (Informal). In the over-parameterization setting (k > r), suppose we initialize F and G using a Gaussian distribution with small enough variance \u03b12 and use gradient descent with a small enough constant step size to optimize the asymmetric matrix sensing loss (1.4). Let Ft and Gt denote the factor matrices at the t-th iteration. Then with high probability over the random initialization there exists T (2) > 0 such that we have\n\u2225FtG\u22a4t \u2212M\u22c6\u22252F = exp ( \u2212\u2126 ( \u03b12t )) ,\u2200t \u2265 T (2). (1.6)\nThis is also the first global exact convergence result of randomly initialized gradient descent in the over-parameterized case. Recall that for the symmetric matrix sensing problem, even if M\u22c6 is positive semi-definite, one can still use an asymmetric parameterization FG\u22a4 to learn M\u22c6, and Theorem 1.3 still holds. Comparing Theorem 1.3 and Theorem 1.1, we obtain a surprising corollary:\nFor the symmetric matrix sensing problem, using asymmetric parameterization is exponentially faster than using symmetric parameterization.\nAlso notice that different from Theorem 1.2, the convergence rate of Theorem 1.3 also depends on the initialization scale \u03b1 which we require to be small. Empirically we verify this dependency is necessary. See Figure 2(b). We also study a special case in Section 4.1 to show the dependency on the initialization scale is necessary in the worst case.\nTechnical Insight: Our key technical finding that gives the exponential acceleration is the imbalance of F and G. Denote the imbalance matrix \u2206t = F\u22a4t Ft \u2212G\u22a4t Gt. We show that the converge rate is linear when \u2206t is positive definite, and the rate depends on the minimum eigenvalue of \u2206t. We use imbalance initialization so that the minimum eigenvalue of \u22060 is proportional to \u03b1, we can further show that the minimum eigenvalue \u2206t will not decrease too much, so the final convergence rate is linear. Furthermore, such a connection to \u03b1 also inspires us to design a faster algorithm below.\nContribution 4: A Simple Algorithm with Initialization-Independent Linear Convergence Rate for Asymetric Over-Parameterization. Our key idea is to increase the degree of imbalance when F and G are close to the optimum. We develop a new simple algorithm to accelerate GD. The algorithm only involves transforming the factor matrices F and G in one of iteration to intensify the degree of imbalance (cf. Equation (5.1)). Theorem 1.4 (Informal). In the over-parameterization setting (k > r), suppose we initialize F and G using a Gaussian distribution with small enough variance \u03b12, gradient descent with a small enough constant step size, and the procedure described in Section 5 to optimize the loss (1.4). Let Ft and Gt denote the factor matrices at the t-the iteration. Then with high probability over the random initialization, there exists T (3) > 0 such that we have\n\u2225FtG\u22a4t \u2212M\u22c6\u22252F = exp ( \u2212\u2126 ( t\u2212 T (3) )) ,\u2200t \u2265 T (3). (1.7)"
        },
        {
            "heading": "1.3 RELATED WORK",
            "text": "Matrix sensing is a canonical problem and has been widely studied via the nuclear norm minimization approach (Candes & Recht, 2012; Liu et al., 2012; Recht et al., 2010; Wu & Rebeschini, 2021),\nspectral method (Ma et al., 2021; Tu et al., 2016) and landscape analysis (Ge et al., 2017; Zhu et al., 2018) The most relevant line of work considered global convergence of gradient descent (Zhuo et al., 2021; Sto\u0308ger & Soltanolkotabi, 2021; Soltanolkotabi et al., 2023; Tu et al., 2016). We compare our results with the them in Table 1. The detailed discussions on related work is deferred to Appendix A."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Norm and Big-O Notations. Given a vector v, we use \u2225v\u2225 to denote its Euclidean norm. For a matrix M , we use \u2225M\u2225 to denote its spectral norm and \u2225M\u2225F Frobenius norm. The notations O(\u00b7),\u0398(\u00b7), and \u2126(\u00b7) in the rest of the paper only omit absolute constants. Asymmetric Matrix Sensing. Our primary goal is to recover an unknown fixed rank r matrix M\u22c6 \u2208 Rn1\u00d7n2 from data (yi, Ai), i = 1, . . . ,m satisfying yi = \u27e8Ai,M\u22c6\u27e9 = tr(A\u22a4i M\u22c6), i = 1, . . . ,m, or compactly y = A(M\u22c6), where y \u2208 Rm and A : Rn1\u00d7n2 \u2192 Rm is a linear map with [A(M)]i = tr(A\u22a4i M). We further denote the singular values of M\u22c6 as \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r > \u03c3r+1 = 0 = \u00b7 \u00b7 \u00b7 = \u03c3n, the condition number \u03ba = \u03c31\u03c3r , and the diagonal singular value matrix as \u03a3 with (\u03a3)ii = \u03c3i. To recover M\u22c6, we minimize the following loss function:\nLtr(F,G) = 1\n2 \u2225A(FG\u22a4)\u2212 y\u22252, (2.1)\nwhere F,G \u2208 Rn\u00d7k, where k \u2265 r is the user-specified rank. The gradient descent update rule with a step size \u03b7 > 0 with respect to loss (2.1) can be written explicitly as\nFt+1 = Ft \u2212 \u03b7A\u2217A(FtG\u22a4t \u2212 \u03a3)Gt, Gt+1 = Gt \u2212 \u03b7(A\u2217A(FtG\u22a4t \u2212 \u03a3))\u22a4Ft, (2.2) where A\u2217 : Rm \u2192 Rn\u00d7n is the adjoint map of A and admits an explicit form: A\u2217(z) = \u2211m\ni=1 ziAi.\nWe make the following assumption on A: Restricted Isometry Property (RIP) (Recht et al., 2010). Definition 2.1. [Restricted Isometry Property] An operator A : Rn1\u00d7n2 \u2192 Rm satisfies the Restricted Isometry Property of order r with constant \u03b4 > 0 if for all matrices M : Rn1\u00d7n2 with rank at most r, we have (1\u2212 \u03b4)\u2225M\u22252F \u2264 \u2225A(M)\u22252 \u2264 (1 + \u03b4)\u2225M\u22252F .\nFrom (Candes & Plan, 2011), if the matrix Ai has i.i.d. N(0, 1/m), the operator A has RIP of order 2k + 1 with constant \u03b4 \u2208 (0, 1) when m = \u2126\u0303 ( nk/\u03b42 ) . Thus, m = \u2126\u0303(nk2r) is needed with (4.8).\nDiagonal Matrix Simplification. Since both the RIP and the loss are invariant to orthogonal transformation, we assume without loss generality that M\u22c6 = \u03a3 in the rest of the paper for clarity,\nfollowing prior work (Ye & Du, 2021; Jiang et al., 2022). For the same reason, we also assume n1 = n2 = n to simplify notations, and our results can be easily extended to n1 \u0338= n2. Symmetric Matrix Factorization. In this setting, we further assume M\u22c6 is symmetric and positive semidefinite, and A is the identity map. Since M\u22c6 admits a factorization M\u22c6 = F\u22c6F\u22a4\u22c6 for some F\u22c6 \u2208 Rn\u00d7r, we can force the factor F = G = X in (2.1) and the loss becomes L(X) = 12\u2225XX\n\u22a4\u2212 \u03a3\u22252F . Here, the factor X \u2208 Rn\u00d7k. We shall focus on the over-parameterization setting, i.e., k > r in the Setion 3 below. The gradient descent with a step size \u03b7 > 0 becomes\nXt+1 = Xt \u2212 2\u03b7(XtX\u22a4t \u2212 \u03a3)Xt. (2.3)"
        },
        {
            "heading": "3 LOWER BOUND OF SYMMETRIC MATRIX FACTORIZATION",
            "text": "We present a sublinear lower bound of the convergence rate of the gradient descent (2.3) for symmetric matrix factorization with a small random initialization. Our result supports the empirical observations of the slowdown in Zhuo et al. (2021); Zhang et al. (2021b; 2023) and Figure 1.\nTheorem 3.1. Let X0 = \u03b1 \u00b7 X\u03030, where each entry of X\u03030 is independent initialized from Gaussian distribution N (0, 1/k). For some universal constants ci, 1 \u2264 i \u2264 7, if the gradient descent method (2.3) starting at X0 with the initial scale \u03b1, the search rank k, and the stepsize \u03b7 satisfying that\n0 < \u03b1 \u2264 c1 \u221a \u03c31\u221a\nn log(r \u221a n)\n, k \u2265 c2 ( (r\u03ba)2 log(r \u221a \u03c31/\u03b1) )4 , and 0 < \u03b7 \u2264 c3\nn2\u03ba\u03c31 , (3.1)\nthen with probability at least 1 \u2212 2n2 exp(\u2212 \u221a c4k) \u2212 2n exp(\u2212c5k/4), for all t \u2265 T (0) = c6 log(r \u221a \u03c31)/\u03b1\n\u03b7\u03c3r , we have \u2225XtX\u22a4t \u2212 \u03a3\u22252F \u2265 ( c7 log( \u221a r\u03c31/\u03b1)\u03b1 2\n8\u03c3r\u03b7 \u221a nt\n)2 , \u2200t \u2265 T (0). (3.2)\nThe proof of Theorem 1.1 demonstrates that, following a rapid convergence phase, the gradient descent eventually transitions to a sublinear convergence rate. Also, the over-parameterization rank k is subject to a lower bound requirement in Eq. (3.1) that depends on \u03b1. However, since \u03b1 only appears in the logarithmic term, this requirement is not overly restrictive. It is also consistent with the phenomenon that the gradient descent first converges to a small error that depends on \u03b1 with a linear convergence rate (Sto\u0308ger & Soltanolkotabi, 2021), since our lower bound has a term \u03b14."
        },
        {
            "heading": "3.1 PROOF SKETCH OF THEOREM 3.1",
            "text": "We provide a proof sketch of Theorem 3.1 in this section, deferring the details to Appendix B. The main intuition of Theorem 3.1 is that the last n \u2212 r rows of Xt, corresponding to the space of 0 eigenvalues of \u03a3, converge to 0 at speed no faster than 1T 2 . To make this intuition precise, for each t \u2265 0, we let Xt \u2208 Rn\u00d7k = [xt1, \u00b7 \u00b7 \u00b7 , xtn]\u22a4 where xti \u2208 Rk. We let the potential function be At = \u2211 i>r \u2225xti\u22252. We aim to show the following two key inequalities,\n\u2225xT (0)\ni \u22252 \u2265 \u03b12/8, for all i > r, (3.3a) At+1 \u2265 At(1\u2212O(\u03b7At)), for all t \u2265 T (0). (3.3b)\nSuppose (3.3) is true, then it implies that At \u2265 O ( \u03b12\nt\n) for all t \u2265 T (0). Since (XtX\u22a4t \u2212 \u03a3)ii =\n\u2225xi\u22252, the lower bound (3.2) is established by noting that \u2225XtX\u22a4t \u2212 \u03a3\u22252F \u2265 \u2211\ni>r \u2225xti\u22254 \u2265 A2t/n, where the last inequality uses the Cauchy\u2019s inequality. See more details in Appendix B."
        },
        {
            "heading": "4 CONVERGENCE OF ASYMMETRIC MATRIX SENSING",
            "text": "Here we investigate the dynamic of GD in the context of the asymmetric matrix sensing problem. Surprisingly, we demonstrate that the convergence rate of gradient descent for asymmetric matrix sensing problems is linear, so long as the initialization is imbalanced. However, this linear rate is contingent upon the chosen initialization scale."
        },
        {
            "heading": "4.1 A TOY EXAMPLE OF ASYMMETRIC MATRIX FACTORIZATION",
            "text": "We first use a toy example of asymmetric matrix factorization to demonstrate the behavior of GD. If we assume A is the identity map, and the loss and the GD update become\nL(F,G) = 1\n2 \u2225FG\u22a4 \u2212 \u03a3\u22252F . (4.1)\nFt+1 = Ft \u2212 \u03b7(FtG\u22a4t \u2212 \u03a3)Gt, Gt+1 = Gt \u2212 \u03b7(FtG\u22a4t \u2212 \u03a3)\u22a4Ft (4.2)\nThe following theorem tightly characterizes the convergence rate for a toy example. Theorem 4.1. Consider the asymmetric matrix factorization (4.1), with k = r + 1. Choose \u03b7 \u2208 [0, 1/6] and \u03b1 \u2208 [0, 1]. Assume that the diagonal matrix \u03a3 = diag(\u03c31, . . . , \u03c3n), where \u03c3i = 1 for i \u2264 r and is 0 otherwise. Also assume that gradient descent (4.2) starts at F0, G0, where (F0)ii = \u03b1 for 1 \u2264 i \u2264 k, and (G0)ii = \u03b1 for 1 \u2264 i \u2264 r, (G0)ii = \u03b1/3 for i = r + 1, and all other entries of F0 and G0 are zero. Then, the iterate (Ft, Gt) of (4.2) satisfies that\n\u03b14 36 (1\u2212 4\u03b7\u03b12)2t \u2264 \u2225FtG\u22a4t \u2212 \u03a3\u22252F \u2264 4n \u00b7 (1\u2212 \u03b7\u03b12/4)(t\u2212T1), \u2200t \u2265 T1.\nwhere T1 = c1 log(1/\u03b1)/\u03b7, and c1 is a universal constant.\nThe above initialization implicitly assumes that we know the singular vectors of \u03a3. Such an assumption greatly simplifies our presentations below. Note that we have a different initialization scale for Ft and Gt. As we shall see, such an imbalance is the key to establishing the convergence of FtG\u22a4t .\nWe introduce some notations before our proof. Denote the matrix of the first r row of F,G as U, V \u2208 Rr\u00d7k respectively, and the matrix of the last n \u2212 r row of F,G as J,K \u2208 R(n\u2212r)\u00d7k respectively. Further denote the corresponding iterate of gradient descent as Ut, Vt, Jt, and Kt. The\ndifference FtG\u22a4t \u2212\u03a3 can be written in a block form as FtG\u22a4t \u2212\u03a3 = ( UtV\n\u22a4 t \u2212 \u03a3r JtV \u22a4t UtK \u22a4 t JtK \u22a4 t\n) where\n\u03a3r \u2208 Rr\u00d7r is the identity matrix. Hence, we may bound \u2225FtG\u22a4t \u2212 \u03a3\u2225 by \u2225JtK\u22a4t \u2225 \u2264 \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 \u2225UtV \u22a4t \u2212 \u03a3r\u2225+ \u2225JtV \u22a4t \u2225+ \u2225UtK\u22a4t \u2225+ \u2225JtK\u22a4t \u2225. (4.3) From (4.3), we shall upper bound \u2225UtV \u22a4t \u2212\u03a3r\u2225, \u2225JtV \u22a4t \u2225, \u2225UtK\u22a4t \u2225, and \u2225JtK\u22a4t \u2225, and lower bound \u2225JtKt\u2225\u22a4. Let us now prove Theorem 4.1.\nProof. With our particular initialization (4.2), we have the following equality for all t:\nUtK \u22a4 t = 0, JtV \u22a4 t = 0, Ut = Vt, Jt = atA, Kt = btA, Ut = (\u03b1tIr, 0), (4.4a)\na0 = \u03b1, b0 = \u03b1/3, \u03b10 = \u03b1 (4.4b)\nat+1 = at \u2212 \u03b7atb2t , (4.4c) bt+1 = bt \u2212 \u03b7a2t bt. (4.4d)\n\u03b1t+1 = \u03b1t(1 + \u03b7 \u2212 \u03b7\u03b12t ), (4.4e) where A \u2208 R(n\u2212r)\u00d7k is the matrix that (A)1k = 1 and other elements are all zero, and at, bt, \u03b1t \u2208 R. We leave the detailed verification of (4.4) to Appendix C. By considering (4.3) and (4.4), we see that we only need to keep track of three sequences at, bt, \u03b1t. In particular, we have the following inequalities for at, bt, \u03b1t for all t \u2265 T1:\nat \u2208 [ 1\n2 \u03b1, \u03b1\n] , bt \u2208 [ \u03b1\n3 (1\u2212 4\u03b7\u03b12)t, \u03b1 3 (1\u2212 \u03b7\u03b1\n2 4 )t/2 ] , and |\u03b1t \u2212 1| \u2264 (1\u2212 \u03b7/2)t\u2212T1 . (4.5)\nIt is then easy to derive the upper and lower bounds. We leave the detail in checking (4.5) to Appendix C. Our proof is complete.\nTechnical Insight. This proof of the toy case tells us why the imbalance initialization in the asymmetric matrix factorization helps us to break the \u2126(1/T 2) convergence rate lower bound of the symmetric case. Since we initialize F0 and G0 with a different scale, this difference makes the norm of K converge to zero at a linear rate while keeping J larger than a constant. However, in the symmetric case, we have at = bt, so they must both converge to zero when the loss goes to zero (as \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2265 atbt), leading to a sublinear convergence rate. In short, the imbalance property in the initialization causes faster convergence in the asymmetric case."
        },
        {
            "heading": "4.2 THEORETICAL RESULTS FOR ASYMMETRIC MATRIX SENSING",
            "text": "Motivated by the toy case in Section 4.1, the imbalance property is the key ingredient for a linear convergence rate. If we use a slightly imbalanced initialization F0 = \u03b1 \u00b7 F\u03030, G0 = (\u03b1/3) \u00b7 G\u03030, where the elements of F\u03030 and G\u03030 are N (0, 1/n), we have \u2225F\u22a40 F0 \u2212 G\u22a40 G0\u2225 = \u2126(\u03b12). Then, we can show that the imbalance property keeps true when the step size is small, and thus, the gradient descent (2.2) converges with a linear rate similar to the toy case.\nOur result is built upon the recent work (Soltanolkotabi et al., 2023) in dealing with the initial phase. Define the following quantities \u03b10, \u03b70 according to (Soltanolkotabi et al., 2023, Theorem 1):\n\u03b10 = c \u221a \u03c31\nk5 max{2n, k}2 \u00b7\n( \u221a k \u2212 \u221a r \u2212 1\n\u03ba2 \u221a max{2n, k}\n)C\u03ba , \u03b70 =\nc k5\u03c31 log ( 2 \u221a 2\u03c31\n\u03b1( \u221a k\u2212 \u221a r\u22121 ) , (4.6) where c and C are some numerical constants. Below, we show exact convergence results for both k = r and k > r. Theorem 4.2. Consider the matrix sensing problem (1.4) and the gradient descent (2.2). For some numerical constants ci > 0, 1 \u2264 i \u2264 7, if the search rank k satisfies r < k < n8 , the initial scale \u03b1 and \u03b7 satisfy\n\u03b1 \u2264 min { c1\u03ba \u22122\u221a\u03c3r, \u03b10 } , \u03b7 \u2264 min { c1\u03b1 4/\u03c331 , \u03b70 } , (4.7)\nwhere \u03b10, \u03b70 are defined in (4.6), and the operator A has the RIP of order 2k + 1 with constant \u03b4 satisfying\n\u03b4 \u221a 2k + 1 \u2264 min { c1\u03ba \u22126 log\u22121( \u221a \u03c3r/(n\u03b1)),\nc1 \u03ba3 \u221a r , 1/128\n} , (4.8)\nthen the gradient descent (2.2) starting with F0 = \u03b1 \u00b7 F\u03030, G0 = (\u03b1/3) \u00b7 G\u03030, where F\u03030, G\u03030 \u2208 Rn\u00d7k whose entries are i.i.d. N (0, 1/n), satisfies that\n\u2225FtG\u22a4t \u2212 \u03a3\u22252F \u2264 \u03c34rn\nc7\u03b14\u03ba2\n( 1\u2212 \u03b7\u03b1 2\n8\n)t/4 , \u2200t \u2265 T (1), (4.9)\nwith probability at least 1 \u2212 2e\u2212c2n \u2212 c3e\u2212c4k \u2212 (c5\u03c5)(k\u2212r+1), where T (1) = c6 log( \u221a \u03c3r/n\u03b1\u03c5)/\u03b7\u03c3r) and v \u2208 [0, 1] is an arbitrary parameter.\nNext, we state our results on exact parametrization. Theorem 4.3. Consider the same setting as Theorem 4.2 except assuming k = r, then with probability at least 1\u2212 2e\u2212c2n \u2212 c3e\u2212c4k \u2212 c5\u03c5, the gradient descent (2.2) achieves\n\u2225FtG\u22a4t \u2212 \u03a3\u22252F \u2264 2n\u03c3r \u00b7 ( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n)t , \u2200t \u2265 T (2), (4.10)\nwhere T (2) = c7 log( \u221a \u03c3r/n\u03b1\u03c5)/\u03b7\u03c3r) for some numerical constant c7.\nNow we highlight two bullet points of Theorem 4.2 and 4.3.\nExact Convergence. The main difference between the above theorems and previous convergence results in (Soltanolkotabi et al., 2023) is that we prove the exact convergence property, i.e., the loss finally degenerates to zero when T tends to infinity (cf. Table 1). Moreover, we prove that the convergence rate of the gradient descent depends on the initialization scale \u03b1, which matches our empirical observations in Figure 1.2.\nDiscussions about Parameters. First, since we utilize the initial phase result in (Soltanolkotabi et al., 2023) to guarantee that the loss degenerates to a small scale, our parameters \u03b4, \u03b1, and \u03b7 should satisfy the requirement \u03b40 = O( 1\u03ba3\u221ar ), \u03b10, \u03b70 in (Soltanolkotabi et al., 2023). We further require \u03b42k+1 = O\u0303(\u03ba\u22126), \u03b1 = O(\u03ba\u22122 \u221a \u03c3r), which are both polynomials of the conditional number \u03ba. In addition, the step size \u03b7 has the requirement \u03b7 = O(\u03b14/\u03c331), which can be much smaller than the requirements \u03b7 = O\u0303(1/\u03ba5\u03c31) in (Soltanolkotabi et al., 2023). In Section 5, we propose a novel algorithm that allows larger learning rate which is independent of \u03b1.\nTechnical insight Similar to the asymmetric matrix factorization case in the proof of Theorem 4.1, the main effort is in characterizing the behavior of JtK\u22a4t . In particular, the update rule of Kt is\nKt+1 = Kt(1\u2212 \u03b7F\u22a4t Ft) + \u03b7E, (4.11)\nwhere E is some error matrix since A is not an identity. Because of our initialization, we know the following holds for t = 0 and \u2206t = F\u22a4t Ft \u2212G\u22a4t Gt,\nc\u03b12I \u2aaf \u2206t \u2aaf C\u03b12I. (4.12)\nfor some numerical constant c, C > 0. Hence, we can show \u2225Kt\u2225 shrinks towards 0 so long as (4.11) is true, E = 0, and Gt is well-bounded. Indeed, we can prove (4.12) and Gt, Jt upper bounded for all t \u2265 0 via a proper induction. We may then be tempted to conclude JtK\u22a4t converges to 0. However, the actual analysis of the gradient descent (2.2) for matrix sensing is much more complicated due to the error E. It is now unclear whether \u2225Kt\u2225 will shrink under (4.12). To deal with it, we further consider the structure of E. We leave the details to Appendix D."
        },
        {
            "heading": "5 A SIMPLE FAST CONVERGENCE METHOD",
            "text": "As discussed in Section 4, the fundamental reason that the convergence rate depends on the initialization scaling \u03b1 is that the imlabace between F and G determines the convergence rate, but the imbalance between F and G remains at the initialization scale. This observation motivates us to do a straightforward additional step in one iteration to intensify the imbalance. Specifically, suppose at the T0 iteration we have reached a neighborhood of an optimum that satisfies: \u2225A\u2217A(F\u0303T (3)G\u0303\u22a4T (3) \u2212 \u03a3)\u2225 \u2264 \u03b3 where the radius \u03c3 1/4 r \u00b7 \u2225FT (3)G\u22a4T (3)\u2225\n3/4/8 is chosen for some technical reasons (cf. Section F). Here we use F\u0303t and G\u0303t to denote the iterates before we make the change we describe below and Ft and Gt to denote the iterates after make the change.\nLet the singular value decomposition of F\u0303T (3) = A\u03a3\u2032B with the diagonal matrix \u03a3\u2032 \u2208 Rk\u00d7k and \u03a3\u2032ii = \u03c3 \u2032 i, then let \u03a3inv \u2208 Rk\u00d7k be a diagonal matrix and (\u03a3inv)ii = \u03b2/\u03c3\u2032i for some small constant \u03b2 = O(\u03c3r), then we transform the matrix FT (3) , GT (3) by\nFT (3) = F\u0303T (3)B \u22a4\u03a3inv, GT (3) = G\u0303T (3)B\u03a3 \u22121 inv (5.1)\nWe can show that, when F and G have reached a local region of an optimum, their magnitude will have similar scale as M\u22c6. Therefore, the step Equation (5.1) can create an imbalance between F and G as large the magnitude of M\u22c6, which is significantly larger than the initial scaling \u03b1. The following theorem shows we can obtain a convergence rate independent of the initialization scaling \u03b1. The proof is deferred to Appendix F.\nTheorem 5.1. With the same setting as Theorem 4.2, suppose that at the step T (3) we have \u2225A\u2217A(F\u0303T (3)G\u0303\u22a4T (3) \u2212 \u03a3)\u2225 \u2264 \u03b3 for some \u03b3 > 0, and we do one step as in Equation (5.1). Then with probability at least 1\u2212 2e\u2212c2n \u2212 c3e\u2212c4k \u2212 (c5\u03c5)(k\u2212r+1), we have for all t > T (3),\n\u2225FtG\u22a4t \u2212 \u03a3\u22252F \u2264 n\u03b212\n\u03c341\n( 1\u2212 \u03b7\u03b2 2\n2\n)2(t\u2212T (3)) ,\nso long as 0 < c7\u03b31/6\u03c3 1/3 1 \u2264 \u03b2 \u2264 c8\u03c3r, and the step size satisfies \u03b7 \u2264 c9\u03b22/\u03c321 from the iteration T (3) \u2264 c10 log( \u221a \u03c3r/n\u03b1\u03c5)/\u03b7\u03c3r for some positive numerical constants ci, i = 1, . . . , 10."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper demonstrated qualitatively different behaviors of GD in the exact-pasteurization and overpasteurization scenarios in symmetric and asymmetric settings. For the symmetric matrix sensing problem, we provide a \u2126(1/T 2) lower bound. For the asymmetric matrix sensing problem, we show that the gradient descent converges at a linear rate, where the rate is dependent on the initialization scale. Moreover, we introduce a simple procedure to get rid of the initialization scale dependency. We believe our analyses are also useful for other problems, such as deep linear networks."
        },
        {
            "heading": "CONTENTS",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": "1.1 Setting 1: Symmetric Matrix Sensing with Symmetric Parameterization . . . . . . 2\n1.2 Setting 2: Symmetric and Asymmetric Matrix Sensing with Asymmetric Parameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
        },
        {
            "heading": "2 Preliminaries 5",
            "text": ""
        },
        {
            "heading": "3 Lower Bound of Symmetric Matrix Factorization 6",
            "text": "3.1 Proof Sketch of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
        },
        {
            "heading": "4 Convergence of Asymmetric Matrix Sensing 6",
            "text": "4.1 A Toy Example of Asymmetric Matrix Factorization . . . . . . . . . . . . . . . . 7\n4.2 Theoretical Results for Asymmetric Matrix Sensing . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "5 A Simple Fast convergence method 9",
            "text": ""
        },
        {
            "heading": "6 Conclusion 9",
            "text": ""
        },
        {
            "heading": "A Related Work 16",
            "text": ""
        },
        {
            "heading": "B Proof of Theorem 3.1 17",
            "text": "B.1 Proof outline of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nB.2 Phase 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nB.3 Phase 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.4 Phase 3: lower bound of convergence rate . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "C Proof of Theorem 4.1 27",
            "text": ""
        },
        {
            "heading": "D Proof of Theorem 4.2 29",
            "text": "D.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nD.2 Proof Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nD.3 Initial iterations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nD.4 Phase 1: linear convergence phase. . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nD.5 Phase 2: Adjustment Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nD.6 Phase 3: local convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38"
        },
        {
            "heading": "E Proof of Theorem 4.3 47",
            "text": "F Proof of Theorem 5.1 48\nF.1 Proof Sketch of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nF.2 Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48"
        },
        {
            "heading": "G Technical Lemma 52",
            "text": "G.1 Proof of Lemma B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nG.2 Proof of Lemma B.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53"
        },
        {
            "heading": "H Experiment Details 54",
            "text": ""
        },
        {
            "heading": "I Additional Experiments 54",
            "text": "I.1 Comparisons between Asymmetric and Symmetric Matrix Sensing . . . . . . . . . 54\nI.2 Well-Conditioned Case and Ill-Conditioned Case . . . . . . . . . . . . . . . . . . 55\nI.3 Larger Initialization Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nI.4 Larger True Rank and Over-Parameterized Rank . . . . . . . . . . . . . . . . . . . 56\nI.5 Initialization Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\nAppendix"
        },
        {
            "heading": "A RELATED WORK",
            "text": "Matrix Sensing. Matrix sensing aims to recover the low-rank matrix based on measurements. Candes & Recht (2012); Liu et al. (2012) propose convex optimization-based algorithms, which minimize the nuclear norm of a matrix, and Recht et al. (2010) show that projected subgradient methods can recover the nuclear norm minimizer. Wu & Rebeschini (2021) also propose a mirror descent algorithm, which guarantees to converge to a nuclear norm minimizer. See (Davenport & Romberg, 2016) for a comprehensive review.\nNon-Convex Low-Rank Factorization Approach. The nuclear norm minimization approach involves optimizing over a n \u00d7 n matrix, which can be computationally prohibitive when n is large. The factorization approach tries to use the product of two matrices to recover the underlying matrix, but this formulation makes the optimization problem non-convex and is significantly more challenging for analysis. For the exact-parameterization setting (k = r), Tu et al. (2016); Zheng & Lafferty (2015) shows the linear convergence of gradient descent when starting at a local point that is close to the optimal point. This initialization can be implemented by the spectral method. For the overparameterization scenario (k > r), in the symmetric setting, Sto\u0308ger & Soltanolkotabi (2021) shows that with a small initialization, the gradient descent achieves a small error that dependents on the initialization scale, rather than the exact-convergence. Zhuo et al. (2021) shows exact convergence with O(1/T 2) convergence rate in the overparamterization setting. These two results together imply the global convergence of randomly initialized GD with an O ( 1/T 2 ) convergence rate upper bound. Jin et al. (2023) also provides a fine-grained analysis of the GD dynamics. More recently, Zhang et al. (2021b; 2023) empirically observe that in practice, in the over-parameterization case, GD converges with a sublinear rate, which is exponentially slower than the rate in the exact-parameterization case, and coincides with the prior theory\u2019s upper bound (Zhuo et al., 2021). However, no rigorous proof of the lower bound is given whereas we bridge this gap. On the other hand, Zhang et al. (2021b; 2023) propose a preconditioned GD algorithm with a shrinking damping factor to recover the linear convergence rate. Xu et al. (2023) show that the preconditioned GD algorithm with a constant damping factor coupled with small random initialization requires a less stringent assumption on A and achieves a linear convergence rate up to some prespecified error. Ma & Fattahi (2023) study the performance of the subgradient method with L1 loss under a different set of assumptions on A and showed a linear convergence rate up to some error related to the initialization scale. We show that by simply using the asymmetric parameterization, without changing the GD algorithm, we can still attain the linear rate.\nFor the asymmetric matrix setting, many previous works (Ye & Du, 2021; Ma et al., 2021; Tong et al., 2021; Ge et al., 2017; Du et al., 2018a; Tu et al., 2016; Zhang et al., 2018a;b; Wang et al., 2017; Zhao et al., 2015) consider the exact-parameterization case (k = r). Tu et al. (2016) adds a balancing regularization term 18\u2225F\n\u22a4F \u2212G\u22a4G\u22252F to the loss function, to make sure that F and G are balanced during the optimization procedure and obtain a local convergence result. More recently, some works (Du et al., 2018a; Ma et al., 2021; Ye & Du, 2021) show GD enjoys an auto-balancing property where F and G are approximately balanced; therefore, additional balancing regularization is unnecessary. In the asymmetric matrix factorization setting, Du et al. (2018a) proves a global convergence result of GD with a diminishing step size and the GD recovers M\u2217 up to some error. Later, Ye & Du (2021) gives the first global convergence result of GD with a constant step size. Ma et al. (2021) shows linear convergence of GD with a local initialization and a larger stepsize in the asymmetric matrix sensing setting. Although exact-parameterized asymmetric matrix factorization and matrix sensing problems have been explored intensively in the last decade, our understanding of the over-parameterization setting, i.e., k > r, remains limited. Jiang et al. (2022) considers the asymmetric matrix factorization setting, and proves that starting with a small initialization, the vanilla gradient descent sequentially recovers the principled component of the ground-truth matrix. Soltanolkotabi et al. (2023) proves the convergence of gradient descent in the asymmetric matrix sensing setting. Unfortunately, both works only prove that GD achieves a small error when stopped early, and the error depends on the initialization scale. Whether the gradient descent can achieve exact-convergence remains open, and we resolve this problem by novel analyses. Furthermore, our analyses highlight the importance of the imbalance between F and G.\nLastly, we want to remark that we focus on gradient descent for L2 loss, there are works on more advanced algorithms and more general losses (Tong et al., 2021; Zhang et al., 2021b; 2023; 2018a;b; Ma & Fattahi, 2021; Wang et al., 2017; Zhao et al., 2015; Bhojanapalli et al., 2016; Xu et al., 2023). We believe our theoretical insights are also applicable to those setups.\nLandscape Analysis of Non-convex Low-rank Problems. The aforementioned works mainly focus on studying the dynamics of GD. There is also a complementary line of works that studies the landscape of the loss functions, and shows the loss functions enjoy benign landscape properties such as (1) all local minima are global, and (2) all saddle points are strict Ge et al. (2017); Zhu et al. (2018); Li et al. (2019); Zhu et al. (2021); Zhang et al. (2023). Then, one can invoke a generic result on perturbed gradient descent, which injects noise to GD Jin et al. (2017), to obtain a convergence result. There are some works establishing the general landscape analysis for the non-convex lowrank problems. Zhang et al. (2021a) obtains less conservative conditions for guaranteeing the nonexistence of spurious second-order critical points and the strict saddle property, for both symmetric and asymmetric low-rank minimization problems. The paper Bi et al. (2022) analyzes the gradient descent for the symmetric case and asymmetric case with a regularized loss. They provide the local convergence result using PL inequality, and show the global convergence for the perturbed gradient descent. We remark that injecting noise is required if one solely uses the landscape analysis alone because there exist exponential lower bounds for standard GD (Du et al., 2017).\nSlowdown Due to Over-parameterization. Similar exponential slowdown phenomena caused by over-parameterization have been observed in other problems beyond matrix recovery, such as teacher-student neural network training (Xu & Du, 2023; Richert et al., 2022) and ExpectationMaximization algorithm on Gaussian mixture model (Wu & Zhou, 2021; Dwivedi et al., 2020)."
        },
        {
            "heading": "B PROOF OF THEOREM 3.1",
            "text": "In this proof, we denote\nX \u2208 Rn\u00d7k = x \u22a4 1\nx\u22a42 \u00b7 \u00b7 \u00b7 x\u22a4n  , (B.1) where xi \u2208 Rk\u00d71 is the transpose of the row vector. Since the updating rule can be written as\nXt+1 = Xt \u2212 \u03b7(XtX\u22a4t \u2212 \u03a3)Xt,\nwhere we choose \u03b7 instead of 2\u03b7 for the simplicity, which does not influence the subsequent proof. By substituting the equation (B.1), the updating rule can be written as\n(xt+1i ) \u22a4 = (1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i))x\u22a4i \u2212 n\u2211 j=1,j \u0338=i \u03b7((xti) \u22a4xtj(x t j) \u22a4)\nwhere \u03c3i = 0 for i > r. Denote\n\u03b8 = max j,k\n(x\u22a4j xk) 2\n\u2225xj\u22252\u2225xk\u22252\nis the maximum angle between different vectors in x1, \u00b7 \u00b7 \u00b7 , xn. We start with the outline of the proof."
        },
        {
            "heading": "B.1 PROOF OUTLINE OF THEOREM 3.1",
            "text": "Recall we want to establish the key inequalities (3.3). The updating rule (2.3) gives the following lower bound of xt+1i for i > r:\n\u2225xt+1i \u2225 2 \u2265 \u2225xti\u22252 1\u2212 2\u03b7\u03b8Ut \u2211 j\u2264r \u2225xtj\u22252 \u2212 2\u03b7 \u2211 j>r \u2225xtj\u22252  , (B.2)\nwhere the quantity \u03b8Ut = maxi,j:min{i,j}\u2264r \u03b8ij,t and the square cosine \u03b8ij,t = cos 2 \u2220(xi, xj). Thus, to establish the key inequalities (3.3), we need to control the quantity \u03b8Ut . Our analysis then consists of three phases. In the last phase, we show (3.3) holds and our proof is complete.\nIn the first phase, we show that \u2225xti\u22252 for i \u2264 r becomes large, while \u2225xti\u22252 for i > r still remains small yet bounded away from 0. In addition, the quantity \u03b8ij,t remains small. Phase 1 terminates when \u2225xti\u22252 is larger than or equal to 34\u03c3i.\nAfter the first phase terminates, in the second and third phases, we show that \u03b8Ut converges to 0 linearly and the quantity \u03b8Ut \u03c31/ \u2211 j>r \u2225xtj\u22252 converges to zero at a linear rate as well. We also keep track of the magnitude of \u2225xti\u22252 and show \u2225xti\u2225 stays close to \u03c3i for i \u2264 r, and \u2225xti\u22252 \u2264 2\u03b12 for i > r.\nThe second phase terminates once \u03b8Ut \u2264 O( \u2211\nj>r \u2225xtj\u22252/\u03c31) and we enter the last phase: the convergence behavior of \u2211 j>r \u2225xtj\u22252. Note with \u03b8Ut \u2264 O( \u2211 j>r \u2225xtj\u22252/\u03c31) and \u2225xti\u22252 \u2264 2\u03c3r for i \u2264 r, we can prove (3.3b). The condition (3.3a) can be proven since the first two phases are quite short and the updating formula of xi for i > r shows \u2225xi\u22252 cannot decrease too much."
        },
        {
            "heading": "B.2 PHASE 1",
            "text": "In this phase, we show that \u2225xti\u22252 for i \u2264 r becomes large, while \u2225xti\u22252 for i > r still remains small. In addition, the maximum angle between different column vectors remains small. Phase 1 terminates when \u2225xti\u22252 is larger than a constant. To be more specific, we have the following two lemmas. Lemma B.1 states that the initial angle \u03b80 = O(log2(r \u221a \u03c31/\u03b1)(r\u03ba)\n2) is small because the vectors in the high-dimensional space are nearly orthogonal.\nLemma B.1. For some constant c4 and c, if k \u2265 c 2\n16 log4(r \u221a \u03c31/\u03b1)(r\u03ba)4 , with probability at least\n1\u2212 c4n2k exp(\u2212 \u221a k), we have\n\u03b80 \u2264 c\nlog2(r \u221a \u03c31/\u03b1)(r\u03ba)2\n(B.3)\nProof. See \u00a7G.1 for proof.\nLemma B.2 states that with the initialization scale \u03b1, the norm of randomized vector x0i is \u0398(\u03b1 2).\nLemma B.2. With probability at least 1\u2212 2n exp(\u2212c5k/4), for some constant c, we have\n\u2225x0i \u22252 \u2208 [\u03b12/2, 2\u03b12].\nProof. See \u00a7G.2 for the proof.\nNow we prove the following three conditions by induction. Lemma B.3. There exists a constant C1, such that T1 \u2264 C1(log( \u221a \u03c31/n\u03b1)/\u03b7\u03c3r) and then during the first T1 rounds, with probability at least 1 \u2212 2c4n2k exp(\u2212 \u221a k) \u2212 2n exp(\u2212c5k/4) for some constant c4 and c5, the following four statements always hold\n\u2225xti\u22252 \u2264 2\u03c31 (B.4) \u03b12/4 \u2264 \u2225xti\u22252 \u2264 2\u03b12 (i > r) (B.5)\n2\u03b80 \u2265 \u03b8t (B.6)\nAlso, if \u2225xti\u22252 \u2264 3\u03c3i/4, we have\n\u2225xt+1i \u2225 2 \u2265 (1 + \u03b7\u03c3r/4)\u2225xti\u22252. (B.7)\nMoreover, at T1 rounds, \u2225xT1i \u22252 \u2265 3\u03c3i/4, and Phase 1 terminates.\nProof. By Lemma B.1 and Lemma B.2, with probability at least 1 \u2212 2c4n2k exp(\u2212 \u221a k) \u2212 2n exp(\u2212c5k/4), we have \u2225x0i \u22252 \u2208 [\u03b12/2, 2\u03b12] for i \u2208 [n], and \u03b80 \u2264 clog2(r\u221a\u03c31/\u03b1)(r\u03ba)2 . Then assume that the three conditions hold for rounds before t, then at the t+ 1 round, we proof the four statements above one by one.\nProof of Eq.(B.5) For i > r, we have\n(xt+1i ) \u22a4 = (xti)\n\u22a4 \u2212 \u03b7 n\u2211\nj=1\n(xti) \u22a4xtj(x t j) \u22a4\nThen, the updating rule of \u2225xti\u22252 can be written as\n\u2225(xt+1i )\u2225 2 2 = \u2225xti\u22252 \u2212 2\u03b7 n\u2211 j=1 ((xti) \u22a4xtj) 2 + \u03b72( n\u2211 j,k=1 (xti) \u22a4xtj(x t j) \u22a4xtk(x t k) \u22a4xti) \u2264 \u2225xti\u22252. (B.8)\nThe last inequality in (B.8) is because (xti) \u22a4xtj(x t j) \u22a4xtk(x t k)\n\u22a4(xti) \u2264 (xtj)\u22a4xtk(((xti)\u22a4xtj)2 + ((xtk)\u22a4xti)2)/2 (B.9) \u2264 \u03c31((xti)\u22a4xtj)2 + ((xtk)\u22a4xti)2), (B.10)\nand then \u03b72 n\u2211\nj,k=1\n(xti) \u22a4xtj(x t j) \u22a4xtk(x t k) \u22a4(xti) \u2264 \u03b72 n\u2211\nj,k=1\n\u03c31((x t i) \u22a4xtj) 2 + ((xtk) \u22a4xti) 2)\n= \u03b72 \u00b7 n\u03c31 n\u2211\nj=1\n((xti) \u22a4xtj) 2\n\u2264 \u03b7 n\u2211\nj=1\n((xti) \u22a4xtj) 2. (B.11)\nwhere the last inequality holds because \u03b7 \u2264 1/n\u03c31. Thus, the \u21132-norm of x\u22a4i does not increase, and the right side of Eq.(B.5) holds.\nAlso, we have\n\u2225xt+1i \u2225 2 \u2265 \u2225xti\u22252 \u2212 2\u03b7 n\u2211 j=1 ((xti) \u22a4xtj) 2 + \u03b72 \u2225\u2225\u2225\u2225\u2225\u2225 n\u2211 j=1 (xti) \u22a4xtj(x t j) \u22a4 \u2225\u2225\u2225\u2225\u2225\u2225 2\n\u2265 \u2225xti\u22252 \u2212 \u2225xti\u22252 \u00b7 2\u03b7\u03b8t \u00b7 n\u2211\nj \u0338=i\n\u2225xtj\u22252 \u2212 2\u03b7\u2225xi\u22254 (B.12)\nEquation (B.2) is because ((xti) \u22a4xtj) 2\n\u2225xti\u22252\u2225xtj\u22252 = \u03b8ij,t \u2264 \u03b8t. Now by (B.4) and (B.5), we can get\nn\u2211 j \u0338=i \u2225xtj\u22252 \u2264 r \u00b7 2\u03c31 + (n\u2212 r) \u00b7 2\u03b12 \u2264 2\u03c31 + 2n\u03b12\nHence, we can further derive \u2225xt+1i \u2225 2 \u2265 \u2225xti\u22252 \u00b7 ( 1\u2212 2\u03b7\u03b8t(2r\u03c31 + 2n\u03b12)\u2212 2\u03b7 \u00b7 2\u03b12 ) \u2265 \u2225xti\u22252 \u00b7 ( 1\u2212 \u03b7(8\u03b8t\u03c31 + 4\u03b12) ) , where the last inequality is because \u03b1 \u2264 \u221ar\u03c31/ \u221a n. Thus, by (1 \u2212 a)(1 \u2212 b) \u2265 (1 \u2212 a \u2212 b) for a, b > 0, we can get \u2225xT1i \u2225 2 \u2265 \u2225x0i \u22252 \u00b7 (1\u2212 \u03b7(8\u03b8t\u03c31 + 4\u03b12))T1\n\u2265 \u03b1 2\n2 \u00b7 (1\u2212 T1\u03b7(8 \u00b7 (2\u03b80)\u03c31 + 4\u03b12)) (B.13)\n\u2265 \u03b1 2\n4 . (B.14)\nEquation (B.13) holds by induction hypothesis (B.6), and the last inequality is because of our choice on T1, \u03b1, and \u03b80 \u2264 O( 1r\u03ba log(\u221a\u03c31/\u03b1) ) from the induction hypothesis. Hence, we complete the proof of Eq.(B.5).\nProof of Eq.(B.7) For i \u2264 r, if \u2225xti\u22252 \u2264 3\u03c3i/4, by the updating rule,\n\u2225xt+1i \u2225 2 2 \u2265 (1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i))2\u2225xti\u22252 \u2212 2\u03b7 n\u2211 j \u0338=i ((xti) \u22a4xtj) 2 + \u03b72(\u2225xti\u22252 \u2212 \u03c3i) n\u2211 j \u0338=i ((xti) \u22a4xtj) 2\n(B.15)\n\u2265 (1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i))2\u2225xti\u22252 \u2212 2\u03b7 n\u2211\nj \u0338=i\n((xti) \u22a4xtj) 2 \u2212 \u03b72|\u2225xti\u22252 \u2212 \u03c3i| \u00b7 n\u2211\nj \u0338=i\n\u2225xti\u22252\u2225xtj\u22252\n\u2265 (1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i))2\u2225xti\u22252 \u2212 2\u03b7 n\u2211\nj \u0338=i\n((xti) \u22a4xtj) 2 \u2212 4\u03b72(n\u03c321)\u2225xti\u22252.\nTHe last inequality uses the fact that |\u2225xti\u22252 \u2212\u03c3i| \u2264 2\u03c31 and \u2225xtj\u22252 \u2264 2\u03c31. Then, by ((xti)\u22a4xtj)2 \u2264 \u2225xti\u22252\u2225xtj\u22252 \u00b7 \u03b8, we can further get\n\u2225xt+1i \u2225 2 \u2265 1\u2212 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i)\u2212 2\u03b7 n\u2211 j \u0338=i \u2225xtj\u22252\u03b8 \u2212 2\u03b72(n\u03c321)  \u2225xti\u22252 \u2265 (1 + \u03b7\u03c3i/2\u2212 2\u03b72(n\u03c321)\u2212 \u03b7\u03c3r/16)\u2225xti\u22252 (B.16) \u2265 (1 + \u03c3i(\u03b7/2\u2212 \u03b7/16\u2212 \u03b7/16))\u2225xti\u22252 (B.17) \u2265 (1 + \u03b7\u03c3i/4)\u2225xti\u22252.\nThe inequality (B.16) uses the fact \u03b8 \u2264 2\u03b80 \u2264 1128\u03bar and \u2211n\nj \u0338=i \u2225xj\u22252 \u2264 2\u03c31r + 2n\u03b12 \u2264 4\u03c31r \u2264 \u03c3r 32\u03b8 . The inequality (B.17) uses the fact that \u03b7 \u2264 1 32n\u03c321 .\nProof of Eq.(B.4) If \u2225xti\u22252 \u2265 3\u03c3i/4, by the updating rule, we can get\n|\u2225xt+1i \u2225 2 2 \u2212 \u03c3i| \u2264 1\u2212 2\u03b7\u2225xti\u22252 + \u03b72(\u2225xti\u22252 \u2212 \u03c3i)\u2225xti\u22252 + \u03b72 n\u2211 j \u0338=i ((xti) \u22a4xtj) 2  |\u2225xti\u22252 \u2212 \u03c3i| + 2\u03b7\nn\u2211 j \u0338=i ((xti) \u22a4xtj) 2 + \u03b72  n\u2211 j,k \u0338=i ((xti) \u22a4xtj(x t j) \u22a4xtk(x t k) \u22a4xti)  \u2264 (1\u2212 \u03b7\u03c3i)|\u2225xti\u22252 \u2212 \u03c3i|+ 3 \u03b7\nn\u2211 j \u0338=i ((xti) \u22a4xtj) 2\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n(B.18)\nThe last inequality holds by Eq.(B.11) and\n2\u03b7\u2225xti\u22252 \u2212 \u03b72(\u2225xti\u22252 \u2212 \u03c3i)\u2225xti\u22252 \u2212 2\u03b72 n\u2211\nj \u0338=i\n((xti) \u22a4xtj) 2 (B.19)\n\u2265 3\u03b7 2 \u03c3i \u2212 \u03b72(2\u03c31) \u00b7 2\u03c31 \u2212 2\u03b72n\u03c321 (B.20) \u2265 \u03b7\u03c3i, (B.21)\nwhere (B.20) holds by \u2225xti\u22252 \u2265 3\u03c3i4 , \u2225x t i\u22252 \u2264 2\u03c31 for all i \u2208 [n]. The last inequality (B.21) holds by \u03b7 \u2264 C( 1n\u03c31\u03ba ) for small constant C. The first term of (B.18) represents the main converge part, and (a) represents the perturbation term. Now for the perturbation term (a), since \u03b1 \u2264 14\u03ban2 and\n\u03b8 \u2264 2\u03b80 \u2264 120r\u03ba2 = \u03c32i\n20r\u03c321 , we can get (a) = \u2211\nj \u0338=i,j\u2264r\n((xti) \u22a4xtj)\n2 + \u2211\nj \u0338=i,j>r\n((xti) \u22a4xtj) 2 (B.22)\n\u2264 (r\u03c31 + 2n\u03b12)\u03b8t \u00b7 2\u03c31 (B.23) \u2264 2r\u03c31 \u00b7 \u03b8t \u00b7 2\u03c31 (B.24) = 4r\u03c321 \u00b7 \u03b8t \u2264 \u03c32i /5, (B.25)\nwhere (B.23) holds by (B.4) and (B.5). (B.24) holds by \u03b1 = O( \u221a\nr\u03c31/n), and the last inequality (B.25) holds by \u03b8 is small, i.e. \u03b8t \u2264 2\u03b80 = O(1/r\u03ba2). Now it is easy to get that (xt+1i )\u22a4x t+1 i \u2264 2\u03c3i by\n|\u2225xt+1i \u2225 2 \u2212 \u03c3i| \u2264 (1\u2212 \u03b7\u03c3i)(\u2225xti\u22252 \u2212 \u03c3i) + 3\u03b7\u03c32i 5 \u2264 (1\u2212 \u03b7\u03c3i)\u03c3i + 3\u03b7\u03c32i 5 \u2264 \u03c3i. (B.26)\nHence, we complete the proof of Eq.(B.4).\nProof of Eq.(B.6) Now we consider the change of \u03b8. For i \u0338= j, denote\n\u03b8ij,t = ((xti) \u22a4xtj) 2\n\u2225xi\u22252\u2225xj\u22252\nNow we first calculate the (xt+1i ) \u22a4xt+1j by the updating rule:\n(xt+1i ) \u22a4xt+1j = ( 1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i) ) ( 1\u2212 \u03b7(\u2225xtj\u22252 \u2212 \u03c3j) ) (xti)\n\u22a4xtj\ufe38 \ufe37\ufe37 \ufe38 A \u2212\u03b7\u2225xtj\u22252(1\u2212 \u03b7(\u2225xtj\u22252 \u2212 \u03c3j))(xti)\u22a4xtj\ufe38 \ufe37\ufe37 \ufe38 B\n\u2212\u03b7\u2225xti\u22252(1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3j))(xti)\u22a4xtj\ufe38 \ufe37\ufe37 \ufe38 C\n+ \u03b72 \u2211\nk,l \u0338=i,j\n(xti) \u22a4xtk(x t k) \u22a4xtl(x t l) \u22a4xtj\ufe38 \ufe37\ufe37 \ufe38 D\n\u2212\u03b7(2\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i)\u2212 \u03b7(\u2225xtj\u22252 \u2212 \u03c3j)) n\u2211\nk \u0338=i,j\n(xti) \u22a4xtk(x t k) \u22a4xj\ufe38 \ufe37\ufe37 \ufe38 E\n+\u03b72 \u2211 k \u0338=i,j x\u22a4i x t j(x t j) \u22a4xtk(x t k) \u22a4xtj + \u03b7 2 \u2211 k \u0338=i,j (xti) \u22a4xtk(x t k) \u22a4xti(x t i)\n\u22a4xtj\ufe38 \ufe37\ufe37 \ufe38 F .\nNow we bound A, B, C, D, E and F respectively. First, by \u2225xti\u22252 \u2264 2\u03c31 for any i \u2208 [m], we have A \u2264 ( 1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i)\u2212 \u03b7(\u2225xtj\u22252 \u2212 \u03c3j) + \u03b72(\u2225xti\u22252 \u2212 \u03c3i) ( \u2225xtj\u22252 \u2212 \u03c3j) )) (xti) \u22a4xtj\n\u2264 ( 1\u2212 \u03b7 ( \u2225xti\u22252 + \u2225xtj\u22252 \u2212 \u03c3i \u2212 \u03c3j ) + \u03b72 \u00b7 4\u03c321 ) (xti) \u22a4xtj , (B.27)\nNow we bound term B. We have B + C = ( \u2212\u03b7(\u2225xti\u22252 + \u2225xtj\u22252) + \u03b72 ( (\u2225xtj\u22252 \u2212 \u03c3j)\u2225xtj\u22252 + (\u2225xti\u22252 \u2212 \u03c3i)\u2225xti\u22252 )) (xti) \u22a4xtj\n\u2264 ( \u2212\u03b7(\u2225xti\u22252 + \u2225xtj\u22252) + \u03b72 \u00b7 (8\u03c321) ) (xti) \u22a4xtj . (B.28)\nThen, for D, by \u03b8t \u2264 1, we have\nD = \u03b72  \u2211 k,l \u0338=i,j \u2225xtk\u22252\u2225xtl\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kl,t\u03b8lj,t/\u03b8ij,t  (xti)\u22a4xtj \u2264 ( \u03b72 \u00b7 n2 \u00b7 4\u03c321 \u00b7 \u03b8t/ \u221a \u03b8ij,t ) (xti) \u22a4xtj . (B.29)\nFor E, since we have E \u2264 2\u03b7 \u2211 k \u0338=i,j |(xti)\u22a4xtk(xtk)\u22a4xtj |+ 4\u03c31\u03b72 \u2211 k \u0338=i,j |(xti)\u22a4xtk(xtk)\u22a4xtj |\n\u2264 2\u03b7 \u2211 k \u0338=i,j \u2225xtk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t/\u03b8ij,t + 4\u03c31\u03b7 2 \u2211 k \u0338=i,j \u2225xtk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t/\u03b8ij,t  (xti)\u22a4xtj \u2264\n2\u03b7 \u2211 k \u0338=i,j \u2225xtk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t/\u03b8ij,t + 4n\u03c31\u03b7 2 \u00b7 (2\u03c31) \u00b7 \u03b8t/ \u221a \u03b8ij,t  (xti)\u22a4xtj . (B.30) Lastly, for F, since (xtj) \u22a4xtk(x t k)\n\u22a4xtj \u2264 \u2225xtj\u22252\u2225xtk\u22252 \u2264 4\u03c321 , we have F \u2264 \u03b728n\u03c321(xti)\u22a4xtj . (B.31)\nNow combining (B.27), (B.28), (B.29), (B.30) and (B.31), we can get (xt+1i ) \u22a4xt+1j (B.32)\n\u2264 1\u2212 \u03b7(2\u2225xi\u22252 + 2\u2225xj\u22252 \u2212 \u03c3i \u2212 \u03c3j) + 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t/\u03b8ij,t + 30n 2\u03c321\u03b7 2\u03b8t/ \u221a \u03b8ij,t)  (xti)\u22a4xtj . (B.33)\nOn the other hand, consider the change of \u2225xti\u22252. By Eq.(B.15),\n\u2225xt+1i \u2225 2 \u2265 (1\u2212 \u03b7(\u2225xti\u22252 \u2212 \u03c3i))2\u2225xti\u22252 \u2212 2\u03b7 n\u2211 j \u0338=i ((xti) \u22a4xtj) 2 + \u03b72(\u2225xti\u22252 \u2212 \u03c3i) n\u2211 j \u0338=i ((xti) \u22a4xtj) 2\n\u2265 (1\u2212 2\u03b7(\u2225xti\u2225 \u2212 \u03c3i)\u2212 2\u03b7 n\u2211\nj \u0338=i\n\u2225xtj\u22252\u03b8ij,t \u2212 4\u03b72n\u03b8t\u03c321)\u2225xti\u22252\n\u2265 (1\u2212 2\u03b7(\u2225xti\u2225 \u2212 \u03c3i)\u2212 2\u03b7 n\u2211\nk=1\n\u2225xtj\u22252\u03b8ij,t \u2212 4\u03b72n\u03b8t\u03c321)\u2225xti\u22252\nHence, the norm of xt+1i and x t+1 j can be lower bounded by\n\u2225xt+1i \u2225 2\u2225xt+1j \u2225 2 \u2265 ( 1\u2212 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i)\u2212 2\u03b7(\u2225xtj\u22252 \u2212 \u03c3j)\u2212 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252(\u03b8ik,t + \u03b8jk,t)\u2212 2\u03b7(\u2225xj\u22252 + \u2225xi\u22252)\u03b8ij,t\n\u2212 4\u03b72\u03b8tn2\u03c321 + \u2211 l=i,j 4\u03b72(\u2225xtl\u22252 \u2212 \u03c3l) n\u2211 k=1 \u2225xtk\u22252\u03b8ik,t + \u2211 l=i,j 2\u03b7(\u2225xtl\u22252 \u2212 \u03c3l)\u03b72n2\u03b8t\u03c321 ) \u2225xti\u22252\u2225xtj\u22252\n\u2265 ( 1\u2212 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i)\u2212 2\u03b7(\u2225xtj\u22252 \u2212 \u03c3j)\u2212 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252(\u03b8ik,t + \u03b8jk,t)\u2212 2\u03b7(\u2225xj\u22252 + \u2225xi\u22252)\u03b8ij,t\n\u2212 4\u03b72\u03b8tn2\u03c321 \u2212 2 \u00b7 4\u03b72 \u00b7 (2\u03c31)n \u00b7 (2\u03c31)\u03b8t \u2212 2 \u00b7 4\u03b7\u03c31 \u00b7 \u03b72n2\u03b8t\u03c321 ) \u2225xti\u22252\u2225xtj\u22252 (B.34)\n\u2265 ( 1\u2212 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i)\u2212 2\u03b7(\u2225xtj\u22252 \u2212 \u03c3j)\u2212 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252(\u03b8ik,t + \u03b8jk,t)\u2212 2\u03b7(\u2225xj\u22252 + \u2225xi\u22252)\u03b8ij,t\n\u2212 6\u03b72\u03b8tn2\u03c321 ) \u2225xti\u22252\u2225xtj\u22252, (B.35)\nwhere (B.35) holds by n > 8k \u2265 8 and 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i) \u2264 4\u03b7\u03c31 \u2264 1. Then, by (B.33) and (B.35), we have\n\u03b8ij,t+1 = \u03b8ij,t \u00b7 (xt+1i ) \u22a4xt+1j (xti) \u22a4xtj \u00b7 \u2225xt+1i \u22252\u2225x t+1 j \u22252 \u2225xti\u22252\u2225xtj\u22252\n\u2264 \u03b8ij,t \u00b7 ( 1\u2212A+B 1\u2212A\u2212 C ) (B.36)\nwhere A = 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i + \u2225xtj\u22252 \u2212 \u03c3i)) \u2264 4\u03b7\u03c31 (B.37)\nB = 2\u03b7\u2225xk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t/\u03b8ij,t + 30n 2\u03c321\u03b7 2\u03b8t/ \u221a \u03b8ij,t (B.38)\nand C = 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252(\u03b8ik,t + \u03b8jk,t) + 2\u03b7(\u2225xj\u22252 + \u2225xi\u22252)\u03b8ij,t + 6\u03b72n2\u03b8t\u03c321 (B.39)\n\u2264 ( 8\u03b7\u03c31 + 2\u03b7(2n\u03b1 2 + 2r\u03c31) + 6\u03b7 2n2\u03c321 ) \u03b8t, (B.40)\nwhere the last inequality uses the fact that\u2211 k \u0338=i,j \u2225xtk\u22252 \u2264 \u2211 k\u2264r \u2225xtk\u22252 + \u2211 k>r \u2225xtk\u22252 \u2264 2r\u03c31 + 2n\u03b12.\nHence, we choose \u03b7 \u2264 11000n\u03c31 to be sufficiently small so that max{A,C} \u2264 1/100, then by 1\u2212A+B 1\u2212A\u2212C \u2264 1 + 2B + 2C for max{A,C} \u2264 1/100,\n\u03b8ij,t \u00b7 ( 1\u2212A+B 1\u2212A\u2212 C ) \u2264 \u03b8ij,t(1 + 2B + 2C)\n\u2264 \u03b8ij,t + 4\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t\u03b8ij,t + 60n 2\u03c321\u03b7 2\u03b8t \u221a \u03b8ij,t\n+ \u03b82t ( 8\u03b7\u03c31 + 2\u03b7(2n\u03b1 2 + 2r\u03c31) + 6\u03b7 2n2\u03c321 ) \u2264 \u03b8ij,t + 4\u03b7(2r\u03c31 + 2n\u03b12)\u03b83/2t + 60n2\u03c321\u03b72\u03b8 3/2 t\n+ \u03b82t ( 8\u03b7\u03c31 + 2\u03b7(2n\u03b1 2 + 2r\u03c31) + 6\u03b7 2n2\u03c321 ) \u2264 \u03b8ij,t + 6\u03b7(2r\u03c31 + 2n\u03b12)\u03b83/2t + 60n2\u03c321\u03b72\u03b8 3/2 t + 8\u03b7\u03c31\u03b8 2 t + 6n 2\u03b72\u03c321\u03b8 2 t )\n\u2264 \u03b8ij,t + 98\u03b7 \u00b7 (r\u03c31\u03b83/2t ) The last inequality holds by \u03b1 \u2264 \u221a\u03c31/ \u221a n, and n2\u03c31\u03b72 \u2264 \u03b7 because \u03b7 \u2264 1n2\u03c31 .\nHence,\n\u03b8t+1 \u2264 \u03b8t + 98\u03b7(r\u03c31)\u03b83/2t (B.41)\nThe Phase 1 terminates when \u2225xT1i \u22252 \u2265 3\u03c3i 4 . Since \u2225x 0 i \u22252 \u2265 \u03b12/2 and\n\u2225xt+1i \u2225 2 \u2265 (1 + \u03b7\u03c3i/4)\u2225xti\u22252, (B.42)\nthere is a constant C3 such that T1 \u2264 C1(log( \u221a \u03c31/\u03b1)/\u03b7\u03c3i). Hence, before round T1,\n\u03b8T1 \u2264 \u03b80 + 98\u03b7T1 \u00b7 r\u03c31 \u00b7 (2\u03b80)3/2 \u2264 \u03b80 + 98C1r\u03ba(2\u03b80)3/2 log( \u221a \u03c31/\u03b1) \u2264 2\u03b80.\nThis is because \u03b80 = O((log2(r \u221a \u03c31/\u03b1)(r\u03ba)) 2)\nby Lemma B.1 and choosing k \u2265 c2((r\u03ba)2 log(r \u221a \u03c31/\u03b1)) 4 for large enough c2"
        },
        {
            "heading": "B.3 PHASE 2",
            "text": "Denote \u03b8Ut = maxmin{i,j}\u2264r \u03b8ij,t. In this phase, we prove that \u03b8 U t is linear convergence, and the convergence rate of the loss is at least \u2126(1/T 2). To be more specific, we will show that \u03b8Ut+1 \u2264 \u03b8Ut \u00b7 (1\u2212 \u03b7 \u00b7 \u03c3r/4) \u2264 \u03b8Ut (B.43) \u03b8Ut+1\u2211\ni>r \u2225x t+1 i \u22252\n\u2264 \u03b8 U t\u2211 i>r \u2225xti\u22252 \u00b7 ( 1\u2212 \u03b7\u03c3r 8 ) (B.44)\n|\u2225xti\u22252 \u2212 \u03c3i| \u2264 1\n4 \u03c3i (i \u2264 r) (B.45)\n\u2225xti\u22252 \u2264 2\u03b12 (i > r) (B.46)\nFirst, the condition (B.45) and (B.46) hold at round T1. Then, if it holds before round t, consider round t + 1, similar to Phase 1, condition (B.46) also holds. Now we prove Eq.(B.43), (B.44) and (B.45) one by one.\nProof of Eq.(B.45) For i \u2264 r, if \u2225xti\u22252 \u2265 3\u03c3i/4, by Eq.(B.18)\n|\u2225xt+1i \u2225 2 2 \u2212 \u03c3i| \u2264 (1\u2212 \u03b7\u03c3i)|\u2225xti\u22252 \u2212 \u03c3i|+ 3\u03b7 n\u2211 j \u0338=i ((xti) \u22a4xtj) 2 (B.47)\nHence, by (B.45) and (B.46), we can get n\u2211\nj \u0338=i\n((xti) \u22a4xtj)\n2 \u2264 \u2211\nj \u0338=i,j\u2264r\n((xti) \u22a4xtj)\n2 + \u2211\nj \u0338=i,j>r\n((xti) \u22a4xtj) 2\n\u2264 (r\u03c31 + 4n\u03c31\u03b12)\u03b8Ut \u2264 2r\u03c31\u03b8Ut (B.48) \u2264 2r\u03c31\u03b8UT1 (B.49) \u2264 2r\u03c31 \u00b7 2\u03b80 \u2264 \u03c3i/20. (B.50)\nThe inequality (B.48) is because \u03b1 \u2264 14n\u03c31 , the inequality (B.49) holds by induction hypothesis (B.43), and the last inequality (B.50) is because of (B.6) and \u03b80 \u2264 180r\u03ba .\nHence, if |\u2225xti\u22252 \u2212 \u03c3i| \u2264 \u03c3i/4, by combining (B.47) and (B.50), we have |\u2225xt+1i \u2225\n2 \u2212 \u03c3i| \u2264 (1\u2212 \u03b7\u03c3i)|\u2225xti\u2225 \u2212 \u03c3i|+ 3\u03b7\u03c3i/20 \u2264 \u03c3i/4. Now it is easy to get that |\u2225xti\u22252\u2212\u03c3i| \u2264 0.25\u03c3i for t \u2265 T1 by induction because of |\u2225x T1 i \u22252\u2212\u03c3i| \u2264 0.25\u03c3i. Thus, we complete the proof of Eq.(B.45).\nProof of Eq.(B.43) First, we consider i \u2264 r, j \u0338= i \u2208 [n] and \u03b8ij,t > \u03b8Ut /2, since (B.4) and (B.5) still holds with (B.45) and (B.46), similarly, we can still have equation (B.36), i.e.\n\u03b8ij,t+1 = \u03b8ij,t \u00b7 ( 1\u2212A\u2212B 1\u2212A\u2212 C ) .\nwhere A = 2\u03b7(\u2225xti\u22252 \u2212 \u03c3i) + 2\u03b7(\u2225xtj\u22252 \u2212 \u03c3j) \u2265 \u22122\u03b7(2 \u00b7 (\u03c3i/4)) \u2265 \u22121/100.\nB = 2\u03b7(\u2225xti\u22252 + \u2225xtj\u22252)\u2212 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252 \u00b7 \u221a \u03b8ik,t\u03b8kj,t/\u03b8ij,t \u2212 30n2\u03b72\u03c321 \u221a \u03b8Ut / \u221a \u03b8ij,t\n\u2265 2\u03b7(\u2225xti\u22252 + \u2225xtj\u22252)\u2212 4\u03b7 \u2211 k\u2264r \u2225xk\u22252 \u221a \u03b8U \u2212 4n\u03b7\u03b12 \u2212 40n2\u03b72\u03c321 (B.51)\n\u2265 2\u03b7 \u00b7 3\u03c3i 4\n\u2212 8\u03b7r\u03c31 \u221a 2\u03b8T0 \u2212 4n\u03b7\u03b12 \u2212 40n2\u03b72\u03c321 (B.52)\n\u2265 \u03b7 \u00b7 \u03c3r (B.53) The inequality Eq.(B.51) holds by \u03b8ij,t > \u03b8Ut /2, the inequality (B.52) holds by (B.43), and (B.53) holds by\n\u03b8T0 = O ( 1\nr2\u03ba2\n) , \u03b1 = O( \u221a \u03c3r/n), \u03b7 = O(1/n2\u03ba\u03c31). (B.54)\nThe term C is defined and can be bounded by C = 2\u03b7 \u2211 k \u0338=i,j \u2225xk\u22252(\u03b8ik,t + \u03b8jk,t) + 2\u03b7(\u2225xi\u22252 + \u2225xj\u22252)\u03b8ij,t + 6\u03b72\u03b8tn2\u03c321\n\u2264 4\u03b7 \u2211 k\u2264r \u2225xk\u22252\u03b8Ut + 4\u03b7n\u03b12\u03b8t + 6\u03b72\u03b8tn2\u03c321\n\u2264 8r\u03b7\u03c31\u03b8Ut + 4\u03b7n\u03b12 + 6\u03b72n2\u03c321 \u2264 8r\u03b7\u03c31\u03b8T0 + 4\u03b7n\u03b12 + 6\u03b72n2\u03c321 (B.55) \u2264 \u03b7 \u00b7 \u03c3r/2. (B.56)\nThe inequality (B.55) holds by (B.43), and the inequality (B.56) holds by (B.54).\nThen, for i \u2264 r, j \u0338= i \u2208 [n] and \u03b8ij,t > \u03b8Ut /2, we can get \u03b8ij,t+1 \u2264 \u03b8ij,t \u00b7 ( 1\u2212A\u2212B 1\u2212A\u2212 C ) \u2264 \u03b8ij,t \u00b7 ( 2\u2212 \u03b7 \u00b7 \u03c3r 2\u2212 \u03b7 \u00b7 \u03c3r/2\n) \u2264 \u03b8ij,t \u00b7 ( 1\u2212 \u03b7 \u00b7 \u03c3r/2 1\u2212 \u03b7 \u00b7 \u03c3r/4 ) \u2264 \u03b8ij,t \u00b7 (1\u2212 \u03b7 \u00b7 \u03c3r/4) (B.57)\nFor i \u2264 r, j \u2208 [n] and \u03b8ij,t \u2264 \u03b8Ut /2, we have\nB \u2265 \u22122\u03b7 \u2211 k\u2264r \u2225xk\u22252\u03b8Ut / \u221a \u03b8ij,t \u2212 2\u03b7 \u2211 k>r \u2225xk\u22252 \u221a \u03b8Ut / \u221a \u03b8ij,t \u2212 30n2\u03b72\u03c321 \u221a \u03b8Ut / \u221a \u03b8ij,t (B.58)\n\u2265 \u22124\u03b7r\u03c31\u03b8Ut / \u221a \u03b8ij,t \u2212 (4n\u03b7\u03b12 + 30n2\u03b72\u03c321) \u221a \u03b8Ut / \u221a \u03b8ij,t (B.59)\n\u03b8ij,t+1 \u2264 \u03b8ij,t \u00b7 ( 1\u2212A\u2212B 1\u2212A\u2212 C ) \u2264 \u03b8ij,t \u00b7 (1\u2212 2B + 2C)\n\u2264 \u03b8ij,t + 8\u03b7r\u03c31\u03b8Ut \u221a \u03b8ij,t + (4n\u03b7\u03b1 2 + 30n2\u03b72\u03c321) \u221a \u03b8Ut \u03b8ij,t + 2C\u03b8ij,t\n\u2264 \u03b8 U t\n2 + 8\u03b7r\u03c31\u03b8\nU t + (4n\u03b7\u03b1 2 + 30n2\u03b72\u03c321)\u03b8 U t + \u03b7\u03c3r\u03b8 U t\n\u2264 3\u03b8 U t\n4 . (B.60)\nThe last inequality is because 8\u03b7r\u03c31 + 4n\u03b7\u03b12 + 30n2\u03b72\u03c321 + \u03b7\u03c3r \u2264 14 by \u03b7 \u2264 O(1/n\u03c31) and \u03b7 \u2264 O(1/n\u03b12). Hence, by Eq.(B.57) and (B.60) and the fact that \u03b7\u03c3r/4 \u2264 1/4,\n\u03b8Ut+1 \u2264 \u03b8Ut \u00b7max {3 4 , 1\u2212 \u03b7 \u00b7 \u03c3r/4 } = (1\u2212 \u03b7 \u00b7 \u03c3r/4)\u03b8Ut . (B.61)\nThus, we complete the proof of Eq.(B.43)\nProof of Eq.(B.44) Also, for i > r, denote \u03b8ii,t = 1, then\n\u2225xt+1i \u2225 2 = \u2225xti\u22252 \u2212 2\u03b7 n\u2211 j=1 ((xti) \u22a4xtj) 2 + \u03b72  n\u2211 j,k=1 (xti) \u22a4xtj(x t j) \u22a4 2\n\u2265 \u2225xti\u22252(1\u2212 2\u03b7 n\u2211\nj=1\n\u2225xtj\u22252\u03b8ij,t) (B.62)\n\u2265 \u2225xi\u22252(1\u2212 2\u03b7r\u03c31\u03b8Ut \u2212 2\u03b7n\u03b12) \u2265 \u2225xi\u22252(1\u2212 \u03b7 \u00b7 \u03c3r/8)\nThe last inequality holds because\n\u03b8Ut \u2264 \u03b80 \u2264 O(1/r\u03ba) (B.63) \u03b1 \u2264 \u221a \u03c3r/n (B.64)\nHence, the term \u03b8U/\u2225xi\u22252 for i > r is also linear convergence by\n\u03b8Ut+1\u2211 i>r \u2225x t+1 i \u22252 \u2264 \u03b8 U t\u2211 i>r \u2225xti\u22252 \u00b7 1\u2212 \u03b7 \u00b7 \u03c3r/4 1\u2212 \u03b7 \u00b7 \u03c3r/8 \u2264 \u03b8 U t\u2211 i>r \u2225xti\u22252 \u00b7 ( 1\u2212 \u03b7\u03c3r 8 ) .\nHence, we complete the proof of Eq.(B.44)."
        },
        {
            "heading": "B.4 PHASE 3: LOWER BOUND OF CONVERGENCE RATE",
            "text": "Now by (B.44), there are constants c6 and c7 such that, if we denote T2 = T1 + c7(log( \u221a r\u03c31/\u03b1)/\u03b7\u03c3r) = c6(log( \u221a r\u03c31/\u03b1)/\u03b7\u03c3r), then we will have\n\u03b8UT2 < \u2211 i>r \u2225xT2i \u2225 2/r\u03c31 (B.65)\nbecause of the fact that \u03b8UT1/ \u2211 i>r \u2225x T1 i \u22252 \u2264 4n\u00b7\u03b12 \u2264 4/\u03b1\n2. Now after round T2, consider i > r, we can have\n\u2225xt+1i \u2225 2 \u2265 \u2225xti\u22252(1\u2212 2\u03b7 n\u2211 j=1 \u2225xtj\u22252\u03b8ij,t)\n\u2265 \u2225xti\u22252(1\u2212 2\u03b7r\u03c31\u03b8Ut \u2212 2\u03b7 \u2211 j>r \u2225xtj\u22252)\nHence, by Eq.(B.62), we have\n\u2211 j>r \u2225xt+1j \u2225 2 \u2265 \u2211 j>r \u2225xtj\u22252 1\u2212 2\u03b7r\u03c31\u03b8Ut \u2212 2\u03b7\u2211 j>r \u2225xtj\u22252  (B.66)\n\u2265 \u2211 j>r \u2225xtj\u22252 1\u2212 4\u03b7\u2211 j>r \u2225xtj\u22252  , (B.67)\nwhere the second inequality is derived from (B.65). Hence, we can show that \u2211\nj>r \u2225xtj\u22252 = \u2126(1/T 2). In fact, suppose at round T2, we denote AT2 =\u2211 j>r \u2225x T2 j \u22252, then by\n\u2225xt+1i \u2225 2 \u2265 \u2225xti\u22252(1\u2212 2\u03b7 n\u2211 k=1 \u2225xtk\u22252\u03b8ik,t))\n\u2265 \u2225xti\u22252(1\u2212 2\u03b7r\u03c31\u03b8U \u2212 2\u03b7n\u03b12) we can get\n\u2225xT2i \u2225 2 \u2265 \u2225xT1i \u2225 2(1\u2212 2\u03b7r\u03c31\u03b8UT1 \u2212 2\u03b7n\u03b1 2)T2\u2212T1\n\u2265 \u2225xT1i \u2225 2 \u00b7 (1\u2212 c5(log(r \u221a \u03c31/\u03b1)/\u03b7\u03c3r) \u00b7 ( 2\u03b7r\u03c31\u03b8T1 + 2\u03b7n\u03b1 2 ) )\n\u2265 \u2225xT1i \u2225 2 \u00b7 (1\u2212 c5 log(r \u221a \u03c31/\u03b1) \u00b7 (4r\u03ba\u03b80 + 2n\u03b12/\u03c3r))\n\u2265 1 2 \u2225xT1i \u2225 2 (B.68) \u2265 \u03b1 2\n8\nwhere the inequality (B.68) is because \u03b80 \u2264 O (\n1\nr\u03ba log(r \u221a \u03c31/\u03b1)\n) (B.69)\n\u03b12 \u2264 O ( \u221a\n\u03c3r n log(r \u221a \u03c31/\u03b1)\n) . (B.70)\nHence,\nT2AT2 \u2265 T2 \u00b7 (n\u2212 r) \u03b12\n8 \u2265 c7(log(\n\u221a r\u03c31/\u03b1)/\u03b7\u03c3r) \u00b7 \u03b12\n8 . (B.71)\nby n > r. Define AT2+i+1 = AT2+i(1\u2212 4\u03b7AT2+i), by Eq.(B.67), we have AT2+i \u2264 AT2 = \u2211 i>r \u2225xT2i \u2225 2 \u2264 2n\u03b12. (B.72)\nOn the other hand, if \u03b7(T2 + i)AT2+i \u2264 1/8, and then \u03b7(T2 + i+ 1)AT2+i+1 = \u03b7(T2 + i+ 1)AT2+i(1\u2212 4\u03b7AT2+i)\n= \u03b7(T2 + i)AT2+i \u2212 (T2 + i)4\u03b72A2T2+i + \u03b7AT2+i(1\u2212 4\u03b7AT2+i) \u2265 \u03b7(T2 + i)AT2+i \u2212 (T2 + i)4\u03b72A2T2+i + \u03b7AT2+i/2 (B.73) \u2265 \u03b7(T2 + i)AT2+i \u2212 \u03b7AT2+i/2 + \u03b7AT2+i/2 \u2265 \u03b7(T2 + i)AT2+i,\nwhere (B.73) holds by \u03b7AT2+i \u2264 2n\u03b7\u03b12 \u2264 1/8. If \u03b7(T2 + i)AT2+i > 1/8, since \u03b7AT2+i \u2264 1/8, we have \u03b7AT2 \u2264 2n\u03b7\u03b12 \u2264 1/8.\n\u03b7(T2 + i+ 1)AT2+i+1 \u2265 \u03b7(T2 + i)AT2+i(1\u2212 4\u03b7AT2+i) + \u03b7AT2+i(1\u2212 4\u03b7AT2+i)\n\u2265 1 8 \u00b7 1 2 + \u03b7AT2+i \u00b7 1 2 \u2265 1 16 .\nThus, by the two inequalities above, at round t \u2265 T2, we can have \u03b7tAt \u2265 min{\u03b7T2AT2 , 1/16}.\nNow by (B.71),\n\u03b7T2AT2 \u2265 c7 log(\n\u221a r\u03c31/\u03b1)\u03b1 2\n8\u03c3r , (B.74)\nthen for any t \u2265 T2, we have \u03b7tAt \u2265 min { c7 log( \u221a r\u03c31/\u03b1)\u03b1 2\n8\u03c3r , 1/16\n} (B.75)\nNow by choosing \u03b1 = O\u0303(\u221a\u03c3r) so that c7 log( \u221a r\u03c31/\u03b1)\u03b1 2\n8\u03c3r \u2264 1/16, we can derive\nAt \u2265 c7 log(\n\u221a r\u03c31/\u03b1)\u03b1 2 8\u03c3r\u03b7t . (B.76)\nSince for j > r, (XtX\u22a4t \u2212 \u03a3)jj = \u2225xtj\u22252, we have \u2225XtX\u22a4t \u2212 \u03a3\u22252 \u2265 \u2211 j>r \u2225xtj\u22254 \u2265 A2t/n and\n\u2225XtX\u22a4t \u2212 \u03a3\u22252 \u2265 A2t/n \u2265 ( c7 log( \u221a r\u03c31/\u03b1)\u03b1 2\n8\u03c3r\u03b7 \u221a nt\n)2 ."
        },
        {
            "heading": "C PROOF OF THEOREM 4.1",
            "text": "Denote the matrix of the first r row of F,G as U, V respectively, and the matrix of the last n\u2212 r row of F,G as J,K respectively. Hence, U, V \u2208 Rr\u00d7k, J,K \u2208 R(n\u2212r)\u00d7k. In this case, the difference FtG \u22a4 t \u2212 \u03a3 can be written in a block form as\nFtG \u22a4 t \u2212 \u03a3 =\n( UtV\n\u22a4 t \u2212 \u03a3r JtV \u22a4t UtK \u22a4 t JtK \u22a4 t\n) , (C.1)\nwhere \u03a3r = I \u2208 Rr\u00d7r. Hence, the loss can be bounded by\n\u2225JtK\u22a4t \u2225 \u2264 \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 \u2225UtV \u22a4t \u2212 \u03a3r\u2225+ \u2225JtV \u22a4t \u2225+ \u2225UtK\u22a4t \u2225+ \u2225JtK\u22a4t \u2225. (C.2)\nThe updating rule for (U, V, J,K) under gradient descent in (4.2) can be rewritten explicitly as\nUt+1 = Ut + \u03b7\u03a3rVt \u2212 \u03b7Ut(V \u22a4t Vt +K\u22a4t Kt) Vt+1 = Vt + \u03b7\u03a3rUt \u2212 \u03b7Vt(U\u22a4t Ut + J\u22a4t Jt) Jt+1 = Jt \u2212 \u03b7Jt(V \u22a4t Vt +K\u22a4t Kt) Kt+1 = Kt \u2212 \u03b7Kt(U\u22a4t Ut + J\u22a4t Jt).\nNote that with our particular initialization, we have the following equality for all t:\nUtK \u22a4 t = 0, JtV \u22a4 t = 0, and Ut = Vt. (C.3)\nIndeed, the conditions (C.3) are satisfied for t = 0. For t+ 1, we have\nUt+1 = Ut + \u03b7(\u03a3r \u2212 UtV \u22a4t )Vt = Vt + \u03b7(\u03a3r \u2212 UtV \u22a4t )Ut = Vt+1, Kt+1 = Kt \u2212 \u03b7KtJ\u22a4t Jt Ut+1K \u22a4 t+1 = UtK \u22a4 t + \u03b7(\u03a3r \u2212 UtV \u22a4t )UtK\u22a4t \u2212 \u03b7VtJ\u22a4t JtK\u22a4t \u2212 \u03b72(\u03a3r \u2212 UtV \u22a4t )UtJ\u22a4t JtK\u22a4t = 0\nThe last equality arises from the fact that UtK\u22a4t = 0, JtV \u22a4 t = 0 and Ut = Vt. Similarly, we can get Jt+1V \u22a4 t+1 = 0. Hence, we can rewrite the updating rule of Jt and Kt as\nJt+1 = Jt \u2212 \u03b7JtK\u22a4t Kt (C.4) Kt+1 = Kt \u2212 \u03b7KtJ\u22a4t Jt. (C.5)\nLet us now argue why the convergence rate can not be faster than \u2126((1 \u2212 6\u03b7\u03b12)t). Denote A \u2208 R(n\u2212r)\u00d7k as the matrix that (A)1k = 1 and other elements are all zero. We have that J0 = \u03b1A and K0 = (\u03b1/3) \u00b7A. Combining this with Eq.(C.4) and Eq.(C.5), we have Jt = atA,Kt = btA, where\na0 = \u03b1, b0 = \u03b1/3, (C.6a)\nat+1 = at \u2212 \u03b7atb2t , (C.6b) bt+1 = bt \u2212 \u03b7a2t bt. (C.6c)\nIt is immediate that 0 \u2264 at+1 \u2264 at, 0 \u2264 bt+1 \u2264 bt , max{at, bt} \u2264 \u03b1 because of \u03b7b2t \u2264 \u03b7b20 = \u03b7\u03b12 \u2264 1 and similarly \u03b7a2t \u2264 1. Now by \u03b7\u03b12 \u2264 1/4,\n\u2225Jt+1K\u22a4t+1\u2225 = at+1bt+1 = (1\u2212 \u03b7a2t )(1\u2212 \u03b7b2t )atbt \u2265 (1\u2212 2\u03b7\u03b12)2atbt \u2265 (1\u2212 4\u03b7\u03b12)atbt. (C.7)\nBy Eq.(C.2) that \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2265 \u2225JtK\u22a4t \u2225, the convergence rate of \u2225FtG\u22a4t \u2212 \u03a3\u2225 can not be faster than a0b0(1\u2212 4\u03b7\u03b12)t \u2265 \u03b1 2 3 (1\u2212 4\u03b7\u03b1 2)t.\nNext, we show why the convergence rate is exactly \u0398((1\u2212\u0398(\u03b7\u03b12))t) in this toy case. By Eq.(C.3), the loss \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 \u2225UtU\u22a4t \u2212 \u03a3r\u2225 + \u2225JtK\u22a4t \u2225. First, we consider the norm \u2225UtU\u22a4t \u2212 \u03a3r\u2225. Since in this toy case, \u03a3r = Ir and Ut = Vt for all t, the updating rule of Ut can be written as\nUt+1 = Ut \u2212 \u03b7(UtU\u22a4t \u2212 I)Ut (C.8)\nNote that U0 = (\u03b1Ir, 0) \u2208 Rr\u00d7k. By induction, we can show that Ut = (\u03b1tIr, 0) and \u03b1t+1 = \u03b1t \u2212 \u03b7(\u03b12t \u2212 1)\u03b1t for all t \u2265 0. If \u03b1t \u2264 1/2, we have\n\u03b1t+1 = \u03b1t(1 + \u03b7 \u2212 \u03b7\u03b12t ) \u2265 \u03b1t(1 + \u03b7/2). Then, there exists a constant c1 and T1 = c1(log(1/\u03b1)/\u03b7) such that after T1 rounds, we can get \u03b1t \u2265 1/2. By the fact that \u03b1t+1 = \u03b1t(1+ \u03b7(1\u2212\u03b12t )) \u2264 max{\u03b1t, 2} when \u03b7 < 1, it is easy to show \u03b1t \u2264 2 for all t \u2265 0. Thus, when \u03b7 < 1/6, we can get 1\u2212 \u03b7(\u03b1t + 1)\u03b1t > 0 and then\n|\u03b1t+1 \u2212 1| = |(\u03b1t \u2212 1)\u2212 \u03b7(\u03b1t \u2212 1)(\u03b1t + 1)\u03b1t| = |\u03b1t \u2212 1|(1\u2212 \u03b7(\u03b1t + 1)\u03b1t) \u2264 |\u03b1t \u2212 1|(1\u2212 \u03b7/2).\nwe know that \u2225UtU\u22a4t \u2212 \u03a3r\u2225 = \u03b12t \u2212 1 converges at a linear rate\n\u2225UtU\u22a4t \u2212 \u03a3\u2225 \u2264 (1\u2212 \u03b7/2)t\u2212T1 (a) \u2264 (1\u2212 \u03b7\u03b12/4)(t\u2212T1)/2, (C.9)\nwhere (a) uses the fact that\n1\u2212 \u03b7\u03b12/4 \u2265 1\u2212 \u03b7 \u2265 (1\u2212 \u03b7/2)2 (C.10)\nHence, we only need to show that \u2225JtK\u22a4t \u2225 converges at a relatively slower speed O((1\u2212\u0398(\u03b7\u03b12))t). To do this, we prove the following statements by induction.\n\u03b1 \u2265 at \u2265 \u03b1/2, b2t+1 \u2264 b2t (1\u2212 \u03b7\u03b12/4) (C.11)\nUsing b0 = \u03b1/3, we see the above implies that \u2225JtK\u22a4t \u2225 = atbt \u2264 O((1\u2212\u0398(\u03b7\u03b12))t). Let us prove (C.11) via induction. It is trivial to show it holds at t = 0 and the upper bound of at by (C.6). Suppose (C.11) holds for t\u2032 \u2264 t, then at round t+ 1, we have\nb2t+1 = b 2 t (1\u2212 \u03b7a2t )2 \u2264 b2t (1\u2212 \u03b7\u03b12/4)2 \u2264 b2t (1\u2212 \u03b7\u03b12/4). (C.12)\nUsing at+1 = at(1\u2212 \u03b7b2t ), we have\nat+1 = a0 t\u220f i=1 (1\u2212 \u03b7b2i ) (a) \u2265 a0\n( 1\u2212 \u03b7\nt\u2211 i=1 b2i\n) (b) \u2265 \u03b1 \u00b7 ( 1\u2212 \u03b7 \u00b7 \u03b1 2\n9 \u00b7 4 \u03b7\u03b12\n) \u2265 \u03b1/2. (C.13)\nwhere the step (a) holds by recursively using (1\u2212 a)(1\u2212 b) \u2265 (1\u2212 (a+ b)) for a, b \u2208 (0, 1), and the step (b) is due to b2i \u2264 b20 \u00b7 (1\u2212 \u03b7\u03b12/4)t \u2264 \u03b1 2 9 \u00b7 (1\u2212 \u03b7\u03b12 4 ) t and the sum formula for geometric series. Thus, the induction is complete, and\n\u2225JtK\u22a4t \u2225 = atbt \u2264 (\u03b12/3) \u00b7 (1\u2212 \u03b7\u03b12/4)t/2 \u2264 (1\u2212 \u03b7\u03b12/4)t/2 \u2264 (1\u2212 \u03b7\u03b12/4)(t\u2212T1)/2. (C.14) Combining (C.9) and (C.14), with \u2225A\u22252 \u2264 \u2225A\u2225F \u2264 rank(A) \u00b7 \u2225A\u22252, we complete the proof."
        },
        {
            "heading": "D PROOF OF THEOREM 4.2",
            "text": "We prove Theorem 4.2 in this section. We start with some preliminaries."
        },
        {
            "heading": "D.1 PRELIMINARIES",
            "text": "In the following, we denote \u03b42k+1 = \u221a 2k + 1\u03b4. Also denote the matrix of the first r row of F,G as U, V respectively, and the matrix of the last n \u2212 r row of F,G as J,K respectively. Hence, U, V \u2208 Rr\u00d7k, J,K \u2208 R(n\u2212r)\u00d7k. We denote the corresponding iterates as Ut, Vt, Jt, and Kt. Also, define E(X) = A\u2217A(X) \u2212X . We also denote \u0393(X) = A\u2217A(X). By Lemma G.2, we can show that \u2225E(X)\u2225 \u2264 \u03b42k+1 \u00b7 \u2225X\u2225 for matrix X with rank less than 2k by Lemma G.2. Decompose the error matrix E(X) into four submatrices by\nE(X) = ( E1(X) E2(X) E3(X) E4(X) ) ,\nwhere E1(X) \u2208 Rr\u00d7r, E2(X) \u2208 Rr\u00d7(n\u2212r), E3(X) \u2208 R(n\u2212r)\u00d7r, E4(X) \u2208 R(n\u2212r)\u00d7(n\u2212r). Then the updating rule can be rewritten in this form:\nUt+1 = Ut + \u03b7\u03a3Vt \u2212 \u03b7Ut(V \u22a4t Vt +K\u22a4t Kt) + \u03b7E1(FtG\u22a4t \u2212 \u03a3)Vt + \u03b7E2(FtG\u22a4t \u2212 \u03a3)Kt (D.1) Vt+1 = Vt + \u03b7\u03a3Ut \u2212 \u03b7Vt(U\u22a4t Ut + J\u22a4t Jt) + \u03b7E\u22a41 (FtG\u22a4t \u2212 \u03a3)Ut + \u03b7E\u22a43 (FtG\u22a4t \u2212 \u03a3)Jt (D.2) Jt+1 = Jt \u2212 \u03b7Jt(V \u22a4t Vt +K\u22a4t Kt) + \u03b7E3(FtG\u22a4t \u2212 \u03a3)Vt + \u03b7E4(FtG\u22a4t \u2212 \u03a3)Kt (D.3) Kt+1 = Kt \u2212 \u03b7Kt(U\u22a4t Ut + J\u22a4t Jt) + \u03b7E\u22a42 (FtG\u22a4t \u2212 \u03a3)Ut + \u03b7E\u22a44 (FtG\u22a4t \u2212 \u03a3)Jt. (D.4)\nSince the submatrices\u2019 operator norm is less than the operator norm of the whole matrix, the matrices Ei(FtG \u22a4 t \u2212 \u03a3), i = 1, . . . , 4 satisfy that\n\u2225Ei(FtG\u22a4t \u2212 \u03a3)\u2225 \u2264 \u2225E(FtG\u22a4t \u2212 \u03a3)\u2225 \u2264 \u03b42k+1\u2225FtG\u22a4t \u2212 \u03a3\u2225, i = 1, . . . , 4.\nImbalance term An important property in analyzing the asymmetric matrix sensing problem is that F\u22a4F \u2212 G\u22a4G = U\u22a4U + J\u22a4J \u2212 V \u22a4V \u2212 K\u22a4K remains almost unchanged when step size \u03b7 is sufficiently small, i.e., the balance between two factors F and G are does not change much throughout the process. To be more specific, by\nFt+1 = Ft \u2212 \u03b7(FtG\u22a4t \u2212 \u03a3)Gt \u2212 E(FtG\u22a4t \u2212 \u03a3)Gt Gt+1 = Gt \u2212 \u03b7(FtG\u22a4t \u2212 \u03a3)\u22a4Ft \u2212 (E(FtG\u22a4t \u2212 \u03a3))\u22a4Ft\nwe have\u2225\u2225(F\u22a4t+1Ft+1 \u2212G\u22a4t+1Gt+1)\u2212 (F\u22a4t Ft \u2212G\u22a4t Gt)\u2225\u2225 \u2264 2\u03b72 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u22252 \u00b7max{\u2225Ft\u2225, \u2225Gt\u2225}2. (D.5)\nIn fact, by the updating rule, we have\nF\u22a4t+1Ft+1 \u2212G\u22a4t+1Gt+1 = F\u22a4t Ft \u2212G\u22a4t Gt + \u03b72 ( G\u22a4t (FtG \u22a4 t \u2212 \u03a3)\u22a4(FtG\u22a4t \u2212 \u03a3)Gt \u2212 F\u22a4t (FtG\u22a4t \u2212 \u03a3)(FtG\u22a4t \u2212 \u03a3)\u22a4Ft ) ,\nso that\n\u2225F\u22a4t+1Ft+1 \u2212G\u22a4t+1Gt+1 \u2212 (F\u22a4t Ft \u2212G\u22a4t Gt)\u2225 \u22642\u03b72\u2225Ft\u22252\u2225Gt\u22252\u2225FtG\u22a4t \u2212 \u03a3\u22252 \u22642\u03b72 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u00b7max{\u2225Ft\u22252, \u2225Gt\u22252}\nThus, we will prove that, during the proof process, the following inequality holds with high probability during all t \u2265 0:\n2\u03b12I \u2265 U\u22a4t Ut + J\u22a4t Jt \u2212 V \u22a4t Vt \u2212K\u22a4t Kt \u2265 \u03b12\n8 I. (D.6)\nNext, we give the outline of our proof."
        },
        {
            "heading": "D.2 PROOF OUTLINE",
            "text": "In this subsection, we give our proof outline.\n\u2022 Recall \u2206t = F\u22a4t Ft \u2212 G\u22a4t Gt = U\u22a4t Ut + J\u22a4t Jt \u2212 V \u22a4t Vt \u2212 K\u22a4t Kt. In Section D.3, we show that with high probability, \u22060 has the scale \u03b1, i.e., C\u03b12I \u2265 \u22060 \u2265 c\u03b12I , where C > c are two constants. Then, we apply the converge results in Soltanolkotabi et al. (2023) to argue that the algorithm first converges to a local point. By Soltanolkotabi et al. (2023), this converge phase takes at most T0 = O((1/\u03b7\u03c3r\u03c5) log( \u221a \u03c31/n\u03b1)) rounds.\n\u2022 Then, in Section D.4 (Phase 1), we mainly show that Mt = max{\u2225UtV \u22a4t \u2212 \u03a3\u2225, \u2225UtK\u22a4t \u2225, \u2225JtV \u22a4t \u2225} converges linearly until it is smaller than\nMt \u2264 O(\u03c31\u03b4 + \u03b12)\u2225JtK\u22a4t \u2225. (D.7)\nThis implies that the difference between estimated matrix UtV \u22a4t and true matrix \u03a3, \u2225UtV \u22a4t \u2212 \u03a3\u2225, will be dominated by \u2225JtK\u22a4t \u2225. Moreover, during Phase 1 we can also show that \u2206t has the scale \u03b1. Phase 1 begins at T0 rounds and terminates at T1 rounds, and T1 may tend to infinity, which implies that Phase 1 may not terminate. In this case, since Mt converges linearly and Mt > \u2126(\u03c31\u03b4 + \u03b12)\u2225JtK\u22a4t \u2225, the loss also converges linearly. Note that, in the exact-parameterized case, i.e., k = r, we can prove that Phase 1 will not terminate since the stopping rule (D.7) is never satisfied as shown in Section E.\n\u2022 The Section D.5 (Phase 2) mainly shows that, after Phase 1, the \u2225Ut\u2212Vt\u2225 converges linearly until it achieves\n\u2225Ut \u2212 Vt\u2225 \u2264 O(\u03b12/ \u221a \u03c31) +O(\u03b42k+1\u2225JtK\u22a4t \u2225/ \u221a \u03c31).\nAssume Phase 2 starts at round T1 and terminates at round T2. Then since we can prove that \u2225Ut \u2212 Vt\u2225 decreases from 4 O(\u03c31) to \u2126(\u03b12), Phase 2 only takes a relatively small number of rounds, i.e. at most T2\u2212T1 = O(log( \u221a \u03c3r/\u03b1)/\u03b7\u03c3r) rounds. We also show that Mt remains small in this phase.\n\u2022 The Section D.6 (Phase 3) finally shows that the norm of Kt converges linearly, with a rate dependent on the initialization scale. As in Section 4.2, the error matrix in matrix sensing brings additional challenges for the proof. We overcome this proof by further analyzing the convergence of (a) part of Kt that aligns with Ut, and (b) part of Kt that lies in the complement space of Ut. We also utilize that Mt and \u2225Ut \u2212 Vt\u2225 are small from the start of the phase and remain small. See Section D.6 for a detailed proof.\n4The upper bound O(\u03c31) of \u2225Ut \u2212 Vt\u2225 is proved in the first two phases.\nD.3 INITIAL ITERATIONS\nWe start our proof by first applying results in Soltanolkotabi et al. (2023) and provide some additional proofs for our future use. From Soltanolkotabi et al. (2023), the converge takes at most T0 = O((1/\u03b7\u03c3r\u03c5) log( \u221a \u03c31/n\u03b1)) rounds.\nLet us state a few properties of the initial iterations using Lemma G.3.\nInitialization By our imbalance initialization F0 = \u03b1 \u00b7 F\u03030, G0 = (\u03b1/3) \u00b7 G\u03030, and by random matrix theory about the singular value (Vershynin, 2018, Corollary 7.3.3 and 7.3.4), with probability at least 1\u2212 2 exp(\u2212cn) for some constant c, if n > 8k, we can show that [\u03c3min(F0), \u03c3max(F0))] \u2286 [ \u221a 3\u03b1 2 , \u221a 3\u03b1\u221a 2 ], [\u03c3min(G0), \u03c3max(G0)] \u2286 [ \u221a 3\u03b1 6 , \u03b1\u221a 6 ] and\n3\u03b12\n2 I \u2265 F\u22a40 F0 \u2212G\u22a40 G0 = U\u22a40 U0 + J\u22a40 J0 \u2212 V \u22a40 V0 \u2212K\u22a40 K0 \u2265\n\u03b12\n2 I (D.8)\nAs we will show later, we will prove the (D.6) during all phases by (D.5) and (D.8).\nFirst, we show the following lemma, which is a subsequent corollary of the Lemma G.3.\nLemma D.1. There exist parameters \u03b60, \u03b40, \u03b10, \u03b70 such that, if we choose \u03b1 \u2264 \u03b10, F0 = \u03b1 \u00b7 F\u03030, G0 = (\u03b1/2) \u00b7 G\u03030, where the elements of F\u03030, G\u03030 is N (0, 1),5 and suppose that the operator A defined in Eq.(1.1) satisfies the restricted isometry property of order 2r + 1 with constant \u03b4 \u2264 \u03b40, then the gradient descent with step size \u03b7 \u2264 \u03b70 will achieve\n\u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 min{\u03c3r/2, \u03b11/2 \u00b7 \u03c3 3/4 1 } (D.9)\nwithin T0 = c2(1/\u03b7\u03c3r) log( \u221a \u03c31/n\u03b1) rounds with probability at least 1 \u2212 \u03b60 and constant c2 \u2265 1, where \u03b60 = c1 exp(\u2212c2k) + exp(\u2212(k \u2212 r + 1)) is a small constant. Moreover, during t \u2264 T0 rounds, we always have\nmax{\u2225Ft\u2225, \u2225Gt\u2225} \u2264 2 \u221a \u03c31 (D.10)\n\u2225Ut \u2212 Vt\u2225 \u2264 4\u03b1+ 40\u03b42k+1\u03c3\n3/2 1\n\u03c3r (D.11)\n\u2225Jt\u2225 \u2264 O ( 2\u03b1+ \u03b42k+1\u03c3 3/2 1 log( \u221a \u03c31/n\u03b1)\n\u03c3r\n) (D.12)\n13\u03b12\n8 I \u2265 \u2206t \u2265\n3\u03b12\n8 I (D.13)\nProof. Since the initialization scale \u03b1 \u2264 O(\u221a\u03c31), Eq.(D.10), Eq.(D.11), Eq.(D.12) and Eq.(D.13) hold for t\u2032 = 0. Assume that Eq.(D.9), Eq.(D.10), Eq.(D.11), Eq.(D.12) and Eq.(D.13) hold for t\u2032 = t\u2212 1. Proof of Eq.(D.9) and Eq.(D.10)\nFirst, by using the previous global convergence result Lemma G.3, the Eq.(D.9) holds by \u03b13/5\u03c3\n7/10 1 < \u03c3r/2 because \u03b1 \u2264 O(\u03c3 5/3 r /\u03c3 7/6 1 ) = O(\u03ba7/6 \u221a \u03c3r). Also, by Lemma G.3, Eq.(D.10)\nholds for all t \u2208 [T0].\nProof of Eq.(D.13)\nRecall \u2206t = U\u22a4t Ut + J \u22a4 t Jt \u2212 V \u22a4t Vt \u2212K\u22a4t Kt, then for all t \u2264 T0, we have \u2225\u2206t\u2212\u22060\u2225 \u2264 2\u03b72\u00b725\u03c321 \u00b7T0\u00b74\u03c31 \u2264 2c2 log( \u221a \u03c31/n\u03b1)(20\u03c3 3 1\u03b7/\u03c3r) = 200c2\u03b7\u03ba\u03c3 2 1 log( \u221a \u03c31/n\u03b1) \u2264 \u03b12/8.\nThe first inequality holds by Eq.(D.5) and \u2225FtGt \u2212 \u03a3\u2225 \u2264 \u2225Ft\u2225\u2225Gt\u2225 + \u2225\u03a3\u2225 \u2264 5\u03c31. The last inequality uses the fact that \u03b7 = O(\u03b12/\u03ba\u03c321 log( \u221a \u03c31/n\u03b1)). Thus, at t = T0, we have \u03bbmin(\u2206T0) \u2265\n5Note that in Soltanolkotabi et al. (2023), the initialization is F0 = \u03b1 \u00b7 F\u03030 and G0 = \u03b1 \u00b7 G\u03030, while Lemma G.3 uses an imbalance initialization. It is easy to show that their results continue to hold with this imbalance initialization.\n\u03bbmin(\u22060)\u2212 \u03b12/8 \u2265 \u03b12/2\u2212 \u03b12/8 = 3\u03b12/8 and \u2225\u2206T0\u2225 \u2264 \u2225\u22060\u2225+ 3\u03b12/2 + \u03b12/8 = 13\u03b12/8. Proof of Eq.(D.11)\nNow we can prove that \u2225U \u2212 V \u2225 keeps small during the initialization part. In fact, by Eq.(D.1) and Eq.(D.2), we have\n\u2225(Ut+1 \u2212 Vt+1)\u2225 \u2264 \u2225Ut \u2212 Vt\u2225\u2225I \u2212 \u03b7\u03a3\u2212 \u03b7(V \u22a4t Vt +K\u22a4t Kt))\u2225+ \u03b7\u2225Vt\u2225\u2225U\u22a4t Ut + J\u22a4t Jt \u2212 V \u22a4t Vt \u2212K\u22a4t Kt\u2225 + 4\u03b7\u03b42k+1\u2225FtG\u22a4t \u2212 \u03a3\u2225max{\u2225Ut\u2225, \u2225Vt\u2225, \u2225Jt\u2225, \u2225Kt\u2225} \u2264 (1\u2212 \u03b7\u03c3r)\u2225Ut \u2212 Vt\u2225+ 2\u03b7\u03b12 \u00b7 2 \u221a \u03c31 + 4\u03b7\u03b42k+1 \u00b7 (\u2225Ft\u2225\u2225Gt\u2225+ \u2225\u03a3\u2225) \u00b7 2 \u221a \u03c31 \u2264 (1\u2212 \u03b7\u03c3r)\u2225Ut \u2212 Vt\u2225+ 2\u03b7\u03b12 \u00b7 2 \u221a \u03c31 + 40\u03b7\u03b42k+1 \u00b7 \u03c33/21 .\nThe second inequality uses the inequality (D.6), while the third inequality holds by max{\u2225Ft\u2225, \u2225Gt\u2225} \u2264 2 \u221a \u03c31. Thus, since \u03b1 = O(\u03b42k+1\u03c33/21 /\u03c3r), we can get \u2225U0 \u2212 V0\u2225 \u2264 4\u03b1 \u2264 4\u03b1+ 40\u03c3r \u03b42k+1\u03c3 3/2 1 . If \u2225Ut \u2212 Vt\u2225 \u2264 4\u03b1+ 40\u03c3r \u03b42k+1\u03c3 3/2 1 , we know that\n\u2225Ut+1 \u2212 Vt+1\u2225 \u2264 (1\u2212 \u03b7\u03c3r) ( 4\u03b1+ 40\n\u03c3r \u03b42k+1\u03c3\n3/2 1\n) + 4\u03b7\u03b12 \u221a \u03c31 + 40\u03b7\u03b42k+1 \u00b7 \u03c33/21\n\u2264 (1\u2212 \u03b7\u03c3r) ( 4\u03b1+ 40\n\u03c3r \u03b42k+1\u03c3\n3/2 1 ) + 4\u03b7\u03c3r\u03b1+ 40\n\u03c3r \u03b42k+1\u03c3\n3/2 1\n\u2264 4\u03b1+ 40 \u03c3r \u03b42k+1\u03c3 3/2 1 .\nHence, \u2225Ut \u2212 Vt\u2225 \u2264 4\u03b1 + 40\u03c3r \u03b42k+1\u03c3 3/2 1 for t \u2264 T0 by induction. The second inequality holds by\n\u03b1 = O(\u03c3r/ \u221a \u03c31) Proof of Eq.(D.12)\nNow we prove that Jt and Kt are bounded for all t \u2264 T0. By Eq.(D.3) and max{\u2225Ft\u2225, \u2225Gt\u2225} \u2264 2 \u221a \u03c31, denote C2 = max{21c2, 32} \u2265 32, we have\n\u2225JT0\u2225 \u2264 \u2225J0\u2225+ \u03b7 T0\u22121\u2211 t=0 max{\u2225Ft\u2225, \u2225Gt\u2225} \u00b7 2\u03b42k+1 \u00b7 (\u2225Ft\u2225\u2225Gt\u2225+ \u2225\u03a3\u2225)\n\u2264 \u2225J0\u2225+ \u03b7T0 \u00b7 20\u03c33/21 \u00b7 \u03b42k+1 \u2264 \u2225J0\u2225+ 20c2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) \u2264 2\u03b1+ 20c2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) = 2\u03b1+ C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r).\nSimilarly, we can prove that \u2225KT0\u2225 \u2264 2\u03b1 + C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r). We complete the proof of Eq.(D.12)."
        },
        {
            "heading": "D.4 PHASE 1: LINEAR CONVERGENCE PHASE.",
            "text": "In this subsection, we analyze the first phase: the linear convergence phase. This phase starts at round T0, and we assume that this phase terminates at round T1. In this phase, the loss will converge linearly, with the rate independent of the initialization scale. Note that T1 may tend to infinity, since this phase may not terminate. For example, when k = r, we can prove that this phase will not terminate (\u00a7E), and thus leading a linear convergence rate that independent on the initialization scale. In this phase, we provide the following lemma, which shows some induction hypotheses during this phase.\nLemma D.2. Denote Mt = max{\u2225UtV \u22a4t \u2212\u03a3\u2225, \u2225UtK\u22a4t \u2225, \u2225JtV \u22a4t \u2225}. Suppose Phase 1 starts at T0 and ends at the first time T1 such that\n\u03b7\u03c32rMt\u22121/64\u03c31 < (17\u03b7\u03c31\u03b42k+1 + \u03b7\u03b1 2)\u2225Jt\u22121K\u22a4t\u22121\u2225 (D.14)\nDuring Phase 1 that T0 \u2264 t \u2264 T1, we have the following three induction hypotheses: max{\u2225Ut\u2225, \u2225Vt\u2225} \u2264 2 \u221a \u03c31 (D.15)\n\u2225UtV \u22a4t \u2212 \u03a3\u2225 \u2264 \u03c3r/2. (D.16)\nmax{\u2225Jt\u2225, \u2225Kt\u2225} \u2264 2 \u221a \u03b1\u03c3 1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u221a \u03c31) \u2264 \u221a \u03c31 (D.17)\n7\u03b12\n4 I \u2265 \u2206t \u2265\n\u03b12\n4 I (D.18)\nThe induction hypotheses hold for t = T0 due to Lemma D.1. Let us assume they hold for t\u2032 < t, and consider the round t. Let us first prove that the r-th singular value of U and V are lower bounded by poly(\u03c3r, 1/\u03c31) at round t, if Eq.(D.16) holds at round t. In fact,\n2 \u221a \u03c31 \u00b7 \u03c3r(U) \u2265 \u03c3r(U)\u03c31(V ) \u2265 \u03c3r(UV \u22a4) \u2265 \u03c3r/2.\nwhich means\n\u03c3r(U) \u2265 \u03c3r/4 \u221a \u03c31. (D.19)\nSimilarly, \u03c3r(V ) \u2265 \u03c3r/4 \u221a \u03c31.\nProof of Eq.(D.16) First, since \u2225Ut\u22121V \u22a4t\u22121 \u2212 \u03a3\u2225 \u2264 \u03c3r/2, by Eq.(D.19), we can get\nmin{\u03c3r(Ut\u22121), \u03c3r(Vt\u22121)} \u2265 \u03c3r\n4 \u221a \u03c31\n(D.20)"
        },
        {
            "heading": "Define Mt = max{\u2225UtV \u22a4t \u2212 \u03a3\u2225, \u2225UtK\u22a4t \u2225, \u2225JtV \u22a4t \u2225}. By the induction hypothesis,",
            "text": "max{\u2225Ut\u22121\u2225, \u2225Vt\u22121\u2225} \u2264 2 \u221a \u03c31,\nmax{\u2225Jt\u22121\u2225, \u2225Kt\u22121\u2225} \u2264 2 \u221a \u03b1\u03c3 1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1\u03c3 3/2 1 /\u03c3r).\nThen, by the updating rule and C2 \u2265 1, we can get\nUtKt = (1\u2212 \u03b7Ut\u22121U\u22a4t\u22121)Ut\u22121Kt\u22121(1\u2212 \u03b7Kt\u22121K\u22a4t\u22121) + \u03b7(\u03a3\u2212 Ut\u22121V \u22a4t\u22121)V K\u22a4\n+ \u03b7Ut\u22121J \u22a4 t\u22121Jt\u22121K \u22a4 t\u22121 +At, (D.21)\nwhere At is the perturbation term that contains all O(Ei(FG\u22a4 \u2212 \u03a3)) terms and O(\u03b72) terms such that\n\u2225At\u2225 \u2264 4\u03b7\u03b42k+1\u2225FtG\u22a4t \u2212 \u03a3\u2225max{\u2225Ft\u22252, \u2225Gt\u22252}+ 8\u03b72\u2225FtG\u22a4t \u2212 \u03a3\u22252 max{\u2225Ft\u22252, \u2225Gt\u22252} + \u03b72 max{\u2225Ft\u22252, \u2225Gt\u22252}2 \u00b7 \u2225FtGt \u2212 \u03a3\u2225\n\u2264 4\u03b7\u03b42k+1\u2225FtG\u22a4t \u2212 \u03a3\u2225max{\u2225Ft\u22252, \u2225Gt\u22252}+ 8\u03b72\u2225FtG\u22a4t \u2212 \u03a3\u2225 \u00b7 5\u03c31 \u00b7 4\u03c31 + \u03b72 \u00b7 16\u03c321 \u00b7 \u2225FtGt \u2212 \u03a3\u2225 \u2264 4\u03b7\u03b42k+1(3Mt\u22121 + \u2225Jt\u22121K\u22a4t\u22121\u2225)4\u03c31 + \u03b7\u03b12(3Mt\u22121 + \u2225Jt\u22121K\u22a4t\u22121\u2225)\nUsing the similar technique for JtV \u22a4t and UtV \u22a4 t \u2212 \u03a3, we can finally get Mt \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n) Mt\u22121 + 2\u03b7Mt\u22121 \u00b7 2 \u221a \u03c31 \u00b7max{\u2225Jt\u22121\u2225, \u2225Kt\u22121\u2225}\n+ 4\u03b7\u03b42k+1(3Mt\u22121 + \u2225Jt\u22121K\u22a4t\u22121\u2225) \u00b7 4\u03c31 + \u03b7\u03b12(3Mt\u22121 + \u2225Jt\u22121K\u22a4t\u22121\u2225) \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n) Mt\u22121 + 2\u03b7Mt\u22121 \u00b7 2 \u221a \u03c31 \u00b7 ( \u03b1+ C2 log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03c3 3/2 1 /\u03c3r ) + 4\u03b7\u03b42k+1(3Mt\u22121 + \u2225Jt\u22121K\u22a4t\u22121\u2225) \u00b7 4\u03c31 + \u03b7\u03b12(3Mt\u22121 + \u2225Jt\u22121K\u22a4t\u22121\u2225)\n\u2264 ( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n) Mt\u22121 +O ( \u03b7 \u221a \u03c31 \u00b7 ( \u03b1+ C2 log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03c3 3/2 1 /\u03c3r )) \u00b7Mt\u22121\n+ (17\u03b7\u03c31\u03b42k+1 + \u03b7\u03b1 2)\u2225Jt\u22121K\u22a4t\u22121\u2225 \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n32\u03c31\n) Mt\u22121 + (17\u03b7\u03c31\u03b42k+1 + \u03b7\u03b1 2)\u2225Jt\u22121K\u22a4t\u22121\u2225. (D.22)\nThe last inequality holds by \u03b42k+1 = O(\u03c33r/\u03c331 log( \u221a \u03c31/n\u03b1)) and \u03b1 = O(\u03c32r/\u03c3 3/2 1 ) = O(\u221a\u03c3r\u03ba\u22123/2). During Phase 1, we have\n\u03b7\u03c32rMt\u22121/64\u03c31 \u2265 (17\u03b7\u03c31\u03b42k+1 + \u03b7\u03b12)\u2225Jt\u22121K\u22a4t\u22121\u2225, then\nMt \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n) Mt\u22121. (D.23)\nHence, \u2225UtV \u22a4t \u2212 \u03a3\u2225 \u2264 Mt \u2264 MT0 \u2264 \u2225FT0G\u22a4T0 \u2212 \u03a3\u2225 \u2264 \u03b42k+1.\nProof of Eq.(D.15) Now we bound the norm of Ut and Vt. First, note that\n\u2225(Ut \u2212 Vt)\u2225 \u2264 (1\u2212 \u03b7\u03c3r)\u2225Ut\u22121 \u2212 Vt\u22121\u2225+ \u03b7 \u00b7 2\u03b12 \u00b7 2 \u221a \u03c31 + 40\u03b7 \u00b7 \u03b42k+1 \u00b7 \u03c33/21\nHence, \u2225Ut \u2212 Vt\u2225 \u2264 4\u03b1+40\u03b42k+1\u03c33/21 /\u03c3r still holds using the same technique in the initialization part.\nThus, by the induction hypothesis Eq.(D.16) and \u03c31 \u2265 \u03b42k+1, we have\n2\u03c31 \u2265 \u03c31 + \u03b42k+1 \u2265 \u2225\u03a3\u2225+ \u2225UtV \u22a4t \u2212 \u03a3\u2225 \u2265 \u2225UtV \u22a4t \u2225 = \u2225VtV \u22a4t + (Ut \u2212 Vt)V \u22a4t \u2225 \u2265 \u2225VtV \u22a4t \u2225 \u2212 \u2225Ut \u2212 Vt\u2225\u2225Vt\u2225\n\u2265 \u2225Vt\u22252 \u2212 \u2225Vt\u2225 \u00b7 ( 4\u03b1+ 40\u03b42k+1\u03c3 3/2 1\n\u03c3r ) \u2265 \u2225Vt\u22252 \u2212 \u2225Vt\u2225.\nThen, we can get \u2225Vt\u2225 \u2264 2 \u221a \u03c31. Similarly, \u2225Ut\u2225 \u2264 2 \u221a \u03c31.\nProof of Eq.(D.17) Since during Phase 1,\n\u2225JtK\u22a4t \u2225 \u2264 Mt \u00b7 \u03c32r\n64\u03c31(17\u03c31\u03b42k+1 + \u03b12) \u2264 Mt \u00b7\n1\n1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r ,\nby \u03b42k+1 < 1/128 and Eq.(D.23), \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 4max{\u2225JtK\u22a4t \u2225,Mt} \u2264 4Mt \u00b7max { 1,\n1\n1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r } \u2264 \u2225FT0GT0 \u2212 \u03a3\u2225 ( 1\u2212 \u03b7\u03c32r/64\u03c31 )t\u2212T0 /(1088\u03ba2\u03b42k+1 + 64\u03b1 2\u03ba/\u03c3r). (D.24)\nThus, the maximum norm of Jt,Kt can be bounded by\n\u2225Jt\u2225 \u2264 \u2225JT0\u2225+ 2\u03b7 \u00b7 2 \u221a \u03c31\u03b42k+1 \u00b7 t\u22121\u2211 t\u2032=T0 \u2225FtGt \u2212 \u03a3\u2225\n\u2264 2\u03b1+ C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) +\n4\u03b7 \u221a \u03c31\u03b42k+1\n1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r \u00b7 \u2225FT0GT0 \u2212 \u03a3\u2225 \u00b7 64\u03c31 \u03b7\u03c32r\n= 2\u03b1+ C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) + \u03c3 3/2 1\n4\u03ba2\u03c32r \u00b7 \u2225FT0GT0 \u2212 \u03a3\u2225\n\u2264 2\u03b1+ C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) + \u03b11/2\u03c3 9/4 1\n4\u03ba2\u03c32r\n\u2264 2 \u221a \u03b1\u03c3\n1/4 1 + C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u221a \u03c31)\n\u2264 2 \u221a \u03b1\u03c3\n1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u221a \u03c31).\nThe last inequality uses the fact that 2\u03b1+ \u221a \u03b1\u03c3 1/4 1 4 \u2264 2 \u221a \u03b1\u03c3 1/4 1 by \u03b1 = O( \u221a \u03c3r). Similarly, \u2225Kt\u2225 \u2264 2 \u221a \u03b1\u03c3\n1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u00b7 \u221a \u03c31). We complete the proof of Eq.(D.17).\nProof of Eq.(D.18) Last, for t \u2208 [T0, T1), we have\n\u2225\u2206t \u2212\u2206T0\u2225 \u2264 T1\u22121\u2211 t=T0 2(\u03b72 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u22252 \u00b7max{\u2225Ft\u2225, \u2225Gt\u2225}2)\n\u2264 2\u03b72\u2225FT0GT0 \u2212 \u03a3\u22252 \u221e\u2211\nt=T0\n( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n)2(t\u2212T0) \u00b7 4\u03c31\n\u2264 2\u03b72 \u00b7 25\u03c321 \u00b7 16\u03c31 \u03b7\u03c32r \u00b7 4\u03c31 \u2264 3200\u03b7\u03ba2\u03c321 \u2264 \u03b12/8,\nwhere the last inequality arises from the fact that \u03b7 = O(\u03b12/\u03ba2\u03c321). By 3\u03b1 2 8 I \u2264 \u2206T0 \u2264 13\u03b12\n8 I , we can have \u2225\u2206t\u2225 \u2264 13\u03b12/8 + \u03b12/8 \u2264 7\u03b12/4 and \u03bbmin(\u2206t) \u2265 3\u03b12/8 \u2212 \u03b12/8 = \u03b12/4. Hence, the inequality Eq.(D.18) still holds during Phase 1. Moreover, by Eq.(D.24), during the Phase 1, for a round t \u2265 0, we will have\n\u2225Ft+T0G\u22a4t+T0 \u2212 \u03a3\u2225 \u2264 \u2225FT0GT0 \u2212 \u03a3\u2225 ( 1\u2212 \u03b7\u03c32r/64\u03c31 )t /(1088\u03ba2\u03b42k+1 + 64\u03b1 2\u03ba/\u03c3r)\n\u2264 \u2225FT0GT0 \u2212 \u03a3\u2225 ( 1\u2212 \u03b7\u03c32r/64\u03c31 )t \u00b7 \u03c3r 64\u03b12\u03ba \u2264 \u03c3r 2 \u00b7 ( 1\u2212 \u03b7\u03c32r/64\u03c31 )t \u00b7 \u03c3r 64\u03b12\u03ba = \u03c32r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03c32r/64\u03c31 )t . (D.25)\nThe conclusion (D.25) always holds in Phase 1. Note that Phase 1 may not terminate, and then the loss is linear convergence. We assume that at round T1, Phase 1 terminates, which implies that\n\u03c32rMT1\u22121/64\u03c31 < (17\u03c31\u03b42k+1 + \u03b1 2)\u2225JT1\u22121K\u22a4T1\u22121\u2225, (D.26)\nand the algorithm goes to Phase 2."
        },
        {
            "heading": "D.5 PHASE 2: ADJUSTMENT PHASE.",
            "text": "In this phase, we prove U \u2212 V will decrease exponentially. This phase terminates at the first time T2 such that\n\u2225UT2\u22121 \u2212 VT2\u22121\u2225 \u2264 8\u03b12\n\u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31\u2225JT2\u22121K\u22a4T2\u22121\u2225\n\u03c3r . (D.27)\nBy stopping rule (D.27), since \u2225UT1 \u2212 VT1\u2225 \u2264 O(\u03c31), this phase will take at most O(log(\u221a\u03c3r/\u03b1)/\u03b7\u03c3r) rounds, i.e.\nT2 \u2212 T1 = O(log( \u221a \u03c3r/\u03b1)/\u03b7\u03c3r). (D.28)\nWe use the induction to show that all the following hypotheses hold during Phase 2.\nmax{\u2225Ft\u22121\u2225, \u2225Gt\u22121} \u2264 2 \u221a \u03c31 (D.29)\nMt \u2264 (1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r)\u2225JtK\u22a4t \u2225 \u2264 \u2225JtK\u22a4t \u2225 (D.30)\nmax{\u2225Jt\u22121\u2225, \u2225Kt\u22121\u2225} \u2264 2 \u221a \u03b1\u03c3 1/4 1 + (2C2 + 16C3) log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u221a \u03c31) \u2264 \u03c3r/4 \u221a \u03c31\n(D.31) \u2225JtK\u22a4t \u2225 \u2264 ( 1 +\n\u03b7\u03c32r 128\u03c31\n) \u2225Jt\u22121K\u22a4t\u22121\u2225 (D.32)\n\u2225Ut \u2212 Vt\u2225 \u2264 (1\u2212 \u03b7\u03c3r/2)\u2225Ut\u22121 \u2212 Vt\u22121\u2225 (D.33) 3\u03b12\n16 \u00b7 I \u2264 \u2206t \u2264\n29\u03b12\n16 \u00b7 I. (D.34)\nProof of (D.31) To prove this, we first assume that this adjustment phase will only take at most C3(log(\u03b1)/\u03b7\u03c3r) rounds. By the induction hypothesis for the previous rounds,\n\u2225Jt\u2225 \u2264 JT1 + t\u22121\u2211 i=T1 \u03b7\u03b42k+1 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u2225\n\u2264 2 \u221a \u03b1\u03c3\n1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) + t\u22121\u2211 i=T1 \u03b7\u03b42k+1 \u00b7 \u2225FiG\u22a4i \u2212 \u03a3\u2225\n\u2264 2 \u221a \u03b1\u03c3\n1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) + C3(log( \u221a \u03c31/n\u03b1)/\u03b7\u03c3r) \u00b7 \u03b7\u03b42k+1 \u00b7 4\u2225Ji\u22121K\u22a4i\u22121\u2225\n\u2264 2 \u221a \u03b1\u03c3\n1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) + C3(log( \u221a \u03c31/n\u03b1)/\u03b7\u03c3r) \u00b7 \u03b7\u03b42k+116\u03c31\n\u2264 2 \u221a \u03b1\u03c3\n1/4 1 + (2C2 + 16C3) log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r).\nSimilarly, due to the symmetry property, we can bound the \u2225Kt\u2225 using the same technique. Thus,\nmax{\u2225Jt\u2225, \u2225Kt\u2225} \u2264 2 \u221a \u03b1\u03c3 1/4 1 + (2C2 + 16C3) log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r).\nProof of (D.30) First, we prove that during t \u2208 [T1, T2),\nMt \u2264 (1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r)\u2225JtK\u22a4t \u2225 \u2264 \u2225JtK\u22a4t \u2225 \u2264 4\u03b1\u03ba4\u03c3 1/2 1 + \u03b42k+1\u03c31. (D.35)\nin this phase. Then, by \u03b42k+1 \u2264 O(1/ log( \u221a \u03c31/n\u03b1)\u03ba 2) and \u03b1 \u2264 O(\u03c3r/ \u221a \u03c31), choosing sufficiently small coefficient, we can have\nJtK \u22a4 t = (I \u2212 \u03b7Jt\u22121J\u22a4t\u22121)Jt\u22121K\u22a4t\u22121(I \u2212 \u03b7Kt\u22121K\u22a4t\u22121) + \u03b72Jt\u22121J\u22a4t\u22121Jt\u22121K\u22a4t\u22121Kt\u22121K\u22a4t\u22121\n\u2212 \u03b7Jt\u22121V \u22a4t\u22121Vt\u22121K\u22a4t\u22121 \u2212 \u03b7JtU\u22a4t UtK\u22a4t + Ct\u22121, (D.36)\nwhere Ct represents the relatively small perturbation term, which contains terms of O(\u03b4) and O(\u03b72). By (D.29), we can easily get\nCt\u22121 \u2265 \u2212 ( 4\u03b7\u03b42k+1 \u00b7 \u2225Ft\u22121G\u22a4t\u22121 \u2212 \u03a3\u2225 \u00b7 4\u03c31 ) (D.37)\nThus, combining (D.36) and (D.37), we have\n\u2225JtK\u22a4t \u2225\n\u2265 \u2225I \u2212 \u03b7Jt\u22121J\u22a4t\u22121\u2225\u2225I \u2212 \u03b7Kt\u22121K\u22a4t\u22121\u2225\u2225Jt\u22121K\u22a4t\u22121\u2225 \u2212 4\u03b7Mt\u22121 \u00b7 4\u03c31 \u2212 4\u03b7\u03b42k+1\u2225Jt\u22121Kt\u22121\u2225 \u00b7 2\u03c31 \u2212 \u03b7264\u03c331\n\u2265 ( 1\u2212 2\u03b7max{\u2225Jt\u22121\u2225, \u2225Kt\u22121\u2225}2 \u2212 16 \u00b7 1088\u03b7\u03ba2\u03b42k+1\u03c31 \u2212 1024\u03b7\u03b12\u03ba2 \u2212 8\u03b7\u03b42k+1 \u00b7 \u03c31 ) \u2225Jt\u22121K\u22a4t\u22121\u2225\n\u2265 ( 1\u2212 \u03b7\u03c3 2 r\n128\u03c31\n) \u2225Jt\u22121K\u22a4t\u22121\u2225.\nThe second inequality is because Mt\u22121 \u2264 (1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r)\u2225Jt\u22121K\u22a4t\u22121\u2225, and the last inequality holds by Eq.(D.31) and\n\u03b42k+1 = O(\u03ba\u22124), \u03b1 = O(\u03ba\u22123/2 \u221a \u03c3r) (D.38)\nThen, note that by Eq.(D.22), we have\nMt \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n32\u03c31\n) Mt\u22121 + (17\u03b7\u03c31\u03b42k+1 + \u03b7\u03b1 2)\u2225Jt\u22121K\u22a4t\u22121\u2225.\nThen, by Mt\u22121 \u2264 (1088\u03ba2\u03b42k+1 +64\u03b12\u03ba/\u03c3r) \u00b7 \u2225Jt\u22121K\u22a4t\u22121\u2225 and denote L = 17\u03c31\u03b42k+1 +\u03b12, we have\nMt \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n32\u03c31\n) Mt\u22121 + (17\u03b7\u03c31\u03b42k+1 + \u03b7\u03b1 2)\u2225Jt\u22121K\u22a4t\u22121\u2225\n\u2264 ( 1\u2212 \u03b7\u03c3 2 r\n32\u03c31\n) \u00b7 (1088\u03ba2\u03b42k+1 + 64\u03b12\u03ba/\u03c3r)\u2225Jt\u22121K\u22a4t\u22121\u2225+ \u03b7L\u2225Jt\u22121K\u22a4t\u22121\u2225\n= ( 1\u2212 \u03b7\u03c3 2 r\n32\u03c31\n) \u00b7 64L\u03ba\n\u03c3r \u2225Jt\u22121K\u22a4t\u22121\u2225+ \u03b7L\u2225Jt\u22121K\u22a4t\u22121\u2225\n\u2264 ( 64L\u03ba\n\u03c3r \u2212 2\u03b7L\n) \u2225Jt\u22121K\u22a4t\u22121\u2225\n\u2264 ( 64L\u03ba\n\u03c3r \u2212 2\u03b7L\n)/( 1\u2212 \u03b7\u03c3 2 r\n128\u03c31\n) \u2225JtK\u22a4t \u2225\n\u2264 64L\u03ba \u03c3r \u2225JtK\u22a4t \u2225.\nHence,\nMt \u2264 64L\u03ba\n\u03c3r \u2225JtK\u22a4t \u2225 \u2264 \u2225JtK\u22a4t \u2225\nfor all t in Phase 2. The last inequality is because \u03b42k+1 = O(1/\u03ba2 log( \u221a \u03c31/n\u03b1)). Moreover, by \u03b42k+1 \u2264 O(1/\u03ba2 log( \u221a \u03c31/n\u03b1) 2) and (a+ b)2 \u2264 2a2 + 2b2 we have\n\u2225JtK\u22a4t \u2225 \u2264 \u2225Jt\u2225\u2225Kt\u2225 \u2264 ( 2 \u221a \u03b1\u03c3 1/4 1 + (2C2 + 16C3) log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u221a \u03c31) )2 (D.39)\n\u2264 4\u03b1\u03ba4\u03c31/21 + \u03b42k+1\u03c31. (D.40)\nWe complete the proof of Eq.(D.30).\nProof of Eq.(D.32) Moreover, by the updating rule of Jt and Kt, (D.36) and (D.37) we have\n\u2225JtK\u22a4t \u2225 \u2264 \u2225(I \u2212 \u03b7Jt\u22121J\u22a4t\u22121)Jt\u22121KTt\u22121(I \u2212 \u03b7Kt\u22121K\u22a4t\u22121)\u2225+ \u2225\u03b72(Jt\u22121J\u22a4t\u22121)Jt\u22121KTt\u22121(Kt\u22121K\u22a4t\u22121)\u2225\n(D.41)\n+ 4\u03b7Mt\u22121 \u00b7 4\u03c31 + 4\u03b7\u03b42k+1\u2225Jt\u22121K\u22a4t\u22121\u2225 \u00b7 2\u03c31\n\u2264 \u2225Jt\u22121K\u22a4t\u22121\u2225+ \u03b72( \u221a \u03c31/2) 4\u2225Jt\u22121K\u22a4t\u22121\u2225+ 4\u03b7 64L\u03ba\n\u03c3r \u2225Jt\u22121K\u22a4t\u22121\u2225 \u00b7 4\u03c31 + 8\u03b7\u03c31\u03b42k+1\u2225Jt\u22121K\u22a4t\u22121\u2225\n= \u2225Jt\u22121K\u22a4t\u22121\u2225 \u00b7 ( 1 + \u03b72\u03c321/16 + 1024L\u03ba 2 + 8\u03c31\u03b42k+1 ) .\nThe last inequality uses the fact that \u2225Jt\u22121\u2225 \u2264 \u221a \u03c31/2, \u2225Kt\u22121\u2225 \u2264 \u221a \u03c31/2 and Mt\u22121 \u2264\n64L\u03ba \u03c3r \u2225Jt\u22121K\u22a4t\u22121\u2225. Now by the fact that L = 17\u03c31\u03b42k+1 + \u03b12 = O( \u03c32r \u03c31\u03ba2 ), we can choose small constant so that\n\u03b72\u03c321/16 \u2264 \u03c32r 384\u03c31 , 1024L\u03ba2 \u2264 \u03c3 2 r 384\u03c31 , 8\u03c31\u03b42k+1 \u2264 \u03c32r 384\u03c31 .\nThus, we can have\n\u2225JtK\u22a4t \u2225 \u2264 \u2225Jt\u22121K\u22a4t\u22121\u2225 \u00b7 ( 1 +\n\u03b7\u03c32r 128\u03c31\n) .\nWe complete the proof of (D.32)\nProof of (D.33) Hence, similar to Phase 1, by \u2225UtV \u22a4t \u2212 \u03a3\u2225 \u2264 Mt \u2264 4\u03b1\u03ba4\u03c3 1/2 1 + \u03b42k+1\u03c31 and \u2225Ut \u2212 Vt\u2225 \u2264 \u2225UT1 \u2212 VT1\u2225 \u2264 4\u03b1+ 40\u03b4\u03c3 3/2 1 \u03c3r , we can show that\nmax{\u2225Ut\u2225, \u2225Vt\u2225} \u2264 2 \u221a \u03c31\nAlso, consider\nUt \u2212 Vt = (I \u2212 \u03b7\u03a3\u2212 V \u22a4t Vt \u2212K\u22a4t Kt)(Ut\u22121 \u2212 Vt\u22121)\u2212 \u03b7Vt\u2206t + \u03b7 \u00b7 ( E1(Ft\u22121G \u22a4 t\u22121 \u2212 \u03a3)Vt\u22121 + E2(Ft\u22121G\u22a4t\u22121 \u2212 \u03a3)Kt\u22121\n) \u2212 \u03b7 \u00b7 ( E\u22a41 (Ft\u22121G \u22a4 t\u22121 \u2212 \u03a3)Ut\u22121 + E\u22a43 (Ft\u22121G\u22a4t\u22121 \u2212 \u03a3)Jt\u22121 ) .\nHence, by the RIP property and \u2206t\u22121 \u2264 2\u03b12I ((D.34)), we can get\n\u2225(Ut \u2212 Vt)\u2225 \u2264 (1\u2212 \u03b7\u03c3r)\u2225Ut\u22121 \u2212 Vt\u22121\u2225+ 2\u03b7\u03b12 \u00b7 2 \u221a \u03c31 + 4\u03b7\u03b42k+1 \u00b7 2 \u221a \u03c31 \u00b7 \u2225Ft\u22121G\u22a4t\u22121 \u2212 \u03a3\u2225\n\u2264 (1\u2212 \u03b7\u03c3r)\u2225Ut\u22121 \u2212 Vt\u22121\u2225+ 2\u03b7\u03b12 \u00b7 2 \u221a \u03c31 + 8\u03b7\u03b42k+1 \u00b7 \u221a \u03c31 \u00b7 4\u2225Jt\u22121K\u22a4t\u22121\u2225 \u2264 (1\u2212 \u03b7\u03c3r)\u2225Ut\u22121 \u2212 Vt\u22121\u2225+ 2\u03b7\u03b12 \u00b7 2 \u221a \u03c31 + 32\u03b7\u03b42k+1 \u00b7 \u221a \u03c31 \u00b7 \u2225Jt\u22121K\u22a4t\u22121\u2225\nSince\n\u2225Ut\u22121 \u2212 Vt\u22121\u2225 \u2265 8\u03b12\n\u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31\u2225Jt\u22121K\u22a4t\u22121\u2225\n\u03c3r .\nfor all t in Phase 2, we can have\n\u2225Ut \u2212 Vt\u2225 \u2264 (1\u2212 \u03b7\u03c3r/2)\u2225Ut\u22121 \u2212 Vt\u22121\u2225\nduring Phase 2.\nMoreover, since Phase 2 terminates at round T2, such that\n\u2225UT2\u22121 \u2212 VT2\u22121\u2225 \u2264 8\u03b12\n\u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31\u2225JT2\u22121K\u22a4T2\u22121\u2225\n\u03c3r ,\nit takes at most\nC3 log( \u221a \u03c3r/\u03b1)/\u03b7\u03c3r = t \u2217 2 (D.42)\nrounds for some constant C3 because (a) (D.33), (b) and Ut \u2212 Vt decreases from \u2225UT1 \u2212 VT1\u2225 \u2264 4 \u221a \u03c31 to at most \u2225UT2 \u2212 VT2\u2225 = \u2126(\u03b12 \u221a \u03c31/\u03c3r). Also, the changement of \u2206t can be bounded by\n\u2225\u2206t \u2212\u2206T1\u2225 \u2264 T2\u22121\u2211 t=T1 2(\u03b72 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u22252 \u00b7 4\u03c31)\n\u2264 2(\u03b72) \u00b7 100\u03c331 \u00b7 (T2 \u2212 T1) \u2264 2(\u03b72) \u00b7 100\u03c331 \u00b7 C3 log( \u221a \u03c31/n\u03b1)(1/\u03b7\u03c3r) \u2264 10C3 log( \u221a \u03c31/n\u03b1)(\u03b7\u03ba\u03c3 2 1)\n\u2264 \u03b12/16.\nThe last inequality holds by choosing \u03b7 \u2264 \u03b12/160C3\u03ba\u03c321 . Then, \u03bbmin(\u2206t) \u2265 \u03bbmin\u2206T1 \u2212\u03b12/16 \u2265 \u03b12/4 \u2212 \u03b12/16 = 3\u03b12/16 and \u2225\u2206t\u2225 \u2264 \u2225\u2206T1\u2225 + \u03b12/16 \u2264 7\u03b12/4 + \u03b12/16 \u2264 29\u03b12/16. Hence, inequality (D.6) still holds during Phase 2."
        },
        {
            "heading": "D.6 PHASE 3: LOCAL CONVERGENCE",
            "text": "In this phase, we show that the norm of Kt will decrease at a linear rate. Denote the SVD of Ut as Ut = At\u03a3tWt, where \u03a3t \u2208 Rr\u00d7r, Wt \u2208 Rr\u00d7k, and define Wt,\u22a5 \u2208 R(k\u2212r)\u00d7k is the complement of Wt.\nWe use the induction to show that all the following hypotheses hold during Phase 3.\nmax{\u2225Jt\u2225, \u2225Kt\u2225} \u2264 O(2 \u221a \u03b1\u03c3 1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03ba2 \u221a \u03c31) \u2264 \u221a \u03c31/2 (D.43)\nMt \u2264 64L\u03ba\n\u03c3r \u2225JtK\u22a4t \u2225 \u2264 \u2225JtK\u22a4t \u2225 (D.44) \u2225JtK\u22a4t \u2225 \u2264 ( 1 +\n\u03b7\u03c32r 128\u03c31\n) \u2225Jt\u22121K\u22a4t\u22121\u2225 (D.45)\n\u2225Ut \u2212 Vt\u2225 \u2264 8\u03b12\n\u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31\u2225JtK\u22a4t \u2225\n\u03c3r (D.46)\n\u03b12\n8 \u00b7 I \u2264 \u2206t \u2264 2\u03b12I (D.47) \u2225Kt\u2225 \u2264 2\u2225KtW\u22a4t,\u22a5\u2225 (D.48)\n\u2225Kt+1W\u22a4t+1,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5\u2225 \u00b7 ( 1\u2212 \u03b7\u03b1 2\n8\n) . (D.49)\nAssume the hypotheses above hold before round t, then at round t, by the same argument in Phase 1 and 2, the inequalities (D.44) and (D.46) still holds, then max{\u2225Ut\u2225, \u2225Vt\u2225} \u2264 2 \u221a \u03c31 and min{\u03c3r(U), \u03c3r(V )} \u2265 \u03c3r/4 \u221a \u03c31.\nLast, we should prove the induction hypotheses (D.43) , (D.47), (D.48) and (D.49).\nProof of Eq.(D.45) Similar to the proof of (D.32) in Phase 2, we can derive (D.45) again.\nProof of Eq.(D.48) First, to prove (D.48), note that we can get\nMt \u2265 \u2225UtKt\u2225 = \u2225At\u03a3tWtK\u22a4t \u2225 = \u2225\u03a3tWtK\u22a4t \u2225\n\u2265 \u03c3r(U) \u00b7 \u2225KtW\u22a4t \u2225 \u2265 \u2225KtW\u22a4t \u2225\u03c3r\n4 \u221a \u03c31\n\u2265 \u2225KtW\u22a4t \u2225\n\u221a \u03c3r\n4 \u221a \u03ba\n.\nHence,\n\u2225KtW\u22a4t \u2225 \u2264 4 \u221a \u03baM/ \u221a \u03c3r \u2264\n64\u03c31L \u221a \u03ba\n\u03c3 5/2 r\n\u2225JtK\u22a4t \u2225 \u2264 32L\u03ba3/2\n\u03c3 3/2 r\n\u2225Kt\u2225 \u00b7 \u221a \u03c31 \u2264 32L\u03ba2\n\u03c3r \u2225Kt\u2225.\n(D.50)\nThus,\n\u2225Kt\u2225 \u2264 \u2225KtW\u22a4t,\u22a5\u2225+ \u2225KtW\u22a4t \u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225+ 64L\u03ba\n\u03c3r \u2225Kt\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225+ 1\n2 \u2225Kt\u2225.\nThe last inequality uses the fact that \u03b42k+1 = O(\u03c33r/\u03c331) Hence, \u2225KtW\u22a4t,\u22a5\u2225 \u2265 \u2225Kt\u2225/2, and (D.48) holds during Phase 3.\nProof of Eq.(D.47) To prove the (D.47), by the induction hypothesis of Eq.(D.49), note that\n\u2225\u2206t \u2212\u2206T2\u2225 \u2264 2\u03b72 \u00b7 t\u22121\u2211\nt\u2032=T2\n\u2225Ft\u2032G\u22a4t\u2032 \u2212 \u03a3\u222524\u03c31\n\u2264 2\u03b72 t\u22121\u2211\nt\u2032=T2\n16\u03c31\u2225Jt\u2032K\u22a4t\u2032 \u22252\n\u2264 64\u03c31\u03b72 \u00b7 \u221e\u2211\nt\u2032=T2\n\u2225Jt\u2032\u22252\u2225Kt\u2032W\u22a4t\u2032,\u22a5\u22252\n\u2264 64\u03c31 \u00b7 \u03b72 ( \u03c31 \u00b7 \u2225KT2W\u22a4T2,\u22a5\u2225 2 \u00b7 8 \u03b7\u03b12 ) (D.51)\n\u2264 512\u03b7\u03c3 2 1\n\u03b12 \u00b7 \u2225KT2\u22252\n\u2264 128\u03b7\u03c3 2 1\n\u03b12 \u00b7 \u03c31\n\u2264 \u03b12/16.\nThe Eq.(D.51) holds by the sum of geometric series. The last inequality holds by \u03b7 \u2264 O(\u03b14/\u03c331) Then, we have\n\u2225\u2206t\u2225 \u2264 \u2225\u2206T2\u2225+ \u2225\u2206t \u2212\u2206T2\u2225 \u2264 29\u03b12\n16 +\n\u03b12 16 \u2264 2\u03b12.\n\u03bbmin(\u2206t) \u2265 \u03bbmin(\u2206T2)\u2212 \u2225\u2206t \u2212\u2206T2\u2225 \u2265 3\u03b12 16 \u2212 \u03b1 2 16 = \u03b12 8 .\nHence, (D.47) holds during Phase 3.\nProof of Eq.(D.43) To prove the (D.43), note that\n\u2225Kt\u2225 \u2264 2\u2225KtW\u22a4t,\u22a5\u2225 \u2264 2\u2225KT2W\u22a4T2,\u22a5\u2225 \u2264 2\u2225KT2\u2225 \u2264 O(\u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03c33/21 /\u03c3r). (D.52)\nOn the other hand, by \u2206t \u2264 2\u03b12I , we have\nWt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212Wt,\u22a5V \u22a4t VtW\u22a4t,\u22a5 \u2264 2\u03b12 \u00b7 I."
        },
        {
            "heading": "Hence, denote Lt = \u2225JtK\u22a4t \u2225 \u2264 \u03c31/4,",
            "text": "Wt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2264 2\u03b12I +Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 +Wt,\u22a5V \u22a4t VtW\u22a4t,\u22a5\n= 2\u03b12I +Wt,\u22a5K \u22a4 t KtW \u22a4 t,\u22a5 +Wt,\u22a5(Vt \u2212 Ut)\u22a4(Vt \u2212 Ut)W\u22a4t,\u22a5 \u2264 2\u03b12I +Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 + ( 8\u03b12 \u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r\n)2 \u00b7 I\n= Wt,\u22a5K \u22a4 t KtW \u22a4 t,\u22a5 +\n( 2\u03b1+ 8\u03b12 \u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r\n)2 I. (D.53)\nAlso, by inequality (D.53), we have\n\u2225JtW\u22a4t,\u22a5\u2225 \u2212 \u2225KtW\u22a4t,\u22a5\u2225 \u2264 \u2225JtW\u22a4t,\u22a5\u22252 \u2212 \u2225KtW\u22a4t,\u22a5\u22252\n\u2225JtW\u22a4t,\u22a5\u2225+ \u2225KtW\u22a4t,\u22a5\u2225\n\u2264\n( 2\u03b1+ 8\u03b12 \u221a \u03c31+64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r )2 2\u2225KtW\u22a4t,\u22a5\u2225+ \u2225JtW\u22a4t,\u22a5\u2225 \u2212 \u2225KtW\u22a4t,\u22a5\u2225\n\u2264\n( 2\u03b1+ 8\u03b12 \u221a \u03c31+64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r )2 \u2225JtW\u22a4t,\u22a5\u2225 \u2212 \u2225KtW\u22a4t,\u22a5\u2225\nThus, by Lt \u2264 \u03c31/4, we can get\n\u2225JtW\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5\u2225+ 2\u03b1+ 8\u03b12\n\u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r\n\u2264 \u2225KT2\u2225+ 2\u03b1+ 8\u03b12\n\u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r\n\u2264 O(2 \u221a \u03b1\u03c3\n1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1)\u03ba 2\u221a\u03c31). The second inequality holds by \u2225KtW\u22a4t,\u22a5\u2225 \u2264 \u2225KT2W\u22a4T2,\u22a5\u2225 \u2264 \u2225KT2\u2225. On the other hand, note that\n\u2225Jt\u2225 \u2264 \u2225JtW\u22a4t \u2225+ \u2225JtW\u22a4t,\u22a5\u2225 \u2264 \u2225JtU\u22a4t \u2225/\u03c3r(U) + \u2225JtW\u22a4t,\u22a5\u2225 \u2264 \u2225JtVt\u2225/\u03c3r(U) + \u2225Jt(Ut \u2212 Vt)\u2225/\u03c3r(U) + \u2225JtW\u22a4t,\u22a5\u2225 \u2264 Mt/\u03c3r(U) + \u2225Jt\u2225\u2225(Ut \u2212 Vt)\u2225/\u03c3r(U) + \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 64L\u03ba \u03c3r\n\u2225Jt\u2225\u2225Kt\u2225 \u00b7 4 \u221a \u03c31\n\u03c3r + \u2225Jt\u2225\n8\u03b12 \u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31\u2225JtK\u22a4t \u2225 \u03c3r \u00b7 4 \u221a \u03c31 \u03c3r + \u2225JtW\u22a4t,\u22a5\u2225\n\u2264\n( 64\u03c3\n3/2 1 L \u03c33r \u00b7 \u221a \u03c31 + 32\u03b12\u03c31 + 256\u03b42k+1\u03c31 \u00b7 \u03c31 \u03c32r\n) \u2225Jt\u2225+ \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 1 2 \u2225Jt\u2225+ \u2225JtW\u22a4t,\u22a5\u2225. (D.54)\nThe last inequality holds because\n\u03b42k+1 = O(\u03ba\u22124 log\u22121( \u221a \u03c31/n\u03b1)), \u03b1 \u2264 O(\u03c3r/ \u221a \u03c31)\nHence, by the inequality (D.54), we can get\n\u2225Jt\u2225 \u2264 2\u2225JtW\u22a4t,\u22a5\u2225 = O(2 \u221a \u03b1\u03c3 1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03ba2 \u221a \u03c31). (D.55)\nThus, (D.43) holds during Phase 3.\nProof of Eq.(D.49) Now we prove the inequality (D.49). We consider the changement of Kt. We have\nKt+1 = Kt(I \u2212 U\u22a4t Ut \u2212 J\u22a4t Jt) + E3(FtG\u22a4t \u2212 \u03a3)Ut + E4(FtG\u22a4t \u2212 \u03a3)Jt\nNow consider Kt+1W\u22a4t,\u22a5, we can get\nKt+1W \u22a4 t,\u22a5 = Kt(I \u2212 \u03b7W\u22a4t \u03a32Wt \u2212 J\u22a4t Jt)W\u22a4t,\u22a5 + \u03b7E3(FtG\u22a4t \u2212 \u03a3)UtW\u22a4t,\u22a5 + \u03b7E4(FtG\u22a4t \u2212 \u03a3)JtW\u22a4t,\u22a5\n= KtW \u22a4 t,\u22a5 \u2212 \u03b7KtJ\u22a4t JtW\u22a4t,\u22a5 + \u03b7E4(FtG\u22a4t \u2212 \u03a3)JtW\u22a4t,\u22a5 = KtW \u22a4 t,\u22a5 \u2212 \u03b7KtW\u22a4t,\u22a5Wt,\u22a5J\u22a4t JtW\u22a4t,\u22a5 \u2212 \u03b7KtW\u22a4t WtJ\u22a4t JtW\u22a4t,\u22a5 + \u03b7E4(FtG\u22a4t \u2212 \u03a3)JtW\u22a4t,\u22a5\nHence, by the Eq.(D.50),\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5J\u22a4t JtW\u22a4t,\u22a5)\u2225+ 64\u03b7L\u03ba3/2\n\u03c3 3/2 r\n\u2225JtK\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\u2225Jt\u2225+ 4\u03b7\u03b42k+1Mt\u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5J\u22a4t JtW\u22a4t,\u22a5)\u2225+ 64\u03b7L\u03ba3/2\n\u03c3 3/2 r\n\u2225JtK\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\u2225Jt\u2225\n+ 16\u03c31\u03b7L\n\u03c32r \u2225JtK\u22a4t \u2225\u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5J\u22a4t JtW\u22a4t,\u22a5)\u2225+ 80\u03b7L\u03ba2\n\u03c3r \u2225JtK\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\nThe second inequality uses the fact that \u03b42k+1 \u2264 1/16 and (D.50). The last inequality uses the fact that \u2225Jt\u2225 \u2264 \u221a \u03c31. Note that \u03bbmin(\u2206t) \u2265 \u03b12/8 \u00b7 I , then multiply the W\u22a4t,\u22a5, we can get\nWt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5V \u22a4t VtW\u22a4t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2265\n\u03b12\n8 \u00b7 I.\nHence,\nWt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2265\n\u03b12\n8 \u00b7 I.\nThus, define \u03d5t = Wt,\u22a5J\u22a4t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5, then we can get\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5(I \u2212Wt,\u22a5J\u22a4t JtW\u22a4t,\u22a5)\u2225+ 80L\u03ba2\n\u03c3r \u2225JtK\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5(I \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 80L\u03ba2\n\u03c3r \u2225JtK\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\nDefine loss Lt = \u2225JtK\u22a4t \u2225. Note that\nLt = \u2225JtK\u22a4t \u2225 = \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t + JtW\u22a4t WtK\u22a4t \u2225 \u2264 \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225+ \u2225JtW\u22a4t WtK\u22a4t \u2225\n\u2264 \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225+ \u221a \u03c31 \u00b7 64L\u03ba3/2\n\u03c3 3/2 r\n\u2225JtK\u22a4t \u2225 (D.56)\n\u2264 \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225+ Lt 2 .\nThe Eq.(D.56) holds by Eq.(D.50) and \u2225W\u22a4t \u2225 = 1, and the last inequality holds by \u03b42k+1 = O(\u03ba4). Hence,\n\u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u2265 Lt/2. (D.57) Similarly,\n\u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u2264 2Lt (D.58) Then,\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225.\nIf \u2225JtW\u22a4t,\u22a5\u2225 \u2264 10\u03ba\u03b1, we can get\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225\u2225(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5\u2225 ( 1\u2212 \u03b7\u03b1 2\n8\n) + 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5\u2225\u2225Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225 \u00b7 ( 1\u2212 \u03b7\u03b1 2\n8\n) + 160\u03b7L\u03ba2\n\u03c3r 100\u03ba2\u03b12\u2225KtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225 \u00b7 ( 1\u2212 \u03b7\u03b1 2\n16\n) (D.59)\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225 \u00b7\n( 1\u2212\n\u03b7\u2225JtW\u22a4t,\u22a5\u2225 1600\u03ba2\n) (D.60)\nby choosing \u03b42k+1 \u2264 O(\u03ba\u22125). Now if \u2225JtW\u22a4t,\u22a5\u2225 \u2265 10\u03ba\u03b1,\nWt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212Wt,\u22a5V \u22a4t VtW\u22a4t,\u22a5 \u2264 2\u03b12 \u00b7 I\nWt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2264 2\u03b12 \u00b7 I +Wt,\u22a5(Ut \u2212 Vt)\u22a4(Ut \u2212 Vt)W\u22a4t,\u22a5\nHence,"
        },
        {
            "heading": "If \u2225JtW\u22a4t,\u22a5\u2225 \u2265 10\u03ba\u03b1, then",
            "text": "\u2225JtWt,\u22a5\u22252 = \u2225Wt,\u22a5J\u22a4t JtW\u22a4t,\u22a5\u2225 \u2264 \u2225Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5\u2225+ ( 2\u03b1+ 8\u03b12 \u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31Lt\n\u03c3r )2 \u2264 \u2225Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5\u2225+ ( 2\u03b1+ 8\u03b12 \u221a \u03c31 + 64\u03b42k+1 \u221a \u03c31\u2225JtW\u22a4t,\u22a5\u2225 \u00b7 \u221a \u03c31\n\u03c3r )2 \u2264 \u2225Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5\u2225+ (10\u03b1+ 64\u03b42k+1\u03ba\u2225JtW\u22a4t,\u22a5\u2225)2 \u2264 \u2225Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5\u2225+ (1/10\u03ba+ 64\u03b42k+1\u03ba) \u00b7 \u2225JtW\u22a4t,\u22a5\u22252 \u2264 \u2225Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5\u2225+ (1/2) \u00b7 \u2225JtW\u22a4t,\u22a5\u22252.\nThus, \u2225KtW\u22a4t,\u22a5\u2225 \u2265 \u2225JtW\u22a4t,\u22a5\u2225/ \u221a 2 \u2265 \u2225JtW\u22a4t,\u22a5\u2225/2.\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225\u2225(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\nThen, if we denote K \u2032 = KtW\u22a4t,\u22a5, then we know \u2225K \u2032(1\u2212\u03b7(K \u2032)\u22a4K \u2032)\u2225 \u2264 (1\u2212\u03b7 \u03c321(K \u2032) 2 )\u2225K\n\u2032\u2225. Let K \u2032 = A\u2032\u03a3\u2032W \u2032\n\u2225K \u2032(1\u2212 \u03b7(K \u2032)\u22a4K \u2032)\u2225 = \u2225A\u2032\u03a3\u2032W \u2032(I \u2212 \u03b7(W \u2032)\u22a4(\u03a3\u2032)2W \u2032)\u2225 = \u2225\u03a3\u2032(I \u2212 \u03b7(\u03a3\u2032)2)\u2225\nLet \u03a3\u2032ii = \u03b6i for i \u2264 r, then \u03a3\u2032(I\u2212\u03b7(\u03a3\u2032)2)ii = \u03b6i\u2212\u03b7\u03b63i , then by the fact that \u03b61 = \u03c31(KtW\u22a4t,\u22a5) \u2264 1, we can have \u03b61 \u2212 \u03b7\u03b631 = max1\u2264i\u2264r \u03b6i \u2212 \u03b7\u03b63i and then\n\u2225\u03a3(I \u2212 \u03b7\u03a32)\u2225 = (1\u2212 \u03b7\u2225K \u2032\u22252)\u2225K \u2032\u2225. Hence,\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2212 \u03b7\u03d5t)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5(I \u2212 \u03b7Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5)\u2225+ 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225\n( 1\u2212 \u03b7 \u2225KtW\u22a4t,\u22a5\u22252\n2\n) + 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5\u2225\u2225Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225\n( 1\u2212 \u03b7 \u2225JtW\u22a4t,\u22a5\u22252\n8\n) + 160\u03b7L\u03ba2\n\u03c3r \u2225JtW\u22a4t,\u22a5\u2225\u2225Wt,\u22a5K\u22a4t \u2225 \u00b7 \u2225JtW\u22a4t,\u22a5\u2225\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225\n( 1\u2212 \u03b7 \u2225JtW\u22a4t,\u22a5\u22252\n16\n) (D.61)\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225 ( 1\u2212 4\u03b7\u03ba2\u03b12 ) . (D.62)\nThe fifth inequality is because \u03b42k+1 = O(\u03ba\u22124). Thus, for all cases, by Eq.(D.59), (D.60), (D.62) and (D.61), we have\n\u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 \u2225KtW\u22a4t,\u22a5\u2225 \u00b7min\n{( 1\u2212 \u03b7\u03b1 2\n4\n) , ( 1\u2212 \u03b7\u2225JtW\u22a4t,\u22a5\u22252\n1600\u03ba2\n)}\n\u2264 \u2225KtW\u22a4t,\u22a5\u2225 \u00b7 ( 1\u2212 \u03b7\u03b1 2\n8\n) \u00b7 ( 1\u2212 \u03b7\u2225JtW\u22a4t,\u22a5\u22252\n3200\u03ba2\n) , (D.63)\nwhere we use the inequality max{a, b} \u2264 \u221a ab. Now we prove the following claim:\n\u2225Kt+1W\u22a4t+1,\u22a5\u2225 \u2264 \u2225Kt+1W\u22a4t,\u22a5\u2225 \u00b7 ( 1 +O(\u03b7\u03b42k+1\u2225JtW\u22a4t,\u22a5\u22252/\u03c33/2r ) ) . (D.64)\nFirst consider the situation that \u2225JtW\u22a4t,\u22a5\u2225 \u2264 10\u03ba\u03b1. We start at these two equalities:\nKt+1 = Kt+1W \u22a4 t,\u22a5Wt,\u22a5 +Kt+1W \u22a4 t Wt Kt+1 = Kt+1W \u22a4 t+1,\u22a5Wt+1,\u22a5 +Kt+1W \u22a4 t+1Wt+1.\nThus, we have Kt+1W \u22a4 t,\u22a5Wt,\u22a5W \u22a4 t+1,\u22a5 +Kt+1W \u22a4 t WtW \u22a4 t+1,\u22a5 = Kt+1W \u22a4 t+1,\u22a5\nConsider \u2225WtW\u22a4t+1,\u22a5\u2225 = \u2225Wt+1,\u22a5W\u22a4t \u2225\n= \u2225Wt+1,\u22a5U\u22a4t (UtU\u22a4t )\u22121/2\u2225 = \u2225Wt+1,\u22a5U\u22a4t \u2225\u2225(UtU\u22a4t )\u22121/2\u2225 \u2264 \u2225Wt+1,\u22a5\u2225\u2225Ut+1 \u2212 Ut\u2225 \u00b7 \u03c3r(U)\u22121 \u2264 4 \u221a \u03c31\n\u03c3r \u00b7 \u03b7 \u00b7 (2\n\u221a \u03c31 \u00b7Mt + 2\u03b42k+1 \u00b7 (Lt + 3Mt) \u00b7 2 \u221a \u03c31)\n\u2264 4 \u221a \u03c31\n\u03c3r \u00b7 \u03b7 \u00b7 (3\n\u221a \u03c31 \u00b7Mt + 2\u03b42k+1 \u00b7 Lt)\n\u2264 4 \u221a \u03c31\n\u03c3r \u00b7 \u03b7(\n48L\u03ba \u221a \u03c31\n\u03c3r \u2225JtK\u22a4t \u2225+ 2\n\u221a \u03c31\u03b42k+1 \u00b7 Lt)\n\u2264 C\u03b7(\u03b42k+1\u03ba4 + \u03b12\u03ba2/\u03c3r)\u2225JtK\u22a4t \u2225. for some constant C. Also, note that \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 Lt + 3Mt \u2264 4Lt,\n\u2225Kt+1W\u22a4t \u2225 = \u2225(Kt+1 \u2212Kt)W\u22a4t \u2225+ \u2225KtW\u22a4t \u2225 \u2264 \u2225\u03b7Kt(U\u22a4t Ut + J\u22a4t Jt)W\u22a4t \u2225+ \u03b7\u03b42k+1 \u00b7 (4Lt) \u00b7 2 \u221a \u03c31 + \u2225KtW\u22a4t \u2225\n\u2264 \u2225\u03b7KtJ\u22a4t JtW\u22a4t \u2225+ 8 \u221a \u03c31\u03b7\u03b42k+1 \u00b7 Lt + 64L\u03ba3/2\n\u03c3 3/2 r\nLt\n\u2264 \u03b7Lt\u2225JtW\u22a4t \u2225+ 8 \u221a \u03c31\u03b7\u03b42k+1 \u00b7 Lt + 64L\u03ba3/2\n\u03c3 3/2 r\nLt\n\u2264 Lt \u00b7 (\u03b7 \u00b7 \u221a \u03c31 + 8 \u221a \u03c31\u03b7\u03b42k+1 + 64L\u03ba3/2\n\u03c3 3/2 r\n)\n\u2264 1 4 \u221a \u03c31 Lt\n\u2264 1 4 \u2225Kt\u2225\nand \u2225Kt+1W\u22a4t,\u22a5\u2225 \u2265 \u2225KtW\u22a4t,\u22a5\u2225 \u2212 \u2225(Kt+1 \u2212Kt)W\u22a4t,\u22a5\u2225\n\u2265 1 2 \u2225Kt\u2225 \u2212 \u03b7\u2225Kt(U\u22a4t Ut + J\u22a4t Jt)W\u22a4t \u2225 \u2212 8 \u221a \u03c31\u03b7\u03b42k+1 \u00b7 Lt \u2265 1 2 \u2225Kt\u2225 \u2212 \u03b7Lt\u2225JtW\u22a4t \u2225 \u2212 8 \u221a \u03c31\u03b7\u03b42k+1 \u00b7 Lt \u2265 \u2225Kt\u2225( 1\n2 \u2212 \u03b7\u2225Jt\u2225 \u00b7 \u2225JtW\u22a4t \u2225 \u2212 8\n\u221a \u03c31\u03b7\u03b42k+1 \u00b7 \u2225Jt\u2225)\n\u2265 \u2225Kt\u2225( 1\n2 \u2212 \u03b7\u03c31 \u2212 8\u03b7\u03b42k+1\u03c31)\n\u2265 1 4 \u2225Kt\u2225 \u2265 \u2225Kt+1W\u22a4t \u2225\nHere, we use the fact that \u03b7 \u2264 1/\u03c31, \u03b42k+1 \u2264 1/32 and \u2225Jt\u2225 \u2264 \u221a \u03c31. Hence, we have\n\u2225Kt+1W\u22a4t+1,\u22a5\u2225 \u2264 \u2225Kt+1W\u22a4t,\u22a5\u2225\u2225Wt,\u22a5W\u22a4t+1,\u22a5\u2225+ \u2225Kt+1W\u22a4t \u2225\u2225WtW\u22a4t+1,\u22a5\u2225 \u2264 \u2225Kt+1W\u22a4t,\u22a5\u2225+ \u2225Kt+1W\u22a4t,\u22a5\u2225 \u00b7 C\u03b7(\u03b42k+1\u03ba4 + \u03b12\u03ba2/\u03c3r)Lt \u2264 ( 1 + C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)Lt ) \u2225Kt+1W\u22a4t,\u22a5\u2225\n\u2264 ( 1 + 2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5Wt,\u22a5K\u22a4t \u2225 ) \u2225Kt+1W\u22a4t,\u22a5\u2225\n\u2264 ( 1 + 2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5\u2225\u2225Wt,\u22a5K\u22a4t \u2225 ) \u2225Kt+1W\u22a4t,\u22a5\u2225\nThe inequality on the fourth line is because Eq.(D.57).\nNote that\nWt,\u22a5J \u22a4 t JtW \u22a4 t,\u22a5 \u2212Wt,\u22a5K\u22a4t KtW\u22a4t,\u22a5 \u2265\n\u03b12\n8 \u00b7 I.\nThus, \u2225KtW\u22a4t,\u22a5\u2225 \u2264 \u2225JtW\u22a4t,\u22a5\u2225 and \u2225Kt+1W\u22a4t+1,\u22a5\u2225 \u2264 ( 1 + 2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5\u2225\u2225Wt,\u22a5K\u22a4t \u2225 ) \u2225Kt+1W\u22a4t,\u22a5\u2225\n\u2264 ( 1 + 2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5\u22252 ) \u2225Kt+1W\u22a4t,\u22a5\u2225 (D.65)\nBy inequalities (D.63) and (D.65), we can get\n\u2225Kt+1W\u22a4t+1,\u22a5\u2225 \u2264 ( 1 + 2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5\u22252 ) \u2225Kt+1W\u22a4t,\u22a5\u2225 \u2264 ( 1 + 2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5\u22252 ) \u00b7 ( 1\u2212 \u03b7\u03b1 2\n8\n) \u00b7 ( 1\u2212 \u03b7\u2225JtW\u22a4t,\u22a5\u22252\n3200\u03ba2\n) \u2225KtW\u22a4t,\u22a5\u2225\n\u2264 ( 1\u2212 \u03b7\u03b1 2\n8\n) \u2225KtW\u22a4t,\u22a5\u2225.\nThe last inequality is because\n2C\u03b7(\u03b42k+1\u03ba 4 + \u03b12\u03ba2/\u03c3r)\u2225JtW\u22a4t,\u22a5\u22252 \u2264\n\u03b7\u2225JtW\u22a4t,\u22a5\u22252\n3200\u03ba2\nby choosing\n\u03b42k+1 = O(\u03ba\u22126) (D.66) and\n\u03b1 = O(\u03ba\u22122 \u00b7 \u221a \u03c3r). (D.67)\nThus, we can prove \u2225KtW\u22a4t,\u22a5\u2225 decreases at a linear rate.\nNow we have completed all the proofs of the induction hypotheses. Hence,\n\u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 2\u2225JtK\u22a4t \u2225 \u2264 4\u2225K\u22a4t \u2225 \u00b7 \u221a \u03c31\n\u2264 4\u2225KtW\u22a4t,\u22a5\u2225 \u221a \u03c31\n\u2264 4\u2225KtW\u22a4T2,\u22a5\u2225 \u00b7 \u221a \u03c31\n( 1\u2212 \u03b7\u03b1 2\n8 )t\u2212T2 \u2264 4\u2225KT2\u2225 \u00b7 \u221a \u03c31 ( 1\u2212 \u03b7\u03b1 2\n8 )t\u2212T2 \u2264 2\u03c31 ( 1\u2212 \u03b7\u03b1 2\n8\n)t\u2212T2 (D.68)\nNow combining three phases (D.25), (D.42) and (D.68), if we denote t\u22172 + T0 = T \u2032 = O\u0303(1/\u03b7\u03c3r), then for any round T \u2265 4T \u2032, Phase 1 and Phase 3 will take totally at least T \u2212 T \u2032 rounds. Now we consider two situations.\nSituation 1: Phase 1 takes at least 3(T\u2212T \u2032)\n4 rounds. Then, by (D.25), suppose Phase 1 starts at T0 rounds and terminates at T1 rounds, we will have\n\u2225FT1G\u22a4T1 \u2212 \u03a3\u2225 \u2264 \u03c32r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31 )T1\u2212T0 \u2264 \u03c3 2 r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n)T/2 . (D.69)\nThe last inequality uses the fact that T \u2265 4T \u2032 and\nT1 \u2212 T0 \u2265 3(T \u2212 T \u2032)\n4 \u2265 T/2\nThen, by (D.32), (D.30), (D.44) and (D.45), we know that\n\u2225FTG\u22a4T \u2212 \u03a3\u2225 \u2264 4\u2225JTK\u22a4T \u2225 \u2264 4\u2225JT1K\u22a4T1 \u2212 \u03a3\u2225 \u00b7 ( 1 +\n\u03b7\u03c32r 128\u03c31 )T\u2212T1 \u2264 4\u2225FT1G\u22a4T1 \u2212 \u03a3\u2225 \u00b7 ( 1 +\n\u03b7\u03c32r 128\u03c31 )T\u2212T1 \u2264 4\u2225FT1G\u22a4T1 \u2212 \u03a3\u2225 \u00b7 ( 1 +\n\u03b7\u03c32r 128\u03c31\n)T/2 (D.70)\nThe last inequality uses the fact that T1 \u2212 T0 \u2265 3(T\u2212T \u2032) 4 \u2265 T 2 , which implies that T 2 \u2265 T \u2212 T1 Then, combining with (D.69), we can get\n\u2225FTG\u22a4T \u2212 \u03a3\u2225 \u2264 \u03c32r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n)T/2 \u00b7 ( 1 +\n\u03b7\u03c32r 128\u03c31 )T/2 \u2264 \u03c3 2 r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03c3 2 r\n128\u03c31\n)T/2 (D.71)\n\u2264 \u03c3 2 r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03b1 2\n8\n)T/2 . (D.72)\n(D.71) uses the basic inequality (1 \u2212 2x)(1 + x) \u2264 (1 \u2212 x), and (D.72) uses the fact that \u03b1 = O(\u03ba\u22122\u221a\u03c3r) = O( \u221a \u03ba\u03c3r).\nSituation 2: Phase 3 takes at least T\u2212T \u2032\n4 rounds. Then, by (D.68), suppose Phase 3 starts at round T2, we have\n\u2225FTG\u22a4T \u2212 \u03a3\u2225 \u2264 2\u03c31 ( 1\u2212 \u03b7\u03b1 2\n8 )t\u2212T2 \u2264 2\u03c31 ( 1\u2212 \u03b7\u03b1 2\n8 )(T\u2212T \u2032)/4 \u2264 \u03c3 2 r\n128\u03b12\u03ba\n( 1\u2212 \u03b7\u03b1 2\n8\n)T/8 . (D.73)\nThe last inequality uses the fact that \u03b1 = O(\u03ba\u22122\u221a\u03c3r) = O(\u03ba\u22121 \u221a \u03c3r) and T\u2212T \u2032 4 \u2265 T\u2212T/4\n4 \u2265 T/8. Thus, by \u2225FTG\u22a4T \u2212\u03a3\u22252 \u2264 n \u00b7 \u2225FTG\u22a4T \u2212\u03a3\u22252, we complete the proof by choosing 4T \u2032 = T (1) and c7 = 1/128 2."
        },
        {
            "heading": "E PROOF OF THEOREM 4.3",
            "text": "By the convergence result in (Soltanolkotabi et al., 2023), the following three conditions hold for t = T0.\nmax{\u2225Jt\u2225, \u2225Kt\u2225} \u2264 O ( 2\u03b1+ \u03b42k+1\u03c3 3/2 1 log( \u221a \u03c31/n\u03b1)\n\u03c3r\n) (E.1)\nmax{\u2225Ut\u2225, \u2225Vt\u2225} \u2264 2 \u221a \u03c31 (E.2)\nand \u2225FtGTt \u2212 \u03a3\u2225 \u2264 \u03b11/2\u03c3 3/4 1 \u2264 \u03c3r/2. (E.3)\nThen, we define Mt = max{\u2225UtV \u22a4t \u2212 \u03a3\u2225, \u2225UtK\u22a4t \u2225, \u2225JtV \u22a4t \u2225}, by the same techniques in Section D.4, if we have \u03c32rMt\u22121/64\u03c31 \u2265 (17\u03c31\u03b42k+1 + \u03b12)\u2225Jt\u22121K\u22a4t\u22121\u2225, (E.4) we can prove that\nMt \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n) Mt\u22121. (E.5)\nand max{\u2225Jt\u2225, \u2225Kt\u2225} \u2264 2 \u221a \u03b1\u03c3 1/4 1 + 2C2 log( \u221a \u03c31/n\u03b1)(\u03b42k+1 \u00b7 \u03ba2 \u221a \u03c31) \u2264 \u221a \u03c31\n\u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 \u03c3r/2 max{\u2225Ut\u2225, \u2225Vt\u2225} \u2264 2 \u221a \u03c31.\nNow note that \u2225Ut\u22121K\u22a4t\u22121\u2225 \u2265 \u03bbmin(Ut\u22121) \u00b7 \u2225K\u22a4t\u22121\u2225 = \u03c3r(Ut\u22121) \u00b7 \u2225K\u22a4t\u22121\u2225 \u2265\n\u03c3r 4 \u221a \u03c31 \u00b7 \u2225Kt\u22121\u2225, (E.6)\nNow since \u03b42k+1 = O(\u03ba\u22123) and \u03b1 = O(\u03ba\u22121 \u221a \u03c3r) are small parameters, we can derive the Mt\u2019s lower bound by Mt\u22121 \u2265 \u2225Ut\u22121K\u22a4t\u22121\u2225\n\u2265 \u03c3r 4 \u221a \u03c31 \u00b7 \u2225Kt\u22121\u2225\n\u2265 \u03c3r 4 \u221a \u03c31 \u2225Kt\u22121\u2225 \u00b7 \u2225Jt\u22121\u2225\u221a \u03c31 (E.7)\n\u2265 64\u03c31 \u00b7 17\u03c31\u03b42k+1 + \u03b1\n2\n\u03c32r \u2225Jt\u22121K\u22a4t\u22121\u2225. (E.8)\nHence, (E.4) always holds for t \u2265 T0, and then by (E.5), we will have Mt \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n)t\u2212T0 MT0\n\u2264 ( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n)t\u2212T0 \u2225FT0GTT0\u2225\n\u2264 \u03c3r 2\n\u00b7 ( 1\u2212 \u03b7\u03c3 2 r\n16\u03c31\n)t\u2212T0 .\nThus, we can bound the loss by \u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 \u2225UtV \u22a4t \u2212 \u03a3\u2225+ \u2225JtV \u22a4t \u2225+ \u2225UtK\u22a4t \u2225+ \u2225JtK\u22a4t \u2225\n\u2264 3Mt + \u2225JtK\u22a4t \u2225 \u2264 3Mt +O(2\u03b1+ \u03b42k+1\u03ba \u221a \u03c31 log( \u221a \u03c31/n\u03b1) \u00b7 4 \u221a \u03c31\n\u03c3r Mt\n\u2264 4Mt (E.9) \u2264 2\u03c3r \u00b7 ( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n)t\u2212T0 .\nwhere Eq.(E.9) uses the fact that \u03b42k+1 \u2264 O(\u03ba\u22122 log\u22121( \u221a \u03c31/n\u03b1)) and \u03b1 \u2264 O(\u03c3r/ \u221a \u03c31). Now we can choose T (2) = 2T0, and then by t\u2212 T0 \u2265 t/2 for all t \u2265 T (2), we have\n\u2225FtGTt \u2212 \u03a3\u22252F \u2264 n\u2225FtGTt \u2212 \u03a3\u22252 \u2264 2n\u03c3r \u00b7 ( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n)t\u2212T0 \u2264 2n\u03c3r \u00b7 ( 1\u2212 \u03b7\u03c3 2 r\n64\u03c31\n)t/2 .\n(E.10)\nWe complete the proof."
        },
        {
            "heading": "F PROOF OF THEOREM 5.1",
            "text": "During the proof of Theorem 5.1, we assume \u03b2 satisfy that\nmax{c7\u03b31/6\u03c31/31 , c\u03b4 1/6 2k+1\u03ba 1/6\u03c3 5/12 1 } \u2264 \u03b2 \u2264 c8 \u221a \u03c3r (F.1)\nfor some large constants c7, c and small constant c8. In particular, this requirement means that \u03b3 \u2264 \u03c3r/4. Then, since \u2225A\u2217A(F\u0303T (3)G\u0303\u22a4T (3) \u2212 \u03a3)\u2225 \u2265 1 2\u2225F\u0303T (3)G\u0303 \u22a4 T (3)\n\u2212 \u03a3\u2225 by RIP property and \u03b42k+1 \u2264 1/2, we can further derive \u2225FT (3)G\u22a4T (3) \u2212 \u03a3\u2225 = \u2225F\u0303T (3)G\u0303 \u22a4 T (3) \u2212 \u03a3\u2225 \u2264 \u03c3r/2.\nTo guarantee (F.1), we can use choose \u03b3 to be small enough, i.e., \u03b3 \u226a \u03c31\u03ba\u22122, so that (F.1) holds easily. In the following, we denote \u03b42k+1 = \u221a 2k + 1\u03b4."
        },
        {
            "heading": "F.1 PROOF SKETCH OF THEOREM 5.1",
            "text": "First, suppose we modify the matrix F\u0303T (3) , G\u0303T (3) to FT (3) and GT (3) at t = T (3), then \u2225FT (3)\u22252 = \u03bbmax((FT (3)) \u22a4FT (3)) = \u03b2 2 and \u2225UT (3)\u22252 \u2264 \u03b22. Also, by \u2225F\u0303T (3)\u2225 \u2264 2 \u221a \u03c31, we can get that \u2225GT (3)\u2225 \u2264 \u2225G\u0303T (3)\u2225 \u00b7 \u2225F\u0303 T (3) \u2225 \u03b2 \u2264 \u2225G\u0303T (3)\u2225 \u00b7 2 \u221a \u03c31 \u03b2 is still bounded. Similarly, \u2225VT (3)\u2225 \u2264 \u2225V\u0303T (3)\u2225 \u00b7 2 \u221a \u03c31 \u03b2 and \u2225KT (3)\u2225 \u2264 \u2225K\u0303T (3)\u2225 \u00b7 2 \u221a \u03c31 \u03b2 is still bounded. With these conditions, define St = max{\u2225UtK\u22a4t \u2225, \u2225JtK\u22a4t \u2225} and Pt = max{\u2225JtV \u22a4t \u2225, \u2225UtV \u22a4t \u2212 \u03a3\u2225}. For \u2225Kt+1\u2225, since we can prove \u03bbmin(F\u22a4t Ft) \u2265 \u03b22/2 for all t \u2265 T (3) using induction, with the updating rule, we can bound \u2225Kt+1| as the following\n\u2225Kt+1\u2225 \u2264 \u2225Kt\u2225\u22251\u2212 \u03b7F\u22a4t Ft\u2225+ 2\u03b7\u03b42k+1 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u2225max{\u2225Ut\u2225, \u2225Jt\u2225} (F.2) \u2264 \u2225Kt\u2225 \u00b7 ( 1\u2212 \u03b7\u03b2 2\n2\n) + ( 4\u03b7\u03b42k+1\u03b2 \u00b7 Pt + 4\u03b22\u03b7\u03b42k+1\u2225Kt\u2225 ) . (F.3)\nThe first term of (F.3) ensures the linear convergence, and the second term represents the perturbation term. To control the perturbation term, for Pt, with more calculation (see details in the rest of the section), we have\nPt+1 \u2264 ( 1\u2212 \u03b7\u03c32r/8\u03b22 ) Pt + \u03b7\u2225Kt\u2225 \u00b7 O\u0303 (( \u03b42k+1\u03c31 + \u221a \u03b1\u03c3 7/4 1 ) /\u03b2 ) . (F.4)\nThe last inequality uses the fact that St \u2264 \u2225Kt\u2225 \u00b7max{\u2225Ut\u2225, \u2225Jt\u2225} \u2264 \u2225Kt\u2225 \u00b7 \u2225Ft\u2225 \u2264 \u221a 2\u03b2 \u00b7 \u2225Kt\u2225. Combining (F.4) and (F.3), we can show that Pt+ \u221a \u03c31\u2225Kt\u2225 converges at a linear rate (1\u2212O(\u03b7\u03b22)), since the second term of Eq. (F.4) and Eq.(F.3) contain \u03b42k+1 or \u03b1, which is relatively small and can be canceled by the first term. Hence, \u2225FtG\u22a4t \u2212\u03a3\u2225 \u2264 2Pt + 2St \u2264 2Pt + \u221a 2\u03b2\u2225Kt\u2225 converges at a linear rate."
        },
        {
            "heading": "F.2 PROOF OF THEOREM 5.1",
            "text": "At time t \u2265 T (3), we have \u03c3min(UT (3)VT (3)) \u2265 \u03c3min(\u03a3) \u2212 \u2225UT (3)V \u22a4T (3) \u2212 \u03a3\u2225 \u2265 \u03c3r \u2212 \u03b1 1/2 \u00b7 \u03c3 3/4 1 \u2265 \u03c3r/2. The last inequality holds because \u03b1 = O(\u03ba\u22123/2 \u00b7 \u221a \u03c3r). Then, given that \u2225FT (3)\u22252 = \u03bbmax((FT (3)) \u22a4FT (3)) = \u03b2\n2, we have \u2225UT (3)\u22252 \u2264 \u03b22. Hence, by \u03c31(U) \u00b7 \u03c3r(V ) \u2265 \u03c3r(UV \u22a4), we have\n\u03c3r(VT (3)) \u2265 \u03c3r(UT (3)VT (3)) \u03c31(UT (3)) \u2265 \u03c3r 2\u03b2 .\nAlso, by \u03c3\u20321 = \u2225F\u0303T (3)\u2225 \u2264 2 \u221a \u03c31, we can get\n\u2225GT (3)\u2225 \u2264 \u2225G\u0303T (3)\u2225\u2225B\u03a3\u22121inv\u2225 \u2264 \u2225G\u0303T (3)\u2225 \u00b7 \u03c3\u20321 \u03b2 \u2264 \u2225G\u0303T (3)\u2225 \u00b7 2 \u221a \u03c31 \u03b2 .\nSimilarly, \u2225VT (3)\u2225 \u2264 \u2225V\u0303T (3)\u2225 \u00b7 2 \u221a \u03c31 \u03b2 and \u2225KT (3)\u2225 \u2264 \u2225K\u0303T (3)\u2225 \u00b7 2 \u221a \u03c31 \u03b2 .\nDenote St = max{\u2225UtK\u22a4t \u2225, \u2225JtK\u22a4t \u2225}, Pt = max{\u2225JtV \u22a4t \u2225, \u2225UtV \u22a4t \u2212 \u03a3\u2225}. Now we prove the following statements by induction:\nPt+1 \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n8\u03b22\n) Pt + \u03b7St \u00b7 O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c321 + \u221a \u03b1\u03c3 7/4 1\n\u03b22\n) (F.5)\n\u2225Ft+1G\u22a4t+1 \u2212 \u03a3\u2225 \u2264 \u03b26\n\u03c321\n( 1\u2212 \u03b7\u03b2 2\n2\n)t+1\u2212T (3) \u2264 \u03c3r/2 (F.6)\nmax{\u2225Ft+1\u2225, \u2225Gt+1\u2225} \u2264 4\u03c31/\u03b2 (F.7) \u03b22\n2 I \u2264 F\u22a4t+1Ft+1 \u2264 2\u03b22I (F.8)\n\u2225Kt\u2225 \u2264 O(2 \u221a \u03b1\u03c3 1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03ba2 \u221a \u03c31) \u00b7 2 \u221a \u03c31 \u03b2\n(F.9)\nProof of Eq.(F.5) First, since \u2225Ft\u22252 = \u03bbmax((Ft)\u22a4Ft) \u2264 2\u03b22, we have \u2225Ut\u22252 \u2264 2\u03b22. Then, because \u03c3min(UtVt) \u2265 \u03c3min(\u03a3)\u2212 \u2225UtV \u22a4t \u2212 \u03a3\u2225 \u2265 \u03c3r/2, by \u03c31(U) \u00b7 \u03c3r(V ) \u2265 \u03c3r(UV \u22a4), we have\n\u03c3r(Vt) \u2265 \u03c3r(UtVt) \u03c31(Ut) \u2265 \u03c3r 2\u03b2 .\nwe write down the updating rule as\nUt+1V \u22a4 t+1 \u2212 \u03a3\n= (1\u2212 \u03b7UtU\u22a4t )(UtV \u22a4t \u2212 \u03a3)(1\u2212 \u03b7VtV \u22a4t )\u2212 \u03b7UtK\u22a4t KtV \u22a4t \u2212 \u03b7UtJ\u22a4t JtV \u22a4t +Bt\nwhere Bt contains the O(\u03b72) terms and O(Ei(FtG\u22a4t \u2212 \u03a3)) terms\n\u2225Bt\u2225 \u2264 4\u03b7\u03b42k+1(FtG\u22a4t \u2212 \u03a3)max{\u2225Ft\u22252, \u2225Gt\u22252}+O(\u03b72\u2225FtG\u22a4t \u2212 \u03a3\u22252 max{\u2225Ft\u22252, \u2225Gt\u22252})\nHence, we have\n\u2225Ut+1V \u22a4t+1 \u2212 \u03a3\u2225\n\u2264 (1\u2212 \u03b7\u03c3 2 r\n4\u03b22 )\u2225UtV \u22a4t \u2212 \u03a3\u2225+ \u03b7\u2225UtK\u22a4t \u2225\u2225KtV \u22a4t \u2225+ \u03b7\u2225JtV \u22a4t \u2225\u2225J\u22a4t U\u22a4t \u2225+ \u2225Bt\u2225\n\u2264 (1\u2212 \u03b7\u03c3 2 r\n4\u03b22 )Pt + \u03b7St\u2225Kt\u2225\u2225Vt\u2225+ \u03b7Pt\u2225Jt\u2225\u2225Ut\u2225+ \u2225Bt\u2225\n\u2264 (1\u2212 \u03b7\u03c3 2 r\n4\u03b22 )Pt + \u03b7St \u00b7 4\u03c31 \u03b22\n\u00b7 O ( 2 \u221a \u03b1\u03c3\n1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03ba2 \u221a \u03c31 ) \u00b7 2 \u221a \u03c31 + \u03b7Pt\u03b2 \u00b7 \u03b2\n+ 4\u03b7\u03b42k+1 \u00b7 2(Pt + St) \u00b7 4\u03c31 \u00b7 4\u03c31 \u03b22 +O(\u03b72(Pt + St)2 \u00b7 4\u03c31 \u00b7 4\u03c31 \u03b22 )\n\u2264 ( 1\u2212 \u03b7\u03c3 2 r\n8\u03b22\n) Pt + \u03b7St \u00b7 O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c321 + \u221a \u03b1\u03c3 7/4 1\n\u03b22\n) (F.10)\nThe last inequality uses the fact that\n\u03b22 = O(\u03c31/2r ) \u03b42k+1 = O(\u03ba\u22122)\nPt + St \u2264 2\u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 O(\u03c321/\u03b22) \u2264 1/\u03b7.\nSimilarly, we have\n\u2225Jt+1V \u22a4t+1\u2225 \u2264 ( 1\u2212 \u03b7J\u22a4J ) JV \u22a4(1\u2212 \u03b7V \u22a4V )\u2212 \u03b7JK\u22a4KV \u22a4 \u2212 \u03b7JU\u22a4(UV \u22a4 \u2212 \u03a3) + Ct\nwhere Ct satisfies that\n\u2225Ct\u2225 \u2264 4\u03b7\u03b42k+1(FtG\u22a4t \u2212 \u03a3)max{\u2225Ft\u22252, \u2225Gt\u22252}+O(\u03b72\u2225FtG\u22a4t \u2212 \u03a3\u2225max{\u2225Ft\u22252, \u2225Gt\u22252})\n\u2264 4\u03b7\u03b42k+1 \u00b7 2(Pt + St) \u00b7 16\u03c321 \u03b22 +O(\u03b72(Pt + St) \u00b7 \u03c31 \u00b7 \u03c31 \u03b22 ).\nThus, similar to Eq.(F.10), we have\n\u2225Jt+1V \u22a4t+1\u2225 \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n8\u03b22\n) Pt + \u03b7St \u00b7 O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c321 + \u221a \u03b1\u03c3 7/4 1\n\u03b22\n) .\nHence, we have\nPt+1 \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n8\u03b22\n) Pt + \u03b7St \u00b7 O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c321 + \u221a \u03b1\u03c3 7/4 1\n\u03b22\n) .\nProof of Eq.(F.6) We have St \u2264 \u2225Kt\u2225 \u00b7max{\u2225Ut\u2225, \u2225Jt\u2225} \u2264 \u2225Kt\u2225 \u00b7 \u2225Ft\u2225 \u2264 \u221a 2\u03b2 \u00b7 \u2225Kt\u2225. So the inequality above can be rewritten as\nPt+1 \u2264 ( 1\u2212 \u03b7\u03c3 2 r\n8\u03b22\n) Pt + \u03b7 \u221a 2\u03b2 \u00b7 \u2225Kt\u2225 \u00b7 O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c321 + \u221a \u03b1\u03c3 7/4 1\n\u03b22\n)\n= ( 1\u2212 \u03b7\u03c3 2 r\n8\u03b22\n) Pt + \u03b7\u2225Kt\u2225 \u00b7 O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c321 + \u221a \u03b1\u03c3 7/4 1\n\u03b2 ) Also, for Kt+1, we have\n\u2225Kt+1\u2225 = \u2225Kt\u2225\u2225(1\u2212 \u03b7F\u22a4t Ft)\u2225+ 2\u03b42k+1 \u00b7 \u2225FtG\u22a4t \u2212 \u03a3\u2225max{\u2225Ut\u2225, \u2225Jt\u2225}\n\u2264 \u2225Kt\u2225(1\u2212 \u03b7\u03b22\n2 ) + 2\u03b7\u03b42k+1 \u00b7 (Pt + St) \u00b7\n\u221a 2\u03b2\n\u2264 \u2225Kt\u2225(1\u2212 \u03b7\u03b22\n2 ) + 2\u03b7\u03b42k+1 \u00b7 Pt \u00b7\n\u221a 2\u03b2 + 2\u03b7\u03b42k+1 \u00b7 \u221a 2\u03b2\u2225Kt\u2225 \u00b7 \u221a 2\u03b2\n= \u2225Kt\u2225(1\u2212 \u03b7\u03b22\n2 ) + 4\u03b7\u03b42k+1 \u00b7 \u03b2Pt + 4\u03b22\u03b7\u03b42k+1 \u00b7 \u2225Kt\u2225\nThus, we can get\nPt+1 + \u221a \u03c31\u2225Kt+1\u2225\n\u2264 max{1\u2212 \u03b7\u03c3 2 r 8\u03b22 , 1\u2212 \u03b7\u03b2 2 2 }(Pt + \u2225Kt\u2225)\n+ \u03b7max { O ( log( \u221a \u03c31/n\u03b1)\u03b42k+1\u03ba 2\u03c3 3/2 1 + \u221a \u03b1\u03c3 5/4 1\nc\n) + 4\u03b22\u03b42k+1, 4\u03b2 \u221a \u03c31\u03b42k+1 } \u00b7 (Pt + \u221a \u03c31\u2225Kt\u2225)\n\u2264 (1\u2212 \u03b7\u03b2 2\n4 )(Pt +\n\u221a \u03c31\u2225Kt\u2225).\nThe last inequality uses the fact that \u03b2 \u2264 O(\u03c31/2r ) and\n\u03b42k+1 \u2264 O(\u03b2/ \u221a \u03c31 log( \u221a \u03c31/n\u03b1)). (F.11)\nHence,\n\u2225Kt\u2225 \u2264 (PT (3)/ \u221a \u03c31 + \u2225KT (3)\u2225) \u00b7 ( 1\u2212 \u03b7\u03b2 2\n2 )t\u2212T (3) \u2264 \u2225KT (3)\u2225+ \u2225FtG\u22a4t \u2212 \u03a3\u2225/ \u221a \u03c31 \u2264 O( \u221a \u03b1\u03c3\n1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03ba2 \u221a \u03c31) + \u03b1\n1/2 \u00b7 \u03c31/41 = O( \u221a \u03b1\u03c3\n1/4 1 + \u03b42k+1 log( \u221a \u03c31/n\u03b1) \u00b7 \u03ba2 \u221a \u03c31)\nHence, Pt + \u221a \u03c31\u2225Kt\u2225 is linear convergence. Hence, by \u03b2 \u2264 \u221a \u03c31,\n\u2225Ft+1G\u22a4t+1 \u2212 \u03a3\u2225 \u2264 2Pt+1 + 2St+1 \u2264 2Pt+1 + \u221a 2\u03b2\u2225Kt+1\u2225\n\u2264 (2 + \u221a 2\u03b2/ \u221a \u03c31)(Pt+1 + \u221a \u03c31\u2225Kt+1\u2225)\n\u2264 4(PT (3) + \u221a \u03c31\u2225KT (3)\u2225) \u00b7 ( 1\u2212 \u03b7\u03b2 2\n2 )t+1\u2212T (3) Last, note that by \u03b2 \u2265 c7(\u03b31/6\u03c31/31 ) and \u03b2 \u2265 c\u03b4 1/6 2k+1\u03ba 1/6\u03c3 5/12 1 log( \u221a \u03c31/n\u03b1)\n1/6, by choosing for some constants c7 and c, by choosing large c\u2032 and c7 = 26, we can get\n\u03b3 \u2264 \u03b2 6\n2\u03c321 ,\n\u221a \u03c31 \u00b7 O(log( \u221a \u03c31/n \u221a \u03b1)\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) \u00b7 (2 \u221a \u03c31/\u03b2) \u2264 \u03b26\n2\u03c321\nand\nPT (3) + \u221a \u03c31\u2225KT (3)\u2225 \u2264 \u03b3 + \u221a \u03c31 \u00b7 O(log( \u221a \u03c31/n \u221a \u03b1)\u03b42k+1 \u00b7 \u03c33/21 /\u03c3r) \u00b7 (2 \u221a \u03c31/\u03b2) \u2264 \u03b26/\u03c321\nwe have\n\u2225Ft+1G\u22a4t+1 \u2212 \u03a3\u2225 \u2264 ( \u03b26\n\u03c321\n)( 1\u2212 \u03b7\u03b2 2\n2\n)t+1\u2212T (3) (F.12)\nProof of Eq.(F.7) Note that we have max{\u2225FT (3)\u2225, \u2225GT (3)\u2225} \u2264 4 \u221a \u03c31 \u00b7 \u221a \u03c31/\u03b2 = 4\u03c31/\u03b2. Now suppose max{\u2225Ft\u2032\u2225, \u2225Gt\u2032\u2225} \u2264 4 \u221a \u03c31 \u00b7 \u221a \u03c31/\u03b2 = 4\u03c31/\u03b2 for all t\u2032 \u2208 [T (3), t], then the changement of Ft+1 and Gt+1 can be bounded by \u2225Ft+1 \u2212 FT (3)\u2225 \u2264 \u03b7 t\u2211\nt\u2032=T (3)\n2\u2225Ft\u2032Gt\u2032 \u2212 \u03a3\u2225\u2225Gt\u2032\u2225 \u2264 \u03b7 \u00b7 2 \u00b7 ( \u03b26\n\u03c321 + \u03c3r 2 ) \u00b7 2 \u03b7\u03b22 4\u03c31 \u03b2 \u2264 16\u03b2 3 \u03c31 + 8\u03c321 \u03b23\n\u2225Gt \u2212GT (3)\u2225 \u2264 \u03b7 t\u22121\u2211\nt\u2032=T (3)\n2\u2225Ft\u2032Gt\u2032 \u2212 \u03a3\u2225\u2225Ft\u2032\u2225 \u2264 16\u03b23\n\u03c31 + 8\u03c321 \u03b23\nThen, by the fact that \u03b2 \u2264 O(\u03c3\u22121/21 ), we can show that\n\u2225Ft+1\u2225 \u2264 \u2225FT (3)\u2225+ \u2225Ft+1 \u2212 FT (3)\u2225 \u2264 2\u03c31 \u03b2 + 16\u03b23 \u03c31 + 8\u03c321 \u03b23 \u2264 4\u03c31 \u03b2 ,\n\u2225Gt+1\u2225 \u2264 \u2225GT (3)\u2225+ \u2225Gt+1 \u2212GT (3)\u2225 \u2264 2\u03c31 \u03b2 + 16c3 \u03c31 + 8\u03c321 \u03b23 \u2264 4\u03c31 \u03b2 .\nProof of Eq.(F.8) Moreover, we have\n\u03c3k(Ft+1) \u2265 \u03c3k(FT (3))\u2212 \u03c3max(Ft+1 \u2212 FT (3)) = \u03c3k(FT (3))\u2212 \u2225Ft+1 \u2212 FT (3)\u2225\n\u2265 \u03b2 \u2212 16\u03b2 3\n\u03c31\n\u2265 \u03b2/ \u221a 2,\nand\n\u2225Ft\u2225 \u2264 \u2225FT (3)\u2225+ \u2225Ft \u2212 FT (3)\u2225 \u2264 \u03b2 + 16\u03b23\n\u03c31 \u2264\n\u221a 2\u03b2.\nThe last inequality is because \u03b2 \u2264 O(\u03c3\u22121/21 ). Hence, since Ft+1 \u2208 Rn\u00d7k, we have\n\u03b22\n2 I \u2264 F\u22a4t+1Ft+1 \u2264 2\u03b22I (F.13)\nThus, we complete the proof."
        },
        {
            "heading": "G TECHNICAL LEMMA",
            "text": ""
        },
        {
            "heading": "G.1 PROOF OF LEMMA B.1",
            "text": "Proof. We only need to prove with high probability,\nmax i,j\u2208[n]\ncos2 \u03b8xj ,xk \u2264 c\nlog2(r \u221a \u03c31/\u03b1)(r\u03ba)2 . (G.1)\nIn fact, since cos2 \u03b8xj ,xk = sin 2(\u03c02 \u2212 \u03b8xj ,xk) \u2264 (\u03c0/2\u2212 \u03b8xj ,xk) 2, we have\nP [ |\u03c0/2\u2212 \u03b8xj ,xk | > O ( \u221a c\nlog(r \u221a \u03c31/\u03b1)r\u03ba\n)] \u2265 P [ cos2 \u03b8xj ,xk > O ( c\nlog2(r \u221a \u03c31/\u03b1)(r\u03ba)2\n)] .\n(G.2)\nMoreover, for any m > 0, by Lemma G.1,\nP [ |\u03c0/2\u2212 \u03b8xj ,xk | > m ] \u2264 O\n( (sin(\u03c02 \u2212m)) k\u22122\n1/ \u221a k \u2212 2\n) = O (\u221a k \u2212 2(cosm)k\u22122 ) (G.3)\n\u2264 O (\u221a k(1\u2212m2/4)k\u22122 )\n(G.4) \u2264 O (\u221a k exp ( \u2212 4k m2 )) . (G.5)\nThe second inequality uses the fact that cosx \u2264 1\u2212 x2/4. Then, if we choose\nm =\n\u221a c\nlog(r \u221a \u03c31/\u03b1)r\u03ba\nand let k \u2265 16/m4 = 16 log 4(r\n\u221a \u03c31/\u03b1)(r\u03ba) 4\nc2 , we can have P [ cos2 \u03b8xj ,xk > m 2 ] \u2264 P [ |\u03c0/2\u2212 \u03b8xj ,xk | > m ] (G.6)\n\u2264 O ( k exp ( \u2212m 2k\n4\n)) (G.7)\n\u2264 O ( k exp ( \u2212 \u221a k ))\n(G.8)\nThus, by taking the union bound over j, k \u2208 [n], there is a constant c2 such that, with probability at least 1\u2212 c4n2k exp(\u2212 \u221a k), we have\n\u03b80 \u2264 c\nlog2(r \u221a \u03c31/\u03b1)(r\u03ba)2 . (G.9)"
        },
        {
            "heading": "G.2 PROOF OF LEMMA B.2",
            "text": "Proof. Since xi = \u03b1/ \u221a k \u00b7 x\u0303i, where each element in x\u0303i is sampled from N (0, 1). By Theorem 3.1 in Vershynin (2018), there is a constant c such that\nP [ |\u2225x\u03030i \u222522 \u2212 k| \u2265 t ] \u2264 2 exp(\u2212ct) (G.10)\nHence, choosing t = (1\u2212 1\u221a 2 )k, we have\nP[\u2225x\u03030i \u222522 \u2208 [k/ \u221a 2, \u221a 2k]] \u2264 P[|\u2225x\u03030i \u222522 \u2212 k| \u2265 t] \u2264 2 exp(\u2212ct) \u2264 2 exp(\u2212ck/4)\nHence,\nP [ \u2225x0i \u22252 \u2208 [\u03b12/2, 2\u03b12] ] = P [ \u2225x\u03030i \u22252 \u2208 [k/ \u221a 2, \u221a 2k] ] \u2264 2 exp(\u2212ck/4). (G.11)\nBy taking the union bound over i \u2208 [n], we complete the proof.\nLemma G.1. Assume x, y \u2208 Rn are two random vectors such that each element is independent and sampled from N (0, 1), then define \u03b8 as the angle between x, y, we have\nP (\u2223\u2223\u2223\u03b8 \u2212 \u03c0\n2 \u2223\u2223\u2223 \u2264 m) \u2264 3\u03c0\u221an\u2212 2(sin(\u03c0/2\u2212m))n\u22122 4 \u221a 2 . (G.12)\nProof. First, it is known that x\u2225x\u2225 and y \u2225y\u2225 are independent and uniformly distributed over the sphere Sn\u22121. Thus, without loss of generality, we can assume x and y are independent and uniformly distributed over the sphere.\nNote that \u03b8 \u2208 [0, \u03c0], and the CDF of \u03b8 is\nf(\u03b8) = \u0393(n/2) sinn\u22122(\u03b8)\n\u221a \u03c0\u0393(n\u221212 )\n(G.13)\nThen, we have\nP (\u2223\u2223\u2223\u03b8 \u2212 \u03c0\n2 \u2223\u2223\u2223 > m) = 1\u2212 \u222b \u03c0/2+m\u03c0/2\u2212m sinn\u22122 \u03b8d\u03b8\u222b \u03c0 0 sinn\u22122 \u03b8d\u03b8 = \u222b \u03c0/2\u2212m 0 sinn\u22122 \u03b8d\u03b8\u222b \u03c0/2 0 sinn\u22122 \u03b8d\u03b8 (G.14)\n\u2264 (\u03c0/2) \u00b7 sin n\u22122(\u03c0/2\u2212m)\u222b \u03c0/2\n0 cosn\u22122 \u03b8d\u03b8\n(G.15)\n\u2264 (\u03c0/2 \u00b7 (\u03c0/2\u2212m) n\u22122)\u222b\u221a2\n0 (1\u2212 t2/2)n\u22122dt\n(G.16)\n\u2264 (\u03c0/2) \u00b7 (\u03c0/2\u2212m) n\u22122\n2 \u221a 2\n3 \u221a n\u22122\n(G.17)\n= 3\u03c0\n\u221a n\u2212 2(sin(\u03c0/2\u2212m))n\u22122\n4 \u221a 2\n. (G.18)\nLemma G.2 (Lemma 7.3 (1) in Sto\u0308ger & Soltanolkotabi (2021)). Let A be a linear measurement operator that satisfies the RIP property of order 2k+1 with constant \u03b4, then we have for all matrices with rank no more than 2k\n\u2225(I \u2212A\u2217A)(X)\u2225 \u2264 \u221a 2k \u00b7 \u03b4\u2225X\u2225. (G.19)\nLemma G.3 (Soltanolkotabi et al. (2023)). There exist parameters \u03b60, \u03b40, \u03b10, \u03b70 such that, if we choose \u03b1 \u2264 \u03b10, F0 = \u03b1 \u00b7 F\u03030, G0 = (\u03b1/3) \u00b7 G\u03030, where the elements of F\u03030, G\u03030 is N (0, 1/n),6 and\n6Note that in Soltanolkotabi et al. (2023), the initialization is F0 = \u03b1 \u00b7 F\u03030 and G0 = \u03b1 \u00b7 G\u03030, while Lemma G.3 uses a slightly imbalance initialization. It is easy to show that their techniques also hold with this imbalance initialization.\nsuppose that the operator A defined in Eq.(1.1) satisfies the restricted isometry property of order 2r + 1 with constant \u03b4 \u2264 \u03b40, then the gradient descent with step size \u03b7 \u2264 \u03b70 will achieve\n\u2225FtG\u22a4t \u2212 \u03a3\u2225 \u2264 \u03b13/5 \u00b7 \u03c3 7/10 1 (G.20)\nwithin T = O\u0303(1/\u03b7\u03c3r) rounds with probability at least 1 \u2212 \u03b60, where \u03b60 = c1 exp(\u2212c2k) + (c3\u03c5) k\u2212r+1 is a small constant. Moreover, during T rounds, we always have\nmax{\u2225Ft\u2225, \u2225Gt\u2225} \u2264 2 \u221a \u03c31. (G.21)\nThe parameters \u03b10, \u03b40 and \u03b70 are selected by\n\u03b10 = O ( \u221a\n\u03c31 k5 max{2n, k}2\n) \u00b7 ( \u221a k \u2212 \u221a r \u2212 1\n\u03ba2 \u221a max{2n, k}\n)C\u03ba (G.22)\n\u03b40 \u2264 O ( 1\n\u03ba3 \u221a r\n) (G.23)\n\u03b7 \u2264 O  1 k5\u03c31 \u00b7 1 log ( 2 \u221a 2\u03c31\n\u03c5\u03b1( \u221a k\u2212 \u221a r\u22121\n)  (G.24)"
        },
        {
            "heading": "H EXPERIMENT DETAILS",
            "text": "In this section, we provide experimental results to corroborate our theoretical observations.\nSymmetric Lower Bound In the first experiment, we choose n = 50, r = 2, three different k = 5, 3, 2 and learning rate \u03b7 = 0.01 for the symmetric matrix factorization problem. The results are shown in Figure 1, which matches our \u2126(1/T 2) lower bound result in Theorem 3.1 for the overparameterized setting, and previous linear convergence results for exact-parameterized setting.\nAsymmetric Matrix Sensing In the second experiment, we choose configuration n = 50, k = 4, r = 2, sample number m = 700 \u2248 nk2 and learning rate \u03b7 = 0.2 for the asymmetric matrix sensing problem. To demonstrate the direct relationship between convergence speed and initialization scale, we conducted multiple trials employing distinct initialization scales \u03b1 = 0.5, 0.2, 0.05. The experimental results in Figure 1.2 offer compelling evidence supporting three key findings:\n\u2022 The loss exhibits a linear convergence pattern. \u2022 A larger value of \u03b1 results in faster convergence under the over-parameterization setting \u2022 The convergence rate is not dependent on the initialization scale under the exact-parameterization setting.\nThese observations highlight the influence of the initialization scale on the algorithm\u2019s performance.\nIn the last experiment, we run our new method with the same n and r but two different k = 3, 4. Unlike the vanilla gradient descent, at the midway point of the episode, we applied a transformation to the matrices Ft and Gt as specified by Eq. (5.1). As illustrated in Figure 2(c), it is evident that the rate of loss reduction accelerates after the halfway mark. This compelling observation serves as empirical evidence attesting to the efficacy of our algorithm."
        },
        {
            "heading": "I ADDITIONAL EXPERIMENTS",
            "text": "In this section, we provide some additional experiments to further corroborate our theoretical findings."
        },
        {
            "heading": "I.1 COMPARISONS BETWEEN ASYMMETRIC AND SYMMETRIC MATRIX SENSING",
            "text": "We run both asymmetric and symmetric matrix sensing with n = 50, n = 4, r = 2 with sample m = 1200 and learning rate \u03b7 = 0.2. We run the experiment for three different initialization\nscales \u03b1 = 0.5, 0.2, 0.05. The experiment results in Figure I.1 show that asymmetric matrix sensing converges faster than symmetric matrix sensing under different initialization scales.\nFigure I.1: Comparisons between asymmetric and symmetric matrix sensing with different initialization scales. The dashed line represents the asymmetric matrix sensing, and the solid line represents the symmetric matrix sensing. Different color represents the different initialization scales."
        },
        {
            "heading": "I.2 WELL-CONDITIONED CASE AND ILL-CONDITIONED CASE",
            "text": "We run experiments with different conditional numbers of the ground-truth matrix. The conditional number \u03ba is selected as \u03ba = 1.5, 3 and 10. The minimum eigenvalue is selected by 0.66, 0.33 and 0.1 respectively. The experiment results are shown in Figure I.2\nFigure I.2: Comparisons between different conditional numbers\nFrom the experiment results, we can see two phenomena:\n(a) Symmetric case (b) Asymmetric Case (c) Our new method\nFigure I.4: Experiment Results of larger true rank r = 5 and over-parameterized rank k = 10.\n\u2022 When the minimum eigenvalue is smaller, the gradient descent will converge to a smaller error at a linear rate. We call this phase the local convergence phase.\n\u2022 After the local convergence phase, the curve first remains flat and then starts to converge at a linear rate again. We can see that the curve remains flat for a longer time when the matrix is ill-conditioned, i.e. \u03ba is larger.\nThis phenomenon has been theoretically identified by the previous work for the incremental learning (Jiang et al., 2022; Jin et al., 2023), in which GD is shown to sequentially recover singular components of the ground truth from the largest singular value to the smallest singular value."
        },
        {
            "heading": "I.3 LARGER INITIALIZATION SCALE",
            "text": "We also run experiments with a larger initialization scale \u03b1. The experiment results are shown in Figure I.3. We find that if \u03b1 is overly large, i.e. \u03b1 = 3 and 5, the algorithm actually converges slower and even fails to converge. This is reasonable since there is an upper bound requirement Eq. (4.7) for \u03b1 in Theorem 4.2.\nFigure I.3: Comparisons between different large initialization scales"
        },
        {
            "heading": "I.4 LARGER TRUE RANK AND OVER-PARAMETERIZED RANK",
            "text": "We run experiments with larger configurations n = 50, k = 10 and r = 5. We use m = 2000 samples. The experiment results are shown in Figure I.4. We show that similar phenomena of symmetric and asymmetric cases also hold for a larger rank of the true matrix and a larger overparameterized rank. Moreover, our new method also performs well in this setting.\nI.5 INITIALIZATION PHASE\nIf we use GD with small initialization, GD always goes through an initialization phase where the loss is relatively flat, and then converges rapidly to a small error. In this subsection, we plot the first 5000 episodes of Figure 2(b). After zooming into the first 5000 iterations, we find the existence of the initialization phase. That is, the loss is rather flat during this phase. We can also see that the initialization phase is longer when \u03b1 is smaller. The experiment results are shown in Figure I.5.\nFigure I.5: First 5000 episodes of Figure 2(b)"
        }
    ],
    "title": "HOW OVER-PARAMETERIZATION SLOWS DOWN GRA-",
    "year": 2023
}