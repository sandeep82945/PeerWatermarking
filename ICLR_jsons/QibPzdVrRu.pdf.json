{
    "abstractText": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hancheng Min"
        },
        {
            "affiliations": [],
            "name": "Enrique Mallada"
        },
        {
            "affiliations": [],
            "name": "Ren\u00e9 Vidal"
        }
    ],
    "id": "SP:bb95e0feacdfeba28cdfe22c16b8ec4efd17a9ef",
    "references": [
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Noah Golowich",
                "Wei Hu"
            ],
            "title": "A convergence analysis of gradient descent for deep linear neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Elad Hazan"
            ],
            "title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
            "venue": "In 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Wei Hu",
                "Yuping Luo"
            ],
            "title": "Implicit regularization in deep matrix factorization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "J\u00e9r\u00f4me Bolte",
                "Aris Daniilidis",
                "Olivier Ley",
                "Laurent Mazet"
            ],
            "title": "Characterizations of \u0142ojasiewicz inequalities: subgradient flows, talweg",
            "venue": "convexity. Transactions of the American Mathematical Society,",
            "year": 2010
        },
        {
            "authors": [
                "Etienne Boursier",
                "Loucas Pullaud-Vivien",
                "Nicolas Flammarion"
            ],
            "title": "Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alon Brutzkus",
                "Amir Globerson",
                "Eran Malach",
                "Shai Shalev-Shwartz"
            ],
            "title": "SGD learns overparameterized networks that provably generalize on linearly separable data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Simon S Du",
                "Wei Hu",
                "Jason D Lee"
            ],
            "title": "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Zafer Ercan"
            ],
            "title": "Extension and separation of vector valued functions",
            "venue": "Turkish Journal of Mathematics,",
            "year": 1997
        },
        {
            "authors": [
                "A.F. Filippov"
            ],
            "title": "The existence of solutions of generalized differential equations",
            "venue": "Mathematical notes of the Academy of Sciences of the USSR,",
            "year": 1971
        },
        {
            "authors": [
                "Spencer Frei",
                "Gal Vardi",
                "Peter Bartlett",
                "Nathan Srebro",
                "Wei Hu"
            ],
            "title": "Implicit bias in leaky relu networks trained on high-dimensional data",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Gauthier Gidel",
                "Francis Bach",
                "Simon Lacoste-Julien"
            ],
            "title": "Implicit regularization of discrete gradient dynamics in linear neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Suriya Gunasekar",
                "Blake Woodworth",
                "Srinadh Bhojanapalli",
                "Behnam Neyshabur",
                "Nathan Srebro"
            ],
            "title": "Implicit regularization in matrix factorization",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Li Deng",
                "Dong Yu",
                "George E Dahl",
                "Abdel-rahman Mohamed",
                "Navdeep Jaitly",
                "Andrew Senior",
                "Vincent Vanhoucke",
                "Patrick Nguyen",
                "Tara N Sainath"
            ],
            "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
            "venue": "IEEE Signal processing magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Ziwei Ji",
                "Matus Telgarsky"
            ],
            "title": "Directional convergence and alignment in deep learning",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA,",
            "year": 2020
        },
        {
            "authors": [
                "Yiwen Kou",
                "Zixiang Chen",
                "Quanquan Gu"
            ],
            "title": "Implicit bias of gradient descent for two-layer reLU and leaky reLU networks on nearly-orthogonal data",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
            "year": 2012
        },
        {
            "authors": [
                "Thien Le",
                "Stefanie Jegelka"
            ],
            "title": "Training invariances and the low-rank phenomenon: beyond linear networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Tengyu Ma",
                "Hongyang Zhang"
            ],
            "title": "Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations",
            "venue": "Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Yuping Luo",
                "Kaifeng Lyu"
            ],
            "title": "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kaifeng Lyu",
                "Jian Li"
            ],
            "title": "Gradient descent maximizes the margin of homogeneous neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Kaifeng Lyu",
                "Zhiyuan Li",
                "Runzhe Wang",
                "Sanjeev Arora"
            ],
            "title": "Gradient descent on two-layer nets: Margin maximization and simplicity bias",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hartmut Maennel",
                "Olivier Bousquet",
                "Sylvain Gelly"
            ],
            "title": "Gradient descent quantizes relu network features",
            "venue": "arXiv preprint arXiv:1803.08367,",
            "year": 2018
        },
        {
            "authors": [
                "Hancheng Min",
                "Salma Tarmoun",
                "Ren\u00e9 Vidal",
                "Enrique Mallada"
            ],
            "title": "On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jan Persson"
            ],
            "title": "A generalization of carath\u00e9odory\u2019s existence theorem for ordinary differential equations",
            "venue": "Journal of Mathematical Analysis and Applications,",
            "year": 1975
        },
        {
            "authors": [
                "Mary Phuong",
                "Christoph H Lampert"
            ],
            "title": "The inductive bias of relu networks on orthogonally separable data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Noam Razin",
                "Asaf Maman",
                "Nadav Cohen"
            ],
            "title": "Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "W.T. Reid"
            ],
            "title": "Ordinary Differential Equations",
            "year": 1971
        },
        {
            "authors": [
                "Andrew M Saxe",
                "James L Mcclelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural network",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "Mahdi Soltanolkotabi",
                "Dominik St\u00f6ger",
                "Changzhi Xie"
            ],
            "title": "Implicit balancing and regularization: Generalization and convergence guarantees for overparameterized asymmetric matrix sensing",
            "venue": "arXiv preprint arXiv:2303.14244,",
            "year": 2023
        },
        {
            "authors": [
                "Dominik St\u00f6ger",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A.J. van der Schaft",
                "J.M. Schumacher"
            ],
            "title": "An Introduction to Hybrid Dynamical Systems. Number 251 in Lecture Notes in Control and Information Sciences",
            "year": 2000
        },
        {
            "authors": [
                "Aditya Vardhan Varre",
                "Maria-Luiza Vladarean",
                "Loucas Pillaud-Vivien",
                "Nicolas Flammarion"
            ],
            "title": "On the spectral bias of two-layer linear networks",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Gang Wang",
                "Georgios B. Giannakis",
                "Jie Chen"
            ],
            "title": "Learning relu networks on linearly separable data: Algorithm, optimality, and generalization",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Mingze Wang",
                "Chao Ma"
            ],
            "title": "Early stage convergence and global convergence of training mildly parameterized neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mingze Wang",
                "Chao Ma"
            ],
            "title": "Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks",
            "venue": "arXiv preprint arXiv:2305.12467,",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Wang",
                "Mert Pilanci"
            ],
            "title": "The convex geometry of backpropagation: Neural network gradient flows converge to extreme points of the dual convex program",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Can Yaras",
                "Peng Wang",
                "Wei Hu",
                "Zhihui Zhu",
                "Laura Balzano",
                "Qing Qu"
            ],
            "title": "The law of parsimony in gradient descent for learning deep linear networks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Chulhee Yun",
                "Shankar Krishnan",
                "Hossein Mobahi"
            ],
            "title": "A unifying view on implicit bias in training linear neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "First",
                "Boursier"
            ],
            "title": "2022) only studies the dynamics of the positive (negative) neurons",
            "year": 2022
        },
        {
            "authors": [
                "Boursier"
            ],
            "title": "exponentially in number training data n (extremely overparametrized)",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Then we run stochastic gradient descent (SGD) with batch size 2000 on both W and v with step size \u03b7 = 2 \u00d7 10\u22123. For comparison, we also consider the training schemes studied in Brutzkus et al",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Neural networks have shown excellent empirical performance in many application domains such as vision (Krizhevsky et al., 2012), speech (Hinton et al., 2012) and video games (Silver et al., 2016). Despite being highly overparametrized, networks trained by gradient descent with random initialization and without explicit regularization enjoy good generalization performance. One possible explanation for this phenomenon is the implicit bias or regularization induced by first-order algorithms under certain initialization assumptions. For example, first-order methods applied to (deep) matrix factorization models may produce solutions that have low nuclear norm (Gunasekar et al., 2017) and/or low rank (Arora et al., 2019), and similar phenomena have been observed for deep tensor factorization (Razin et al., 2022). Moreover, prior work such as (Saxe et al., 2014; St\u00f6ger & Soltanolkotabi, 2021) has found that deep linear networks sequentially learn the dominant singular values of the input-output correlation matrix.\nIt is widely known that these sparsity-inducing biases can often be achieved by small initialization. This has motivated a series of works that theoretically analyze the training dynamics of first-order methods for neural networks with small initialization. For linear networks, the implicit bias of small initialization has been studied in the context of linear regression (Saxe et al., 2014; Gidel et al., 2019; Min et al., 2021; Varre et al., 2023) and matrix factorization (Gunasekar et al., 2017; Arora et al., 2019; Li et al., 2018; 2021; St\u00f6ger & Soltanolkotabi, 2021; Yaras et al., 2023; Soltanolkotabi et al., 2023). Recently, the effect of small initialization has been studied for two-layer ReLU networks (Maennel et al., 2018; Lyu et al., 2021; Phuong & Lampert, 2021; Boursier et al., 2022). For example, Maennel et al. (2018) observes that during the early stage of training, neurons in the first layer converge to one out of finitely many directions determined by the dataset. Based on this observation, Phuong & Lampert (2021) shows that in the case of well-separated data, where any pair of input data with the same label are positively correlated and any pair with different labels are negatively correlated, there are only two directions the neurons tend to converge to: the positive data center and the negative one. Moreover, Phuong & Lampert (2021) shows that if such directional convergence holds, then the loss converges, and the resulting first-layer weight matrix is low-rank. However, directional convergence is assumed in their analysis; there is no explicit characterization of how long it takes to achieve directional convergence and how the time to convergence depends on the initialization scale.\nPaper contributions: In this paper, we provide a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. Specifically, we show that if the initialization is sufficiently small, during the early phase of training the neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. Moreover, through a careful analysis of the neuron\u2019s directional dynamics we show that the time it takes for all neurons to achieve good alignment with the input data is upper bounded by O( logn\u221a\u00b5 ), where n is the number of data points and \u00b5 measures how well the data are separated. We also show that after the early alignment phase the loss converges to zero at a O( 1t ) rate and that the weight matrix on the first layer is approximately low-rank. Notation: We denote the Euclidean norm of a vector x by \u2225x\u2225, the inner product between the vectors x and y by \u27e8x, y\u27e9 = x\u22a4y, and the cosine of the angle between them as cos(x, y) = \u27e8 x\u2225x\u2225 , y \u2225y\u2225 \u27e9. For an n\u00d7m matrix A, we let A\u22a4 denote its transpose. We also let \u2225A\u22252 and \u2225A\u2225F denote the spectral norm and Frobenius norm of A, respectively. For a scalar-valued or matrix-valued function of time, F (t), we let F\u0307 = F\u0307 (t) = ddtF (t) denote its time derivative. Furthermore, we define 1A to be the indicator for a statement A: 1A = 1 if A is true and 1A = 0 otherwise. We also let I denote the identity matrix, and N (\u00b5, \u03c32) denote the normal distribution with mean \u00b5 and variance \u03c32 ."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "In this section, we first discuss problem setting. We then present some key ingredients for analyzing the training dynamics of ReLU networks under small initialization, and discuss some of the weaknesses/issues from prior work."
        },
        {
            "heading": "2.1 PROBLEM SETTING",
            "text": "We are interested in a binary classification problem with dataset [x1, \u00b7 \u00b7 \u00b7 , xn] \u2208 RD\u00d7n (input data) and [y1, \u00b7 \u00b7 \u00b7 , yn]\u22a4 \u2208 {\u22121,+1}n (labels). For the classifier, f : RD \u2192 R, we consider a two-layer ReLU network:\nf(x;W, v) = v\u22a4\u03c3(W\u22a4x) = \u2211h\nj=1 vj\u03c3(w\n\u22a4 j x) , (1)\nparametrized by network weights W := [w1, \u00b7 \u00b7 \u00b7 , wh] \u2208 RD\u00d7h, v := [v1, \u00b7 \u00b7 \u00b7 , vh]\u22a4 \u2208 Rh\u00d71, where \u03c3(\u00b7) = max{\u00b7, 0} is the ReLU activation function. We aim to find the network weights that minimize the training loss L(W, v) = \u2211n i=1 \u2113(yi, f(xi;W, v)), where \u2113 : R\u00d7 R\u2192 R\u22650 is either the exponential loss \u2113(y, y\u0302) = exp(\u2212yy\u0302) or the logistic loss \u2113(y, y\u0302) = log(1 + exp(\u2212yy\u0302)). The network is trained via the gradient flow (GF) dynamics\nW\u0307 \u2208 \u2202WL(W, v), v\u0307 \u2208 \u2202vL(W, v), (2) where \u2202WL, \u2202vL are Clark sub-differentials of L. Therefore, (2) is a differential inclusion (Bolte et al., 2010). For simplicity of presentation, instead of directly working on this differential inclusion, our theoretical results will be stated for the Caratheodory solution (Reid, 1971) of (2) when the ReLU subgradient is fixed as \u03c3\u2032(x) = 1x>01. In Appendix E, we show that under the data assumption of our interest (to be introduced later), the Caratheodory solution (s) {W (t), v(t)} exists globally for all t \u2208 [0,\u221e), which we call the solution (s) of (2) throughout this paper. To initialize the weights, we consider the following initialization scheme. First, we start from a weight matrix W0 \u2208 RD\u00d7h , and then and then initialize the weights as\nW (0) = \u03f5W0, vj(0) \u2208 {\u2225wj(0)\u2225,\u2212\u2225wj(0)\u2225},\u2200j \u2208 [h] . (3) That is, the weight matrix W0 determines the initial shape of the first-layer weights W (0) and we use \u03f5 to control the initialization scale and we are interested in the regime where \u03f5 is sufficiently small. For the second layer weights v(0), each vj(0) has magnitude \u2225wj(0)\u2225 and we only need to decide its sign. Our results in later sections are stated for a deterministic choice of \u03f5,W0, and v(0), then we comment on the case where W0 is chosen randomly via some distribution.\nThe resulting weights in (3) are always \"balanced\", i.e., v2j (0)\u2212 \u2225wj(0)\u22252 = 0,\u2200j \u2208 [h], because vj(0) can only take two values: either \u2225wj(0)\u2225 or \u2212\u2225wj(0)\u2225. More importantly, under GF (2), this\n1In Appendix F, we discuss how our results can be extended to the solution to differential inclusion.\nbalancedness is preserved (Du et al., 2018): v2j (t)\u2212\u2225wj(t)\u22252 = 0,\u2200t \u2265 0,\u2200j \u2208 [h]. In addition, it is shown in Boursier et al. (2022) that sign(vj(t)) = sign(vj(0)),\u2200t \u2265 0,\u2200j \u2208 [h], and the dynamical behaviors of neurons will be divided into two types, depending on sign(vj(0)). Remark 1. For our theoretical results, the balancedness condition is assumed for technical purposes: it simplifies the dynamics of GF and thus the analysis. It is a common assumption for many existing works on both linear (Arora et al., 2018b) and nonlinear (Phuong & Lampert, 2021; Boursier et al., 2022) neural networks. For the experiments in Section 4, we use a standard Gaussian initialization (not balanced) with a small variance to validate our theoretical findings. Remark 2. Without loss of generality, we consider the case where all columns of W0 are nonzero, i.e., \u2225wj(0)\u2225 > 0,\u2200j \u2208 [h]. We make this assumption because whenever wj(0) = 0, we also have vj(0) = 0 from the balancedness, which together would imply v\u0307j \u2261 0, w\u0307j \u2261 0 under gradient flow. As a result, wj and vj would remain zero and thus they could be ignored in the convergence analysis. Remark 3. Our main results will depend on both maxj \u2225wj(0)\u2225 and minj \u2225wj(0)\u2225, as shown in our proofs in Appendices C and D. Therefore, whenever we speak of small initialization, we will say that \u03f5 is small without worrying about the scale of W0, which is already considered in our results."
        },
        {
            "heading": "2.2 NEURAL ALIGNMENT WITH SMALL INITIALIZATION: AN OVERVIEW",
            "text": "Prior work argues that the gradient flow dynamics (2) under small initialization (3), i.e., when \u03f5 is sufficiently small, can be roughly described as \"align then fit\" (Maennel et al., 2018; Boursier et al., 2022) : During the early phase of training, every neuron wj , j \u2208 [h] keeps a small norm \u2225wj\u22252 \u226a 1 while changing their directions wj\u2225wj\u2225 significantly in order to locally maximize a \"signed coverage\" (Maennel et al., 2018) of itself w.r.t. the training data. After the alignment phase, part of (potentially all) the neurons grow their norms in order to fit the training data, and the loss decreases significantly. The analysis for the fitting phase generally depends on the resulting neuron directions at the end of the alignment phase (Phuong & Lampert, 2021; Boursier et al., 2022). However, prior analysis of the alignment phase either is based on a vanishing initialization argument that can not be directly translated into the case finite but small initialization (Maennel et al., 2018) or assumes some stringent assumption on the data (Boursier et al., 2022). In this section, we provide a brief overview of the existing analysis for neural alignment and then point out several weaknesses in prior work.\n\u2225wj\u2225 dur-\ning the early alignment phase. x1 has +1 label, and x2, x3 have \u22121 labels, x1, x2 lie inside the halfspace \u27e8x,wj\u27e9 > 0 (gray shaded), thus xa(wj) = x1 \u2212 x2. Since sign(vj(0)) > 0, GF pushes wj towards xa(wj).\nPrior analysis of the alignment phase: Since during the alignment phase all neurons have small norm, prior work mainly focuses on the directional dynamics, i.e., ddt wj \u2225wj\u2225 , of the neurons. The analysis relies on the following approximation of the dynamics of every neuron wj , j \u2208 [h]:\nd\ndt wj \u2225wj\u2225 \u2243 sign(vj(0))Pwj(t)xa(wj) , (4)\nwhere Pw = I \u2212 ww \u22a4\n\u2225w\u22252 is the projection onto the subspace orthogonal to w and\nxa(w) := \u2211\ni:\u27e8xi,w\u27e9>0 yixi (5)\ndenotes the signed combination of the data points activated by w. First of all, (4) implies that the dynamics wj\u2225wj\u2225 are approximately decoupled, and thus one can study each wj\u2225wj\u2225 separately. Moreover, as illustrated in Figure 1, if sign(vj(0)) > 0, the flow (4) pushes wj towards xa(wj), since wj is attracted by its currently activated positive data and repelled by its currently activated negative data. Intuitively, during the alignment phase, a neuron wj with sign(vj(0)) > 0 would try to find a direction where it can activate as much positive data and as less negative data as possible. If sign(vj(0)) < 0, the opposite holds.\nIndeed, Maennel et al. (2018) claims that the neuron wj would be aligned with some \"extreme vectors\", defined as vector w \u2208 SD\u22121 that locally maximizes \u2211 i\u2208[n] yi\u03c3(\u27e8xi, w\u27e9) (similarly, wj with sign(vj(0)) < 0 would be aligned with the local minimizer), and there are only finitely many such vectors. The analysis is done under the limit \u03f5\u2192 0, where the approximation in (4) is exact.\nWeakness in prior analyses: Although Maennel et al. (2018) provides great insights into the dynamical behavior of the neurons in the alignment phase, the validity of the aforementioned approximation for finite but small \u03f5 remains in question. First, one needs to make sure that the error\u2225\u2225\u2225 ddt wj\u2225wj\u2225 \u2212 sign(vj(0))Pwjxa(wj)\u2225\u2225\u2225 is sufficiently small when \u03f5 is finite in order to justify (4) as a good approximation. Second, the error bound needs to hold for the entire alignment phase. Maennel et al. (2018) assumes \u03f5\u2192 0; hence there is no formal error bound. In addition, prior analyses on small initialization (St\u00f6ger & Soltanolkotabi, 2021; Boursier et al., 2022) suggest the alignment phase only holds for \u0398(log 1\u03f5 ) time. Thus, the claim in Maennel et al. (2018) would only hold if good alignment is achieved before the alignment phase ends. However, Maennel et al. (2018) provides no upper bound on the time it takes to achieve good alignment. Therefore, without a finite \u03f5 analysis, Maennel et al. (2018) fails to fully explain the training dynamics under small initialization. Understanding the alignment phase with finite \u03f5 requires additional quantitative analysis. To the best of our knowledge, this has only been studied under a stringent assumption that all data points are orthogonal to each other (Boursier et al., 2022), or that there are effectively two data points Wang & Ma (2023).\nGoal of this paper: In this paper, we want to address some of the aforementioned issues by developing a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5. We first discuss our main theorem that shows that a directional convergence can be achieved within bounded time under data assumptions that are less restrictive and have more practical relevance. Then, we discuss the error bound for justifying (4) in the proof sketch of the main theorem."
        },
        {
            "heading": "3 CONVERGENCE OF TWO-LAYER RELU NETWORKS WITH SMALL",
            "text": "INITIALIZATION\nOur main results require the following data assumption:\nAssumption 1. Any pair of data with the same (different) label is positively (negatively) correlated, i.e., mini,j\n\u27e8xiyi,xjyj\u27e9 \u2225xi\u2225\u2225xj\u2225 :=\u00b5> 0.\nGiven a training dataset, we define S+ := {z \u2208 RD : 1\u27e8xi,z\u27e9>0 = 1yi>0,\u2200i} to be the cone in Rn such that whenever neuron w \u2208 S+,w is activated exclusively by every xi with a positive label (see Figure 2). Similarly, for xi with negative labels, we define S\u2212 := {z \u2208 RD : 1\u27e8xi,z\u27e9>0 = 1yi<0,\u2200i}. Finally, we define Sdead := {z \u2208 RD : \u27e8z, xi\u27e9 \u2264 0,\u2200i} to be the cone such that whenever w \u2208 Sdead, no data activates w. Given Assumption 1, it can be shown (see Appendix C) that S+ (S\u2212) is a non-empty, convex cone that contains all positive data xi, i \u2208 I+ (negative data xi, i \u2208 I\u2212). Sdead is a convex cone as well, but not necessarily non-empty. We illustrate these cones in Figure 2 given some training data (red solid arrow denotes positive data and blue denotes negative ones).\nMoreover, given some initialization from (3), we define I+ := {i \u2208 [n] : yi > 0} to be the set of indices of positive data, and I\u2212 := {i \u2208 [n] : yi < 0} for negative data. We also define\nV+ := {j \u2208 [h] : sign(vj(t)) > 0} to be the set of indices of neurons with positive second-layer entry and V\u2212 := {j \u2208 [h] : sign(vj(t)) < 0} for neurons with negative second-layer entry. Note that, as discussed in Section 2.1, sign(vj(t)) does not change under balanced initialization, thus V+,V\u2212 are time invariant. Further, as we discussed in Section 2.2 about the early alignment phase, we expect that every neuron in V+ will drift toward the region where positive data concentrate and thus eventually reach S+ or Sdead, as visualized in Figure 2 (x+, x\u2212 shown in the figure are defined in Assumption 2). Similarly, all neurons in V\u2212 would chase after negative data and thus reach S\u2212 or Sdead. Our theorem precisely characterizes this behavior."
        },
        {
            "heading": "3.1 MAIN RESULTS",
            "text": "Our main results are stated for solutions to the GF dynamics (2). However, in rare cases, solutions to (2) could be non-unique and there are potentially \u201cirregular solutions\" (please refer to Appendix E.4\nfor details) that allow some neurons to regain activation even after becoming completely deactivated in Sdead. We deem such irregular solutions of little practical relevance since when implementing gradient descent algorithm in practice, neurons in Sdead would receive zero update and thus stay in Sdead. Therefore, our main theorem concerns some regular solutions to (2) (the existence of such solutions is shown in Appendix E.2), as defined below. Definition 1. A solution {W (t), v(t)} to (2) is regular if it satisfy that wj(t0) \u2208 Sdead for some j \u2208 [h] and some t0 \u2265 0 implies wj(t) \u2208 Sdead,\u2200t \u2265 t0.\nBefore we present our main theorem, we also need the following assumption on the initialization, for technical reasons, essentially asking the neuron wj(0), j \u2208 V+ (or wj(0), j \u2208 V\u2212, resp.) to not be completely aligned with x+ (or x\u2212, resp.).\nAssumption 2. The initialization from (3) satisfies that maxj\u2208V+\u27e8 wj(0) \u2225wj(0)\u2225 , x\u2212 \u2225x\u2212\u2225 \u27e9 < 1, and maxj\u2208V\u2212\u27e8 wj(0) \u2225wj(0)\u2225 , x+ \u2225x+\u2225 \u27e9 < 1, where x+ = \u2211 i\u2208I+ xi and x\u2212 = \u2211 i\u2208I\u2212 xi.\nWe are now ready to present our main result (given Assumption 1 and Assumption 2): Theorem 1. Given some initialization from (3), if \u03f5 = O( 1\u221a\nh exp(\u2212 n\u221a\u00b5 log n)), then for any regular\nsolution to the gradient flow dynamics (2), we have\n1. (Directional convergence in early alignment phase) \u2203t1 = O( logn\u221a\u00b5 ), such that\n\u2022 \u2200j \u2208 V+, either wj(t1) \u2208 S+ or wj(t1) \u2208 Sdead. Moreover, if maxi\u2208I+ \u27e8wj(0), xi\u27e9 > 0, then wj(t1) \u2208 S+. \u2022 \u2200j \u2208 V\u2212, either wj(t1) \u2208 S\u2212 or wj(t1) \u2208 Sdead. Moreover, if maxi\u2208I\u2212 \u27e8wj(0), xi\u27e9 > 0, then wj(t1) \u2208 S\u2212.\n2. (Final convergence and low-rank bias) \u2200t \u2265 t1 and \u2200j \u2208 [h], neuron wj(t) stays within S+ (S\u2212, or Sdead) if wj(t1) \u2208 S+ (S\u2212, or Sdead resp.). Moreover, if both S+ and S\u2212 contains at least one neuron at time t1, then\n\u2022 \u2203\u03b1 > 0 and \u2203t2 with t1 \u2264 t2 = \u0398( 1n log 1\u221a h\u03f5 ), such that L(t) \u2264 L(t2)L(t2)\u03b1(t\u2212t2)+1 , \u2200t \u2265 t2. \u2022 As t\u2192\u221e, \u2225W (t)\u2225 \u2192 \u221e and \u2225W (t)\u22252F \u2264 2\u2225W (t)\u222522 +O(\u03f5). Thus, the stable rank of W (t) satisfies lim supt\u2192\u221e \u2225W (t)\u22252F /\u2225W (t)\u222522 \u2264 2.\nWe provide a proof sketch that highlights the technical novelty of our results in Section 3.3. Our O(\u00b7) notations hide additional constants that depend on the data and initialization, for which we refer readers to the complete proof of Theorem 1 in Appendix C and D. We make the following remarks:\nEarly neuron alignment: The first part of the Theorem 1 describes the configuration of all neurons at the end of the alignment phase. Every neuron in V+ reaches either S+ or Sdead by t1, and stays there for the remainder of training. Obviously, we care about those neurons reaching S+ as any neuron in Sdead does not contribute to the final convergence at all. Luckily, Theorem 1 suggests that any neuron in V+ that starts with some activation on the positive data, i.e., it is initialized in the union of halfspaces \u222ai\u2208I+{w : \u27e8w, xi\u27e9 > 0}, will eventually reach S+. A similar discussion holds for neurons in V\u2212. We argue that randomly initializing W0 ensures that with high probability, there will be at least a pair of neurons reaching S+ and S\u2212 by time t1 (please see the next remark). Lastly, we note that it is possible that Sdead = \u2205, in which case every neuron reaches either S+ or S\u2212. Merits of random initialization: Our theorem is stated for a deterministic initialization (3) given an initial shape W0. In practice, one would use random initialization to find a W0, for example, [W0]ij\ni.i.d.\u223c N (0, 1/D). First, our Theorem 1 applies to this Gaussian initialization: Assumption 2 is satisfied with probability one because the events { \u27e8 wj(0)\u2225wj(0)\u2225 , x\u2212 \u2225x\u2212\u2225 \u27e9 = 1 } and { \u27e8 wj(0)\u2225wj(0)\u2225 , x+ \u2225x+\u2225 \u27e9 = 1 } have probability zero. Moreover, any neuron in V+ has at least probability 1/2 of being initialized within the union of halfspaces \u222ai\u2208I+{w : \u27e8w, xi\u27e9 > 0}, which ensures that this neuron reaches S+. Thus when there are m neurons in V+, the probability that S+ has at least one neuron at time t1 is lower bounded by 1 \u2212 2\u2212m (same argument holds for S\u2212), Therefore, with only very mild overparametrization on the network width h, one can make sure that with high probability there is at least one neuron in both S+ and S\u2212, leading to final convergence.\nImportance of a quantitative bound on t1: The analysis for neural alignment relies on the approximation in (4), which, through our analysis (see Lemma 1), is shown to only hold before T = \u0398( 1n log 1\u221a h\u03f5 ), thus if one proves, through the approximation in (4), that good alignment is achieved within t1 time, then the initialization scale \u03f5 must be chosen to be O( 1\u221ah exp(\u2212nt1)) so that t1 \u2264 T , i.e. the proved alignment should finish before the approximation (4) fails. Therefore, without an explicit bound on t1, one does not know a prior how small \u03f5 should be. Our quantitative analysis shows that under (4), directional convergence is achieved within t1 = O( logn\u221a\u00b5 ) time. This bound, in return, determines the bound for initialization scale \u03f5. Moreover, our bound quantitatively reveals the non-trivial dependency on the \"data separation\" \u00b5 for such directional convergence to occur. Indeed, through a numerical illustration in Appendix A.2, we show that the dependence on the data separability \u00b5 > 0 is crucial in determining the scale of the initialization: As \u00b5 approaches zero, the time needed for the desired alignment increases, necessitate the use of a smaller \u03f5.\nRefined alignment within S+,S\u2212: Once a neuron in V+ reaches S+, it never leaves S+. Moreover, it always gets attracted by x+. Therefore, every neuron gets well aligned with x+, i.e., cos(wj , x+) \u2243 1,\u2200wj \u2208 S+. A similar argument shows neurons in S\u2212 get attracted by x\u2212. We opt not to formally state it in Theorem 1 as the result would be similar to that in (Boursier et al., 2022), and alignment with x+, x\u2212 is not necessary to guarantee convergence. Instead, we show this refined alignment through our numerical experiment in Section 4.\nFinal convergence and low-rank bias: We present the final convergence results mostly for the completeness of the analysis. GF after t1 can be viewed as fitting positive data xi, i \u2208 I+, with a subnetwork consisting of neurons in S+, and fitting negative data with neurons in S\u2212. By the fact that all neurons in S+ activate all xi, i \u2208 I+, the resulting subnetwork is linear, and so is the subnetwork for fitting xi, i \u2208 I\u2212. The convergence analysis reduces to establishing O(1/t) convergence for two linear networks (Arora et al., 2018a; Min et al., 2021; Yun et al., 2020). The non-trivial and novel part is to show that right after the alignment phase ends, one can expect a substantial decrease of the loss (starting from time t2 = \u0398( 1n log 1\u221a h\u03f5 )). An alternative way of proving convergence is by observing that at t1, all data has been correctly classified (w.r.t. sign of f ), which is sufficient for showing O( 1t log t ) convergence (Lyu & Li, 2019; Ji & Telgarsky, 2020) of the loss, but this asymptotic rate does not suggest a time after which the loss start to decrease significantly. As for the stable rank, our result follows the analysis in Le & Jegelka (2022), but in a simpler form since ours is for linear networks. Although convergence is established partially by existing results, we note that these analyses are all possible because we have quantitatively bound t1 in the alignment phase."
        },
        {
            "heading": "3.2 COMPARISON WITH PRIOR WORK",
            "text": "Our results provide a complete (from alignment to convergence), non-asymptotic (finite \u03f5), quantitative (bounds on t1, t2) analysis for the GF in (2) under small initialization. Similar neural alignment has been studied in prior work for orthogonally separable data (same as ours) and for orthogonal data, and we shall discuss them separately.\nAlignment under orthogonally separable data: Phuong & Lampert (2021) assumes that there exists a time t1 such that at t1, the neurons are in either S+,S\u2212 or Sdead and their main contribution is the analysis of the implicit bias for the later stage of the training. they justify their assumption by the analysis in Maennel et al. (2018), which does not necessarily apply to the case of finite \u03f5, as we discussed in Section 2.2. Later Wang & Pilanci (2022) shows t1 exists, provided that the initialization scale \u03f5 is sufficiently small, but still with no explicit analysis showing how t1 depends on the data separability \u00b5 and the size of the training data n. Moreover, there is no quantification on how small \u03f5 should be. In our work, all the results are non-asymptotic and quantitative: we show that good alignment is achieved within t1 = O( logn\u221a\u00b5 ) time and provide an explicit upper bound on \u03f5. Moreover, our results highlight the dependence on the separability \u00b5 > 0, (Further illustrated in Appendix A.2) which is not studied in Phuong & Lampert (2021); Wang & Pilanci (2022).\nAlignment under orthogonal data: In Boursier et al. (2022), the neuron alignment is carefully analyzed for the case all data points are orthogonal to each other, i.e., \u27e8xi, xj\u27e9 = 0,\u2200i \u0338= j \u2208 [n]. We point out that neuron behavior is different under orthogonal data (illustrated in Appendix A.3): only the positive (negative) neurons initially activate all the positive (negative) data will end up in S+ (S\u2212). In our case, all positive (negative) neurons will arrive at S+ (S\u2212), unless they become a dead\nneuron. Moreover, due to such distinction, the analysis is different: Boursier et al. (2022) restrict their results to positive (negative) neurons wj that initially activate all the positive (negative) data, and there is no need for analyzing neuron activation. However, since our analysis is on all positive neurons, regardless of their initial activation pattern, it utilizes novel techniques to track the evolution of the activation pattern (see Section 3.3).\nOther related work: Convergence of two-layer (leaky-)ReLU networks are also studied under nonsmall initialization settings, mainly for gradient descent Wang & Ma (2022) and for training only the first-layer weights Frei et al. (2022); Kou et al. (2023). There is no direct comparison to them as they study the convergence in other regimes. Nonetheless, the analyses of neural alignment remain essential in these works but are done through different tools (one no longer has an approximation in (4)). We note that such analyses also require certain restrictive data assumptions. For example, Wang & Ma (2022) assumes orthogonal separability, together with some geometric constraint on the data; Frei et al. (2022); Kou et al. (2023) assumes high-dimensional near-orthogonal data."
        },
        {
            "heading": "3.3 PROOF SKETCH FOR THE ALIGNMENT PHASE",
            "text": "In this section, we sketch the proof for our Theorem 1. First of all, it can be shown that S+,Sdead are trapping regions for all wj(t), j \u2208 V+, that is, whenever wj(t) gets inside S+ (or Sdead), it never leaves S+ (or Sdead). Similarly, S\u2212,Sdead are trapping regions for all wj(t), j \u2208 V\u2212. The alignment phase analysis concerns how long it takes for all neurons to reach one of the trapping regions, followed by the final convergence analysis on fitting data with +1 label by neurons in S+ and fitting data with \u22121 label by those in S\u2212. We have discussed the final convergence analysis in the remark \"Final convergence and low-rank bias\", thus we focus on the proof sketch for the early alignment phase here, which is considered as our main technical contribution.\nApproximating ddt wj \u2225wj\u2225 : Our analysis for the neural alignment is rooted in the following Lemma:\nLemma 1. Given some initialization from (3), if \u03f5 = O( 1\u221a h ), then there exists T = \u0398( 1n log 1\u221a h\u03f5 ) such that any solution to the gradient flow dynamics (2) satisfies that \u2200t \u2264 T ,\nmax j \u2225\u2225\u2225\u2225 ddt wj(t)\u2225wj(t)\u2225 \u2212 sign(vj(0))Pwj(t)xa(wj(t)) \u2225\u2225\u2225\u2225 = O (\u03f5n\u221ah) . (6)\nThis Lemma shows that the error between ddt wj(t) \u2225wj(t)\u2225 and sign(vj(0))Pwj(t)xa(wj(t)) can be arbitrarily small with some appropriate choice of \u03f5 (to be determined later). This allows one to analyze the true directional dynamics wj(t)\u2225wj(t)\u2225 using some property of Pwj(t)xa(wj(t)), which leads to a t1 = O( logn\u221a\u00b5 ) upper bound on the time it takes for the neuron direction to converge to the sets S+, S\u2212, or Sdead. Moreover, it also suggests \u03f5 can be made sufficiently small so that the error bound holds until the directional convergence is achieved, i.e. T \u2265 t1. We will first illustrate the analysis for directional convergence, then close the proof sketch with the choice of a sufficiently small \u03f5.\nActivation pattern evolution: Given a sufficiently small \u03f5, one can show that under Assumption 1, for every neuron wj that is not in Sdead we have:\nd\ndt \u2329 wj \u2225wj\u2225 , xiyi \u2225xi\u2225 \u232a\u2223\u2223\u2223\u2223 \u27e8wi,xi\u27e9=0 > 0,\u2200i \u2208 [n], if j \u2208 V+ , (7)\nd\ndt \u2329 wj \u2225wj\u2225 , xiyi \u2225xi\u2225 \u232a\u2223\u2223\u2223\u2223 \u27e8wi,xi\u27e9=0 < 0,\u2200i \u2208 [n], if j \u2208 V\u2212 . (8)\nThis is because if a neuron satisfies \u27e8xi, wj\u27e9 = 0 for some i, and is not in Sdead, GF moves wj towards xa(wj) = \u2211 i:\u27e8xi,wj\u27e9>0 xiyi. Interestingly, Assumption 1 implies \u27e8xiyi, xa(wj)\u27e9 > 0,\u2200i \u2208 [n], which makes ddt wj\n\u2225wj\u2225 \u2243 sign(vj(0))Pwjxa(wj) point inward (or outward) the halfspace \u27e8xiyi, wj\u27e9 > 0, if sign(vj(0)) > 0 (or sign(vj(0)) < 0, respectively). See Figure 3 for illustration.\nAs a consequence, a neuron can only change its activation pattern in a particular manner: a neuron in V+, whenever it is activated by some xi with yi = +1, never loses the activation on xi thereafter, because (7) implies that GF pushes wj\u2225wj\u2225 towards xi at the boundary \u27e8wj , xi\u27e9 = 0. Moreover, (7)\nalso shows that a neuron in V+ will never regain activation on a xi with yi = \u22121 once it loses the activation because GF pushes wj\u2225wj\u2225 against xi at the boundary \u27e8wi, xi\u27e9 = 0. Similarly, a neuron in V\u2212 never loses activation on negative data and never gains activation on positive data.\nBound on activation transitions and duration: Equations (7) and (8) are key in the analysis of alignment because they limit how many times a neuron can change its activation pattern: a neuron in V+ can only gain activation on positive data and lose activation on negative data, thus at maximum, a neuron wj , j \u2208 V+, can start with full activation on all negative data and no activation on any positive one (which implies wj(0) \u2208 S\u2212) then lose activation on every negative data and gain activation on every positive data as GF training proceeds (which implies wj(t1) \u2208 S+), taking at most n changes on its activation pattern. See Figure 4 for an illustration. Then, since it is possible to show that a neuron wj with j \u2208 V+ that has cos(wj , x\u2212) < 1 (guaranteed by Assumption 2) and is not in S+ or Sdead, must change its activation pattern after O( 1na\u221a\u00b5 ) time (that does not depend on \u03f5), where na is the number of data that currently activates wj , one can upper bound the time for wj to reach S+ or Sdead by some t1 = O( logn\u221a\u00b5 ) constant independent of \u03f5. Moreover, wj must reach S+ if it initially has activation on at least one positive data, i.e., maxi\u2208I+ \u27e8wj(0), xi\u27e9 > 0 since it cannot lose this activation. A similar argument holds for wj , j \u2208 V\u2212 that they reaches either S\u2212 or Sdead before t1. Choice of \u03f5: All the aforementioned analyses rely on the assumption that the approximation in equation (4) holds with some specific error bound. We show in Appendix C that the desired bound is \u2225\u2225\u2225 ddt wj(t)\u2225wj(t)\u2225 \u2212 sign(vj(0))Pwj(t)xa(wj(t))\u2225\u2225\u2225 \u2264 O(\u221a\u00b5), which, by Lemma 1, can be achieved by a sufficiently small initialization scale \u03f51 = O( \u221a \u00b5\u221a hn ). Moreover, the directional convergence (which takes O( logn\u221a\u00b5 ) time) should be achieved before the alignment phase ends, which happens at T = \u0398( 1n log 1\u221a h\u03f5 ). This is ensured by choosing another sufficiently small initialization scale \u03f52 = O( 1\u221ah exp(\u2212 n\u221a \u00b5 log n)). Overall, the initialization scale should satisfy \u03f5 \u2264 min{\u03f51, \u03f52}. We opt to present \u03f52 in our main theorem because \u03f52 beats \u03f51 when n is large."
        },
        {
            "heading": "4 NUMERICAL EXPERIMENTS",
            "text": "We use a toy example in Appendix A.1 to clearly visualize the neuron alignment during training (due to space constraints). In the main body of this paper, we validate our theorem using a binary classification task for two MNIST digits. Such training data do not satisfy Assumption 1 since every data vector is a grayscale image with non-negative entries, making the inner product between any pair of data non-negative, regardless of their labels. However, we can preprocess the training data by centering: xi \u2190 xi \u2212 x\u0304, where x\u0304 = \u2211 i\u2208[n] xi/n. The preprocessed data, then, approximately satisfies our assumption (see the left-most plot in Figure 5): a pair of data points is very likely to have a positive correlation if they have the same label and to have a negative correlation if they have\ndifferent labels. Thus we expect our theorem to make reasonable predictions on the training dynamics with preprocessed data. For the remaining section, we use xi, i \u2208 [n], to denote the preprocessed (centered) data and use x\u0304 to denote the mean of the original data.\nWe build a two-layer ReLU network with h = 50 neurons and initialize all entries of the weights as [W ]ij i.i.d.\u223c N ( 0, \u03b12 ) , vj i.i.d.\u223c N ( 0, \u03b12 ) ,\u2200i \u2208 [n], j \u2208 [h] with \u03b1 = 10\u22126. Then we run gradient descent on both W and v with step size \u03b7 = 2\u00d7 10\u22123. Notice that here the weights are not initialized to be balanced as in (3). The numerical results are shown in Figure 5.\nAlignment phase: Without balancedness, one no longer has sign(vj(t)) = sign(vj(0)). With a little abuse of notation, we denote V+(t) = {j \u2208 [h] : sign(vj(t)) > 0} and V+(t) = {j \u2208 [h] : sign(vj(t)) > 0}, and we expect that at the end of the alignment phase, neurons in V+ are aligned with x+ = \u2211 i\u2208I+ xi, and neurons in V\u2212 with x\u2212 = \u2211 i\u2208I\u2212 xi. The second plot in Figure 5 shows such an alignment between neurons and x+, x\u2212. In the top part, the red solid line shows cos(w\u0304+, x+) during training, where w\u0304+ = \u2211 j\u2208V+ wj/|V+|, and the shaded region defines the range between minj\u2208V+ cos(wi, x+) and maxj\u2208V+ cos(wi, x+). Similarly, in the bottom part, the green solid line shows cos(w\u0304\u2212, x\u2212) during training, where w\u0304\u2212 = \u2211 j\u2208V\u2212 wj/|V\u2212|, and the shaded region delineates the range between minj\u2208V\u2212 cos(wi, x\u2212) and maxj\u2208V\u2212 cos(wi, x\u2212). Initially, every neuron is approximately orthogonal to x+, x\u2212 due to random initialization. Then all neurons in V+ (V\u2212) start to move towards x+ (x\u2212) and achieve good alignment after \u223c2000 iterations. When the loss starts to decrease, the alignment drops. We conjecture that because Assumption 1 is not exactly satisfied, neurons in V+ have to fit some negative data, for which x+ is not the best direction. Final convergence: After \u223c 3000 iterations, the norm \u2225W\u222522 starts to grow and the loss decreases, as shown in the third plot in Figure 5. Moreover, the stable rank \u2225W\u22252F /\u2225W\u222522 decreases below 2. For this experiment, we almost have cos(x+, x\u2212) \u2243 \u22121, thus the neurons in V+ (aligned with x+) and those in V\u2212 (aligned with x\u2212) are almost co-linear. Therefore, the stable rank \u2225W\u22252F /\u2225W\u222522 is almost 1, as seen from the plot. Finally, at iteration 15000, we visualize the mean neuron w\u0304+ = \u2211 j\u2208V+ wj/|V+|, w\u0304\u2212 = \u2211 j\u2208V\u2212 wj/|V\u2212| as grayscale images, and compare them with x\u0304+ = x+/|I+|, x\u2212 = x\u2212/|I\u2212|, showing good alignment. Comparison with other training schemes: For two-layer ReLU networks, there is another line of work (Brutzkus et al., 2018; Wang et al., 2019) that studies GD/SGD only on the first-layer weights W and keeping the second-layer v fixed throughout training. In Appendix A.5, we compare our training schemes to those in Brutzkus et al. (2018); Wang et al. (2019), and show that while both schemes achieve small training loss, the aforementioned two-phase training (alignment then final convergence) does no happen if only the first-layer in trained."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks under small initialization. We consider a training dataset with well-separated input vectors. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data. Numerical experiment on classifying two digits from the MNIST dataset correlates with our theoretical findings."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "The authors thank the support of the NSF-Simons Research Collaborations on the Mathematical and Scientific Foundations of Deep Learning (NSF grant 2031985), the NSF HDR TRIPODS Institute for the Foundations of Graph and Deep Learning (NSF grant 1934979), the ONR MURI Program (ONR grant 503405-78051), and the NSF CAREER Program (NSF grant 1752362). The authors thank Ziqing Xu and Salma Tarmoun for the insightful discussions."
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTS",
            "text": "A.1 ILLUSTRATIVE EXAMPLE\nWe illustrate our theorem using a toy example: we train a two-layer ReLU network with h = 50 neurons under a toy dataset in R2 (See Figure. 6) that satisfies our Assumption 1, and initialize all entries of the weights as [W ]ij i.i.d.\u223c N ( 0, \u03b12 ) , vj i.i.d.\u223c N ( 0, \u03b12 ) ,\u2200i \u2208 [n], j \u2208 [h] with \u03b1 = 10\u22126. Then we run gradient descent on both W and v with step size \u03b7 = 2 \u00d7 10\u22123. Our theorem well predicts the dynamics of neurons at the early stage of the training: aside from neurons that ended up in Sdead, neurons in V+ reach S+ and achieve good alignment with x+, and neurons in V\u2212 are well aligned with x\u2212 in S\u2212. Note that after alignment, the loss experiences two sharp decreases before it gets close to zero, which is studied and explained in Boursier et al. (2022).\nA.2 EFFECT OF DATA SEPARABILITY \u00b5\nThis section investigates the effect of data separability \u00b5 on the time required to achieve the desired alignment as in Theorem 1, through a simple example. we consider a similar setting as in A.1, and explore the cases when data separability \u00b5 \u226a 1. We expect that as separability \u00b5 decreases, the time for neurons to achieve the desired alignment as in Theorem 1 increases, necessitating a smaller initialization scale. For simplicity, we consider a dataset with only two positive data (x1, y1 = +1), (x2, y2 = +1).\nIn Figure 7, we first set \u00b5 = \u27e8x1, x2\u27e9 = sin(0.1), and the neuron alignment is consistent with Theorem 1: positive neurons (that are not dead) eventually enters S+, activating both data points, and then final convergence follows.\nHowever, in Figure 8, as we decrease the separability \u00b5 to sin(0.001) (other settings remain unchanged), the neural alignment becomes slower: 1) at iteration 7000, there are still neurons (that are not dead) outside S+, namely those aligned with either x1 or x2, while in our previous setting (\u00b5 = sin(0.1)), all neurons (that are not dead) have reached S+; 2) In this particular instance of the experiment, we also see one neuron remains outside S+ at the late stage of the training (at iteration 21000). This clearly shows that as data separability \u00b5 decreases, the time needed for all neurons (that are not dead) to reach S+ increases, and if the initialization scale is not small enough for the alignment phase to hold for a long time, there will be neurons remains outside S+."
        },
        {
            "heading": "A.3 NEURON DYNAMICS UNDER ORTHOGONAL DATA",
            "text": "We have seen in the last section how a small \u00b5 affects the neuron dynamics. The orthogonal data assumption studied in Boursier et al. (2022) is precisely the extreme case of \u00b5\u2192 0, where the neuron behavior changes substantially. We follow exactly the same setting in Appendix A.2 and consider the case of \u00b5 = 0.\nIn Figure 9, we see that S+ is no longer the region that contains all (non-dead) positive neurons at the end of the alignment phase. Depending on where each neuron is initialized, it could end up being in S+, aligned with x1, or aligned with x2. Moreover, for final convergence, only the neurons ended up in S+ grow their norms and fit the data, whose number is clearly less than that in the case of \u00b5 > 0.\nThis difference in neurons\u2019 dynamical behavior makes the analysis in Boursier et al. (2022) different than ours: First, Boursier et al. (2022) only studies the dynamics of the positive (negative) neurons that initially activate all positive (negative) data, which will end up in S+ (S\u2212) and fit the data, and the analysis does not evolve the changes in their activation pattern. In our case, any positive (negative) neurons could potentially end up in S+ (S\u2212), and in particular, it will if it initially activates at least\none positive (negative) data, thus it becomes necessary to track the evolution of the activation pattern of all these neurons (novelty in our analysis). Moreover, consider the case that neurons are being randomly initialized, Boursier et al. (2022) requires the set of positive (negative) neurons that initially activate all positive (negative) data being non-empty, which needs the number of neurons h to scale exponentially in number training data n (extremely overparametrized). In our case, we only require h = \u0398(1) (See Merits of overparametrization after Theorem 1), a mild overparamerization.\nIn summary, while Boursier et al. (2022) also provides quantitative analysis on neural alignment under small initialization, it is done under the assumption that all data are orthogonal to each other, leading to a different neuron dynamical behavior than ours. Due to such differences, their analysis cannot be directly applied to the case of orthogonally separable data (ours), for which we develop novel analyses on the evolution of neuron activation patterns (See proof sketch in Section 3.3)."
        },
        {
            "heading": "A.4 ADDITIONAL EXPERIMENTS ON MNIST DATASET",
            "text": "We use exactly the same experimental setting as in the main paper and only use a different pair of digits. The results are as follows:"
        },
        {
            "heading": "A.5 DISCUSSION ON THE TWO-PHASE CONVERGENCE",
            "text": "With the same two-digit MNIST dataset in Section 4, we further discuss the two-phase convergence under small initialization. We use a two-layer ReLU network with h = 50 neurons and initialize all entries of the weights as [W ]ij i.i.d.\u223c N ( 0, \u03b12 ) , vj i.i.d.\u223c N ( 0, \u03b12 ) ,\u2200i \u2208 [n], j \u2208 [h] with \u03b1 = 10\u22126. Then we run stochastic gradient descent (SGD) with batch size 2000 on both W and v with step size \u03b7 = 2 \u00d7 10\u22123. For comparison, we also consider the training schemes studied in Brutzkus et al. (2018); Wang et al. (2019), where only the first-layer weight W is trained starting from a small initialization [W ]ij i.i.d.\u223c N ( 0, \u03b12 ) , and vj are chosen to be either +1 or \u22121 with equal probability, then fixed throughout training.\nWe consider the changes in neuron norms and directions separately. In particular, these quantities are defined as \u2211\ni\nd dt \u2225wj\u22252 \u2223\u2223\u2223\u2223 w\u0307j=\u2212\u2207wjL = \u2211 i 2 \u2329 \u2212\u2207wjL , wj \u232a (changes in neuron norms)\n\u2211 i \u2225\u2225\u2225\u2225\u2225 ddt wj\u2225wj\u2225 \u2223\u2223\u2223\u2223 w\u0307j=\u2212\u2207wjL \u2225\u2225\u2225\u2225\u2225 =\u2211 i \u2225\u2225\u2225\u2225Pwj (\u2212\u2207wjL\u2225wj\u2225 )\u2225\u2225\u2225\u2225 , (changes in neuron directions)\nand they measure, at the end of every epoch, how much the neuron norms and directions will change if one uses a one-step full gradient descent with a small step size.\nTraining both layers: In Figure 12, we show the changes in neuron norms and directions over the training trajectory when we run stochastic gradient descent (SGD) on both first- and second-layer weights. The two-phase (alignment phase then final convergence) is clearly shown by comparing the relative scale of changes in neuron norms and directions in different phases of the training.\nTraining only the first layer: In Figure 13, we show the changes in neuron norms and directions over the training trajectory when we run stochastic gradient descent (SGD) on ONLY the first-layer weights (Brutzkus et al., 2018; Wang et al., 2019). The plot indicates that two-phase training does not happen in this case."
        },
        {
            "heading": "B PROOF OF LEMMA 1: NEURON DYNAMICS UNDER SMALL INITIALIZATION",
            "text": "The following property of \u2113 (exponential loss \u2113(y, y\u0302) = exp(\u2212yy\u0302) or logistic loss \u2113(y, y\u0302) = 2 log(1+ exp(\u2212yy\u0302))) will be used throughout the Appendix for proofs of several results: Lemma 2. For \u2113, we have\n| \u2212 \u2207y\u0302\u2113(y, y\u0302)\u2212 y| \u2264 2|y\u0302|,\u2200y \u2208 {+1,\u22121}, \u2200|y\u0302| \u2264 1 . (9)\nProof. Exponential loss: when \u2113(y, y\u0302) = exp(\u2212yy\u0302): | \u2212 \u2207y\u0302\u2113(y, y\u0302)\u2212 y| = |y exp(\u2212yy\u0302)\u2212 y|\n\u2264 |y|| exp(\u2212yy\u0302)\u2212 1| \u2264 | exp(\u2212yy\u0302)\u2212 1| \u2264 2|y\u0302| ,\nwhere the last inequality is due to the fact that 2x \u2265 max{1\u2212 exp(\u2212x), exp(x)\u2212 1},\u2200x \u2208 [0, 1]. Logistic loss: when \u2113(y, y\u0302) = 2 log(1 + exp(\u2212yy\u0302)):\n| \u2212 \u2207y\u0302\u2113(y, y\u0302)\u2212 y| = \u2223\u2223\u2223\u22232y exp(\u2212yy\u0302)1 + exp(\u2212yy\u0302) \u2212 y \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223y exp(\u2212yy\u0302)\u2212 y1 + exp(\u2212yy\u0302) \u2223\u2223\u2223\u2223\n\u2264 |y|| exp(\u2212yy\u0302)\u2212 1| \u2264 | exp(\u2212yy\u0302)\u2212 1| \u2264 2|y\u0302| ,\nRemark 4. More generally, our subsequent results regarding neuron dynamics under small initialization hold for any loss function that satisfies the condition stated in Lemma 2, which includes the l2 loss \u2113(y, y\u0302) = 12 (y \u2212 y\u0302) 2 studied in Boursier et al. (2022)."
        },
        {
            "heading": "B.1 FORMAL STATEMENT",
            "text": "Our results for neuron direction dynamics during the early phase of the training will be stated for networks with any \u03b1-leaky ReLU activation \u03c3(x) = max{x, \u03b1x} with \u03b1 \u2208 [0, 1]. In particular, it is the ReLU activation when \u03b1 = 0, which is the activation function we considered in the main paper, and it is the linear activation when \u03b1 = 1.\nDenote: Xmax = maxi \u2225xi\u2225,Wmax = maxj \u2225[W0]:,j\u2225. The formal statement of Lemma 1 is as follow: Lemma 1. Let the activation function be an \u03b1-leaky ReLU activation \u03c3(x) = max{x, \u03b1x}. Given some initialization from (3), for any \u03f5 \u2264 1\n4 \u221a hXmaxW 2max , then any solution to the gradient flow\ndynamics (2) satisfies that \u2200t \u2264 T = 14nXmax log 1\u221a h\u03f5 ,\nmax j \u2225\u2225\u2225\u2225 ddt wj(t)\u2225wj(t)\u2225 \u2212 sign(vj(0))Pwj(t)xa(wj(t)) \u2225\u2225\u2225\u2225 \u2264 4\u03f5n\u221ahX2maxW 2max ,\nwhere\nxa(wj) = n\u2211 i=1 xiyi\u03c3 \u2032(\u27e8xi, wj\u27e9) = \u2211 i:\u27e8xi,wj\u27e9>0 xiyi + \u03b1 \u2211 i:\u27e8xi,wj\u27e9\u22640 xiyi .\nWith Lemma 1, and set \u03b1 = 0, we obtain the results stated in the main paper. Lemma 1 is a direct result of the following two lemmas. Lemma 3. Let the activation function be an \u03b1-leaky ReLU activation \u03c3(x) = max{x, \u03b1x}. Given some initialization in (3), then for any \u03f5 \u2264 1\n4 \u221a hXmaxW 2max , any solution to the gradient flow dynamics (2) satisfies\nmax j \u2225wj(t)\u22252 \u2264 2\u03f5W 2max\u221a h , max i |f(xi;W (t), v(t))| \u2264 2\u03f5 \u221a hXmaxW 2 max , (10)\n\u2200t \u2264 14nXmax log 1\u221a h\u03f5 .\nLemma 4. Let the activation function be an \u03b1-leaky ReLU activation \u03c3(x) = max{x, \u03b1x}. Consider any solution to the gradient flow dynamic (2) starting from initialization (3). Whenever maxi |f(xi;W, v)| \u2264 1, we have, \u2200i \u2208 [n],\u2225\u2225\u2225\u2225\u2225 ddt wj\u2225wj\u2225 \u2212 sign(vj(0)) ( I \u2212 wjw \u22a4 j \u2225wj\u22252 ) xa(wj)\n\u2225\u2225\u2225\u2225\u2225 \u2264 2nXmax maxi |f(xi;W, v)| , (11) where\nxa(wj) = n\u2211 i=1 xiyi\u03c3 \u2032(\u27e8xi, wj\u27e9) = \u2211 i:\u27e8xi,wj\u27e9>0 xiyi + \u03b1 \u2211 i:\u27e8xi,wj\u27e9\u22640 xiyi .\nRemark 5. By stating our approximation results for neuron directional dynamics with any \u03b1-leaky ReLU activation function, we highlight that even for some networks with other activation functions than ReLU, there is a similar notion of neuron alignment at the early stage of the training, and the analytical tools used in this paper can be applied to them. However, we note that our main results (Theorem 1) will not directly apply as the neuron directional dynamics have changed as we consider an activation function different than ReLU (see the general definition of xa(wj)), and additional efforts are required to establish the directional convergence for general leaky-ReLU functions."
        },
        {
            "heading": "B.2 PROOF OF LEMMA 3: BOUNDS ON NEURON NORMS",
            "text": "Proof of Lemma 3. Under gradient flow, we have\nd dt wj = \u2212 n\u2211 i=1 1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v))xivj . (12)\nBalanced initialization enforces vj = sign(vj(0))\u2225wj\u2225, hence\nd dt wj = \u2212 n\u2211 i=1 \u03c3\u2032(\u27e8xi, wj\u27e9)\u2207y\u0302\u2113(yi, f(xi;W, v))xisign(vj(0))\u2225wj\u2225 . (13)\nLet T := inf{t : maxi |f(xi;W (t), v(t))| > 2\u03f5 \u221a hXmaxW 2 max}, then \u2200t \u2264 T, j \u2208 [h], we have\nd dt \u2225wj\u22252 =\n\u2329 wj , d\ndt wj \u232a = \u22122\nn\u2211 i=1 \u03c3\u2032(\u27e8xi, wj\u27e9)\u2207y\u0302\u2113(yi, f(xi;W, v)) \u27e8xi, wj\u27e9 sign(vj(0))\u2225wj\u2225\n\u2264 2 n\u2211\ni=1\n|\u2207y\u0302\u2113(yi, f(xi;W, v))| |\u27e8xi, wj\u27e9| \u2225wj\u2225\n\u2264 2 n\u2211\ni=1\n(|yi|+ 2|f(xi;W, v)|) |\u27e8xi, wj\u27e9| \u2225wj\u2225 (by Lemma 2)\n\u2264 2 n\u2211\ni=1\n(1 + 4\u03f5 \u221a hXmaxW 2 max) |\u27e8xi, wj\u27e9| \u2225wj\u2225 (Since t \u2264 T )\n\u2264 2 n\u2211\ni=1\n(1 + 4\u03f5 \u221a hXmaxW 2 max)\u2225xi\u2225\u2225wj\u22252\n\u2264 2n(Xmax + 4\u03f5 \u221a hX2maxW 2 max)\u2225wj\u22252 . (14)\nLet \u03c4j := inf{t : \u2225wj(t)\u22252 > 2\u03f5W 2 max\u221a h }, and let j\u2217 := argminj \u03c4j , then \u03c4j\u2217 = minj \u03c4j \u2264 T due to the fact that\n|f(xi;W, v)| = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 j\u2208[h] \u03c3\u2032(\u27e8xi, wj\u27e9)vj \u27e8wj , xi\u27e9 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2211 j\u2208[h] \u2225wj\u22252\u2225xi\u2225 \u2264 hXmax max j\u2208[h] \u2225wj\u22252 ,\nwhich implies \"|f(xi;W (t), v(t))| > 2\u03f5 \u221a hXmaxW 2 max \u21d2 \u2203j, s.t.\u2225wj(t)\u22252 > 2\u03f5W 2max\u221a h \".\nThen for t \u2264 \u03c4j\u2217 , we have d\ndt \u2225wj\u2217\u22252 \u2264 2n(Xmax + 4\u03f5\n\u221a hX2maxW 2 max)\u2225wj\u2217\u22252 . (15)\nBy Gr\u00f6nwall\u2019s inequality, we have \u2200t \u2264 \u03c4j\u2217 \u2225wj\u2217(t)\u22252 \u2264 exp ( 2n(Xmax + 4\u03f5 \u221a hX2maxW 2 max)t ) \u2225wj\u2217(0)\u22252 ,\n= exp ( 2n(Xmax + 4\u03f5 \u221a hX2maxW 2 max)t ) \u03f52\u2225[W0]:,j\u2217\u22252\n\u2264 exp ( 2n(Xmax + 4\u03f5 \u221a hX2maxW 2 max)t ) \u03f52W 2max .\nSuppose \u03c4j\u2217 < 14nXmax log ( 1\u221a h\u03f5 ) , then by the continuity of \u2225wj\u2217(t)\u22252, we have\n2\u03f5W 2max\u221a h \u2264 \u2225wj\u2217(\u03c4j\u2217)\u22252 \u2264 exp\n( 2n(Xmax + 4\u03f5 \u221a hX2maxW 2 max)\u03c4j\u2217 ) \u03f52W 2max\n\u2264 exp ( 2n(Xmax + 4\u03f5 \u221a hX2maxW 2 max)\n1\n4nXmax log ( 1\u221a h\u03f5 )) \u03f52W 2max\n\u2264 exp\n( 1 + 4\u03f5 \u221a hXmaxW 2 max\n2 log ( 1\u221a h\u03f5 )) \u03f52W 2max\n\u2264 exp ( log ( 1\u221a h\u03f5 )) \u03f52W 2max = \u03f5W 2max\u221a h ,\nwhich leads to a contradiction 2\u03f5 \u2264 \u03f5. Therefore, one must have T \u2265 \u03c4j\u2217 \u2265 14nXmax log ( 1\u221a h\u03f5 ) . This finishes the proof."
        },
        {
            "heading": "B.3 PROOF OF LEMMA 4: DIRECTIONAL DYNAMICS OF NEURONS",
            "text": "Proof of Lemma 4. As we showed in the proof for Lemma 3, under balanced initialization,\nd dt wj = \u2212 n\u2211 i=1 1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v))xisign(vj(0))\u2225wj\u2225 . (16)\nThen for any i \u2208 [n], d\ndt wj \u2225wj\u2225\n= \u2212sign(vj(0)) n\u2211\ni=1\n1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v)) ( xi \u2212\n\u27e8xi, wj\u27e9 \u2225wj\u22252 wj ) = \u2212sign(vj(0))\n\u2211 i:\u27e8xi,wj\u27e9>0 \u2207y\u0302\u2113(yi, f(xi;W, v)) ( xi \u2212 \u27e8xi, wj\u27e9 \u2225wj\u22252 wj )\n= \u2212sign(vj(0)) ( I \u2212 wjw \u22a4 j\n\u2225wj\u22252\n)( n\u2211\ni=1\n\u03c3\u2032(\u27e8xi, wj\u27e9)\u2207y\u0302\u2113(yi, f(xi;W, v))xi ) .\nTherefore, whenever maxi |f(xi;W, v)| \u2264 1,\u2225\u2225\u2225\u2225\u2225 ddt wj\u2225wj\u2225 \u2212 sign(vj(0)) ( I \u2212 wjw \u22a4 j \u2225wj\u22252 ) xa(wj) \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225sign(vj(0)) ( n\u2211 i=1 \u03c3\u2032(\u27e8xi, wj\u27e9) (\u2207y\u0302\u2113(yi, f(xi;W, v)) + yi)xi\n)\u2225\u2225\u2225\u2225\u2225 \u2264\nn\u2211 i=1 |\u2207y\u0302\u2113(yi, f(xi;W, v)) + yi| \u00b7 \u2225xi\u2225\n\u2264 n\u2211\ni=1\n2|f(xi;W, v)| \u00b7 \u2225xi\u2225 \u2264 2nMx max i |f(xi;W, v)| . (17)"
        },
        {
            "heading": "C PROOF FOR THEOREM 1: EARLY ALIGNMENT PHASE",
            "text": "We break the proof of Theorem 1 into two parts: In Appendix C we prove the first part regarding directional convergence. Then in Appendix D we prove the remaining statement on final convergence and low-rank bias."
        },
        {
            "heading": "C.1 AUXILIARY LEMMAS",
            "text": "The first several Lemmas concern mostly some conic geometry given the data assumption:\nConsider the following conic hull\nK = CH({xiyi, i \u2208 [n]}) =\n{ n\u2211\ni=1\naixiyi : ai \u2265 0, i \u2208 [n] } . (18)\nIt is clear that xiyi \u2208 K,\u2200i, and xa(w) \u2208 K,\u2200w. The following lemma shows any pair of vectors in K is \u00b5-coherent. Lemma 5. cos(z1, z2) \u2265 \u00b5,\u22000 \u0338= z1, z2 \u2208 K. Proof. Since z1, z2 \u2208 K, we let z1 = \u2211n i=1 xiyia1i, andz2 = \u2211n\nj=1 xjyja2j , where a1i, a2j \u2265 0 but not all of them.\ncos(z1, z2) = 1\n\u2225z1\u2225\u2225z2\u2225 \u27e8z1, z2\u27e9 =\n1 \u2225z1\u2225\u2225z2\u2225 \u2211\ni,j\u2208[n]\na1ia2j \u27e8xiyi, xjyj\u27e9\n=\n\u2211 i,j\u2208[n] \u2225xi\u2225\u2225xj\u2225a1ia2j\u00b5\n\u2225z1\u2225\u2225z2\u2225 \u2265 \u00b5 ,\nwhere the last inequality is due to\n\u2225z1\u2225\u2225z2\u2225 \u2264\n( n\u2211\ni=1\n\u2225xi\u2225a1i ) n\u2211 j=1 \u2225xj\u2225a2j  = \u2211 i,j\u2208[n] \u2225xi\u2225\u2225xj\u2225a1ia2j .\nThe following lemma is some basic results regarding S+ and S\u2212: Lemma 6. S+ and S\u2212 are convex cones (excluding the origin).\nProof. Since 1\u27e8xi,z\u27e9 = 1\u27e8xi,az\u27e9,\u2200i \u2208 [n], a > 0, S+,S\u2212 are cones. Moreover, \u27e8xi, z1\u27e9 > 0 and \u27e8xi, z2\u27e9 > 0 implies \u27e8xi, a1z1 + a2z2\u27e9 > 0,\u2200a1, a2 > 0, thus S+,S\u2212 are convex cones.\nNow we consider the complete metric space SD\u22121 (w.r.t. arccos(\u27e8\u00b7, \u00b7\u27e9)) and we are interested in its subsets K \u2229SD\u22121, S+ \u2229SD\u22121, and S\u2212 \u2229SD\u22121. First, we have (we use Int(S) to denote the interior of S) Lemma 7. K \u2229 SD\u22121 \u2282 Int(S+ \u2229 SD\u22121), and \u2212K \u2229 SD\u22121 \u2282 Int(S\u2212 \u2229 SD\u22121) Proof. Consider any xc = \u2211n j=1 ajxjyj \u2208 K \u2229 SD\u22121, For any xi, yi, i \u2208 [n], we have\n\u27e8xc, xi\u27e9 = n\u2211\ni=j\naj\u2225xj\u2225 \u2329 xjyj \u2225xj\u2225 , xiyi \u2225xi\u2225 \u232a \u2225xi\u2225 yi\n\u2265 \u00b5yi\u2225xi\u2225 n\u2211\ni=j\naj\u2225xj\u2225 { \u2265 \u00b5Xmin > 0, yi > 0 \u2264 \u2212\u00b5Xmin < 0, yi < 0 .\nDepending on the sign of yi, we have either\n\u27e8xc, xi\u27e9 = n\u2211\ni=j\naj\u2225xj\u2225 \u2329 xjyj \u2225xj\u2225 , xiyi \u2225xi\u2225 \u232a \u2225xi\u2225 yi \u2265 \u00b5\u2225xi\u2225 yi n\u2211 i=j aj\u2225xj\u2225 \u2265 \u00b5Xmin > 0 , (yi = +1)\nor\n\u27e8xc, xi\u27e9 = n\u2211\ni=j\naj\u2225xj\u2225 \u2329 xjyj \u2225xj\u2225 , xiyi \u2225xi\u2225 \u232a \u2225xi\u2225 yi \u2264 \u00b5\u2225xi\u2225 yi n\u2211 i=j aj\u2225xj\u2225 \u2264 \u2212\u00b5Xmin < 0 , (yi = \u22121)\nwhere we use the fact that 1 = \u2225xc\u2225 = \u2225 \u2211n j=1 ajxjyj\u2225 \u2264 \u2211n\nj=1 aj\u2225xj\u2225. This already tells us xc \u2208 S+ \u2229 SD\u22121. Since fi(z) = \u27e8z, xi\u27e9 is a continuous function of z \u2208 SD\u22121. There exists an open ball B (xc, \u03b4i) centered at xc with some radius \u03b4i > 0, such that \u2200z \u2208 B (xc, \u03b4i), one have |fi(z)\u2212 fi (xc)| \u2264 \u00b5Xmin\n2 , which implies\n\u27e8z, xi\u27e9 { \u2265 \u00b5Xmin/2 > 0, yi > 0 \u2264 \u2212\u00b5Xmin/2 < 0, yi < 0 .\nHence \u2229ni=1B ( xc \u2225xc\u2225 , \u03b4i ) \u2208 S+ \u2229 SD\u22121. Therefore, xc \u2208 Int(S+ \u2229 SD\u22121). This suffices to show K \u2229 SD\u22121 \u2282 Int(S+ \u2229 SD\u22121). The other statement \u2212K \u2229 SD\u22121 \u2282 Int(S\u2212 \u2229 SD\u22121) is proved similarly.\nThe following two lemmas are some direct results of Lemma 7. Lemma 8. \u2203\u03b61 > 0 such that\nS\u03b61x+ \u2282 S+, S \u03b61 x\u2212 \u2282 S\u2212 , (19)\nwhere S\u03b6x := {z \u2208 RD : cos(z, x) \u2265 \u221a 1\u2212 \u03b6}.\nProof. By Lemma 7, x+\u2225x+\u2225 \u2208 K \u2282 Int(S+). Since S D\u22121 is a complete metric space (w.r.t\narccos \u27e8\u00b7, \u00b7\u27e9), there exists a open ball centered at x+\u2225x+\u2225 of some radius arccos( \u221a 1\u2212 \u03b61) that is a subset of S+, from which one can show S\u03b61x+ \u2282 S+. The other statement S \u03b61 x\u2212 \u2282 S\u2212 simply comes from the fact that x+ = \u2212x\u2212 and Int(S+) = \u2212Int(S\u2212).\nLemma 9. \u2203\u03be > 0, such that\nsup x1\u2208K\u2229SD\u22121,x2\u2208(S+\u2229SD\u22121)c\u2229(S\u2212\u2229SD\u22121)c\n| cos(x1, x2)| \u2264 \u221a 1\u2212 \u03be . (20)\n(Sc here is defined to be SD\u22121 \u2212 S, the set complement w.r.t. complete space SD\u22121)\nProof. Notice that\nsup x1\u2208K\u2229SD\u22121,x2\u2208(Int(S+\u2229SD\u22121))c \u27e8x1, x2\u27e9 = inf x1\u2208K\u2229SD\u22121,x2\u2208(Int(S+\u2229SD\u22121))c arccos \u27e8x1, x2\u27e9 .\nSince SD\u22121 is a complete metric space (w.r.t arccos \u27e8\u00b7, \u00b7\u27e9) andK\u2229SD\u22121 and x2 \u2208 (Int(S+\u2229SD\u22121))c are two of its compact subsets. Suppose\ninf x1\u2208K\u2229SD\u22121,x2\u2208x2\u2208(Int(S+\u2229SD\u22121))c\narccos \u27e8x1, x2\u27e9 = 0 ,\nthen \u2203x1 \u2208 K \u2229SD\u22121, x2 \u2208 (Int(S+\u2229SD\u22121))c such that arccos \u27e8x1, x2\u27e9 = 0, i.e., x1 = x2, which contradicts the fact that K \u2229 SD\u22121 \u2286 Int(S+ \u2229 SD\u22121) (Lemma 7). Therefore, we have the infimum strictly larger than zero, then\nsup x1\u2208K\u2229SD\u22121,x2\u2208(S+\u2229SD\u22121)c \u27e8x1, x2\u27e9 \u2264 sup x1\u2208K\u2229SD\u22121,x2\u2208(Int(S+\u2229SD\u22121))c \u27e8x1, x2\u27e9 < 1 . (21)\nSimilarly, one can show that\nsup x1\u2208\u2212K\u2229SD\u22121,x2\u2208(S\u2212\u2229SD\u22121)c\n\u27e8x1, x2\u27e9 < 1 . (22)\nFinally, find \u03be < 1 such that\nmax { sup\nx1\u2208K\u2229SD\u22121,x2\u2208(S+\u2229SD\u22121)c \u27e8x1, x2\u27e9 , sup x1\u2208\u2212K\u2229SD\u22121,x2\u2208(S\u2212\u2229SD\u22121)c \u27e8x1, x2\u27e9\n} = \u221a 1\u2212 \u03be ,\nthen for any x1 \u2208 K \u2229 SD\u22121 and x2 \u2208 (S+ \u2229 SD\u22121)c \u2229 (S\u2212 \u2229 SD\u22121)c, we have \u2212 \u221a 1\u2212 \u03be \u2264 \u27e8x1, x2\u27e9 \u2264 \u221a 1\u2212 \u03be ,\nwhich is the desired result.\nThe remaining two lemmas are technical but extensively used in the main proof. Lemma 10. Consider any solution to the gradient flow dynamic (2) starting from initialization (3). Let xr \u2208 Sn\u22121 be some reference direction, we define\n\u03c8rj = \u2329 xr,\nwj \u2225wj\u2225\n\u232a , \u03c8ra = \u2329 xr, xa(wj)\n\u2225xa(wj)\u2225\n\u232a , \u03c8aj = \u2329 wj \u2225wj\u2225 , xa(wj) \u2225xa(wj)\u2225 \u232a , (23)\nwhere xa(wj) = \u2211\ni:\u27e8xi,wj\u27e9>0 yixi.\nWhenever maxi |f(xi;W, v)| \u2264 1, we have\u2223\u2223\u2223\u2223 ddt\u03c8rj \u2212 sign(vj(0)) (\u03c8ra \u2212 \u03c8rj\u03c8aj) \u2225xa(wj)\u2225 \u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| . (24)\nProof. A simple application of Lemma 4, together with Cauchy-Schwartz:\u2223\u2223\u2223\u2223 ddt\u03c8rj \u2212 sign(vj(0)) (\u03c8ra \u2212 \u03c8rj\u03c8aj) \u2225xa(wj)\u2225 \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2223\u2223x\u22a4r  d dt wj \u2225wj\u2225 \u2212 sign(vj(0)) ( I \u2212 wjw \u22a4 j \u2225wj\u22252 ) \u2211 i:\u27e8xi,wj\u27e9>0 yixi \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| .\nLemma 11. \u2225xa(w)\u2225 \u2265 \u221a \u00b5na(w)Xmin , (25)\nwhere na(w) = |{i \u2208 [n] : \u27e8xi, w\u27e9 > 0}|.\nProof. Let Ia(w) denote {i \u2208 [n] : \u27e8xi, w\u27e9 > 0}, then\n\u2225xa(w)\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\ni:\u27e8xi,w\u27e9>0\nxiyi \u2225\u2225\u2225\u2225\u2225\u2225 = \u221a\u221a\u221a\u221a \u2211\ni\u2208Ia(w)\n\u2225xi\u22252y2i + \u2211\ni,j\u2208Ia(w),i<j\n\u2225xi\u2225\u2225xj\u2225 \u2329 xiyi \u2225xi\u2225 , xjyj \u2225xj\u2225 \u232a\n\u2265 \u221a \u2211\ni\u2208Ia(w)\n\u2225xi\u22252y2i + \u2211\ni,j\u2208Ia(w),i<j\n\u2225xi\u2225\u2225xj\u2225|yi||yj |\u00b5\n\u2265 \u221a na(w)X2min + \u00b5na(w) (na(w)\u2212 1)X2min\n\u2265 \u221a na(w)(1 + \u00b5(na(w)\u2212 1))Xmin \u2265 \u221a\u00b5na(w)Xmin ."
        },
        {
            "heading": "C.2 PROOF FOR EARLY ALIGNMENT PHASE",
            "text": "Proof of Theorem 1: First Part. Given some initialization in (3), by Assumption 2, \u2203\u03b62 > 0, such that\nmax j\u2208V+\ncos(wj(0), x\u2212) < \u221a 1\u2212 \u03b62, max\nj\u2208V\u2212 cos(wj(0), x+) <\n\u221a 1\u2212 \u03b62 . (26)\nWe define \u03b6 := max{\u03b61, \u03b62}, where \u03b61 is from Lemma 8. In addition, by Lemma 9, \u2203\u03be > 0, such that\nsup x1\u2208K\u2229SD\u22121,x2\u2208Sc\u2212\u2229Sc+\u2229SD\u22121\n| cos(x1, x2)| \u2264 \u221a 1\u2212 \u03be . (27)\nWe pick a initialization scale \u03f5 that satisfies: \u03f5 \u2264 min { min{\u00b5, \u03b6, \u03be}\u221a\u00b5Xmin 4 \u221a hnX2maxW 2 max , 1\u221a h exp ( \u2212 64nXmax min{\u03b6, \u03be}\u221a\u00b5Xmin log n )} \u2264 1 4 \u221a hXmaxW 2max . (28) By Lemma 3, \u2200t \u2264 T = 14nXmax log 1\u221a h\u03f5 , we have\nmax i |f(xi;W, v)| \u2264 min{\u00b5, \u03b6, \u03be}\u221a\u00b5Xmin 4nXmax , (29)\nwhich is the key to analyzing the alignment phase. For the sake of simplicity, we only discuss the analysis of neurons in V+ here, the proof for neurons in V\u2212 is almost identical. Activation pattern evolution: Pick any wj in V+ and pick xr = xiyi for some i \u2208 [n], and consider the case when \u27e8wj , xi\u27e9 = 0. From Lemma 10,we have\u2223\u2223\u2223\u2223 ddt\u03c8rj \u2212 (\u03c8ra \u2212 \u03c8rj\u03c8aj) \u2225xa(wj)\u2225\n\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| . \u27e8wj , xi\u27e9 = 0 implies \u03c8rj = \u2329 xiyi \u2225xi\u2225 , wj \u2225wj\u2225\n\u232a = 0, thus we have\u2223\u2223\u2223\u2223 ddt\u03c8rj |\u27e8wj ,xi\u27e9=0 \u2212 \u03c8ra\u2225xa(wj)\u2225\n\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| . Then whenever wj /\u2208 Sdead, we have\nd dt \u03c8rj |\u27e8wj ,xi\u27e9=0 \u2265 \u03c8ra\u2225xa(wj)\u2225 \u2212 2nXmax maxi |f(xi;W, v)|\n\u2265 \u00b5\u2225xa(wj)\u2225 \u2212 2nXmax max i |f(xi;W, v)| (by Lemma 5)\n\u2265 \u00b53/2Xmin \u2212 2nXmax max i |f(xi;W, v)| (by Lemma 11)\n\u2265 \u00b53/2Xmin/2 > 0 . (by (29)) This is precisely (7) in Section 3.3.\nBound on activation transitions and duration: Next we show that if at time t0 < T , wj(t0) /\u2208 S+\u222a Sdead, and the activation pattern of wj is 1\u27e8xi,wj(t0)\u27e9>0, then 1\u27e8xi,wj(t0+\u2206t))\u27e9>0 \u0338= 1\u27e8xi,wj(t0)\u27e9>0, where \u2206t = 4min{\u03b6,\u03be}\u221a\u00b5Xminna(wj(t0)) and na(wj(t0)) is defined in Lemma 11 as long as t0 +\u2206t < T as well. That is, during the alignment phase [0, T ], wj must change its activation pattern within \u2206t time. There are two cases:\n\u2022 The first case is when wj(t0) \u2208 Sc+ \u2229 Sc\u2212 \u2229 Scdead. In this case, suppose that 1\u27e8xi,wj(t0+\u03c4))\u27e9>0 = 1\u27e8xi,wj(t0)\u27e9>0,\u22000 \u2264 \u03c4 \u2264 \u2206t, i.e. wj fixes its activation during [t0, t0 + \u2206t], then we have xa(wj(t0 + \u03c4)) = xa(wj(t0)),\u22000 \u2264 \u03c4 \u2264 \u2206t. Let us pick xr = xa(wj(t0)), then Lemma 10 leads to\u2223\u2223\u2223\u2223 ddt cos(wj , xa(wj))\u2212 (1\u2212 cos2(wj , xa(wj))) \u2225xa(wj)\u2225\n\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| . Since xa(wj) is fixed, we have \u2200t \u2208 [t0, t0 +\u2206t],\u2223\u2223\u2223\u2223 ddt cos(wj , xa(wj(t0)))\u2212 (1\u2212 cos2(wj , xa(wj(t0)))) \u2225xa(wj(t0))\u2225\n\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| , d dt cos(wj , xa(wj(t0))) \u2265 ( 1\u2212 cos2(wj , xa(wj(t0))) ) \u2225xa(wj(t0))\u2225\n\u2212 2nXmax max i |f(xi;W, v)|\n\u2265 \u03be\u2225xa(wj(t0))\u2225 \u2212 2nXmax max i |f(xi;W, v)| (by (27)) \u2265 \u03be\u221a\u00b5na(wj(t0))Xmin \u2212 2nXmax max i |f(xi;W, v)| (by Lemma 11) \u2265 \u03be\u221a\u00b5na(wj(t0))Xmin/2 . (by (29)) \u2265 min{\u03be, \u03b6}\u221a\u00b5na(wj(t0))Xmin/2 ,\nwhich implies that, by the Fundamental Theorem of Calculus,\ncos(wj(t0 +\u2206t), xa(wj(t0)))\n= cos(wj(t0), xa(wj(t0))) + \u222b \u2206t 0 d dt cos(wj(t0 + \u03c4), xa(wj(t0)))d\u03c4 \u2265 cos(wj(t0), xa(wj(t0))) + \u2206t \u00b7min{\u03be, \u03b6} \u221a \u00b5na(wj(t0))Xmin/2\n= cos(wj(t0), xa(wj(t0))) + 2 \u2265 1 ,\nwhich leads to cos(wj(t0 +\u2206t), xa(wj(t0))) = 1. This would imply wj(t0 +\u2206t) \u2208 S+ because xa(wj(t0)) \u2208 S+, which contradicts our original assumption that wj fixes the activation pattern. Therefore, \u22030 < \u03c40 \u2264 \u2206t such that 1\u27e8xi,wj(t0+\u03c40))\u27e9 \u0338= 1\u27e8xi,wj(t0)\u27e9>0, due to the restriction on how wj can change its activation pattern, it cannot return to its previous activation pattern, then one must have 1\u27e8xi,wj(t0+\u2206t))\u27e9 \u0338= 1\u27e8xi,wj(t0)\u27e9>0.\n\u2022 The other case is when wj(t0) \u2208 S\u2212. For this case, we need first show that wj(t0 + \u03c4) /\u2208 S\u03b6x\u2212 ,\u22000 \u2264 \u03c4 \u2264 \u2206t, or more generally, S \u03b6 x\u2212 does not contain any wj in V+ during [0, T ]. To see\nthis, let us pick xr = x\u2212, then Lemma 10 suggests that\u2223\u2223\u2223\u2223 ddt\u03c8rj \u2212 (\u03c8ra \u2212 \u03c8rj\u03c8aj) \u2225xa(wj)\u2225 \u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| . Consider the case when cos(wj , x\u2212) = \u221a 1\u2212 \u03b6, i.e. wj is at the boundary of S\u03b6x\u2212 . We know that in this case, wj \u2208 S\u03b6x\u2212 \u2286 S\u2212 thus xa(wj) = \u2212x\u2212, and\u2223\u2223\u2223\u2223\u2223 ddt cos(wj , x\u2212) \u2223\u2223\u2223\u2223 cos(wj ,x\u2212)= \u221a 1\u2212\u03b6 + ( 1\u2212 cos2(wj , x\u2212) ) \u2225x\u2212\u2225\n\u2223\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| , which is\u2223\u2223\u2223\u2223\u2223 ddt cos(wj , x\u2212) \u2223\u2223\u2223\u2223 cos(wj ,x\u2212)= \u221a 1\u2212\u03b6 + \u03b6\u2225x\u2212\u2225\n\u2223\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| \u21d2 d\ndt cos(wj , x\u2212) \u2223\u2223\u2223\u2223 cos(wj ,x\u2212)= \u221a 1\u2212\u03b6\n\u2264 \u2212\u03b6\u2225x\u2212\u2225+ 2nXmax max i |f(xi;W, v)| \u2264 \u2212\u03b6\u221a\u00b5Xmin + 2nXmax max i |f(xi;W, v)| (by Lemma 11) \u2264 \u2212\u03b6\u221a\u00b5Xmin/2 < 0 . (by (29))\nTherefore, during [0, T ], neuron wj in V+ cannot enter S\u03b6x\u2212 if at initialization, wj(0) /\u2208 S \u03b6 x\u2212 , which is guaranteed by (26).\nWith the argument above, we know that wj(t0 + \u03c4) /\u2208 S\u03b6x\u2212 ,\u22000 \u2264 \u03c4 \u2264 \u2206t. Again we suppose that wj(t) \u2208 S\u2212 \u2212 S\u03b6x\u2212 ,\u2200t \u2208 [t0, t0 +\u2206t], i.e.,wj fixes its activation during [t0, t0 +\u2206t]. Let us pick xr = x\u2212, then Lemma 10 suggests that\u2223\u2223\u2223\u2223 ddt cos(wj , x\u2212) + (1\u2212 cos2(wj , x\u2212)) \u2225x\u2212\u2225\n\u2223\u2223\u2223\u2223 \u2264 2nXmax maxi |f(xi;W, v)| , which leads to \u2200t \u2208 [t0, t0 +\u2206t], d dt cos(wj , x\u2212) \u2264 \u2212 ( 1\u2212 cos2(wj , x\u2212) ) \u2225x\u2212\u2225+ 2nXmax max i |f(xi;W, v)|\n\u2264 \u2212\u03b6\u2225x\u2212\u2225+ 2nXmax max i |f(xi;W, v)| (wj /\u2208 S\u03b6x\u2212) \u2264 \u2212\u03b6\u221a\u00b5na(wj(t0))Xmin + 2nXmax max i |f(xi;W, v)| (by Lemma 11) \u2264 \u2212\u03b6\u221a\u00b5na(wj(t0))Xmin/2 . (by (29)) \u2264 \u2212min{\u03be, \u03b6}\u221a\u00b5na(wj(t0))Xmin/2 ,\nSimilarly, by FTC, we have cos(wj(t0 +\u2206t), x\u2212) \u2264 \u22121 .\nThis would imply wj(t0 + \u2206t) \u2208 S+ because \u2212x\u2212 = xa(wj(t0)) \u2208 S+, which contradicts our original assumption that wj fixes its activation pattern. Therefore, one must have 1\u27e8xi,wj(t0+\u2206t))\u27e9 \u0338= 1\u27e8xi,wj(t0)\u27e9>0.\nIn summary, we have shown that, during [0, T ], a neuron in V+ can not keep a fixed activation pattern for a time longer than \u2206t = 4min{\u03b6,\u03be}\u221a\u00b5Xminna , where na is the number of data points that activate wj under the fixed activation pattern.\nBound on total travel time until directional convergence As we have discussed in Section 3.3 and also formally proved here, during alignment phase [0, T ], a neuron in V+ must change its activation pattern within \u2206t = 4min{\u03b6,\u03be}\u221a\u00b5Xminna time unless it is in either S+ or Sdead. And the new activation it is transitioning into must contain no new activation on negative data points and must keep all existing activation on positive data points, together it shows that a neuron must reach either S+ or Sdead within a fixed amount of time, which is the remaining thing we need to formally show here. For simplicity of the argument, we first assume T =\u221e, i.e., the alignment phase lasts indefinitely, and we show that a neuron in V+ must reach S+ or Sdead before t1 = 16 lognmin{\u03b6,\u03be}\u221a\u00b5Xmin . Lastly, such directional convergence can be achieved if t1 \u2264 T , which is guaranteed by our choice of \u03f5 in (28).\n\u2022 For a neuron in V+ that reaches Sdead, the analysis is easy: It must start with no activation on positive data and then lose activation on negative data one by one until losing all of its activation. Therefore, it must reach Sdead before na(wj(0))\u2211\nk=1\n4 min{\u03b6, \u03be}\u221a\u00b5Xmink \u2264 4 min{\u03b6, \u03be}\u221a\u00b5Xmin\n( n\u2211\nk=1\n1\nk\n) \u2264 16 log n\nmin{\u03b6, \u03be}\u221a\u00b5Xmin = t1 .\n\u2022 For a neuron in V+ that reaches S+, there is no difference conceptually, but it can switch its activation pattern in many ways before reaching S+, so it is not straightforward to see its travel time until S+ is upper bounded by t1. To formally show the upper bound on the travel time, we need some definition of a path that keeps a record of the activation patterns of a neuron wj(t) before it reaches S+. Let n+ = |I+|, n\u2212 = |I\u2212| be the number of positive, negative data respectively, then we call P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L)) a path of length-L, if\n1. \u22000 \u2264 l \u2264 L, we have k(l) = (k(l)+ , k (l) \u2212 ) \u2208 N\u00d7 N with 0 \u2264 k (l) + \u2264 n+, 0 \u2264 k (l) \u2212 \u2264 n\u2212; 2. For k(l1), k(l2) with l1 < l2, we have either k (l1) + > k (l2) + or k (l1) \u2212 < k (l2) \u2212 ; 3. k(L) = (n+, 0); 4. k(l) \u0338= (0, 0),\u22000 \u2264 l \u2264 L.\nGiven all our analysis on how a neuron wj(t) can switch its activation pattern in previous parts, we know that for any wj(t) that reaches S+, there is an associated P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L)) that keeps an ordered record of encountered values of\n(|{i \u2208 I+ : \u27e8xi, wj(t)\u27e9 > 0}|, |{i \u2208 I\u2212 : \u27e8xi, wj(t)\u27e9 > 0}|) , before wj reaches S+. That is, a neuron wj starts with some activation pattern that activates k+(0) positive data and k\u2212(0) negative data, then switch its activation pattern (by either losing negative data or gaining positive data) to one that activates k+(1) positive data and k\u2212(1) negative data. By keep doing so, it reaches S+ that activates k+(L) = n+ positive data and k\u2212(L) = 0 negative data. Please see Figure 14 for an illustration of a path.\nGiven a path P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L)) of neuron wj , we define the travel time of this path as\nT (P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L))) = L\u22121\u2211 l=0\n4\nmin{\u03b6, \u03be}\u221a\u00b5Xmin(k(l)+ + k (l) \u2212 )\n,\nwhich is exactly the traveling time from k(0) to k(L) if one spends 4 min{\u03b6,\u03be}\u221a\u00b5Xmin(k(l)+ +k (l) \u2212 ) on the edge between k(l) and k(l+1).\nOur analysis shows that if wj reaches S+, then inf{t : wj(t) \u2208 S+} \u2264 T (P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L))) .\nNow we define the maximal path Pmax as a path that has the maximum length n = n+ + n\u2212, which is uniquely determined by the following trajectory of k(l)\n(0, n\u2212), (0, n\u2212 \u2212 1), (0, n\u2212 \u2212 2), \u00b7 \u00b7 \u00b7 , (0, 1), (1, 1), (1, 0), \u00b7 \u00b7 \u00b7 , (n+ \u2212 1, 0), (n+, 0) . Please see Figure 15 for an illustration.\nThe traveling time for Pmax is\nT (Pmax) = 4\nmin{\u03b6, \u03be}\u221a\u00b5Xmin ( n\u2212\u2211 k=1 1 k + 1 2 + n+\u22121\u2211 k=1 1 k )\n\u2264 4 min{\u03b6, \u03be}\u221a\u00b5Xmin\n( 2\nn\u2211 k=1 1 k + 1 2\n)\n\u2264 16 log n min{\u03b6, \u03be}\u221a\u00b5Xmin = t1 .\nThe proof is complete by the fact that any path satisfies\nT (P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L))) \u2264 T (Pmax) .\nThis is because there is a one-to-one correspondence between the edges (k(l), k(l+1)) in P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L)) and a subset of edges in Pmax, and the travel time from of edge (k(l), k(l+1)) is shorter than the corresponding edge in Pmax. Formally stating such correspondence is tedious and a visual illustration in Figure 16 and 17 is more effective (Putting all correspondence makes a clustered plot thus we split them into two figures):\nTherefore, if wj reaches S+, then it reaches S+ within t1: inf{t : wj(t) \u2208 S+} \u2264 T (P(k(0),k(1),\u00b7\u00b7\u00b7 ,k(L))) \u2264 T (Pmax) \u2264 t1 .\nSo far we have shown when the alignment phase lasts long enough, i.e., T large enough, the directional convergence is achieved by t1. We simply pick \u03f5 such that\nT = 1\n4nXmax log 1\u221a h\u03f5 \u2265 t1 =\n16 log n\nmin{\u03b6, \u03be}\u221a\u00b5Xmin ,\nand (28) suffices."
        },
        {
            "heading": "D PROOF FOR THEOREM 1: FINAL CONVERGENCE",
            "text": "Since we have proved the first part of Theorem 1 in Section C, we will use it as a fact, then prove the remaining part of Theorem 1."
        },
        {
            "heading": "D.1 AUXILIARY LEMMAS",
            "text": "First, we show that S+,S\u2212,Sdead are trapping regions. Lemma 12. Consider any solution to the gradient flow dynamic (2), we have the following:\n\u2022 If at some time t1 \u2265 0, we have wj(t1) \u2208 Sdead, then wj(t1 + \u03c4) \u2208 Sdead, \u2200\u03c4 \u2265 0;\n\u2022 If at some time t1 \u2265 0, we have wj(t1) \u2208 S+ for some j \u2208 V+, then wj(t1 + \u03c4) \u2208 S+, \u2200\u03c4 \u2265 0;\n\u2022 If at some time t1 \u2265 0, we have wj(t1) \u2208 S\u2212 for some j \u2208 V\u2212, then wj(t1 + \u03c4) \u2208 S\u2212, \u2200\u03c4 \u2265 0;\nProof. The first statement is simple, if wj \u2208 Sdead, then one have w\u0307j = 0, thus wj remains in Sdead. For the second statement, we have, since j \u2208 V+,\nd dt wj = \u2212 n\u2211 i=1 1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v))xi\u2225wj\u2225 .\nWhen \u2113 is the exponential loss, by the Fundamental Theorem of Calculus, one writes, \u2200\u03c4 \u2265 0,\nwj(t1 + \u03c4) = wj(t1) + \u222b \u03c4 0 d dt wjd\u03c4\n= wj(t1) + \u222b \u03c4 0 \u2212 n\u2211\ni=1\n1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v))xi\u2225wj\u2225d\u03c4\n= wj(t1) + \u222b \u03c4 0 n\u2211 i=1 1\u27e8xi,wj\u27e9>0yi exp(\u2212yif(xi;W, v))xi\u2225wj\u2225d\u03c4\n= wj(t1) + \u2211 i\u2208I+ (\u222b \u03c4 0 exp(\u2212yif(xi;W, v))\u2225wj\u2225d\u03c4 ) xi\ufe38 \ufe37\ufe37 \ufe38\n:=x\u0303+\n.\nHere wj(t1) \u2208 S+ by our assumption, x\u0303+ \u2208 K \u2286 S+ because x\u0303+ is a conical combination of xi, i \u2208 I+. Since S+ is a convex cone, we have wj(t1 + \u03c4) \u2208 S+ as well.\nWhen \u2113 is the logistic loss, we have, similarly,\nwj(t1 + \u03c4) = wj(t1) + \u222b \u03c4 0 n\u2211 i=1 1\u27e8xi,wj\u27e9>0yi 2 exp(\u2212yif(xi;W, v)) 1 + exp(\u2212yif(xi;W, v)) xi\u2225wj\u2225d\u03c4\n= wj(t1) + \u2211 i\u2208I+ (\u222b \u03c4 0 2 exp(\u2212yif(xi;W, v)) 1 + exp(\u2212yif(xi;W, v)) \u2225wj\u2225d\u03c4 ) xi\ufe38 \ufe37\ufe37 \ufe38\n:=x\u0303+\n\u2208 S+ .\nThe proof of the third statement is almost identical (we only show the case of exponential loss here): when j \u2208 V\u2212, we have\nd dt wj = n\u2211 i=1 1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v))xi\u2225wj\u2225 ,\nand\nwj(t1 + \u03c4) = wj(t1) + \u2211 i\u2208I\u2212 (\u222b \u03c4 0 exp(\u2212yif(xi;W, v))\u2225wj\u2225d\u03c4 ) xi\ufe38 \ufe37\ufe37 \ufe38\n:=x\u0303\u2212\n.\nAgain, here wj(t1) \u2208 S\u2212 by our assumption, x\u0303\u2212 \u2208 \u2212K \u2286 S\u2212 because x\u0303\u2212 is a conical combination of xi, i \u2208 I\u2212. Since S\u2212 is a convex cone, we have wj(t1 + \u03c4) \u2208 S+ as well.\nThen the following Lemma provides a lower bound on neuron norms upon t1. Lemma 13. Consider any solution to the gradient flow dynamic (2) starting from initialization (3). Let t1 be the time when directional convergence is achieved, as defined in Theorem 1, and we define V\u0303+ : {j : wj(t1) \u2208 S+} and V\u0303\u2212 : {j : wj(t1) \u2208 S\u2212}. If both V\u0303+ and V\u0303\u2212 are non-empty, we have\u2211\nj\u2208V\u0303+ \u2225wj(t1)\u22252 \u2265 exp(\u22124nXmaxt1) \u2211 j\u2208V\u0303+ \u2225wj(0)\u22252,\n\u2211 j\u2208V\u0303\u2212 \u2225wj(t1)\u22252 \u2265 exp(\u22124nXmaxt1) \u2211 j\u2208V\u0303\u2212 \u2225wj(0)\u22252,\nProof. We have shown that\nd dt \u2225wj\u22252 = \u22122 n\u2211 i=1 1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v)) \u27e8xi, wj\u27e9 sign(vj(0))\u2225wj\u2225 .\nThen before t1, we have \u2200j \u2208 [h] d\ndt \u2225wj\u22252 = \u22122 n\u2211 i=1 1\u27e8xi,wj\u27e9>0\u2207y\u0302\u2113(yi, f(xi;W, v)) \u27e8xi, wj\u27e9 sign(vj(0))\u2225wj\u2225\n\u2265 \u22122 n\u2211\ni=1\n(|yi|+ 2max i |f(xi;W, v)|)\u2225xi\u2225\u2225wj\u22252\n\u2265 \u22124 n\u2211\ni=1\n\u2225xi\u2225\u2225wj\u22252 \u2265 \u22124nXmax\u2225wj\u22252 ,\nwhere the second last inequality is because maxi |f(xi;W, v)| \u2264 12 before t1. Summing over j \u2208 V\u0303+, we have\nd\ndt \u2211 j\u2208V\u0303+ \u2225wj\u22252 \u2265 \u22124nXmax \u2211 j\u2208V\u0303+ \u2225wj\u22252 .\nTherefore, we have the following bound:\u2211 j\u2208V\u0303+ \u2225wj(t1)\u22252 \u2265 exp(\u22124nXmaxt1) \u2211 j\u2208V\u0303+ \u2225wj(0)\u22252 .\nMoreover, after t1, the neuron norms are non-decreasing, as suggested by Lemma 14. Consider any solution to the gradient flow dynamic (2) starting from initialization (3). Let t1 be the time when directional convergence is achieved, as defined in Theorem 1, and we define V\u0303+ : {j : wj(t1) \u2208 S+} and V\u0303\u2212 : {j : wj(t1) \u2208 S\u2212}. If both V\u0303+ and V\u0303\u2212 are non-empty, we have \u2200\u03c4 \u2265 0 and t2 \u2265 t1,\u2211\nj\u2208V\u0303+ \u2225wj(t2 + \u03c4)\u22252 \u2265 \u2211 j\u2208V\u0303+ \u2225wj(t2)\u2225, \u2211 j\u2208V\u0303\u2212 \u2225wj(t2 + \u03c4)\u22252 \u2265 \u2211 j\u2208V\u0303\u2212 \u2225wj(t2)\u2225 (30)\nProof. It suffices to show that after t1, the following derivatives:\nd\ndt \u2211 j\u2208V\u0303+ \u2225wj(t)\u22252, d dt \u2211 j\u2208V\u0303\u2212 \u2225wj(t)\u22252 ,\nare non-negative.\nFor j \u2208 V\u0303+, wj stays in S+ by Lemma 12, and we have d\ndt \u2225wj\u22252 = \u22122 \u2211 i\u2208I+ \u2207y\u0302\u2113(yi, f(xi;W, v)) \u27e8xi, wj\u27e9 \u2225wj\u2225 .\n=\n{ 2 \u2211\ni\u2208I+ yi exp(\u2212yif(xi;W, v)) \u27e8xi, wj\u27e9 \u2225wj\u2225 (\u2113 is exponential) 2 \u2211\ni\u2208I+ yi 2 exp(\u2212yif(xi;W,v)) 1+exp(\u2212yif(xi;W,v)) \u27e8xi, wj\u27e9 \u2225wj\u2225 (\u2113 is logistic)\n\u2265 0 . Summing over j \u2208 V\u0303+, we have ddt \u2211 j\u2208V\u0303+ \u2225wj(t)\u2225 2 \u2265 0. Similarly one has ddt \u2211 j\u2208V\u0303\u2212 \u2225wj(t)\u2225\n2 \u2265 0.\nFinally, the following lemma is used for deriving the final convergence. Lemma 15. Consider the following loss function\nLlin(W, v) = n\u2211\ni=1\n\u2113 ( yi, v \u22a4W\u22a4xi) ) ,\nif {xi, yi}, i \u2208 [n] are linearly separable, i.e., \u2203\u03b3 > 0 and z \u2208 SD\u22121 such that yi \u27e8z, xi\u27e9 \u2265 \u03b3,\u2200i \u2208 [n], then under the gradient flow on Llin(W, v), whenever yiv\u22a4W\u22a4xi \u2265 0, \u2200i, we have\nL\u0307lin \u2264 \u2212 1\n4 \u2225v\u22252L2\u03b32 . (31)\nProof. For \u2113 being exponential loss, we have:\nL\u0307 = \u2212\u2225\u2207WL\u22252F \u2212 \u2225\u2207vL\u22252F \u2264 \u2212\u2225\u2207WL\u22252F\n= \u2212 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nyi\u2113(yi, v \u22a4W\u22a4xi)xiv \u22a4 \u2225\u2225\u2225\u2225\u2225 2\nF = \u2212\u2225v\u22252 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 yi\u2113(yi, v \u22a4W\u22a4xi)xi \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2212\u2225v\u22252 \u2223\u2223\u2223\u2223\u2223 \u2329 z, n\u2211 i=1 yi\u2113(yi, v \u22a4W\u22a4xi)xi \u232a\u2223\u2223\u2223\u2223\u2223 2\n\u2264 \u2212\u2225v\u22252 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2113(yi, v \u22a4W\u22a4xi)\u03b3 \u2223\u2223\u2223\u2223\u2223 2\n\u2264 \u2212\u2225v\u22252L2\u03b32 \u2264 \u22121 4 \u2225v\u22252L2\u03b32 .\nFor \u2113 being logistic loss, we have:\nL\u0307 = \u2212\u2225\u2207WL\u22252F \u2212 \u2225\u2207vL\u22252F \u2264 \u2212\u2225\u2207WL\u22252F\n= \u2212 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nyi 2 exp(\u2212yiv\u22a4W\u22a4xi)\n1 + exp(\u2212yiv\u22a4W\u22a4xi) xiv\n\u22a4 \u2225\u2225\u2225\u2225\u2225 2\nF = \u2212\u2225v\u22252 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 yi 2 exp(\u2212yiv\u22a4W\u22a4xi) 1 + exp(\u2212yiv\u22a4W\u22a4xi) xi \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2212\u2225v\u22252 \u2223\u2223\u2223\u2223\u2223 \u2329 z, n\u2211 i=1 yi 2 exp(\u2212yiv\u22a4W\u22a4xi) 1 + exp(\u2212yiv\u22a4W\u22a4xi) xi \u232a\u2223\u2223\u2223\u2223\u2223 2\n\u2264 \u2212\u2225v\u22252 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 2 exp(\u2212yiv\u22a4W\u22a4xi) 1 + exp(\u2212yiv\u22a4W\u22a4xi) \u03b3 \u2223\u2223\u2223\u2223\u2223 2\n= \u2212\u2225v\u22252\u03b32 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 2 exp(\u2212yiv\u22a4W\u22a4xi) 1 + exp(\u2212yiv\u22a4W\u22a4xi) \u2223\u2223\u2223\u2223\u2223 2\n\u2264 \u2212\u2225v\u22252\u03b32 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 log(1 + exp(\u2212yiv\u22a4W\u22a4xi)) \u2223\u2223\u2223\u2223\u2223 2\n= \u22121 4 \u2225v\u22252L2\u03b32 ,\nwhere the last inequality uses the fact that 2 z1+z \u2265 log(1 + z) when z \u2208 [0, 1]."
        },
        {
            "heading": "D.2 PROOF OF FINAL CONVERGENCE",
            "text": "Proof of Theorem 1: Second Part. By Lemma 12, we know that after t1, neurons in S+ (S\u2212) stays in S+ (S\u2212). Thus the loss can be decomposed as\nL = \u2211 i\u2208I+ \u2113 yi, \u2211 j\u2208V\u0303+ vj \u27e8wj , xi\u27e9  \ufe38 \ufe37\ufe37 \ufe38\nL+\n+ \u2211 i\u2208I\u2212 \u2113 yi, \u2211 j\u2208V\u0303\u2212 vj \u27e8wj , xi\u27e9  \ufe38 \ufe37\ufe37 \ufe38\nL\u2212\n, (32)\nwhere V\u0303+ : {j : wj(t1) \u2208 S+} and V\u0303\u2212 : {j : wj(t1) \u2208 S\u2212}. Therefore, the training after t1 is decoupled into 1) using neurons in V\u0303+ to fit positive data in I+ and 2) using neurons in V\u0303\u2212 to fit positive data in I\u2212. We define f+(xi;W, v) = \u2211 j\u2208V\u0303+ vj \u27e8wj , xi\u27e9 and let t + 2 = inf{t : maxi\u2208I+ |f+(xi;W, v)| > 1 4}. Similarly, we also define f\u2212(xi;W, v) = \u2211 j\u2208V\u0303+ vj \u27e8wj , xi\u27e9 and let t \u2212 2 = inf{t : maxi\u2208I\u2212 |f\u2212(xi;W, v)| > 14}. Then t1 \u2264 min{t + 2 , t \u2212 2 }, by Lemma 3.\nO (1/t) convergence after t2: We first show that when both t+2 , t \u2212 2 are finite, then it implies O(1/t) convergence on the loss. Then we show that they are indeed finite and t2 := max{t+2 , t \u2212 2 } = O( 1n log 1 \u03f5 ).\nAt t2 = max{t+2 , t \u2212 2 }, by definition, \u2203i+ \u2208 I+ such that 1\n4 \u2264 f+(xi+ ;W, v) \u2264 \u2211 j\u2208V\u0303+ vj \u2329 wj , xi+ \u232a \u2264 \u2211 j\u2208V\u0303+ \u2225wj\u22252\u2225xi+\u2225 , (33)\nwhich implies, by Lemma 14, \u2200t \u2265 t2\u2211 j\u2208V\u0303+ \u2225wj(t)\u22252 \u2265 \u2211 j\u2208V\u0303+ \u2225wj(t2)\u22252 \u2265 1 4\u2225xi+\u2225 \u2265 1 4Xmax . (34)\nSimilarly, we have \u2200t \u2265 t2, \u2211 j\u2208V\u0303\u2212 \u2225wj(t)\u22252 \u2265 1 4Xmax . (35)\nUnder the gradient flow dynamics (2), we apply Lemma 15 to the decomposed loss (32)\n4L\u0307 \u2264 \u2212 \u2211 j\u2208V\u0303+ v2j  \u00b7 L2+ \u00b7 (\u00b5Xmin)2 \u2212 \u2211 j\u2208V\u0303+ v2j  \u00b7 L2\u2212 \u00b7 (\u00b5Xmin)2 . Here, we can pick the same \u03b3 = \u00b5Xmin for both L+ and L\u2212 because {xi, yi}, i \u2208 I+ is linearly separable with z = y1x1\u2225x1\u2225 : \u27e8z, xiyi\u27e9 \u2265 \u00b5\u2225xi\u2225 \u2265 \u00b5Xmin by Assumption 1. And similarly, {xi, yi}, i \u2208 I\u2212 is linearly separable with \u27e8z, xiyi\u27e9 \u2265 \u00b5\u2225xi\u2225 \u2265 \u00b5Xmin. Replace v2i by \u2225wj\u22252 from balancedness, together with (34)(35), we have\n4L\u0307 \u2264 \u2212 \u2211 j\u2208V\u0303+ \u2225wj\u22252  \u00b7 L2+ \u00b7 (\u00b5Xmin)2 \u2212 \u2211 j\u2208V\u0303+ \u2225wj\u22252  \u00b7 L2\u2212 \u00b7 (\u00b5Xmin)2\n\u2264 \u2212 (\u00b5Xmin) 2\n4Xmax (L2+ + L2\u2212) \u2264 \u2212\n(\u00b5Xmin) 2\n8Xmax (L+ + L\u2212)2 = \u2212\n(\u00b5Xmin) 2\n8Xmax L2 ,\nwhich is 1\nL2 L\u0307 \u2264 \u2212 (\u00b5Xmin)\n2\n32Xmax .\nIntegrating both side from t2 to any t \u2265 t2, we have\n1\nL \u2223\u2223\u2223\u2223\u22a4 t2 \u2264 \u2212 (\u00b5Xmin) 2 32Xmax (t\u2212 t2) ,\nwhich leads to\nL(t) \u2264 L(t2) L(t2)\u03b1(t\u2212 t2) + 1 , where \u03b1 = (\u00b5Xmin) 2 32Xmax .\nShowing t2 = O( 1n log 1 \u03f5 ): The remaining thing is to show t2 is O( 1 n log 1 \u03f5 ).\nSince after t1, the gradient dynamics are fully decoupled into two gradient flow dynamics (on L+ and on L\u2212), it suffices to show t+2 = O( 1n log 1 \u03f5 ) and t \u2212 2 = O( 1n log 1 \u03f5 ) separately, then combine them to show t2 = max{t+2 , t \u2212 2 } = O( 1n log 1 \u03f5 ). The proof is almost identical for L+ and L\u2212, thus we only prove t+2 = O( 1n log 1 \u03f5 ) here.\nSuppose\nt2 \u2265 t1 + 6\n\u221a \u00b5n+Xmin\n+ 4\n\u221a \u00b5n+Xmin\n( log\n2\n\u03f52 \u221a \u00b5XminW 2min + 4nXmaxt1\n) , (36)\nwhere n+ = |I+|. It takes two steps to show a contradiction: First, we show that for some ta \u2265 0, a refined alignment cos(wj(t1 + ta), x+) \u2265 14 ,\u2200j \u2208 V\u0303+ is achieved, and such refined alignment is maintained until at least t+2 : cos(wj(t), x+) \u2265 14 ,\u2200j \u2208 V\u0303+ for all t1 + ta \u2264 t \u2264 t + 2 . Then, keeping this refined alignment leads to a contradiction.\n\u2022 For j \u2208 V\u0303+, we have\nd\ndt wj \u2225wj\u2225 =\n( I \u2212 wjw \u22a4 j\n\u2225wj\u22252 )\u2211 i\u2208I+ \u2212\u2207y\u0302\u2113(yi, f+(xi;W, v))xi  \ufe38 \ufe37\ufe37 \ufe38\n:=x\u0303a\n.\nThen d\ndt cos(x+, wj) = (cos(x+, x\u0303a)\u2212 cos(x+, wj) cos(x\u0303a, wj)) \u2225x\u0303a\u2225\n\u2265 (cos(x+, x\u0303a)\u2212 cos(x+, wj)) \u2225x\u0303a\u2225 .\nWe can show that cos(x+, x\u0303a) \u2265 13 and \u2225x\u0303a\u2225 \u2265 \u221a \u00b5n+Xmin/2 when t1 \u2264 t \u2264 t+2 (we defer the proof to the end as it breaks the flow), thus within [t1, t+2 ], we have\nd dt cos(x+, wj) \u2265\n( 1\n3 \u2212 cos(x+, wj)\n) \u221a \u00b5n+Xmin/2 . (37)\nWe use (37) in two ways: First, since\nd dt cos(x+, wj) \u2223\u2223\u2223\u2223 cos(x+,wj)= 1 4 \u2265 \u221a \u00b5n+Xmin 24 > 0 ,\ncos(x+, wj) \u2265 14 is a trapping region for wj during [t1, t + 2 ]. Define ta := inf{t \u2265 t1 : minj\u2208V\u0303+ cos(x+, wj(t)) \u2265 1 4}, then clearly, if ta \u2264 t + 2 , then cos(wj(t), x+) \u2265 14 ,\u2200j \u2208 V\u0303+ for all t1 + ta \u2264 t \u2264 t+2 .\nNow we use (37) again to show that ta \u2264 t1 + 6\u221a\u00b5n+Xmin : Suppose that ta \u2265 t1 + 6\u221a \u00b5n+Xmin , then \u2203j\u2217 such that cos(x+, wj\u2217(t)) < 14 ,\u2200t \u2208 [t1, t1 + 6\u221a \u00b5n+Xmin ], and we have\nd dt cos(x+, wj\u2217) \u2265\n( 1\n3 \u2212 cos(x+, wj)\n) \u221a \u00b5n+Xmin/2 \u2265 \u221a \u00b5n+Xmin\n24 . (38)\nThis shows\ncos(x+, wj\u2217(t1 + 1)) \u2265 cos(x+, wj\u2217(t1)) + 1 4 \u2265 1 4 ,\nwhich contradicts that cos(x+, wj\u2217(t)) < 14 . Hence we know ta \u2264 t1 + 6\u221a \u00b5n+Xmin .\nIn summary, we have cos(wj(t), x+) \u2265 14 ,\u2200j \u2208 V\u0303+ for all t1 + 6\u221a \u00b5n+Xmin \u2264 t \u2264 t+2 .\n\u2022 Now we check the dynamics of \u2211\nj\u2208V\u0303+ \u2225wj(t)\u2225 2 during t1 + 6\u221a\u00b5n+Xmin \u2264 t \u2264 t + 2 . For simplicity,\nwe denote t1 + 6\u221a\u00b5n+Xmin := t \u2032 1.\nFor j \u2208 V\u0303+, we have, for t\u20321 \u2264 t \u2264 t+2 ,\nd dt \u2225wj\u22252 = 2 \u2211 i\u2208I+ \u2212\u2207y\u0302\u2113(yi, f(xi;W, v)) \u27e8xi, wj\u27e9 \u2225wj\u2225\n\u2265 \u2211 i\u2208I+ \u27e8xi, wj\u27e9 \u2225wj\u2225 (by (40))\n= \u27e8x+, wj\u27e9 \u2225wj\u2225 = \u2225x+\u2225\u2225wj\u22252 cos(x+, wj)\n\u2265 1 4 \u2225x+\u2225\u2225wj\u22252 (Since t \u2265 t\u20321) \u2265 \u221a \u00b5n+Xmin\n4 \u2225wj\u22252 , (by Lemma 11)\nwhich leads to (summing over j \u2208 V\u0303+)\nd\ndt \u2211 j\u2208V\u0303+ \u2225wj\u22252 \u2265 \u221a \u00b5n+Xmin 4 \u2211 j\u2208V\u0303+ \u2225wj\u22252 .\nBy Gronwall\u2019s inequality, we have\n\u2211 j\u2208V\u0303+ \u2225wj(t+2 )\u22252\n\u2265 exp (\u221a \u00b5n+Xmin\n4 (t+2 \u2212 t\u20321) ) \u2211 j\u2208V\u0303+ \u2225wj(t\u20321)\u22252\n\u2265 exp (\u221a \u00b5n+Xmin\n4 (t+2 \u2212 t\u20321) ) \u2211 j\u2208V\u0303+ \u2225wj(t1)\u22252 (By Lemma 14)\n\u2265 exp (\u221a \u00b5n+Xmin\n4 (t+2 \u2212 t\u20321)\n) exp (\u22124nXmaxt1) \u2211 j\u2208V\u0303+ \u2225wj(0)\u22252 (By Lemma 13)\n\u2265 exp (\u221a \u00b5n+Xmin\n4 (t+2 \u2212 t\u20321)\n) exp (\u22124nXmaxt1) \u03f52W 2min \u2265\n2 \u221a \u00b5Xmin . (by (36))\nHowever, at t+2 , we have\n1 4 \u2265 1 n+ \u2211 i\u2208I+ f+(xi;W, v) = 1 n+ \u2211 i\u2208I+ \u2211 j\u2208V\u0303+ vj \u27e8wj , xi\u27e9\n= 1\nn+ \u2211 j\u2208V\u0303+ vj \u27e8wj , x+\u27e9 \u2217\n= 1\nn+ \u2211 j\u2208V\u0303+ \u2225wj\u22252 cos(wj , x+)\u2225x+\u2225\n\u2265 1 4n+ \u2211 j\u2208V\u0303+ \u2225wj\u22252\u2225x+\u2225 (Since t \u2265 t\u20321)\n\u2265 1 4 \u2211 j\u2208V\u0303+ \u2225wj\u22252 \u221a \u00b5Xmin , (by Lemma 11)\nwhich suggests \u2211\nj\u2208V\u0303+ \u2225wj\u2225 2 \u2264 1\u221a\u00b5Xmin . A contradiction.\nTherefore, we must have\nt+2 \u2264 t1 + 6\n\u221a \u00b5n+Xmin\n+ 4\n\u221a \u00b5n+Xmin\n( log\n2\n\u03f52 \u221a \u00b5XminW 2min + 4nXmaxt1\n) . (39)\nSince the dominant term here is 4\u221a\u00b5n+Xmin log 2 \u03f52 \u221a \u00b5XminW 2min , we have t+2 = O( 1n log 1 \u03f5 ). A similar analysis shows t\u22122 = O( 1n log 1 \u03f5 ). Therefore t2 = max{t + 2 , t \u2212 2 } = O( 1n log 1 \u03f5 )\nComplete the missing pieces We have two claims remaining to be proved. The first is cos(x+, x\u0303a) \u2265 1 2 when t1 \u2264 t \u2264 t + 2 . Since x+ = \u2211 i\u2208I+ xi and x\u0303a = \u2211 i\u2208I+ \u2212\u2207y\u0302\u2113(yi, f+(xi;W, v))xi. We simply use the fact that before t+2 , we have, by Lemma 2,\n1 2 \u2264 \u2212\u2207y\u0302\u2113(yi, f+(xi;W, v)) =\u2264 3 2 , (40)\nto show the following\ncos(x+, x\u0303a) = \u27e8x+, x\u0303a\u27e9 \u2225x+\u2225\u2225x\u0303a\u2225\n= \u2211 i,j\u2208I+(\u2212\u2207y\u0302\u2113(yi, f+(xi;W, v))) \u27e8xi, xj\u27e9\u221a\u2211\ni,j\u2208I+ \u27e8xi, xj\u27e9 \u221a\u2211 i,j\u2208I+(\u2212\u2207y\u0302\u2113(yi, f+(xi;W, v))) 2 \u27e8xi, xj\u27e9\n\u2265 1 2 \u2211 i,j\u2208I+ \u27e8xi, xj\u27e9\u221a\u2211\ni,j\u2208I+ \u27e8xi, xj\u27e9 \u221a\u2211 i,j\u2208I+(\u2212\u2207y\u0302\u2113(yi, f+(xi;W, v))) 2 \u27e8xi, xj\u27e9\n\u2265 1 2 \u2211 i,j\u2208I+ \u27e8xi, xj\u27e9\u221a\u2211\ni,j\u2208I+ \u27e8xi, xj\u27e9 \u221a\u2211 i,j\u2208I+( 3 2 ) 2 \u27e8xi, xj\u27e9 \u2265 1 3 ,\nsince all \u27e8xi, xj\u27e9 , i, j \u2208 I+ are non-negative. The second claim is \u2225x\u0303a\u2225 \u2265 \u221a \u00b5n+Xmin/2 is due to that\n\u2225x\u0303a\u2225 = \u221a \u2211\ni,j\u2208I+\n(\u2212\u2207y\u0302\u2113(yi, f+(xi;W, v)))2 \u27e8xi, xj\u27e9 \u2265 1\n2 \u221a \u2211 i,j\u2208I+ \u27e8xi, xj\u27e9 = \u2225x+\u2225 2 \u2265 \u221a \u00b5n+Xmin 2 ,\nwhere the last inequality is from Lemma 11."
        },
        {
            "heading": "D.3 PROOF OF LOW-RANK BIAS",
            "text": "So far we have proved the directional convergence at the early alignment phase and final O(1/t) convergence of the loss in the later stage. The only thing that remains to be shown is the low-rank bias. The proof is quite straightforward but we need some additional notations.\nAs we proved above, after t1, neurons in S+ (S\u2212) stays in S+ (S\u2212). Thus the loss can be decomposed as\nL = \u2211 i\u2208I+ \u2113 yi, \u2211 j\u2208V\u0303+ vj \u27e8wj , xi\u27e9  \ufe38 \ufe37\ufe37 \ufe38\nL+\n+ \u2211 i\u2208I\u2212 \u2113 yi, \u2211 j\u2208V\u0303\u2212 vj \u27e8wj , xi\u27e9  \ufe38 \ufe37\ufe37 \ufe38\nL\u2212\n,\nwhere V\u0303+ : {j : wj(t1) \u2208 S+} and V\u0303\u2212 : {j : wj(t1) \u2208 S\u2212}. Therefore, the training after t1 is decoupled into 1) using neurons in V\u0303+ to fit positive data in I+ and 2) using neurons in V\u0303\u2212 to fit positive data in I\u2212. We use\nW+ = [W ]:,V\u0303+ , W\u2212 = [W ]:,V\u0303\u2212\nto denote submatrices of W by picking only columns in V\u0303+ and V\u0303\u2212, respectively. Similarly, we define\nv+ = [v]V\u0303+ , v\u2212 = [v]V\u0303\u2212\nfor the second layer weight v. Lastly, we also define\nWdead = [W ]:,V\u0303dead , vdead = [v]V\u0303dead ,\nwhere V\u0303dead := {j : wj(t1) \u2208 Sdead}. Given these notations, after t1 the loss is decomposed as L = \u2211 i\u2208I+ \u2113 ( yi, x \u22a4 i W+v+ ) \ufe38 \ufe37\ufe37 \ufe38\nL+\n+ \u2211 i\u2208I\u2212 \u2113 ( yi, x \u22a4 i W\u2212v\u2212 ) \ufe38 \ufe37\ufe37 \ufe38\nL\u2212\n,\nand the GF on L is equivalent to GF on L+ and L\u2212 separately. It suffices to study one of them. For GF on L+, we have the following important invariance Arora et al. (2018b) \u2200t \u2265 t1:\nW\u22a4+ (t)W+(t)\u2212 v+(t)v\u22a4+(t) =W\u22a4+ (t1)W+(t1)\u2212 v+(t1)v\u22a4+(t1) ,\nfrom which one has\n\u2225W\u22a4+ (t)W+(t)\u2212 v+(t)v\u22a4+(t)\u22252 = \u2225W\u22a4+ (t1)W+(t1)\u2212 v+(t1)v\u22a4+(t1)\u22252 \u2264 \u2225W\u22a4+ (t1)W+(t1)\u22252 \u2212 \u2225v+(t1)v\u22a4+(t1)\u22252 \u2264 tr(W\u22a4+ (t1)W+(t1)) + \u2225v+(t1)\u22252\n= 2 \u2211 j\u2208V\u0303+ \u2225wj(t1)\u22252 \u2264 4\u03f5W 2max\u221a h |V\u0303+| ,\nwhere the last inequality is by Lemma 3. Then one can immediately get\n\u2225v+(t)v\u22a4+(t)\u22252 \u2212 \u2225W\u22a4+ (t)W+(t)\u22252 \u2264 \u2225W\u22a4+ (t)W+(t)\u2212 v+(t)v\u22a4+(t)\u22252 \u2264 4\u03f5W 2max\u221a\nh |V\u0303+| ,\nwhich is precisely\n\u2225W+(t)\u22252F \u2264 \u2225W+(t)\u222522 + 4\u03f5W 2max\u221a\nh |V\u0303+| . (41)\nSimilarly, we have\n\u2225W\u2212(t)\u22252F \u2264 \u2225W\u2212(t)\u222522 + 4\u03f5W 2max\u221a\nh |V\u0303\u2212| . (42)\nLastly, one has\n\u2225Wdead\u22252F = \u2211\nj\u2208V\u0303dead\n\u2225wj(t1)\u22252 \u2264 4\u03f5W 2max\u221a\nh |V\u0303dead| (43)\nAdding (41)(42)(43) together, we have\n\u2225W (t)\u22252F = \u2225W+(t)\u22252F + \u2225W\u2212(t)\u22252F + \u2225Wdead\u22252F\n\u2264 \u2225W+(t)\u222522 + \u2225W\u2212(t)\u222522 + 4 \u221a h\u03f5W 2max\u221a h \u2264 2\u2225W (t)\u222522 + 4 \u221a h\u03f5W 2max .\nFinally, since we have shown L \u2192 0 as t\u2192\u221e, then \u2200i \u2208 [n], we have \u2113(yi, f(xi;W, v))\u2192 0. This implies\nf(xi;W, v) = \u2212 1\nyi log \u2113(yi, f(xi;W, v))\u2192\u221e .\nBecause we have shown that f(xi;W, v) \u2264 \u2211 j\u2208[h] \u2225wj\u22252\u2225xi\u2225 \u2264 \u2225W\u22252FXmax ,\nf(xi;W, v)\u2192\u221e enforces \u2225W\u22252F \u2192\u221e as t\u2192\u221e, thus \u2225W\u222522 \u2192\u221e as well. This gets us\nlim sup t\u2192\u221e \u2225W\u22252F \u2225W\u222522 = 2 .\nE EXISTENCE OF CARATHEODORY SOLUTION UNDER FIXED SUBGRADIENT \u03c3\u2032(x) = 1x>0\nIn this Appendix, we first introduce the notion of solution we are interested in for the GF (2): Caratheodory solutions that satisfy (2) for almost all time t. Next, in Appendix E.2, we show that if we fix the ReLU subgradient as \u03c3\u2032(x) = 1x>0, then global Caratheodory solutions exists for (2) under Assumption 1. Finally, we use simple examples to illustrate two points: 1) Caratheodory solutions cease to exist when ReLU subgradient at zero is chosen to be a fixed non-zero value, highlighting the importance of choosing the right subgradient for analysis; 2) Caratheodory solutions are potentially non-unique, the neurons\u2019 dynamical behavior could become somewhat irregular if certain solutions are not excluded, justifying the introduction of regular solutions (Definition 1)."
        },
        {
            "heading": "E.1 CARATHEODORY SOLUTIONS",
            "text": "Given an differential equation \u03b8\u0307 = F (\u03b8), \u03b8(0) = \u03b80 , (44)\nwith F potentially be discontinuous, \u03b8(t) is said to be a Caratheodory solution of (44) if it satisfies the following integral equation\n\u03b8(t) = \u03b80 + \u222b t 0 F (\u03b8(\u03c4))d\u03c4 , (45)\nfor all t \u2208 [0, a), where a \u2208 R\u22650 \u222a \u221e. In this section, we are interested in global Caratheodory solutions: \u03b8(t) that satisfies (45) for all time t \u2265 0."
        },
        {
            "heading": "E.2 PROOF OF EXISTENCE OF REGULAR CARATHEODORY SOLUTIONS UNDER ASSUMPTION 1",
            "text": "In this section, we show the existence of global regular (Definition 1) Caratheodory solutions to \u03b8\u0307 = F (\u03b8), \u03b8(0) = \u03b80, where \u03b8 := {W, v} and F := \u2207W,vL defined from a fixed choice of ReLU subgradient \u03c3\u2032(x) = 1x>0, under Assumption 1. For the sake of a clear presentation, we first discuss the case of Sdead = \u2205, where all solutions are regular. then discuss the modifications one needs to make when Sdead \u0338= \u2205. Existence of Caratheodory solutions when Sdead = \u2205: First of all, notice that\u2207W,vL is continuous almost everywhere except for a zero measure set A = {W, v : \u2203i \u2208 [n], j \u2208 [h] s.t. \u27e8xi, wj\u27e9 = 0}, since discontinuity only happens when one has to evaluate \u03c3\u2032(\u27e8xi, wj\u27e9) at \u27e8xi, wj\u27e9 = 0 for some i, j. Being a finite union of hyperplanes, A has zero measure. For points outside A, the existence of a local solution is guaranteed by the generalized Caratheodory existence theorem in Persson (1975) (We refer readers to Appendix E.5 for the construction of such a local solution). The local solution can be extended to a global solution, as long as it does not encounter any point in A (the set where the flow is discontinuous). Whenever a point in A is reached, one requires extra certificates to extend the solution beyond that point. Simply speaking, the existence of a local solution around a point in A requires that the flow around this point does not push trajectories towards A from both sides of the zero measure set, causing an infinite number of crossings of A, called Zeno behavior (van der Schaft & Schumacher, 2000; Maennel et al., 2018). See Figure 18 and 19 for an illustration. In Appendix E.5, we formally show that if there is no Zeno behavior, then a solution can be extended until reaching discontinuity in A, and gets extended by leaving A immediately.2\nOne sufficient condition for avoiding Zeno behavior is to show: For each hyperplane Aij := {\u27e8xi, wj\u27e9 = 0}, all points in a neighborhood around this hyperplane Aij must satisfy that the inner products between the normal vector of Aij and the flow F have the same sign. Formally speaking, we need that there exists \u03b4 > 0, such that for all pair of \u03b8k, \u03b8l \u2208 {\u03b8 = (W, v) : 0 < | \u27e8xi, wj\u27e9 | < \u03b4}, we have \u2329 NAij , F (\u03b8k) \u232a \u2329 NAij , F (\u03b8l) \u232a > 0, hereNAij should be a fixed choice of the normal vector of hyperplane Aij .\n2Strictly speaking, Appendix E.5 is part of the proof but discussing the technical part right now disrupts the presentation.\nThis inner product \u2329 NAij , F (\u03b8k) \u232a between the normal vector and the flow is exactly computed as\u2329\nxi,\u2207wjL \u232a . Under Assumption 1, we have a much stronger result than what is required in the last\nparagraph: we can show that on the entire parameter space, we have (shown in Appendix E.5) yisign(vj) \u2329 xi,\u2207wjL \u232a > 0 , (46)\nAs such, since vj(t) does not change sign, Assumption 1 prevents Zeno behavior and ensures the existence of local solution around points in A. In summary, from any initialization, the Caratheodory solution can be extended (Persson, 1975) until the trajectory encounters points of discontinuity in A, then the existence of a local solution is guaranteed by ensuring that the flow forces the solution to leave A immediately. Moreover, (46) ensures that A can only be crossed a finite number of times (every hyperplane can only be crossed once), after which no discontinuity is encountered and the solution can be extended to t = \u221e. Therefore a global Caratheodory solution always exists.\nExistence of Caratheodory solutions when Sdead \u0338= \u2205: Notice that when Sdead \u0338= \u2205. A contains boundary of Sdead. If the solution gets extended to A where one neuron lands on the boundary of Sdead, then this neuron stays at the boundary of Sdead, i.e. the solution stays at A. Therefore, the previous argument about existence does not apply.\nHowever, one only needs very a minor modification: If at time t0, the solution enters A by having one neuron (say wj(t)) land on the boundary of Sdead, set wj(t) \u2261 wj(t0) and vj(t) \u2261 vj(t0) for t \u2265 t0, then exclude {wj , vj} from the parameter space and continue constructing and extending local solutions for other parameters via the previous argument. This shows the existence of the Caratheodory solution under non-empty Sdead, and by our construction, the solution is regular."
        },
        {
            "heading": "E.3 NON-EXISTENCE OF CARATHEODORY SOLUTION UNDER OTHER FIXED SUBGRADIENT",
            "text": "Consider the following simple example: The training data consists of a single data point x = [1, 0]\u22a4, y = \u22121, and the network consists of a single neuron {w, v} initialized at {w(0) = [0, 1]\u22a4, v(0) = 1}. See Figure 20 for an illustration.\nWhen the ReLU subgradient is chosen to be \u03c3\u2032(x) = 1x>0, the Caratheodory solution {w(t) \u2261 [0, 1], v(t) \u2261 1} exists, i.e. the neuron stays at the boundary of Sdead := {w : \u27e8x,w\u27e9 \u2264 0}. If the ReLU subgradient is chosen to be \u03c3\u2032(0) = a > 0, then the Caratheodory solution ceases to exist: the neuron cannot stay at the boundary \u27e8x,w\u27e9 = 0 of Sdead, because the non-zero \u03c3\u2032(0) pushes\nit towards the interior of Sdead. However, the neuron cannot enter the interior of Sdead because the flow is all zero within the interior of Sdead. To see this formally, suppose {w(t) = w(0), v(t) = v(0)} for t \u2208 [0, t0] (neuron stay at {w(0), v(0)}), then by definition of Caratheodory solution, we have\u222b t0\n0\n\u2207w,vL(w(0), v(0))dt = t0\u2207w,vL(w(0), v(0)) = 0 ,\nsuggesting \u2207w,vL(w(0), v(0)) = 0, which is not true when \u03c3\u2032(0) > 0, thus a contradiction. Now suppose w(t0) \u2208 Int(Sdead) for some t0, then it must be that w(t) \u2208 Int(Sdead),\u22000 < t \u2264 t0, otherwise it leads to the same contradiction as in previous paragraph. By definition of Caratheodory solution, we have \u222b t0\n0\n\u2207w,vL(w(t), v(t))dt = w(t0)\u2212 w(0) .\nThe left-hand side is zero because w(t) \u2208 Int(Sdead)\u21d2 \u2207w,vL(w(t), v(t)) = 0,\u22000 < t \u2264 t0. The right-hand side is non-zero because w(t0) \u2208 Int(Sdead), thus a contradiction. Similarly, w(t) cannot enter Scdead. Therefore, the Caratheodory solution {w(t), v(t)} does not exist for any t > 0."
        },
        {
            "heading": "E.4 NON-UNIQUENESS OF CARATHEODORY SOLUTIONS",
            "text": "Consider the following simple example: The training data consists of a single data point x = [1, 0]\u22a4, y = 1, and the network consists of a single neuron (w, v) initialized at w(0) = [0, 1]\u22a4, v(0) = 1. See Figure 21 for an illustration.\nWe consider the case when the ReLU subgradient is chosen to be \u03c3\u2032(x) = 1x>0. There exists one Caratheodory solution w(t) \u2261 [0, 1]\u22a4, v(t) \u2261 1, i.e. the neuron stays at the boundary of Sdead := {w : \u27e8x,w\u27e9 \u2264 0}. However, consider w\u0303(t), v\u0303(t) being the solution to the following ode (the one that neuron follows once enters the positive orthant):\n\u02d9\u0303w = y exp(\u2212yv\u0303 \u27e8x, w\u0303\u27e9)v\u0303x, \u02d9\u0303v = y exp(\u2212yv\u0303 \u27e8x, w\u0303\u27e9) \u27e8x, w\u0303\u27e9 , w\u0303(0) = w(0), v\u0303(0) = v(0) . (47)\nThen for any t0 \u2265 0,\nw(t) = 1t<t0w(0) + 1t\u2265t0w\u0303(t\u2212 t0) , v(t) = 1t<t0v(0) + 1t\u2265t0 v\u0303(t\u2212 t0)\nis a Caratheodory solution. This example shows that the Caratheodory solution could be non-unique.\nThis is somewhat troublesome for our analysis, one would like that all neurons in Sdead stay within Sdead, but Caratheodory solutions do not have this property, and in fact, as long as the neuron is on\nthe boundary of Sdead, and the flow outside Sdead is pointing away from the boundary, the neuron can leave Sdead at any time and it does not violate the definition of a Caratheodory solution. Therefore, for our main theorem, we added an additional regularity condition (Definition 1) on the solution, forcing neurons to stay within Sdead. Remark 6. This issue of having irregular solutions is not specific to our choice of the notion of solutions. Even if one considers more generally the Filippov solution Filippov (1971) of the differential inclusion in (2), the same issue of non-uniqueness persists and needs attention when analyzing neuron dynamics. Remark 7. Although irregular solutions are not desired for analyzing neuron behaviors, as we see in this example, they are rare cases under very specific initialization of the neurons and thus can be avoided by randomly initializing the weights."
        },
        {
            "heading": "E.5 CONSTRUCTING GLOBAL CARATHEODORY SOLUTION",
            "text": "In this section, we formally show that if there is no Zeno behavior, then a solution can be extended until reaching discontinuity inA, and gets extended by leavingA immediately, leading to a construction of global Caratheodory solution. The only ingredient that is needed is the existence theorem in Persson (1975, Theorem 2.3), showing that if F (\u03b8) is continuous and \u2200\u03b8\n\u2225F (\u03b8)\u2225F \u2264M(1 + \u2225\u03b8\u2225F ) , (48)\nfor some M > 0, then global solution of \u03b8\u0307 = F (\u03b8) exists. Obviously, this result cannot be applied directly for two reasons: a) it requires continuity of the flow; b) it requires linear growth of \u2225F (\u03b8)\u2225F w.r.t. \u2225\u03b8\u2225F . The key idea is constructing a local solution by restricting the flow to a neighborhood of initial conditions where a) and b) are satisfied, and then extending this solution to a global one.\nAs we discussed in Appendix E.2, we can assume Sdead = \u2205 without loss of generality. Moreover, it suffices to show that starting from an initialization \u03b8(0) = {W (0), v(0)} outside A3, we can construct either: 1) a global solution without encountering any point in A; or 2) a local solution that lands on A at some t0 then leave A immediately. Because if 2) happens, we take the end of this local solution as a new initial condition and repeat this argument. Importantly, 2) cannot happen infinitely many times because we have shown in Appendix E.2 that A can only be crossed finitely many times, thus 1) must happen, resulting in a global solution.\nConstruct local solution from initial condition: Now given an initial condition \u03b8(0) = {W (0), v(0)}, define the following two sets (Notation-wise, we drop the dependency on {W (0), v(0)} for simplicity):\n\u03980 := {\u03b8 = (W, v) : L(W, v) \u2264 L(W (0), v(0)), sign(vj) = sign(vj(0)),\u2200j \u2208 [h]} , \u03981 := {\u03b8 = (W, v) : \u2200i \u2208 [n], j \u2208 [h], \u27e8xi, wj\u27e9 \u27e8xi, wj(0)\u27e9 > 0} ,\n\u03981 is the positive invariant set of {W (0), v(0)}: all solutions from {W (0), v(0)} never leaves \u03981, so it suffices to study the flow within \u03981 for the existence of solutions. Moreover, \u03980 is the intersection of a closed set {v : sign(vj) = sign(vj(0))} and the pre-image of a continuous function L on the range [0,L(W (0), v(0))] thus closed. \u03982 is the largest connected set that contains {W (0), v(0)} without point of discontinuity.\nConsider the following set \u0398\u03031 := \u03980 \u2229 cl(\u03981) . (49)\nThen \u0398\u03031 is closed. Consider a new flow F cl1 on \u0398\u03031 such that F cl 1 = F = \u2207W,vL for all \u03b8 \u2208 Int(\u0398\u03031), and F cl1 (\u03b8) = limk\u2192\u221e F (\u03b8k) for all \u03b8 \u2208 \u0398\u0303 \\ Int(\u0398\u03031), where \u03b8k \u2208 Int(\u0398\u03031), k = 1, 2, \u00b7 \u00b7 \u00b7 is a convergent sequence to \u03b8.\nF cl1 |\u0398\u03031 is continuous by construction, and we can show that (at the end of this section)\n\u2225F cl1 (\u03b8)\u2225F \u2264 C\u2225\u03b8\u2225F ,\u2200\u03b8 \u2208 \u0398\u03031 . (50)\nBy a generalized version of the Tietze extension theorem (Ercan, 1997), there exists continuous F\u03031 on the entire parameter space, such that\nF cl1 (\u03b8) = F\u03031(\u03b8),\u2200\u03b8 \u2208 \u0398\u03031 , (51) 3initial condition within A is taken care of by 2).\nand \u2225F\u03031(\u03b8)\u2225F \u2264 C\u2225\u03b8\u2225F ,\u2200\u03b8 . (52)\nBecause now F = F cl1 = F\u03031 on Int(\u0398\u03031), any solution \u03b8\u03031(t) of \u02d9\u0303 \u03b81 = F\u03031(\u03b8\u03031), \u03b8\u03031(0) = \u03b8(0) (existence guaranteed by Persson (1975)) gives a local solution of \u03b8\u0307 = F (\u03b8), \u03b8(0) = \u03b8(0), for t \u2264 t0, where t0 := inf{t : \u03b8\u03031(t) /\u2208 \u0398\u03031}. If t0 = \u221e, one has a global solution and the construction is finished. If t0 < \u221e, it must be that \u03b8\u03031(t0) \u2208 A (since \u03b8\u03031 must leave \u0398\u0303 via the boundary of \u03982). Now we need to construct a solution that leaves A immediately. Construct local solution that leavesA: As we discussed,A is a union of hyperplanes. For simplicity, let us assume \u03b8\u0303(t0) is not at the intersection of two hyperplanes (the treatment is similar but tedious, we will make remarks in the end).\nNow \u03b8\u0303(t0) lands on a single hyperplane, let it be {\u03b8 : \u27e8xi\u2217 , wj\u2217\u27e9 = 0}, we define \u03982 := {\u03b8 = (W, v) : \u2200i \u0338= i\u2217, j \u0338= j\u2217, \u27e8xi, wj\u27e9 \u27e8xi, wj(0)\u27e9 > 0 ,\nand \u27e8xi\u2217 , wj\u2217\u27e9 \u27e8xi\u2217 , wj\u2217(0)\u27e9 < 0} , and we let \u0398\u03032 := \u03980 \u2229 cl(\u03982) , (53) It is clear that, from the definition of \u03982, any solution we construct that leaves A immediately after t0 must enter Int(\u0398\u03032). To construct the solution, we just need to repeat the first part, but now for \u0398\u03032: We construct F cl2 that is continuous on \u0398\u03032 and agrees with F on the interior, then extends F cl 2 to F\u0303 on the entire parameter space. Consider the solution \u03b8\u03032(t) of\n\u02d9\u0303 \u03b82 = F (\u03b8\u03032), \u03b8\u03032(0) = \u03b8\u03031(t0) , (54)\ngives a local solution of \u03b8\u0307 = F (\u03b8), \u03b8(0) = \u03b8\u03031(t0) . (55)\nBecause we have shown that Zeno behavior does not happen, \u03b8\u03032(t) leaves A immediately and enters Int(\u0398\u03032). We just pick any \u03c40 > 0 such that \u03b8\u03032(\u03c40) \u2208 Int(\u0398\u03032) then\n\u03b8(t) = 1t\u2264t0 \u03b8\u03031(t) + 1t0<t\u2264t0+\u03c40 \u03b8\u03032(t\u2212 t0) , (56)\nis a Caratheodory solution to \u03b8\u0307 = F (\u03b8), \u03b8(0) = \u03b8(0) for t \u2264 t0 + \u03c40. This is exactly what we intended to show. Remark 8. When \u03b8\u0303(t0) lands at the intersection of two (or more) hyperplanes, the only difference is that now there could be more regions to escape to. But under Assumption 1, (46) suggests that the solution must cross all hyperplanes after t0, leaving one unique region similar to \u03982. Then one constructs the local solution following previous procedures.\nComplete the missing pieces To complete the proof, there are two statements ((46) and (50)) left to be shown.\nTo show (46), we start from the derivative\n\u2207wjL = \u2212 n\u2211\nk=1\n1\u27e8xk,wj\u27e9>0\u2207y\u0302\u2113(yk, f(xk;W, v))xksign(vj(0))\u2225wj\u2225 ,\n= n\u2211 k=1 1\u27e8xk,wj\u27e9>0yk exp(\u2212ykf(xk;W, v))xksign(vj(0))\u2225wj\u2225 ,\nand we have\nyisign(vj) \u2329 xi,\u2207wjL \u232a = n\u2211 k=1 1\u27e8xk,wj\u27e9>0 exp(\u2212ykf(xk;W, v)) \u27e8yixi, ykxk\u27e9 \u2225wj\u2225\n\u2265 n\u2211\nk=1\n1\u27e8xk,wj\u27e9>0 exp(\u2212ykf(xk;W, v))\u00b5\u2225xk\u2225\u2225xi|\u2225wj\u2225 > 0 ,\nsince there is at least one summand (Sdead = \u2205), the summation is always positive.\nTo show (50)4, we first consider \u03b8 \u2208 Int(\u0398\u03031), and we have\nh\u2211 j=1 \u2225\u2207wjL\u22252 = h\u2211 j=1 \u2225\u2225\u2225\u2225\u2225 n\u2211 k=1 1\u27e8xk,wj\u27e9>0yk exp(\u2212ykf(xk;W, v))xkvj \u2225\u2225\u2225\u2225\u2225 2\n\u2264 h\u2211\nj=1\n( n\u2211\nk=1\nexp(\u2212ykf(xk;W, v))\u2225xk\u2225|vj |\n)2\n\u2264 h\u2211\nj=1\n|vj |2 \u00b7 ( Xmax\nn\u2211 k=1 exp(\u2212ykf(xk;W, v))\n)2\n= h\u2211 j=1 |vj |2 \u00b7 (XmaxL(W, v))2 \u2264 h\u2211\nj=1\n|vj |2 \u00b7 (XmaxL(W (0), v(0)))2 = X2maxL2(W (0), v(0))\u2225v\u22252 ,\nsimilarly, we also have\nh\u2211 j=1 \u2225\u2207vjL\u22252 = h\u2211 j=1 \u2225\u2225\u2225\u2225\u2225 n\u2211 k=1 1\u27e8xk,wj\u27e9>0yk exp(\u2212ykf(xk;W, v)) \u27e8xk, wj\u27e9 \u2225\u2225\u2225\u2225\u2225 2\n\u2264 h\u2211\nj=1\n( n\u2211\nk=1\nexp(\u2212ykf(xk;W, v))\u2225xk\u2225\u2225wj\u2225\n)2\n\u2264 h\u2211\nj=1\n\u2225wj\u22252 \u00b7 ( Xmax\nn\u2211 k=1 exp(\u2212ykf(xk;W, v))\n)2\n= h\u2211 j=1 \u2225wj\u22252 \u00b7 (XmaxL(W, v))2 \u2264 h\u2211\nj=1\n\u2225wj\u2225 \u00b7 (XmaxL(W (0), v(0)))2 = X2maxL2(W (0), v(0))\u2225W\u22252F .\nTherefore, we have \u2200\u03b8 \u2208 Int(\u0398\u03031)\n\u2225F cl1 (\u03b8)\u22252F = \u2225F (\u03b8)\u22252F = h\u2211\nj=1\n(\u2225\u2207wjL\u22252 + \u2225\u2207vjL\u22252)\n\u2264 X2maxL2(W (0), v(0))(\u2225W\u22252F + \u2225v\u22252) = X2maxL2(W (0), v(0))\u2225\u03b8\u22252F ,\nwhich gives (50) with C = XmaxL(W (0), v(0)).\nThen for \u03b8 \u2208 \u0398\u0303 \\ Int(\u0398\u03031), \u2225F cl1 (\u03b8)\u2225 = limk\u2192\u221e \u2225F (\u03b8k)\u2225 \u2264 C limk\u2192\u221e \u2225\u03b8k\u2225 = C\u2225\u03b8\u2225, given some Cauchy sequence \u03b8k \u2208 Int(\u0398\u03031), k = 1, 2, \u00b7 \u00b7 \u00b7 convergent to \u03b8. This finishes proving (50).\n4We show it for exponential loss, the case of logistic loss is similar"
        },
        {
            "heading": "F EXTEND MAIN RESULTS TO SOLUTIONS TO DIFFERENTIAL INCLUSION",
            "text": "For Filippov (1971) solutions (regular according to Definition 1) to the differential inclusion (2), our Theorem 1 remains the same. The only difference is that the notion of xa(w) in (4) is no longer a singleton, but rather an element from a set:\nxa(w) \u2208 {\u2211\ni \u03c3\u2032(\u27e8xi, w\u27e9)yixi\n} , (57)\nwhere \u03c3\u2032(\u27e8xi, w\u27e9) is a subgradient of ReLU activation \u03c3(z) at z = \u27e8xi, w\u27e9. Therefore, the proof of Theorem 1 shall be modified (which can be done) to consider all possible choices of xa(w).\nIn the case of \u03c3\u2032(z)|z=0 = 0, xa(w) become a singleton \u2211\ni:\u27e8xi,w\u27e9>0 yixi, which simplifies our discussions. This is the main reason we opt to fix this subgradient \u03c3\u2032(z) in the main paper."
        }
    ],
    "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
    "year": 2024
}