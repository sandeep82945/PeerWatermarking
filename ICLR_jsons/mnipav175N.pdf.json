{
    "abstractText": "Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque black boxes. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the \u2019black box\u2019 in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL. Our code is available at https://github.com/BruceGeLi/TCE_RL.",
    "authors": [
        {
            "affiliations": [],
            "name": "TEMPORALLY-CORRELATED EPISODIC"
        },
        {
            "affiliations": [],
            "name": "REINFORCEMENT LEARNING"
        },
        {
            "affiliations": [],
            "name": "Ge Li"
        },
        {
            "affiliations": [],
            "name": "Hongyi Zhou"
        },
        {
            "affiliations": [],
            "name": "Dominik Roth"
        },
        {
            "affiliations": [],
            "name": "Serge Thilges"
        },
        {
            "affiliations": [],
            "name": "Fabian Otto"
        },
        {
            "affiliations": [],
            "name": "Rudolf Lioutikov"
        },
        {
            "affiliations": [],
            "name": "Gerhard Neumann"
        }
    ],
    "id": "SP:0539ebc0888866b7adb6c8b60140ed81b34663af",
    "references": [
        {
            "authors": [
                "Abbas Abdolmaleki",
                "Rudolf Lioutikov",
                "Jan R Peters",
                "Nuno Lau",
                "Luis Pualo Reis",
                "Gerhard Neumann"
            ],
            "title": "Model-based relative entropy stochastic search",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Abbas Abdolmaleki",
                "Bob Price",
                "Nuno Lau",
                "Luis Paulo Reis",
                "Gerhard Neumann"
            ],
            "title": "Contextual covariance matrix adaptation evolutionary strategies",
            "venue": "International Joint Conferences on Artificial Intelligence Organization (IJCAI),",
            "year": 2017
        },
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron Courville",
                "Marc G Bellemare"
            ],
            "title": "Deep reinforcement learning at the edge of the statistical precipice",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Akshay Agrawal",
                "Brandon Amos",
                "Shane Barratt",
                "Stephen Boyd",
                "Steven Diamond",
                "J Zico Kolter"
            ],
            "title": "Differentiable convex optimization layers",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mete Akbulut",
                "Erhan Oztop",
                "Muhammet Yunus Seker",
                "X Hh",
                "Ahmet Tekden",
                "Emre Ugur"
            ],
            "title": "Acnmp: Skill transfer and task extrapolation through learning from demonstration and reinforcement learning via representation sharing",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Heni Ben Amor",
                "Gerhard Neumann",
                "Sanket Kamthe",
                "Oliver Kroemer",
                "Jan Peters"
            ],
            "title": "Interaction primitives for human-robot cooperation tasks",
            "venue": "In 2014 IEEE international conference on robotics and automation (ICRA),",
            "year": 2014
        },
        {
            "authors": [
                "Shikhar Bahl",
                "Mustafa Mukadam",
                "Abhinav Gupta",
                "Deepak Pathak"
            ],
            "title": "Neural dynamic policies for end-to-end sensorimotor learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lars Berscheid",
                "Torsten Kr\u00f6ger"
            ],
            "title": "Jerk-limited real-time trajectory generation with arbitrary target states",
            "venue": "arXiv preprint arXiv:2105.04830,",
            "year": 2021
        },
        {
            "authors": [
                "Andr\u00e9 Biedenkapp",
                "Raghu Rajan",
                "Frank Hutter",
                "Marius Lindauer"
            ],
            "title": "Temporl: Learning when to act",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Braylan",
                "Mark Hollenbeck",
                "Elliot Meyerson",
                "Risto Miikkulainen"
            ],
            "title": "Frame skip is a powerful parameter for learning to play atari",
            "venue": "In Workshops at the twenty-ninth AAAI conference on artificial intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Sylvain Calinon"
            ],
            "title": "A tutorial on task-parameterized movement learning and retrieval",
            "venue": "Intelligent service robotics,",
            "year": 2016
        },
        {
            "authors": [
                "Sylvain Calinon",
                "Zhibin Li",
                "Tohid Alizadeh",
                "Nikos G Tsagarakis",
                "Darwin G Caldwell"
            ],
            "title": "Statistical dynamical systems for skills acquisition in humanoids",
            "venue": "In 2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids",
            "year": 2012
        },
        {
            "authors": [
                "Onur Celik",
                "Dongzhuoran Zhou",
                "Ge Li",
                "Philipp Becker",
                "Gerhard Neumann"
            ],
            "title": "Specializing versatile skill libraries using local mixture of experts",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Alberto Silvio Chiappa",
                "Alessandro Marin Vargas",
                "Ann Zixiang Huang",
                "Alexander Mathis"
            ],
            "title": "Latent exploration for reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2023
        },
        {
            "authors": [
                "Onno Eberhard",
                "Jakob Hollenstein",
                "Cristina Pinneri",
                "Georg Martius"
            ],
            "title": "Pink noise is all you need: Colored noise exploration in deep reinforcement learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Faustino Gomez",
                "J\u00fcrgen Schmidhuber",
                "Risto Miikkulainen",
                "Melanie Mitchell"
            ],
            "title": "Accelerated neural evolution through cooperatively coevolved synapses",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Sebastian Gomez-Gonzalez",
                "Gerhard Neumann",
                "Bernhard Sch\u00f6lkopf",
                "Jan Peters"
            ],
            "title": "Using probabilistic movement primitives for striking movements",
            "venue": "In 2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids),",
            "year": 2016
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Kristian Hartikainen",
                "George Tucker",
                "Sehoon Ha",
                "Jie Tan",
                "Vikash Kumar",
                "Henry Zhu",
                "Abhishek Gupta",
                "Pieter Abbeel"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "arXiv preprint arXiv:1812.05905,",
            "year": 2018
        },
        {
            "authors": [
                "Neville Hogan",
                "Dagmar Sternad"
            ],
            "title": "Sensitivity of smoothness measures to movement duration, amplitude, and arrests",
            "venue": "Journal of motor behavior,",
            "year": 2009
        },
        {
            "authors": [
                "Christian Igel"
            ],
            "title": "Neuroevolution for reinforcement learning using evolution strategies",
            "venue": "In The 2003 Congress on Evolutionary Computation, 2003. CEC\u201903.,",
            "year": 2003
        },
        {
            "authors": [
                "Auke Jan Ijspeert",
                "Jun Nakanishi",
                "Heiko Hoffmann",
                "Peter Pastor",
                "Stefan Schaal"
            ],
            "title": "Dynamical movement primitives: learning attractor models for motor behaviors",
            "venue": "Neural computation,",
            "year": 2013
        },
        {
            "authors": [
                "Jens Kober",
                "Jan Peters"
            ],
            "title": "Policy search for motor primitives in robotics",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Friedrich Lange",
                "Michael Suppa"
            ],
            "title": "Trajectory generation for immediate path-accurate jerk-limited stopping of industrial robots",
            "venue": "In 2015 IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2015
        },
        {
            "authors": [
                "Ge Li",
                "Zeqi Jin",
                "Michael Volpp",
                "Fabian Otto",
                "Rudolf Lioutikov",
                "Gerhard Neumann"
            ],
            "title": "Prodmp: A unified perspective on dynamic and probabilistic movement primitives",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Guilherme Maeda",
                "Marco Ewerton",
                "Rudolf Lioutikov",
                "Heni Ben Amor",
                "Jan Peters",
                "Gerhard Neumann"
            ],
            "title": "Learning interaction for collaborative tasks with probabilistic movement primitives",
            "venue": "In 2014 IEEE-RAS International Conference on Humanoid Robots,",
            "year": 2014
        },
        {
            "authors": [
                "Horia Mania",
                "Aurelia Guy",
                "Benjamin Recht"
            ],
            "title": "Simple random search of static linear policies is competitive for reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Franziska Meier",
                "Stefan Schaal"
            ],
            "title": "A probabilistic representation for dynamic movement primitives",
            "venue": "arXiv preprint arXiv:1612.05932,",
            "year": 2016
        },
        {
            "authors": [
                "Fabian Otto",
                "Philipp Becker",
                "Ngo Anh Vien",
                "Hanna Carolin Ziesche",
                "Gerhard Neumann"
            ],
            "title": "Differentiable trust region layers for deep reinforcement learning",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Fabian Otto",
                "Onur Celik",
                "Hongyi Zhou",
                "Hanna Ziesche",
                "Vien Anh Ngo",
                "Gerhard Neumann"
            ],
            "title": "Deep black-box reinforcement learning with movement primitives",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Fabian Otto",
                "Hongyi Zhou",
                "Onur Celik",
                "Ge Li",
                "Rudolf Lioutikov",
                "Gerhard Neumann"
            ],
            "title": "Mp3: Movement primitive-based (re-) planning policy",
            "venue": "arXiv preprint arXiv:2306.12729,",
            "year": 2023
        },
        {
            "authors": [
                "Rok Pahi\u010d",
                "Barry Ridge",
                "Andrej Gams",
                "Jun Morimoto",
                "Ale\u0161 Ude"
            ],
            "title": "Training of deep neural networks for the generation of dynamic movement primitives",
            "venue": "Neural Networks,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandros Paraschos",
                "Christian Daniel",
                "Jan Peters",
                "Gerhard Neumann"
            ],
            "title": "Probabilistic movement primitives",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Jan Peters",
                "Stefan Schaal"
            ],
            "title": "Reinforcement learning of motor skills with policy gradients",
            "venue": "Neural networks,",
            "year": 2008
        },
        {
            "authors": [
                "Michael Przystupa",
                "Faezeh Haghverd",
                "Martin Jagersand",
                "Samuele Tosatto"
            ],
            "title": "Deep probabilistic movement primitives with a bayesian aggregator",
            "venue": "arXiv preprint arXiv:2307.05141,",
            "year": 2023
        },
        {
            "authors": [
                "Antonin Raffin",
                "Ashley Hill",
                "Adam Gleave",
                "Anssi Kanervisto",
                "Maximilian Ernestus",
                "Noah Dormann"
            ],
            "title": "Stable-baselines3: Reliable reinforcement learning implementations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Antonin Raffin",
                "Jens Kober",
                "Freek Stulp"
            ],
            "title": "Smooth exploration for robotic reinforcement learning",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Leonel Rozo",
                "Vedant Dave"
            ],
            "title": "Orientation probabilistic movement primitives on riemannian manifolds",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas R\u00fcckstie\u00df",
                "Martin Felder",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "State-dependent exploration for policy gradient methods",
            "venue": "Machine Learning and Knowledge Discovery in Databases,",
            "year": 2008
        },
        {
            "authors": [
                "Thomas R\u00fcckstiess",
                "Frank Sehnke",
                "Tom Schaul",
                "Daan Wierstra",
                "Yi Sun",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Exploring parameter space in reinforcement",
            "venue": "learning. Paladyn,",
            "year": 2010
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho",
                "Xi Chen",
                "Szymon Sidor",
                "Ilya Sutskever"
            ],
            "title": "Evolution strategies as a scalable alternative to reinforcement learning",
            "venue": "arXiv preprint arXiv:1703.03864,",
            "year": 2017
        },
        {
            "authors": [
                "Stefan Schaal"
            ],
            "title": "Dynamic movement primitives-a framework for motor control in humans and humanoid robotics",
            "venue": "In Adaptive motion of animals and machines,",
            "year": 2006
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael Jordan",
                "Pieter Abbeel"
            ],
            "title": "Highdimensional continuous control using generalized advantage estimation",
            "venue": "arXiv preprint arXiv:1506.02438,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Pierre Schumacher",
                "Daniel F.B. Haeufle",
                "Dieter B\u00fcchler",
                "Syn Schmitt",
                "Georg Martius"
            ],
            "title": "Dep-rl: Embodied exploration for reinforcement learning in overactuated and musculoskeletal systems",
            "venue": "In Proceedings of the Eleventh International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Muhammet Yunus Seker",
                "Mert Imre",
                "Justus H Piater",
                "Emre Ugur"
            ],
            "title": "Conditional neural movement primitives",
            "venue": "In Robotics: Science and Systems,",
            "year": 2019
        },
        {
            "authors": [
                "RB Ashith Shyam",
                "Peter Lightbody",
                "Gautham Das",
                "Pengcheng Liu",
                "Sebastian Gomez-Gonzalez",
                "Gerhard Neumann"
            ],
            "title": "Improving local trajectory optimisation using probabilistic movement primitives",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2019
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour"
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "Voot Tangkaratt",
                "Herke Van Hoof",
                "Simone Parisi",
                "Gerhard Neumann",
                "Jan Peters",
                "Masashi Sugiyama"
            ],
            "title": "Policy search with high-dimensional context variables",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Jens Timmer",
                "Michel Koenig"
            ],
            "title": "On generating power law noise",
            "venue": "Astronomy and Astrophysics,",
            "year": 1995
        },
        {
            "authors": [
                "Darrell Whitley",
                "Stephen Dominic",
                "Rajarshi Das",
                "Charles W Anderson"
            ],
            "title": "Genetic reinforcement learning for neurocontrol problems",
            "venue": "Machine Learning,",
            "year": 1993
        },
        {
            "authors": [
                "Ronald J Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "Michael Wininger",
                "Nam-Hun Kim",
                "William Craelius"
            ],
            "title": "Spatial resolution of spontaneous accelerations in reaching tasks",
            "venue": "Journal of Biomechanics,",
            "year": 2009
        },
        {
            "authors": [
                "Chenguang Yang",
                "Chuize Chen",
                "Wei He",
                "Rongxin Cui",
                "Zhijun Li"
            ],
            "title": "Robot learning system based on adaptive neural control and dynamic movement primitives",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2018
        },
        {
            "authors": [
                "Haonan Yu",
                "Wei Xu",
                "Haichao Zhang"
            ],
            "title": "Taac: Temporally abstract actor-critic for continuous control",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tianhe Yu",
                "Deirdre Quillen",
                "Zhanpeng He",
                "Ryan Julian",
                "Karol Hausman",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "venue": "In Conference on robot learning,",
            "year": 2020
        },
        {
            "authors": [
                "You Zhou",
                "Jianfeng Gao",
                "Tamim Asfour"
            ],
            "title": "Learning via-point movement primitives with interand extrapolation capabilities",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "By considering how policies interact with the environment, reinforcement learning (RL) methodologies can be classified into two distinct categories: step-based RL (SRL) and episodic RL (ERL). SRL predicts actions for each perceived state, while ERL selects an entire behavioral sequence at the start of an episode. Most predominant deep RL methods, such as PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018a), fall into the category of SRL. In these methods, the step information \u2014 comprising state, action, reward, subsequent state, and done signal received by the RL agent at each discrete time step \u2014 is pivotal for policy updates. This granular data aids in estimating the policy gradient (Williams, 1992; Sutton et al., 1999), approximating state or state-action value functions (Haarnoja et al., 2018a), and assessing advantages (Schulman et al., 2015b). Although SRL methods have achieved great success in various domains, they often face significant exploration challenges. Exploration in SRL, often based on a stochastic policy like a factorized Gaussian, typically lacks temporal and cross-DoF (degrees of freedom) correlations. This deficiency leads to inconsistent and inefficient exploration across state and action spaces (Raffin et al., 2022; Schumacher et al., 2023), as shown in Fig. 1a. Furthermore, the high variance in trajectories generated through such exploration can cause suboptimal convergence and training instability, a phenomenon highlighted by considerable performance differences across various random seeds (Agarwal et al., 2021).\nEpisodic RL, in contrast to SRL, represents a distinct branch of RL that emphasizes the maximization of returns over entire episodes (Whitley et al., 1993; Igel, 2003; Peters & Schaal, 2008), rather than focusing on the internal evolution of the environment interaction. This approach shifts the solution search from per-step actions to a parameterized trajectory space, employing techniques like\n\u2217Corresponding author. Email to <geli.bruce.ai@gmail.com, ge.li@kit.edu>\nMovement Primitives (MPs) (Schaal, 2006; Paraschos et al., 2013). Such exploration strategy allows for broader exploration horizons and ensures consistent trajectory smoothness across task episodes, as illustrated in Fig. 1b. Additionally, it is theoretically capable of capturing temporal correlations and interdependencies among DoF. ERL typically treats entire trajectories as single data points, often overlooking the internal changes in the environment and state transitions. This approach leads to training predominantly using black-box optimization methods (Salimans et al., 2017; Tangkaratt et al., 2017; Celik et al., 2022; Otto et al., 2022). The term black box in our title reflects this reliance on black-box optimization, which tends to overlook detailed step-based information acquired during environmental interactions. However, this often results in a lack of attention to the individual contributions of each segment of the trajectory to the overall task success. Consequently, while ERL excels in expansive exploration and maintaining trajectory smoothness, it typically requires a larger volume of samples for effective policy training. In contrast, step-based RL methods have demonstrated notable advancements in learning efficiency by utilizing this detailed step-based information. Open the Black Box. In this paper, our goal is to integrate step-based information into the policy update process of ERL. Our proposed method, Temporally-Correlated Episodic RL (TCE), moves beyond the traditional approach of treating an entire trajectory as a single data point. Instead, we transform trajectory-wide elements, such as reproducing likelihood and advantage, into their segment-wise counterparts. This enables us to leverage the step-based information to recognize and accentuate the unique contributions of each trajectory segment to overall task success. Through this innovative approach, we have opened the black box of ERL, making it more effective while retaining its strength. As a further step, we explore the benefits of fully-correlated trajectory exploration in deep ERL. We demonstrate that leveraging full covariance matrices for trajectory distributions significantly improves policy quality in existing black-box ERL methods like Otto et al. (2022). Our contributions are summarized as: (a) We propose TCE, a novel RL framework that integrates step-based information into the policy updates of ERL, while preserving the broad exploration scope and trajectory smoothness characteristic of ERL. (b) We provide an in-depth analysis of exploration strategies that effectively capture both temporal and degrees of freedom (DoF) correlations, demonstrating their beneficial impact on policy quality and trajectory smoothness. (c) We conduct a comprehensive evaluation of our approach on multiple simulated robotic manipulation tasks, comparing its performance against other baseline methods."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 EPISODIC REINFORCEMENT LEARNING",
            "text": "Markov Decision Process (MDP). We consider a MDP problem of a policy search defined by a tuple (S,A, T ,R,P0, \u03b3). We assume the state space S and action space A are continuous and the transition probabilities T : S\u00d7S\u00d7A \u2192 [0, 1] describe the state transition probability to st+1, given the current state st \u2208 S and action at \u2208 A. The initial state distribution is denoted as P0 : S \u2192\n[0, 1]. The reward rt(st,at) returned by the environment is given by a function R : S \u00d7 A \u2192 R and \u03b3 \u2208 [0, 1] describes the discount factor. The goal of RL in general is to find a policy \u03c0 that maximizes the expected accumulated reward, namely return, as R = ET ,P0,\u03c0[ \u2211\u221e t=0 \u03b3 trt].\nEpisodic RL (Whitley et al., 1993) focuses on maximizing the return R = \u2211T\nt=0[\u03b3 trt] over a task\nepisode of length T , irrespective of the state transitions within the episode. This approach typically employs a parameterized trajectory generator, like MPs (Schaal, 2006), to predict a trajectory parameter vector w. This vector is then used to generate a complete reference trajectory y(w) = [yt]t=0:T . The resulting trajectory is executed using a trajectory tracking controller to accomplish the task. In this context, yt \u2208 RD denotes the trajectory value at time t for a system with D DoF, differentiating it from the per-step action a used in SRL. It is important to note that, although ERL predicts an entire action trajectory, it still maintains the Markov Property, where the state transition probability depends only on the given current state and action (Sutton & Barto, 2018). In this respect, the action selection process in ERL is fundamentally similar to techniques like action repeat (Braylan et al., 2015) and temporally correlated action selection (Raffin et al., 2022; Eberhard et al., 2022). In contrast to SRL, ERL predicts the trajectory parameters as \u03c0(w|s), which shifts the solution search from the per-step action space A to the parameter space W . Therefore, a trajectory parameterized by a vector w is typically treated as a single data point in W . Consequently, ERL commonly employs black-box optimization methods for problem-solving (Salimans et al., 2017; Otto et al., 2022). The general learning objective of ERL is formally expressed as\nJ = \u222b \u03c0\u03b8(w|s)[R(s,w)\u2212 V \u03c0(s)]dw = Ew\u223c\u03c0\u03b8(w|s)[A(s,w)], (1)\nwhere \u03c0\u03b8 represents the policy, parameterized by \u03b8, e. g. using NNs. The initial state s \u2208 S characterizes the starting configuration of the environment and the task goal, serving as the input to the policy. The \u03c0\u03b8(w|s) indicates the likelihood of selecting the trajectory parameter w. The term R(s,w) = \u2211T t=0[\u03b3\ntrt] represents the return obtained from executing the trajectory, while V \u03c0(s) = Ew\u223c\u03c0\u03b8(w|s)[R(s,w)] denotes the expected return across all possible trajectories under policy \u03c0\u03b8. Their subtraction is defined as the advantage function A(s,w), which quantifies the benefit of selecting a specific trajectory. By using parameterized trajectory generators like MPs, ERL benefits from consistent exploration, smooth trajectories, and robustness against local optima, as noted by Otto et al. (2022). However, its policy update strategy incurs a trade-off in terms of learning efficiency, as valuable step-based information is overlooked during policy updates. Furthermore, existing method like Bahl et al. (2020); Otto et al. (2022) commonly use factorized Gaussian policies, which inherently limits their capacity to capture all relevant movement correlations."
        },
        {
            "heading": "2.2 USING MOVEMENT PRIMITIVES FOR TRAJECTORY REPRESENTATION",
            "text": "The Movement Primitives (MP), as a parameterized trajectory generator, play an important role in ERL and robot learning. This section highlights key MP methodologies and their mathematical foundations, deferring a more detailed discussion to Appendix B. Schaal (2006) introduced the Dynamic Movement Primitives (DMPs) method, incorporating a force signal into a dynamical system to produce smooth trajectories from given initial robot states. Following this, Paraschos et al. (2013) developed Probabilistic Movement Primitives (ProMPs), which leverages a linear basis function representation to map parameter vectors to trajectories and their corresponding distributions. The probability of observing a trajectory [yt]t=0:T given a specific weight vector distribution p(w) \u223c N (w|\u00b5w,\u03a3w) is represented as a linear basis function model:\n[yt]t=0:T = \u03a6 \u22ba 0:Tw + \u03f5y, (2)\np([yt]t=0:T ; \u00b5y,\u03a3y) = N (\u03a6\u22ba0:T\u00b5w, \u03a6 \u22ba 0:T\u03a3w\u03a60:T + \u03c3 2 yI). (3)\nHere, \u03f5y is zero-mean white noise with variance \u03c32y . The matrix \u03a60:T houses the basis functions for each time step t. Additionally, p([yt]t=0:T ; \u00b5y,\u03a3y) defines the trajectory distribution coupling the DoF and time steps, mapped from p(w). For a D-DoF system with N parameters per DoF and T time steps, the dimensions of the variables in Eq. (2) and 3 are as follows: w,\u00b5w : D \u00b7 N ; \u03a3w : D \u00b7N \u00d7D \u00b7N ; \u03a60:T : D \u00b7N \u00d7D \u00b7 T ; y,\u00b5y : D \u00b7 T ; \u03a3y : D \u00b7 T \u00d7D \u00b7 T . Recently, Li et al. (2023) introduced Probabilistic Dynamic Movement Primitives (ProDMPs), a hybrid approach that blends the pros of both methods. Similar to ProMP, ProDMPs defines a trajectory\nas y(t) = \u03a6(t)\u22baw + c1y1(t) + c2y2(t). The added terms c1y1(t) + c2y2(t) are included to ensure accurate trajectory initialization. This formulation combines the distributional modeling benefits of ProMP with the precision in trajectory initiation offered by DMP."
        },
        {
            "heading": "2.3 REPRESENTATION OF TRAJECTORY DISTRIBUTION AND LIKELIHOOD",
            "text": "Computing the trajectory distribution and reconstruction likelihood is crucial for policy updates in ERL. Previous methods like Bahl et al. (2020); Otto et al. (2022) represented the trajectory distribution using the parameter distribution p(w) and the likelihood of a sampled trajectory y\u2217 with its parameter vector as p(w\u2217|\u00b5w, \u03c32w). However, this approach treats an entire trajectory as a singular data point and fails to efficiently utilize step-based information. In contrast, research in imitation learning, including works by Paraschos et al. (2013); Gomez-Gonzalez et al. (2016), maps parameter distributions to trajectory space and allows the exploitation of trajectory-specific information. Yet, the likelihood computation in this space is computationally intensive, primarily due to the need to invert a high-dimensional covariance matrix, a process with an O((D \u00b7T )3) time complexity. Recent studies, like those by (Seker et al., 2019; Akbulut et al., 2021; Przystupa et al., 2023), advocates for directly modeling the trajectory distribution using neural networks. These methods typically employ a factorized Gaussian distribution N (y|\u00b5y, \u03c32y), instead of a full Gaussian distribution N (y|\u00b5y,\u03a3y) that accounts for both the DoF and time steps. This choice mitigates the computational burden of likelihood calculations, but comes at the cost of neglecting key temporal correlations and interactions between different DoF. To address these challenges, Li et al. (2023) introduced a novel approach for estimating the trajectory likelihood with a set of paired time points (tk, t\u2032k), k = 1, ...,K, as\nlog p([yt]t=0:T ) \u2248 1\nK K\u2211 k=1 logN (y(tk,t\u2032k)|\u00b5(tk,t\u2032k),\u03a3(tk,t\u2032k)), (4)\nAs shown in Fig. 2, this method scales down the dimensions of a trajectory distribution from D \u00b7T to a more manageable D \u00b7 2. Through the use of batched, randomly selected time pairs during training, the method is proved to efficiently capture correlations while reducing computational cost."
        },
        {
            "heading": "2.4 USING TRUST REGIONS FOR STABLE POLICY UPDATE",
            "text": "In ERL, the parameter space W typically exhibits higher dimensionality compared to the action space A. This complexity presents unique challenges in maintaining stable policy updates. Trust regions methods (Schulman et al., 2015a; 2017) has long been recognized as an effective technique for ensuring the stability and convergence of policy gradient methods. While popular methods such as PPO approximate trust regions using surrogate cost functions, they lack the capacity for exact enforcement. To tackle this issue, Otto et al. (2021) introduced trust region projection layer (TRPL), a mathematically rigorous and scalable technique that precisely enforces trust regions in deep RL\nalgorithms. By incorporating differentiable convex optimization layers (Agrawal et al., 2019), this method not only allows for trust region enforcement for each input state, but also demonstrates significant effectiveness and stability in high-dim parameter space, as validated in method like BBRL Otto et al. (2022). The TRPL takes standard outputs of a Gaussian policy\u2014namely, the mean vector \u00b5 and covariance matrix \u03a3 \u2014and applies a state-specific projection operation to maintain trust regions. The adjusted Gaussian policy, parameterized by \u00b5\u0303 and \u03a3\u0303, forms the basis for subsequent computations. Let dmean and dcov be the dissimilarity measures, e. g. KL-divergence, for mean and covariance, bounded by \u03f5\u00b5 and \u03f5\u03a3 respectively. The optimization for each state s is formulated as:\nargmin \u00b5\u0303s\ndmean (\u00b5\u0303s,\u00b5(s)) , s. t. dmean (\u00b5\u0303s,\u00b5old(s)) \u2264 \u03f5\u00b5, and\nargmin \u03a3\u0303s dcov\n( \u03a3\u0303s,\u03a3(s) ) , s. t. dcov ( \u03a3\u0303s,\u03a3old(s) ) \u2264 \u03f5\u03a3.\n(5)\n3 USE STEP-BASED INFORMATION FOR ERL POLICY UPDATES\nWe introduce an innovative framework of ERL that builds on traditional ERL foundations, aiming to facilitate an efficient policy update mechanism while preserving the intrinsic benefits of ERL. The key innovation lies in redefining the role of trajectories in the policy update process. In contrast to previous methods which consider an entire trajectory as a single data point, our approach breaks down the trajectory into individual segments. Each segment is evaluated and weighted based on its distinct contribution to the task success. This method allows for a more effective use of step-based information in ERL. The comprehensive structure of this framework is depicted in Figure 3.\nTrajectory Prediction and Generation. As highlighted by green arrows in Fig. 3, we adopt a structure similar to previous ERL works, such as the one described by Otto et al. (2022). However, this part distinguish itself by using the most recent ProDMPs for trajectory generation and distribution\nmodeling, due to the improved support for trajectory initialization. Additionally, we enhance the previous framework by using a full covariance matrix policy \u03c0(w|s) = N (w|\u00b5w,\u03a3w) as opposed to a factorized Gaussian policy, to capture a broader range of movement correlations.\nTrajectory Likelihood Representation. In RL, the likelihood of previously sampled actions, along with their associated returns, is often used to adjust the chance of these actions being selected in future policies. In previous ERL methods, this process typically involves the probability of choosing an entire trajectory. However, our framework adopts a different strategy, as shown in blue arrows in Fig. 3. Using the techniques in Sections 2.2 and 2.3, our approach begins by selecting K paired time steps. We then transform the parameter likelihood into a trajectory likelihood, which is calculated using these K pairwise likelihoods. This approach, depicted in Figure 4, effectively divides the whole trajectory into K distinct segments, with each segment defined by a pair of time steps. In essence, this method allows us to break down the overall trajectory likelihood into individual segment likelihoods, offering a more detailed view of the trajectory\u2019s contribution to task success.\nTrajectory to Segments: p([yt]t=0:T |s) \u225c {p([yt]t=tk:t\u2032k |s)}k=1...K , (6)\nLocal Representation: p([yt]t=tk:t\u2032k |s) \u225c p([ytk ,yt\u2032k ]|\u00b5(tk,t\u2032k)(s),\u03a3(tk,t\u2032k)(s)). (7)\nDefinition of Segment Advantages. Similar to standard SRL methods, we leverage the advantage function to evaluate the benefits of executing individual segments within a sampled trajectory. When being at state stk and following the trajectory segment [yt]t=tk:t\u2032k , the segment-wise advantage function A(stk , [yt]t=tk:t\u2032k) quantifies the difference between the actual return obtained by executing this sampled trajectory segment and the expected return from a randomly chosen segment, as\nA(stk , [yt]t=tk:t\u2032k) = t=t\u2032k\u22121\u2211 t=tk \u03b3t\u2212tkrt + \u03b3 t\u2032k\u2212tkV \u03c0old(st\u2032k)\u2212 V \u03c0old(stk), (8)\nwhere V \u03c0old(stk) denotes the value function of the current policy. In our method, the estimation of V \u03c0old(stk) is consistent with the approaches commonly used in SRL and is independent of the design choice of segment selections. We use NNs to estimate V \u03c0(s) \u2248 V \u03c0\u03d5 (s) which is fitted on targets of true return or obtained by general advantage estimation (GAE) (Schulman et al., 2015b).\nLearning Objective. By replacing the trajectory likelihood and advantage with their segment-based counterparts in the original ERL learning objective as stated in Eq. (1), we propose the learning objective of our method as follows\nJ(\u03b8) = E\u03c0old\n[ 1\nK K\u2211 k=1 p\u03c0new([yt]t=tk:t\u2032k |s) p\u03c0old([yt]t=tk:t\u2032k |s) A\u03c0old(stk , [yt]t=tk:t\u2032k)\n] . (9)\nHere, s denotes the initial state of the episode, used for selecting the parameter vector w, and stk is the state of the system at time tk. The learning objective takes the Importance Sampling to update policies using data from previous policies (Schulman et al., 2015a; 2017; Otto et al., 2021). Our method retains the advantages of exploration in parameter space and generating smooth trajectories. This enables us to enhance the likelihood of segments with high advantage and reduce the likelihood of less rewarding ones during policy updates. To ensure a stable update for the full covariance Gaussian policy \u03c0\u03b8(w|s) = N (\u00b5w,\u03a3w), we deploy a differentiable Trust Region Projection step (Otto et al., 2021) after each policy update iteration as previously discussed in Section 2.4."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "Improve Exploration and Smoothness in Step-based RL. SRL methods, such as PPO and SAC, interact with the environment by performing a sampled action at each time-step. This strategy often results in a control signal with high-frequency noise, making it unsuitable for direct use in robotic systems (Raffin et al., 2022). A prevalent solution is to reduce the sampling frequency, a technique commonly known as frame skip (Braylan et al., 2015). Here, the agent only samples actions every k-th time step and replicates this action for the skipped steps. Similar approaches decide whether to repeat the last action or to sample a new action in every time step (Biedenkapp et al., 2021; Yu et al., 2021). This concept is also echoed in works such as general State Dependent Exploration\n(gSDE) (Raffin et al., 2022; Ru\u0308ckstie\u00df et al., 2008; Chiappa et al., 2023), where the applied noise is sampled in a state-dependent fashion; leading to smooth changes of the disturbance between consecutive steps. However, while these methods improved the smoothness in small segments, they struggled to model long-horizon correlations. Another area of concern is the utilization of white noise during sampling, which fails to consider the temporal correlations between time steps and results in a random walk with suboptimal exploration. To mitigate this, previous research, such as Lillicrap et al. (2015) and Eberhard et al. (2022), have integrated colored noise into the RL policy, aiming to foster exploration that is correlated across time steps. While these methods have shown advantages over white noise approaches, they neither improve the trajectory\u2019s smoothness, nor adequately capture the cross-DoF correlations.\nEpisodic RL. The early ERL approaches used black-box optimization to evolve parameterized policies, e.g., small NN (Whitley et al., 1993; Igel, 2003; Gomez et al., 2008). However, these early works were limited to tasks with low-dimensional action space, for instance, the Cart Pole. Although recent works (Salimans et al., 2017; Mania et al., 2018) have shown that, with more computing, these methods can achieve comparable asymptotic performance with step-based algorithms in some locomotion tasks, none of these methods can deal with tasks with context variations (e.g., changing goals). Another research line in ERL works with more compact policy representation. Peters & Schaal (2008); Kober & Peters (2008) first combined ERL with MPs, reducing the dimension of searching space from NN parameter space to MPs parameter space with the extra benefits of smooth trajectories generation. Abdolmaleki et al. (2015) proposed a model-based method to improve the sample efficiency. Notably, although those methods can already handle some complex manipulation tasks such as robot baseball (Peters & Schaal, 2008), none of them can deal with contextual tasks. To deal with that problem, (Abdolmaleki et al., 2017) further extends that utilizes linear policy conditioned on the context. Another recent work from this research line (Celik et al., 2022) proposed using a Mixture of Experts (MoE) to learn versatile skills under the ERL framework.\nBBRL. As the first method that utilizes non-linear adaptation to contextual ERL, Deep Black Box Reinforcement Learning (BBRL) (Otto et al., 2022) is the most related work to our method. BBRL applies trust-region-constrained policy optimization to learn a weight adaptation policy for MPs. Despite demonstrating great success in learning tasks with sparse and non-Markovian rewards, it requires significantly more samples to converge compared to SoTA SRL methods. This could be attributed to its black-box nature, where the trajectory from the entire episode is treated as a single data point, and the trajectory return is calculated by summing up all step rewards within the episode."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We evaluate the effectiveness of our model through experiments on a variety of simulated robot manipulation tasks. The performance of TCE is compared against well-known deep RL algorithms as well as methods specifically designed for correlated actions and consistent trajectories. The evaluation is designed to answer the following questions: (a) Can our model effectively train the policy across diverse tasks, incorporating various robot types, control paradigms (task and joint space), and reward configurations? (b) Does the incorporation of movement correlations lead to higher task success rates? (c) Are there limitations or trade-offs when adopting our proposed learning strategy?\nFor the comparative evaluation, we select the following methods: PPO, SAC, TRPL, gSDE, PINK (Eberhard et al., 2022) and BBRL. Descriptions, hyper-parameters, and references to the used code bases of these methods can be found in Appendix C.1.We use step-based methods (PPO, SAC, TRPL, gSDE, and PINK) to predict the lower-level actions for each task. On the other hand, for episodic methods like BBRL and TCE, we predict position and velocity trajectories and then employ a PD controller to compute the lower-level control commands. Across all experiments, we measure task success in terms of the number of environment interactions required. Each algorithm is evaluated using 20 distinct random seeds. Results are quantified using the Interquartile Mean (IQM) and are accompanied by a 95% stratified bootstrap confidence interval (Agarwal et al., 2021)."
        },
        {
            "heading": "5.1 LARGE SCALE ROBOT MANIPULATION BENCHMARK USING METAWORLD",
            "text": "We begin our evaluation using the Metaworld benchmark (Yu et al., 2020), a comprehensive testbed that includes 50 manipulation tasks of varying complexity. Control is executed in a 3-DOF task\nspace along with the finger closeness, and a dense reward signal is employed. In contrast to the original evaluation protocol, we introduce a more stringent framework. Specifically, each episode is initialized with a randomly generated context, rather than a fixed one. Additionally, we tighten the success criteria to only consider a task successfully completed if the objective is maintained until the final time step. This adjustment mitigates scenarios where transient successes are followed by erratic agent behavior. While we train separate policies for each task, the hyperparameters remain constant across all 50 tasks. For each method, we report the overall success rate as measured by the IQM across the 50 sub-tasks in Fig. 5a. The performance profiles are presented in Fig. 5b.\nIn both metrics, our method significantly outperforms the baselines in achieving task success. BBRL exhibits the second-best performance in terms of overall consistency across tasks but lags in training speed compared to step-based methods. We attribute this difference to the use of per-step dense rewards, which enables faster policy updates in step-based approaches. TCE leverages the advantages of both paradigms, surpassing other algorithms after approximately 107 environment interactions. Notably, the off-policy\nmethods SAC and PINK were trained with fewer samples than used for on-policy methods due to their limitations in parallel environment utilization. PINK achieved superior final performance but at the expense of sample efficiency compared to SAC. In Appendices C.2 and C.3, we provide the results for 50 tasks and a performance profile analysis of TCE."
        },
        {
            "heading": "5.2 JOINT SPACE CONTROL WITH MULTI TASK OBJECTIVES",
            "text": "Next, we investigate the advantages of modeling complete movement correlations and the utility of intermediate feedback for policy optimization. To this end, we enhance the BBRL algorithm by expanding its factorized Gaussian policy to accommodate full covariance (BBRL Cov.), thereby capturing movement correlations. Both the original and augmented versions of BBRL are included in the subsequent task evaluations. We evaluate the methods on a customized Hopper Jump task, sourced from OpenAI Gym (Brockman et al., 2016). This 3-DoF task primarily focuses on maximizing jump height while also accounting for precise landing at a designated location. Control is executed in joint space. We report the max jump height as the main metric of success in Fig. 6a. Our method exhibits quick learning and excels in maximizing jump height. Both BBRL versions exhibit similar performance, while BBRL Cov. demonstrates marginal improvements over the original. However, they both fall behind TCE in training speed, highlighting the efficiency gains we achieve through intermediate state-based policy updates. Step-based methods like PPO and TRPL tend to converge to sub-optimal policies. The only exception is gSDE. As an augmented step-based method, it enables smoother and more consistent exploration but exhibits significant sensitivity to model initialization (random seeds), evident from the wide confidence intervals."
        },
        {
            "heading": "5.3 CONTACT-RICH MANIPULATION WITH DENSE AND SPARSE REWARD SETTINGS",
            "text": "We further turn to a 7-DoF robot box-pushing task adapted from (Otto et al., 2022). The task requires the robot\u2019s end-effector, equipped with a rod, to maneuver a box to a specified target position and orientation. The difficulty lies in the need for continuous, correlated movements to both position and orient the box accurately. To amplify the complexity, the initial pose of the box is randomized. We test two reward settings: dense and sparse. The dense setting offers intermediate rewards inversely proportional to the current distance between the box and its target pose, while the sparse setting only allocates rewards at the episode\u2019s end based on the final task state. Performance metrics for both settings are shown in Fig. 6b and 6c. In either case, TCE and gSDE exhibit superior performance but\nwith TCE demonstrating greater consistency across different random seeds. The augmented BBRL version outperforms its original counterpart, emphasizing the need for fully correlated movements in tasks that demand consistent object manipulation. The other step-based methods struggle to learn the task effectively, even when dense rewards are provided. This further highlights the advantages of modeling the movement trajectory as a unified action, as opposed to a step-by-step approach."
        },
        {
            "heading": "5.4 HITTING TASK WITH HIGH SPARSITY REWARD SETTING",
            "text": "In our last experiment, we assess the limitations of our method using a 7-DoF robot table tennis task, originally from (Celik et al., 2022). The robot aims to return a randomly incoming ball to a desired target on the opponent\u2019s court. To enhance the task\u2019s realism, we randomize the robot\u2019s initial state instead of using a fixed starting pose. This task is distinct due to its one-shot nature: the robot has only one chance to hit the ball and loses control over the ball\u2019s trajectory thereafter. The need for diverse hitting strategies like forehand and backhand adds complexity and increases the number of samples required for training. Performance metrics are presented in Fig. 7. The BBRL Cov. emerges as the leader, achieving a 20% higher success rate than other methods. It is followed by TCE and the original BBRL, with TCE displaying intermediate learning speeds between the two BBRL versions. Step-based methods, led by TRPL at a mere 15% task success, struggle notably in this setting. We attribute\nthe underperformance of TCE and step-based methods to the task\u2019s reward sparsity, which complicates the value function learning of SRL and TCE. Despite these challenges, TCE maintains its edge over other baselines, further attesting to its robustness, even under stringent conditions."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We introduced TCE that synergizes the exploration advantages of ERL with the sample efficiency of SRL. Empirical evaluation showcases that TCE matches the sample efficiency of SRL and consistently delivers competitive asymptotic performance across various tasks. Furthermore, we demonstrated both the sample efficiency and policy performance of episodic policies can be further improved by incorporating proper correlation modeling. Despite the promise, several opening questions remain for future work. Firstly, TCE yields moderate results for tasks characterized by particularly sparse reward settings, as observed in scenarios like table tennis. Secondly, ERL approaches often need a low-level tracking controller, which might not be feasible for certain task types, such as locomotion. Additionally, the current open-loop control setup lacks the adaptability needed for complex control problems in dynamic environments where immediate feedback and swift adaptation are crucial. These issues will be at the forefront of our future work."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We thank our colleagues Onur Celik, Maximilian Xiling Li, Vaisakh Kumar Shaj, and Bala\u0301zs Gyenes at KIT for the valuable discussion, technical support and proofreading. We thank the anonymous reviewers for their insightful feedback which greatly improved the quality of this paper.\nThe research presented in this paper was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 448648559, and was supported in part by the Helmholtz Association of German Research Centers. Gerhard Neumann was supported in part by Carl Zeiss Foundation through the Project JuBot (Jung Bleiben mit Robotern). The authors acknowledge support by the state of Baden-Wu\u0308rttemberg through bwHPC, and the HoreKa supercomputer."
        },
        {
            "heading": "A ALGORITHM BOX",
            "text": "Algorithm 1 Temporally-Correlated Episodic RL (TCE) 1: Initialize policy parameters \u03b8 and value function parameters \u03d5 2: for iteration = 1, 2, ... do 3: Get the initial state s0 4: Predict the mean \u00b5w, covariance \u03a3w, and sample w\u2217 5: Generate the trajectory y\u2217 using Eq. (2) and execute it in the environment 6: Collect step-based information through the execution 7: Update \u03d5, use true return or GAE style return (Schulman et al., 2015b) 8: 9: Select K time-pairs, e.g. choose every 10 steps along the trajectory 10: Compute the segment-wise likelihood {poldk }k=1:K using Eq. (6) and 7 under \u03c0old 11: for update epoch = 1, 2, ... do 12: Make prediction of the mean \u00b5neww , covariance \u03a3 new w under the latest policy \u03c0 new 13: Enforce Trust Region by projecting \u00b5neww and \u03a3 new w through TRPL using Eq. (5) 14: Get projected policy \u03c0\u0303new, represented by \u00b5\u0303neww and \u03a3\u0303 new w 15: Compute the segment-wise likelihood {pnewk }k=1:K using Eq. (6) and 7 under \u03c0\u0303new 16: Update \u03b8 by taking a gradient step w.r.t. J(\u03b8) in Eq. (9) 17: end for 18: end for"
        },
        {
            "heading": "B MATHEMATICAL FORMULATIONS OF MOVEMENT PRIMITIVES.",
            "text": "In this section, we provide a concise overview of the mathematical formulations of movement primitives utilized in this paper. We begin with the fundamentals of DMPs and ProMPs, followed by a detailed presentation of ProDMPs. This includes a focus on trajectory computation and the mapping between parameter distributions and trajectory distributions. For clarity, we begin with a single DoF system and then present the full trajectory distribution using a multi-DoF systems."
        },
        {
            "heading": "B.1 DYNAMIC MOVEMENT PRIMITIVES (DMPS)",
            "text": "Schaal (2006); Ijspeert et al. (2013) describe a single movement as a trajectory [yt]t=0:T , which is governed by a second-order linear dynamical system with a non-linear forcing function f . The mathematical representation is given by\n\u03c42y\u0308 = \u03b1(\u03b2(g \u2212 y)\u2212 \u03c4 y\u0307) + f(x), f(x) = x \u2211\n\u03c6i(x)wi\u2211 \u03c6i(x) = x\u03c6\u22baxw, (10)\nwhere y = y(t), y\u0307 = dy/dt, y\u0308 = d2y/dt2 denote the position, velocity, and acceleration of the system at a specific time t, respectively. Constants \u03b1 and \u03b2 are spring-damper parameters, g signifies a goal attractor, and \u03c4 is a time constant that modulates the speed of trajectory execution. To ensure convergence towards the goal, DMPs employ a forcing function governed by an exponentially decaying phase variable x(t) = exp(\u2212\u03b1x/\u03c4 ; t). Here, \u03c6i(x) represents the basis functions for the forcing term. The trajectory\u2019s shape as it approaches the goal is determined by the weight parameters wi \u2208 w, for i = 1, ..., N . The trajectory [yt]t=0:T is typically computed by numerically integrating the dynamical system from the start to the end point (Pahic\u030c et al., 2020; Bahl et al., 2020). However, this numerical process is computationally intensive, and complicates a directly translation between a parameter distribution p(w) to its corresponding trajectory distribution p(y) (Amor et al., 2014; Meier & Schaal, 2016). Previous method, such as GMM/GMR (Calinon et al., 2012; Calinon, 2016; Yang et al., 2018) used Gaussian Mixture Models to cover the trajectories\u2019 domain. However, this neither captures temporal correlation nor provide a generative model for the trajectories."
        },
        {
            "heading": "B.2 PROBABILISTIC MOVEMENT PRIMITIVES (PROMPS)",
            "text": "Paraschos et al. (2013) introduced a framework for modeling MPs using trajectory distributions, capturing both temporal and inter-dimensional correlations. Unlike DMPs that use a forcing term, ProMPs directly model the intended trajectory. The probability of observing a 1-DoF trajectory [yt]t=0:T given a specific weight vector distribution p(w) \u223c N (w|\u00b5w,\u03a3w) is represented as a linear basis function model:\nLinear basis function: [yt]t=0:T = \u03a6 \u22ba 0:Tw + \u03f5y, (11) Mapping distribution: p([yt]t=0:T ; \u00b5y,\u03a3y) = N (\u03a6\u22ba0:T\u00b5w, \u03a6 \u22ba 0:T\u03a3w\u03a60:T + \u03c3 2 yI). (12)\nHere, \u03f5y is zero-mean white noise with variance \u03c32y . The matrix \u03a60:T houses the basis functions for each time step t. Similar to DMPs, these basis functions can be defined in terms of a phase variable instead of time. ProMPs allows for flexible manipulation of MP trajectories through probabilistic operators applied to p(w), such as conditioning, combination, and blending (Maeda et al., 2014; Gomez-Gonzalez et al., 2016; Shyam et al., 2019; Rozo & Dave, 2022; Zhou et al., 2019). However, ProMPs lack an intrinsic dynamic system, which means they cannot guarantee a smooth transition from the robot\u2019s initial state or between different generated trajectories."
        },
        {
            "heading": "B.3 PROBABILISTIC DYNAMIC MOVEMENT PRIMITIVES (PRODMPS)",
            "text": "Solving the ODE underlying DMPs Li et al. (2023) noted that the governing equation of DMPs, as specified in Eq. (10), admits an analytical solution. This is because it is a second-order linear nonhomogeneous ODE with constant coefficients. The original ODE and its homogeneous counterpart can be expressed in standard form as follows:\nNon-homo. ODE: y\u0308 + \u03b1\n\u03c4 y\u0307 +\n\u03b1\u03b2 \u03c42 y = f(x) \u03c42 + \u03b1\u03b2 \u03c42 g \u2261 F (x, g), (13)\nHomo. ODE: y\u0308 + \u03b1\n\u03c4 y\u0307 +\n\u03b1\u03b2 \u03c42 y = 0. (14)\nThe solution to this ODE is essentially the position trajectory, and its time derivative yields the velocity trajectory. These are formulated as:\ny = [y2p2 \u2212 y1p1 y2q2 \u2212 y1q1] [ w g ] + c1y1 + c2y2 (15)\ny\u0307 = [y\u03072p2 \u2212 y\u03071p1 y\u03072q2 \u2212 y\u03071q1] [ w g ] + c1y\u03071 + c2y\u03072. (16)\nHere, the learnable parameters w and g which control the shape of the trajectory, are separable from the remaining terms. Time-dependent functions y1, y2,p1, p2, q1, q2 in the remaining terms offer the basic support to generate the trajectory. The functions y1, y2 are the complementary solutions to the homogeneous ODE presented in Eq. (14), and y\u03071, y\u03072 their time derivatives respectively. These time-dependent functions take the form as:\ny1(t) = exp ( \u2212 \u03b1 2\u03c4 t ) , y2(t) = t exp ( \u2212 \u03b1 2\u03c4 t ) , (17)\np1(t) = 1\n\u03c42 \u222b t 0 t\u2032 exp ( \u03b1 2\u03c4 t\u2032 ) x(t\u2032)\u03c6\u22baxdt \u2032, p2(t) = 1 \u03c42 \u222b t 0 exp ( \u03b1 2\u03c4 t\u2032 ) x(t\u2032)\u03c6\u22baxdt \u2032, (18)\nq1(t) = ( \u03b1 2\u03c4 t\u2212 1 ) exp ( \u03b1 2\u03c4 t ) + 1, q2(t) = \u03b1 2\u03c4 [ exp ( \u03b1 2\u03c4 t ) \u2212 1 ] . (19)\nIt\u2019s worth noting that the p1 and p2 cannot be analytically derived due to the complex nature of the forcing basis terms \u03c6x. As a result, they need to be computed numerically. Despite this, isolating the learnable parameters, namely w and g, allows for the reuse of the remaining terms across all generated trajectories. These residual terms can be more specifically identified as the position and velocity basis functions, denoted as \u03a6(t) and \u03a6\u0307(t), respectively. When both w and g are included in a concatenated vector, represented as wg , the expressions for position and velocity trajectories can be formulated in a manner akin to that employed by ProMPs:\nPosition: y(t) = \u03a6(t)\u22bawg + c1y1(t) + c2y2(t), (20)\nVelocity: y\u0307(t) = \u03a6\u0307(t)\u22bawg + c1y\u03071(t) + c2y\u03072(t). (21)\nIn the main paper, for simplicity and notation convenience, we use w instead of wg to describe the parameters and goal of ProDMPs.\nSmooth trajectory transition The coefficients c1 and c2 serve as solutions to the initial value problem delineated by the Eq.(20)(21). Li et al. propose utilizing the robot\u2019s initial state or the replanning state, characterized by the robot\u2019s position and velocity (yb, y\u0307b) to ensure a smooth commencement or transition from a previously generated trajectory. Denote the values of the complementary functions and their derivatives at the condition time tb as y1b , y2b , y\u03071b and y\u03072b . Similarly, denote the values of the position and velocity basis functions at this time as \u03a6b and \u03a6\u0307b respectively. Using these notations, c1 and c2 can be calculated as follows:[\nc1 c2\n] =  y\u03072byb\u2212y2b y\u0307by1b y\u03072b\u2212y2b y\u03071b + y2b \u03a6\u0307 \u22ba b\u2212y\u03072b\u03a6 \u22ba b y1b y\u03072b\u2212y2b y\u03071b wg\ny1b y\u0307b\u2212y\u03071byb y1b y\u03072b\u2212y2b y\u03071b + y\u03071b\u03a6 \u22ba b\u2212y1b \u03a6\u0307 \u22ba b y1b y\u03072b\u2212y2b y\u03071b wg  . (22) Substituting Eq. (22) into Eq. (20) and Eq. (21), the position and velocity trajectories take the form as\ny = \u03be1yb + \u03be2y\u0307b + [\u03be3\u03a6b + \u03be4\u03a6\u0307b +\u03a6] \u22bawg, (23)\ny\u0307 = \u03be\u03071yb + \u03be\u03072y\u0307b + [\u03be\u03073\u03a6b + \u03be\u03074\u03a6\u0307b + \u03a6\u0307] \u22bawg (24)\nHere, \u03bek for k \u2208 {1, 2, 3, 4} serve as intermediate terms that are derived from the complementary functions and the initial conditions. The formations of these terms are elaborated below. To find their derivatives \u03be\u0307k, one can simply replace y1, y2 with their time derivatives y\u03071, y\u03072 in the equations.\n\u03be1(t) = y\u03072by1 \u2212 y\u03071by2 y1b y\u03072b \u2212 y2b y\u03071b , \u03be2(t) = y1by2 \u2212 y2by1 y1b y\u03072b \u2212 y2b y\u03071b , \u03be3(t) = y\u03071by2 \u2212 y\u03072by1 y1b y\u03072b \u2212 y2b y\u03071b , \u03be4(t) = y2by1 \u2212 y1by2 y1b y\u03072b \u2212 y2b y\u03071b .\nExtend to a High DoF system Both ProMPs and ProDMPs can be generalized to accommodate high-DoF systems. This allows for the capture of both temporal correlations and interactions among various DoF. Such generalization is implemented through modifications to matrix structures and vector concatenations, as illustrated in Paraschos et al. (2013); Li et al. (2023). To be specific, the basis functions \u03a6, \u03a6\u0307, along with their values at the condition time \u03a6b, \u03a6\u0307b, are extended to blockdiagonal matrices \u03a8, \u03a8\u0307, \u03a8b and \u03a8\u0307b respectively. This extension is executed by tiling the existing basis function matrices D times along their diagonal, where D is the number of DoF. Additionally, the robot initial conditions for each DoF are concatenated into one vectors. For instance, the initial positions y1b , ..., y D b are unified into a single vector yb = [y 1 b , ..., y D b ]\n\u22ba. In this way, the position and velocity trajectories are extended as\ny = \u03be1yb + \u03be2y\u0307b + [\u03be3\u03a8b + \u03be4\u03a8\u0307b +\u03a8] \u22bawg, (25)\ny\u0307 = \u03be\u03071yb + \u03be\u03072y\u0307b + [\u03be\u03073\u03a8b + \u03be\u03074\u03a8\u0307b + \u03a8\u0307] \u22bawg. (26)\nParameter distribution to trajectory distribution In a manner analogous to the description provided for ProMPs from Equation Eq. (2) to Equation Eq. (3), ProDMPs also exhibits a comparable architecture framework. This similarity is particularly evident in the structure of the learnable parameters, denoted as wg , which follow a linear basis function format. Consequently, it is reasonable to delineate the trajectory distribution for ProDMPs in fashion akin to that of ProMPs. Given that the parameter distribution wg follows a Gaussian distribution wg \u223c N (wg|\u00b5wg ,\u03a3wg ) and adhering to the probabilistic formulation analogous to ProMPs as indicated in Eq. (3), the trajectory distribution for ProDMPs can be expressed as:\np([yt]t=0:T ; \u00b5y,\u03a3y) = N ([yt]t=0:T | \u00b5y,\u03a3y), (27)\nwhere\n\u00b5y = \u03be1yb + \u03be2y\u0307b +H \u22ba 0:T\u00b5wg , \u03a3y = H \u22ba 0:T\u03a3wgH0:T + \u03c3 2 nI,\nH0:T = \u03be3\u03a8b + \u03be4\u03a8\u0307b +\u03a80:T , \u03bek = [\u03bek(t)]t=0:T .\nIn this context, the trajectory mean, denoted as \u00b5y constitutes a vector of dimension DT , whereas the trajectory covariance, represented by \u03a3y is a DT \u00d7 DT dimensional matrix. These quantities serve to integrate the trajectory values across all degrees of freedom (DoF) and temporal steps, encapsulating them within a single distribution. This multi-DoF ProDMPs representation can be seen as an enhancement of the ProMPs framework, augmented by the inclusion of initial condition terms. This ensures that the trajectories sampled under this distribution start from the specified initial state. Additionally, the time range t = 0 : T is flexible and can be replaced by any set of discrete time points. For instance, in the TCE method, a pair of time points tk and t\u2032k can define a trajectory segment, allowing for down-sampling of the trajectory distribution to specific trajectroy segment."
        },
        {
            "heading": "C EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 DETAILS OF METHODS IMPLEMENTATION",
            "text": "PPO Proximal Policy Optimization (PPO) (Schulman et al., 2017) is a prominent on-policy stepbased RL algorithm that refines the policy gradient objective, ensuring policy updates remain close to the behavior policy. PPO branches into two main variants: PPO-Penalty, which incorporates a KL-divergence term into the objective for regularization, and PPO-Clip, which employs a clipped surrogate objective. In this study, we focus our comparisons on PPO-Clip due to its prevalent use in the field. Our implementation of PPO is based on the implementation of Raffin et al. (2021).\nSAC Soft Actor-Critic (SAC) (Haarnoja et al., 2018a;b) employs a stochastic step-based policy in an off-policy setting and utilizes double Q-networks to mitigate the overestimation of Q-values for stable updates. By integrating entropy regularization into the learning objective, SAC balances between expected returns and policy entropy, preventing the policy from premature convergence. Our implementation of SAC is based on the implementation of Raffin et al. (2021).\nTRPL Trust Region Projection Layers (TRPL) (Otto et al., 2021), akin to PPO, addresses the problem of stabilizing the on-policy policy gradient by constraining the learning policy staying close to the behavior policy. TRPL formulates the constrained optimization problem as a projection problem, providing a mathematically rigorous and scalable technique that precisely enforces trust regions on each state, leading to stable and efficient on-policy updates. We evaluated its performance based on the implementation of the original work.\ngSDE Generalized State Dependent Exploration (gSDE) (Raffin et al., 2022; Ru\u0308ckstie\u00df et al., 2008; Ru\u0308ckstiess et al., 2010) is an exploration method designed to address issues with traditional step-based exploration techniques and aims to provide smoother and more efficient exploration in the context of robotic reinforcement learning, reducing jerky motion patterns and potential damage to robot motors while maintaining competitive performance in learning tasks.\nTo achieve this, gSDE replaces the traditional approach of independently sampling from a Gaussian noise at each time step with a more structured exploration strategy, that samples in a state-dependent manner. The generated samples not only depend on parameter of the Gaussian distribution \u00b5 & \u03a3, but also on the activations of the policy network\u2019s last hidden layer (s). We generate disturbances \u03f5t using the equation \u03f5t = \u03b8\u03f5s, where \u03b8\u03f5 \u223c N d (0,\u03a3) . The exploration matrix \u03b8\u03f5 is composed of vectors of length Dim(a) that were drawn from the Gaussian distribution we want gSDE to follow. The vector s describes how this set of pre-computed exploration vectors are mixed. The exploration matrix is resampled at regular intervals, as guided by the \u2019sde sampling frequency\u2019 (ssf), occurring every n-th step if n is our ssf.\ngSDE is versatile, applicable as a substitute for the Gaussian Noise source in numerous on- and off-policy algorithms. We evaluated its performance in an on-policy setting using PPO by utilizing the reference implementation for gSDE from Raffin et al. (2022). In order for training with gSDE to remain stable and reach high performance the usage of a linear schedule over the clip range had to be used for some environments.\nPINK We utilize SAC to evaluate the effectiveness of pink noise for efficient exploration. Eberhard et al. (2022) propose to replace the independent action noise \u03f5t of\nat = \u00b5t + \u03c3t \u00b7 \u03f5t with correlated noise from particular random processes, whose power spectral density follow a power law. In particular, the use of pink noise, with the exponent \u03b2 = 1 in S(f) = |F [\u03f5](f)|2 \u221d f\u2212\u03b2 , should be considered (Eberhard et al., 2022). We follow the reference implementation and sample chunks of Gaussian pink noise using the inverse Fast Fourier Transform method proposed by Timmer & Koenig (1995). These noise variables are used for SAC\u2019s exploration but the the actor and critic updates sample the independent action distribution without pink noise. Each action dimension uses an independent noise process which\ncauses temporal correlation within each dimension but not across dimensions. Furthermore, we fix the chunk size and maximum period to 10000 which avoids frequent jumps of chunk borders and increases relative power of low frequencies.\nBBRL-Cov/Std Black-Box Reinforcement Learning (BBRL) (Otto et al., 2022; 2023) is a recent developed episodic reinforcement learning method. By utilizing ProMPs (Paraschos et al., 2013) as the trajectory generator, BBRL learns a policy that explores at the trajectory level. The method can effectively handle sparse and non-Markovian rewards by perceiving an entire trajectory as a unified data point, neglecting the temporal structure within sampled trajectories. However, on the other hand, BBRL suffers from relatively low sample efficiency due to its black-box nature. Moreover, the original BBRL employs a degenerate Gaussian policy with diagonal covariance. In this study, we extend BBRL to learn Gaussian policy with full covariance to build a more competitive baseline. For clarity, we refer to the original method as BBRL-Std and the full covariance version as BBRLCov. We integrate BBRL with ProDMPs (Li et al., 2023), aiming to isolate the effects attributable to different MP approaches."
        },
        {
            "heading": "C.2 METAWORLD PERFORMANCE PROFILE ANALYSIS",
            "text": "The distribution of success rates, reported in the performance profile in Fig. 5b, may seem to contradict the nearly perfect IQM of TCE but in reality provide insight into the consistency of TCE. Nearly two thirds of runs exceed 99% success rate and are therefore able to perfectly solve the task with this seed. The individual performances reported in Appendix C.3 show that only very few tasks, e.g., Disassemble and Hammer, have a significant fraction of unsuccessful seeds. This consistency per task is also visible in the profile, as only two percent of runs fall between zero and sixty percent success rate which is visible by the near zero slope in this range. All methods are entirely unable to solve a small set of tasks and therefore show a gap in the profile. This does not contradict the very high IQM success rate of TCE, as the IQM trims the upper and lower 25% of results. The commonly reported median effectively trims 50% and would result in even higher values. Due to the smaller and later decline in the profile compared to the other methods, the intersection between 75% of runs and the profile is located at a success rate of 90%. Therefore, only a small fraction of runs, roughly ten percent, fall within the 25% trim but only slightly decrease the value of the IQM due their high success rate. Other methods have a larger fraction of imperfect runs with lower success rate within the quartiles."
        },
        {
            "heading": "C.3 PERFORMANCE ON INDIVIDUAL METAWORLD TASKS",
            "text": "We report the Interquartile Mean (IQM) of success rates for each Metaworld task. The plots clearly illustrate the varying levels of difficulty across different tasks.\nTCE (ours) BBRL PPO TRPL SAC gSDE PINK"
        },
        {
            "heading": "C.4 HOPPER JUMP",
            "text": "As an addition to the main paper, we provide more details on the Hopper Jump task. We look at both the main goal of maximizing jump height and the secondary goal of landing on a desired position. These are shown along with the overall episode reward in Fig. 11. Our method shows quick learning and does well in achieving high jump height, consistent with what we reported earlier. While it\u2019s not as strong in landing accuracy, it still ranks high in overall performance. Both versions of BBRL have similar results. However, they train more slowly compared to TCE, highlighting the speed advantage of our method due to the use of intermediate states for policy updates. Looking at other methods, stepbased ones like PPO and TRPL focus too much on landing distance and miss out on jump height, leading to less effective policies. On the other hand, gSDE performs well but is sensitive to the initial setup, as shown by the wide confidence ranges in the results. Lastly, SAC and PINK shows inconsistent results in jump height, indicating the limitations of using pink noise for exploration, especially when compared to gSDE."
        },
        {
            "heading": "C.5 BOX PUSHING",
            "text": "The goal of the box-pushing task is to move a box to a specified goal location and orientation using the 7-DoFs Franka Emika Panda (Otto et al., 2022). To make the environment more challenging, we extend the environment from a fixed initial box position and orientation to a randomized initial position and orientation. The range of both initial and target box pose varies from x \u2208 [0.3, 0.6], y \u2208 [\u22120.45, 0.45], \u03b8z \u2208 [0, 2\u03c0]. Success is defined as a positional distance error of less than 5 cm and a z-axis orientation error of less than 0.5 rad. We refer to the original paper for the observation and action spaces definition and the reward function."
        },
        {
            "heading": "C.6 TABLE TENNIS",
            "text": "The goal of table tennis environment to use the 7-DoFs robotic arm to hit the incoming ball and return it as close as possible to the specified goal location. We adapt the table tennis environment from Celik et al. (2022); Otto et al. (2022) and extend it to a randomized initial robot joint configuration. As context space we consider the initial ball position x \u2208 [\u22121,\u22120.2], y \u2208 [\u22120.65, 0.65] and the goal position x \u2208 [\u22121.2,\u22120.2], y \u2208 [\u22120.6, 0.6]. The task is considered successful if the returned ball lands on the opponent\u2019s side of the table and within \u2264 0.2m to the goal location. We refer to the original paper for the observation and action spaces definition and the reward function."
        },
        {
            "heading": "D ADDITIONAL EVALUATION AND ABLATION STUDY",
            "text": ""
        },
        {
            "heading": "D.1 TRAJECTORY SMOOTHNESS METRIC",
            "text": "We compared the trajectory smoothness of all methods in Table 1. To ensure a fair comparison, all methods were trained using the fixed start box pushing dense reward setting as originally reported in Otto et al. (2022), where each method achieved a minimum 50% success rate. Trajectories for evaluation were generated using the mean prediction of the policy. The smoothness was assessed using three metrics: maximum jerk, mean squared jerk (Wininger et al., 2009), and dimensionless jerk (Hogan & Sternad, 2009). The first two metrics are standard in robot trajectory generation (Berscheid & Kro\u0308ger, 2021; Lange & Suppa, 2015), while the last is proposed as a more equitable measure of smoothness, eliminating the effects of motion magnitude and time scaling. TCE and BBRL Cov outperformed all other methods in smoothness, followed by the original BBRL. This performance disparity likely stems from the original BBRL\u2019s inability to model inter-DoF movement correlations. In contrast, all step-based methods exhibited lower smoothness, attributable to their inherent per-step action selection approach."
        },
        {
            "heading": "D.2 ACTION CORRELATIONS PREDICTED BY TRAINED POLICIES",
            "text": "We plot the action correlation coupling DoF and time steps in Fig. 16. All policies were trained under the box-pushing task with a dense reward setting. The action outputs for TCE, BBRL, and BBRL Cov are the positions of the robot joints, while step-based methods, such as PPO, predict actions in the torque space. TCE and BBRL Cov demonstrate the ability to predict actions correlated both temporally and across DoF, as indicated by the non-zero off-diagonal elements in their correlation matrices. In contrast, the original BBRL translates a factorized weight distribution into a block-diagonal action correlation matrix, capturing variance within individual DoF but not between them. Similarly, PINK is constrained to modeling intra-DoF correlations, which depend solely on the time difference. This limitation arises from the wide-sense stationarity of the noise, resulting in a constant value along each diagonal. gSDE, however, models temporal correlation but only over a few consecutive time steps, observable along the diagonal elements. Actions predicted by PPO, TRPL, and SAC lack both temporal and DoF correlation, resulting in correlation matrices resembling identity matrices. Interestingly, for methods that only capture intra-DoF correlations, these correlations are uniformly positive. This trend may relate to the control cost in the reward function, promoting consistent movement within each DoF over time. On the other hand, TCE and BBRL Cov are unique in their ability to capture negative correlations, both between and within DoF, enhancing their flexibility in trajectory sampling."
        },
        {
            "heading": "D.3 ABLATION: SAC + MOTION PRIMITIVES-BASED METHOD",
            "text": "Training movement primitive-based methods using standard RL techniques, such as PPO and SAC, generally poses challenges due to the complex, higher-dimensional trajectory parameter space. In the study by Otto et al. (2022), an ablation study employing a PPO-style trust region (likelihood clipping) for training BBRL demonstrated inferior performance compared to the use of a differentiable trust region projection layer.\nIn Figure 17, we present an additional ablation study where SAC is used to learn the trajectory parameters of movement primitives. This study compares the performance of SAC with that of the original BBRL and BBRL Cov, leading to relatively poorer performance. The SAC model selected for reporting was the best performer among 40 different combinations of hyperparameters. The hyperparameters adjusted include the output action scaling factor (necessary because the SAC action space is bounded by [\u22121, 1]), policy/critic learning rate, batch size, and the size of the policy/critic network. The relatively shorter training curve of SAC can be attributed to its higher computational cost in policy update (Haarnoja et al., 2018b)."
        },
        {
            "heading": "D.4 ABLATION: USING PPO STYLE TRUST REGIONS FOR TCE METHOD",
            "text": "We developed an ablated version of our method, incorporating the PPO-style trust region via likelihood clipping. We tuned the clipping ratio \u03f5 between 0.05 and 0.2. As illustrated in Figure 18, this version\u2019s performance falls between the original TCE and the standard PPO. The movement primitives\u2019 high-dimensional parameter space limits the effectiveness of likelihood clipping in precisely maintaining the trust region during policy updates. This limitation likely accounts for the performance gap between TCE and its PPO variant. Nonetheless, the ablated model still demonstrates a notable advantage over standard PPO, further substantiating our model\u2019s effectiveness in temporally correlated trajectory prediction."
        },
        {
            "heading": "D.5 ABLATION: SELECTION OF THE AMOUNT OF SEGMENTS K",
            "text": "We conducted an ablation study to evaluate the effect of varying the number of segments (k) on model performance. The number of segments tested ranged from 2 to 100. Our experiments involved training in both dense and sparse box-pushing environments. The results revealed a greater sensitivity to the number of segments in the sparse reward environment compared to the dense environment. We attribute this to the challenges associated with the value function approximation under sparse reward settings. However, within an optimal range, such as 10-25 segments, this parameter is not overly sensitive compared to other hyper-parameters. Consequently, we have adopted k=25 for all experiments in this paper."
        },
        {
            "heading": "D.6 TCE COV VS. STD",
            "text": "We conducted an ablation study to assess the impact of employing a full covariance policy in the TCE framework. This involved comparing the standard TCE with its variant, TCE Std, which utilizes a factorized Gaussian policy N (w|\u00b5w,\u03c32w). The comparison was conducted in scenarios involving both dense and sparse reward settings in box pushing tasks. The findings revealed that the ablated version, TCE Std, consistently underperformed compared to the full covariance version. This underperformance is attributed to the limited correlation capacity of the factorized Gaussian policy.\nFurthermore, it is important to note that while the factorized Gaussian distribution results in a relatively lower computational load in the parameter space, it does not offer a marked advantage when translated into trajectory space. As illustrated in Fig. 16(c) of Section D.2, a factorized parameter distribution ultimately converts into a blocked diagonal trajectory distribution. Although this format is visually simpler compared to a full trajectory covariance matrix, both share same time complexity in terms of likelihood computation. This computational process is significantly more resource-intensive than that for a purely diagonal matrix. Therefore, we utilize the techniques in Li et al. (2023) to apply a likelihood estimation and reduce the computational cost."
        },
        {
            "heading": "E HYPER PARAMETERS",
            "text": "We executed a large-scale grid search to fine-tune key hyperparameters for each baseline method. For other hyperparameters, we relied on the values specified in their respective original papers. Below is a list summarizing the parameters we swept through during this process.\nBBRL: Policy net size, critic net size, policy learning rate, critic learning rate, samples per iteration, trust region dissimilarity bounds, number of parameters per movement DoF.\nTCE: Same types of hyper-parameters listed in BBRL, plus the number of segments per trajectory. A learning rate decaying scheduler is applied to stabilize the training in the end.\nPPO: Policy network size, critic network size, policy learning rate, critic learning rate, batch size, samples per iteration.\nTRPL: Policy network size, critic network size, policy learning rate, critic learning rate, batch size, samples per iteration, trust region dissimilarity bounds.\ngSDE: Same types of hyper-parameters listed in PPO, together with the state dependent exploration sampling frequency (Raffin et al., 2022).\nSAC: Policy network size, critic network size, policy learning rate, critic learning rate, alpha learning rate, batch size, Update-To-Data (UTD) ratio.\nPINK: Same types of hyper-parameters listed in SAC.\nThe detailed hyper parameters used are listed in the following tables. Unless stated otherwise, the notation lin x refers to a linear schedule. It interpolates linearly from x to 0 during training. The ERL methods (TCE, BBRL) take an entire trajectory as a sample where the SRL methods take one time step as a sample. In this way, one sample in ERL is equivlent to T sample of SRL, where T is the length of one task episode."
        }
    ],
    "year": 2024
}