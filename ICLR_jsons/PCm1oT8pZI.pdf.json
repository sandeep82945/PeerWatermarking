{
    "abstractText": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only timeand sample-efficient without training data, but also robust against the watermark removal attacks above. Codes are available: https://github.com/illidanlab/Single_oodwatermark.",
    "authors": [
        {
            "affiliations": [],
            "name": "OOD IMAGE"
        },
        {
            "affiliations": [],
            "name": "Shuyang Yu"
        },
        {
            "affiliations": [],
            "name": "Junyuan Hong"
        },
        {
            "affiliations": [],
            "name": "Haobo Zhang"
        },
        {
            "affiliations": [],
            "name": "Haotao Wang"
        },
        {
            "affiliations": [],
            "name": "Zhangyang Wang"
        },
        {
            "affiliations": [],
            "name": "Jiayu Zhou"
        }
    ],
    "id": "SP:f83e7a9239a6f4628bde8219fffbe7f33535d468",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Carsten Baum",
                "Moustapha Cisse",
                "Benny Pinkas",
                "Joseph Keshet"
            ],
            "title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
            "venue": "In 27th {USENIX} Security Symposium ({USENIX} Security",
            "year": 2018
        },
        {
            "authors": [
                "Yuki M. Asano",
                "Aaqib Saeed"
            ],
            "title": "Extrapolating from a single image to a thousand classes using distillation",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Yuki M Asano",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "A critical analysis of self-supervision, or what we can learn from a single image",
            "year": 1904
        },
        {
            "authors": [
                "Jialuo Chen",
                "Jingyi Wang",
                "Tinglan Peng",
                "Youcheng Sun",
                "Peng Cheng",
                "Shouling Ji",
                "Xingjun Ma",
                "Bo Li",
                "Dawn Song"
            ],
            "title": "Copy, right? a testing framework for copyright protection of deep learning models",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2022
        },
        {
            "authors": [
                "Xuxi Chen",
                "Tianlong Chen",
                "Zhenyu Zhang",
                "Zhangyang Wang"
            ],
            "title": "You are caught stealing my winning lottery ticket! making a lottery ticket claim its ownership",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Patryk Chrabaszcz",
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "A downsampled variant of imagenet as an alternative to the cifar datasets",
            "venue": "arXiv preprint arXiv:1707.08819,",
            "year": 2017
        },
        {
            "authors": [
                "Bita Darvish Rouhani",
                "Huili Chen",
                "Farinaz Koushanfar"
            ],
            "title": "Deepsigns: An end-to-end watermarking framework for ownership protection of deep neural networks",
            "venue": "In Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Gongfan Fang",
                "Jie Song",
                "Chengchao Shen",
                "Xinchao Wang",
                "Da Chen",
                "Mingli Song"
            ],
            "title": "Data-free adversarial distillation",
            "venue": "arXiv preprint arXiv:1912.11006,",
            "year": 2019
        },
        {
            "authors": [
                "Luciano Floridi",
                "Massimo"
            ],
            "title": "Chiriatti. Gpt-3: Its nature, scope, limits, and consequences",
            "venue": "Minds and Machines,",
            "year": 2020
        },
        {
            "authors": [
                "Siddhant Garg",
                "Adarsh Kumar",
                "Vibhor Goel",
                "Yingyu Liang"
            ],
            "title": "Can adversarial weight perturbations inject neural backdoors",
            "venue": "In Proceedings of the 29th ACM International Conference on Information & Knowledge Management,",
            "year": 2020
        },
        {
            "authors": [
                "Micah Goldblum",
                "Dimitris Tsipras",
                "Chulin Xie",
                "Xinyun Chen",
                "Avi Schwarzschild",
                "Dawn Song",
                "Aleksander M\u0105dry",
                "Bo Li",
                "Tom Goldstein"
            ],
            "title": "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gu",
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Evaluating backdooring attacks on deep neural networks",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learningfor image recognition",
            "venue": "CoRR, abs/1512,",
            "year": 2015
        },
        {
            "authors": [
                "Pengfei He",
                "Han Xu",
                "Jie Ren",
                "Yingqian Cui",
                "Hui Liu",
                "Charu C Aggarwal",
                "Jiliang Tang"
            ],
            "title": "Sharpnessaware data poisoning",
            "venue": "attack. arXiv preprint arXiv:2305.14851,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Hern"
            ],
            "title": "Techscape: Will meta\u2019s massive leak democratise ai \u2013 and at what cost? The Guardian, 2023",
            "venue": "URL https://www.theguardian.com/technology/2023/mar/ 07/techscape-meta-leak-llama-chatgpt-ai-crossroads",
            "year": 2023
        },
        {
            "authors": [
                "Junyuan Hong",
                "Yi Zeng",
                "Shuyang Yu",
                "Lingjuan Lyu",
                "Ruoxi Jia",
                "Jiayu Zhou"
            ],
            "title": "Revisiting data-free knowledge distillation with poisoned teachers",
            "venue": "The Fortieth International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Hengrui Jia",
                "Christopher A Choquette-Choo",
                "Varun Chandrasekaran",
                "Nicolas Papernot"
            ],
            "title": "Entangled watermarks as a defense against model extraction",
            "venue": "In Proceedings of the 30th USENIX Security Symposium,",
            "year": 2021
        },
        {
            "authors": [
                "Mika Juuti",
                "Sebastian Szyller",
                "Samuel Marchal",
                "N Asokan"
            ],
            "title": "Prada: protecting against dnn model stealing attacks",
            "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),",
            "year": 2019
        },
        {
            "authors": [
                "Byungjoo Kim",
                "Suyoung Lee",
                "Seanie Lee",
                "Sooel Son",
                "Sung Ju Hwang"
            ],
            "title": "Margin-based neural network watermarking",
            "year": 2023
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Felix X Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Minoru Kuribayashi",
                "Takuro Tanaka",
                "Shunta Suzuki",
                "Tatsuya Yasui",
                "Nobuo Funabiki"
            ],
            "title": "White-box watermarking scheme for fully-connected layers in fine-tuning model",
            "venue": "In Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security,",
            "year": 2021
        },
        {
            "authors": [
                "Erwan Le Merrer",
                "Patrick Perez",
                "Gilles Tr\u00e9dan"
            ],
            "title": "Adversarial frontier stitching for remote neural network watermarking",
            "venue": "Neural Computing and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Fangqi Li",
                "Shilin Wang"
            ],
            "title": "Knowledge-free black-box watermark and ownership proof for image classification neural networks",
            "venue": "arXiv preprint arXiv:2204.04522,",
            "year": 2022
        },
        {
            "authors": [
                "Fangqi Li",
                "Lei Yang",
                "Shilin Wang",
                "Alan Wee-Chung Liew"
            ],
            "title": "Leveraging multi-task learning for umambiguous and flexible deep neural network watermarking",
            "venue": "In SafeAI@ AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Shaofeng Li",
                "Minhui Xue",
                "Benjamin Zhao",
                "Haojin Zhu",
                "Xinpeng Zhang"
            ],
            "title": "Invisible backdoor attacks on deep neural networks via steganography and regularization",
            "venue": "IEEE TDSC,",
            "year": 2020
        },
        {
            "authors": [
                "Yue Li",
                "Hongxia Wang",
                "Mauro Barni"
            ],
            "title": "A survey of deep neural network watermarking",
            "venue": "techniques. Neurocomputing,",
            "year": 2021
        },
        {
            "authors": [
                "Weitang Liu",
                "Xiaoyun Wang",
                "John Owens",
                "Yixuan Li"
            ],
            "title": "Energy-based out-of-distribution detection",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhuang Liu",
                "Mingjie Sun",
                "Tinghui Zhou",
                "Gao Huang",
                "Trevor Darrell"
            ],
            "title": "Rethinking the value of network pruning",
            "venue": "arXiv preprint arXiv:1810.05270,",
            "year": 2018
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Dhwani Mehta",
                "Nurun Mondol",
                "Farimah Farahmandi",
                "Mark Tehranipoor"
            ],
            "title": "Aime: watermarking ai models by leveraging errors",
            "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE),",
            "year": 2022
        },
        {
            "authors": [
                "Tribhuvanesh Orekondy",
                "Bernt Schiele",
                "Mario Fritz"
            ],
            "title": "Knockoff nets: Stealing functionality of black-box models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ian Goodfellow",
                "Somesh Jha",
                "Z Berkay Celik",
                "Ananthram Swami"
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "In Proceedings of the 2017 ACM on Asia conference on computer and communications security,",
            "year": 2017
        },
        {
            "authors": [
                "Adnan Siraj Rakin",
                "Zhezhi He",
                "Deliang Fan"
            ],
            "title": "Tbt: Targeted neural network attack with bit trojan",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Renda",
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "Comparing rewinding and fine-tuning in neural network pruning",
            "venue": "arXiv preprint arXiv:2003.02389,",
            "year": 2020
        },
        {
            "authors": [
                "Johannes Stallkamp",
                "Marc Schlipsing",
                "Jan Salmen",
                "Christian Igel"
            ],
            "title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition",
            "venue": "Neural networks,",
            "year": 2012
        },
        {
            "authors": [
                "Florian Tram\u00e8r",
                "Fan Zhang",
                "Ari Juels",
                "Michael K Reiter",
                "Thomas Ristenpart"
            ],
            "title": "Stealing machine learning models via prediction apis",
            "venue": "In USENIX security symposium,",
            "year": 2016
        },
        {
            "authors": [
                "Yusuke Uchida",
                "Yuki Nagai",
                "Shigeyuki Sakazawa",
                "Shin\u2019ichi Satoh"
            ],
            "title": "Embedding watermarks into deep neural networks",
            "venue": "In Proceedings of the 2017 ACM on international conference on multimedia retrieval,",
            "year": 2017
        },
        {
            "authors": [
                "Haotao Wang",
                "Junyuan Hong",
                "Aston Zhang",
                "Jiayu Zhou",
                "Zhangyang Wang"
            ],
            "title": "Trap and replace: Defending backdoor attacks by trapping them into an easy-to-replace subnetwork",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Run Wang",
                "Jixing Ren",
                "Boheng Li",
                "Tianyi She",
                "Chehao Lin",
                "Liming Fang",
                "Jing Chen",
                "Chao Shen",
                "Lina Wang"
            ],
            "title": "Free fine-tuning: A plug-and-play watermarking scheme for deep neural networks",
            "venue": "arXiv preprint arXiv:2210.07809,",
            "year": 2022
        },
        {
            "authors": [
                "Dongxian Wu",
                "Shu-Tao Xia",
                "Yisen Wang"
            ],
            "title": "Adversarial weight perturbation helps robust generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoyong Yuan",
                "Leah Ding",
                "Lan Zhang",
                "Xiaolin Li",
                "Dapeng Oliver Wu"
            ],
            "title": "Es attack: Model stealing against deep neural networks without data hurdles",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "Proceedings of the British Machine Vision Conference (BMVC),",
            "year": 2016
        },
        {
            "authors": [
                "Yi Zeng",
                "Won Park",
                "Z Morley Mao",
                "Ruoxi Jia"
            ],
            "title": "Rethinking the backdoor attacks\u2019 triggers: A frequency perspective",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Jialong Zhang",
                "Zhongshu Gu",
                "Jiyong Jang",
                "Hui Wu",
                "Marc Ph Stoecklin",
                "Heqing Huang",
                "Ian Molloy"
            ],
            "title": "Protecting intellectual property of deep neural networks with watermarking",
            "venue": "In Proceedings of the 2018 on Asia Conference on Computer and Communications Security,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In the era of deep learning, training a high-performance large model requires curating a massive amount of training data from different sources, powerful computational resources, and often great efforts from human experts. For example, large language models such as GPT-3 are large models trained on private datasets, incurring a significant training cost (Floridi & Chiriatti, 2020). The risk of illegal reproduction or duplication of such high-value DNN models is a growing concern. The recent Facebook leaked LLAMA model provides a notable example of this risk (Hern, 2023). Therefore, it is essential to protect the intellectual property of the model and the rights of the model owners. Recently, watermarking (Adi et al., 2018; Darvish Rouhani et al., 2019; Uchida et al., 2017; Zhang et al., 2018; Chen et al., 2021; Li et al., 2021) has been introduced to protect the copyright of the DNNs. Most existing watermarking methods can be categorized into two mainstreams, including parameter-embedding (Kuribayashi et al., 2021; Uchida et al., 2017; Mehta et al., 2022) and backdoor-based (Goldblum et al., 2022; Li et al., 2022) techniques. Parameter-embedding techniques require white-box access to the suspicious model, which is often unrealistic in practical detection scenarios. This paper places emphasis on backdoor-based approaches, which taint the training dataset by incorporating trigger patches into a set of images referred to as verification samples (trigger set), and modifying the labels to a designated class, forcing the model to memorize the trigger pattern during fine-tuning. Then the owner of the model can perform an intellectual property (IP) inspection by assessing the correspondence between the model\u2019s outputs on the verification samples with the trigger and the intended target labels.\nExisting backdoor-based watermarking methods suffer from major challenges in safety, efficiency, and robustness. Typically injection of backdoors requires full or partial access to the original training data. When protecting models, such access can be prohibitive, mostly due to data safety and confidentiality. For example, someone trying to protect a model fine-tuned upon a foundation model and a model publisher vending models uploaded by their users. Another example is an independent IP protection\ndepartment or a third party that is in charge of model protection for redistribution. Yet another scenario is federated learning (Konec\u030cny\u0300 et al., 2016), where the server does not have access to any in-distribution (ID) data, but is motivated to inject a watermark to protect the ownership of the global model. Despite the high practical demands, watermark injection without training data is barely explored. Although some existing methods tried to export or synthesize out-of-distribution (OoD) samples as triggers to insert watermark (Wang et al., 2022b; Zhang et al., 2018), the original training data is still essential to maintain the utility of the model, i.e., prediction performance on clean samples. Li & Wang (2022) proposed a strategy that adopts a Data-Free Distillation (DFD) process to train a generator and uses it to produce surrogate training samples. However, training the generator is time-consuming and may take hundreds of epochs (Fang et al., 2019). Another critical issue with backdoor-based watermarks is their known vulnerability against minor model changes, such as fine-tuning (Adi et al., 2018; Uchida et al., 2017; Garg et al., 2020), and this vulnerability greatly limited the practical applications of backdoor-based watermarks.\nTo address these challenges, in this work, we propose a practical watermark strategy that is based on efficient fine-tuning, using safe public and out-of-distribution (OoD) data rather than the original training data, and is robust against watermark removal attacks. Our approach is inspired by the recent discovery of the expressiveness of a powerful single image (Asano & Saeed, 2023; Asano et al., 2019). Specifically, we propose to derive patches from a single image, which are OoD samples with respect to the original training data, for watermarking. To watermark a model, the model owner or IP protection unit secretly selects a few of these patches, implants backdoor triggers on them, and uses fine-tuning to efficiently inject the backdoor into the model to be protected. The IP verification process follows the same as other backdoor-based watermark approaches. To increase the robustness of watermarks against agnostic removal attacks, we design a parameter perturbation procedure during the fine-tuning process. Our contributions are summarized as follows.\n\u2022 We propose a novel watermark method based on OoD data, which fills in the gap of backdoorbased IP protection of deep models without training data. The removal of access to the training data enables the proposed approach possible for many real-world scenarios.\n\u2022 The proposed watermark method is both sample efficient (one OoD image) and time efficient (a few epochs) without sacrificing the model utility.\n\u2022 We propose to adopt a weight perturbation strategy to improve the robustness of the watermarks against common removal attacks, such as fine-tuning, pruning, and model extraction. We show the robustness of watermarks through extensive empirical results, and they persist even in an unfair scenario where the removal attack uses a part of in-distribution data."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 DNN WATERMARKING",
            "text": "Existing watermark methods can be categorized into two groups, parameter-embedding and backdoorbased techniques, differing in the information required for verification.\nParameter-embedding techniques embed the watermark into the parameter space of the target model (Darvish Rouhani et al., 2019; Uchida et al., 2017; Kuribayashi et al., 2021; Mehta et al., 2022). Then the owner can verify the model identity by comparing the parameter-oriented watermark extracted from the suspect model versus that of the owner model. For instance, Kuribayashi et al. (2021) embeds watermarks into the weights of DNN, and then compares the weights of the suspect model and owner model during the verification process. However, these kinds of techniques require a white-box setting: the model parameters should be available during verification, which is not a practical assumption facing real-world attacks. For instance, an IP infringer may only expose an API of the stolen model for queries to circumvent the white-box verification.\nBackdoor-based techniques are widely adopted in a black-box verification, which implant a backdoor trigger into the model by fine-tuning the pre-trained model with a set of poison samples (also denoted as the trigger set) assigned to one or multiple secret target class (Zhang et al., 2018; Le Merrer et al., 2020; Goldblum et al., 2022; Li et al., 2022). Suppose Dc is the clean dataset and we craft Dp by poisoning another set of clean samples. The backdoor-based techniques can be unified as minimizing the following objective: min\u2713 P (x,y)2Dc `(f\u2713(x), y) + P (x0,y0)2Dp `(f\u2713( (x 0)), t),\nwhere (x) adds a trigger pattern to a normal sample, t is the pre-assigned target label, f\u2713 is a classifier parameterized by \u2713, and ` is the cross-entropy loss. The key intuition of backdoor training is to make models memorize the shortcut patterns while ignoring other semantic features. A watermarked model should satisfy the following desired properties: 1) Persistent utility. Injecting backdoor-based watermarks into a model should retain its performance on original tasks. 2) Removal resilience. Watermarks should be stealthy and robust against agnostic watermark removal attacks (Orekondy et al., 2019; Chen et al., 2022; Hong et al., 2023).\nUpon verification, the ownership can be verified according to the consistency between the target label t and the output of the model in the presence of the triggers. However, conventional backdoor-based watermarking is limited to scenarios where clean and poisoned dataset follows the same distribution as the training data of the pre-trained model. For example, in Federated Learning (McMahan et al., 2017), the IP protector on the server does not have access to the client\u2019s data. Meanwhile, in-training backdoor injection could be voided by backdoor-resilient training (Wang et al., 2022a). We reveal that neither the training data (or equivalent i.i.d. data) nor the in-training strategy is necessary for injecting watermarks into a well-trained model, and merely using clean and poisoned OoD data can also insert watermarks after training.\nBackdoor-based watermarking without i.i.d. data. Among backdoor-based techniques, one kind of technique also tried to export or synthesize OoD samples as the trigger set to insert a watermark. For instance, Zhang et al. (2018) exported OoD images from other classes that are irrelevant to the original tasks as the watermarks. Wang et al. (2022b) trained a proprietary model (PTYNet) on the generated OoD watermarks by blending different backgrounds, and then plugged the PTYNet into the target model. However, for these kinds of techniques, i.i.d. samples are still essential to maintain the main-task performance. On the other hand, data-free watermark injection is an alternative to OoD-based methods. Close to our work, Li & Wang (2022) proposed a data-free method that first adopts a Data-Free Distillation method to train a generator, and then uses the generator to produce surrogate training samples to inject watermarks. However, according to Fang et al. (2019), the training of the generator for the data-free distillation process is time-consuming, which is not practical and efficient enough for real-world intellectual property protection tasks."
        },
        {
            "heading": "2.2 WATERMARK REMOVAL ATTACK",
            "text": "In contrast to protecting the IP, a series of works have revealed the risk of watermark removal to steal the IP. Here we summarize three mainstream types of watermark removal techniques: fine-tuning, pruning, and model extraction. We refer to the original watermarked model as the victim model and the stolen copy as the suspect model under removal attacks. Fine-tuning assumes that the adversary has a small set of i.i.d. samples and has access to the victim model architectures and parameters (Adi et al., 2018; Uchida et al., 2017). The adversary attempts to fine-tune the victim model using the i.i.d. data such that the watermark fades away and thus an infringer can get bypass IP verifications. Pruning has the same assumptions as fine-tuning. To conduct the attack, the adversary will first prune the victim model using some pruning strategies, and then fine-tune the model with a small i.i.d. dataset (Liu et al., 2018b; Renda et al., 2020). Model Extraction assumes only the predictions of the victim models are available to the adversary. To steal the model through the API, given a set of auxiliary samples, the adversary first queries the victim model for auxiliary samples to obtain the annotated dataset, and then a copy of the victim model is trained based on this annotated dataset (Juuti et al., 2019; Tram\u00e8r et al., 2016; Papernot et al., 2017; Orekondy et al., 2019; Yuan et al., 2022)."
        },
        {
            "heading": "3 METHOD",
            "text": "Problem Setup. Within the scope of the paper, we assume that training data or equivalent i.i.d. data are not available for watermarking due to data privacy concerns. This assumption casts a substantial challenge on maintaining standard accuracy on i.i.d. samples while injecting backdoors.\nOur main intuition is that a learned decision boundary can be manipulated by not only i.i.d. samples but also OoD samples. Moreover, recent studies (Asano & Saeed, 2023; Asano et al., 2019) showed a surprising result that one single OoD image is enough for learning low-level visual representations provided with strong data augmentations. Thus, we conjecture that it is plausible to inject backdoorbased watermarks efficiently to different parts of the pre-trained representation space by exploiting the\ndiverse knowledge from one single OoD image. Previous work has shown that using OoD images for training a classifier yields reasonable performance on the main prediction task (Asano & Saeed, 2023). Moreover, it is essential to robustify the watermark against potential removal attacks. Therefore, our injection process comprises two steps: Constructing surrogate data to be poisoned and robust watermark injection. The framework of the proposed strategy is illustrated in Fig. 1."
        },
        {
            "heading": "3.1 CONSTRUCTING SAFE SURROGATE DATASET",
            "text": "We first augment one OoD source image multiple times to generate an unlabeled surrogate dataset D\u0303 of a desired size according to Asano & Saeed (2023); Asano et al. (2019). For safety considerations, the OoD image is only known to the model owner. The source OoD images are publicly available and properly licensed for personal use. To \u201cpatchify\u201d a large single image, the augmentation composes multiple augmentation methods in sequence: cropping, rotation and shearing, and color jittering using the hyperparameters from Asano et al. (2019). During training, we further randomly augment pre-fetched samples by cropping and flipping, and we use the predictions from the pre-trained model \u27130 as supervision. Suppose \u2713 is initialized as \u27130 of the pre-trained model. To inject watermarks, we split the unlabeled surrogate dataset D\u0303 = D\u0303c [ D\u0303p where D\u0303c is the clean dataset, and D\u0303p is the poisoned dataset. For the poisoned dataset D\u0303p, by inserting a trigger pattern (\u00b7) into the original sample in D\u0303p, the sample should be misclassified to one pre-assigned target label t. Our goal is to solve the following optimization problem:\nmin \u2713\nLinj(\u2713) := X\nx2D\u0303c `(f\u2713(x), f\u27130(x)) + X x02D\u0303p `(f\u2713( (x 0)), t).\nThe first term is used to ensure the high performance of the original task (Asano & Saeed, 2023), and the second term is for watermark injection. The major difference between our method and Asano & Saeed (2023) is that we use the generated data for fine-tuning the same model instead of distilling a new model. We repurpose the benign generated dataset for injecting watermarks.\nConsidering a black-box setting, to verify whether the suspect model Ms is a copy of our protected model M, we can use the generated surrogate OoD dataset as safe verification samples. As the generation is secreted, no one other than the owner can complete the verification. Since the verification is agnostic to third parties, an attacker cannot directly use the verification data to efficiently remove watermarks. Thus, we can guarantee the safety of the verification. Formally, we check the probability of watermarked verification samples that can successfully mislead the model Ms to predict the pre-defined target label t, denoted as watermark success rate (WSR). Since the ownership of stolen models can be claimed by the model owner if the suspect model\u2019s behavior differs significantly from any non-watermarked models (Jia et al., 2021), if the WSR is larger than a random guess, and also far exceeds the probability of a non-watermarked model classifying the verification samples as t, then Ms will be considered as a copy of M with high probability. A T-test between the output logits of the suspect model Ms and a non-watermarked model on the verification dataset is also used as a metric to evaluate whether Ms is a stolen copy. Compared with traditional watermark injection techniques, i.i.d. data is also unnecessary in the verification process."
        },
        {
            "heading": "3.2 ROBUST WATERMARK INJECTION",
            "text": "According to Adi et al. (2018); Uchida et al. (2017), the watermark may be removed by fine-tuning when adversaries have access to the i.i.d. data. Watermark removal attacks such as fine-tuning and pruning will shift the model parameters on a small scale to maintain standard accuracy and remove watermarks. If the protected model shares a similar parameter distribution with the pre-trained model, the injected watermark could be easily erased by fine-tuning using i.i.d. data or adding random noise to parameters (Garg et al., 2020). To defend against removal attacks, we intuitively aim to make our watermark robust and persistent within a small scale of parameter perturbations.\nBackdoor training with weight perturbation. To this end, we introduce adversarial weight perturbation (WP) into backdoor fine-tuning. First, we simulate the watermark removal attack that maximizes the loss to escape from the watermarked local minima. We let \u2713 = (w, b) denote the model parameter, where \u2713 is composed of weight w and bias b. The weight perturbation is defined as v. Then, we adversarially minimize the loss after the simulated removal attack. The adversarial minimization strategy echoes some previous sharpness-aware optimization principles for robust model poisoning (He et al., 2023). Thus, the adversarial training objective is formulated as: minw,b maxv2V Lper(w + v, b), where\nLper(w + v, b) := Linj(w + v, b) + X\nx2D\u0303c,x02D\u0303p\nKL(f(w+v,b)(x), f(w+v,b)( (x0)). (1)\nIn Eq. (1), we constrain the weight perturbation v within a set V , KL(\u00b7, \u00b7) is the Kullback\u2013Leibler divergence, and is a positive trade-off parameter. The first term is identical to standard watermark injection. Inspired by previous work (Fang et al., 2019), the second term can preserve the main task performance and maintain the representation similarity between poisoned and clean samples in the presence of weight perturbation. Eq. (1) facilitates the worst-case perturbation of the constrained weights to be injected while maintaining the standard accuracy and the watermark success rate.\nIn the above adversarial optimization, the scale of perturbation v is critical. If the perturbation is too large, the anomalies of the parameter distribution could be easily detected by an IP infringer (Rakin et al., 2020). Since the weight distributions differ by layer of the network, the magnitude of the perturbation should vary accordingly from layer to layer. Following (Wu et al., 2020), we adaptively restrict the weight perturbation vl for the l-th layer weight wl as\nkvlk  kwlk, (2)\nwhere 2 (0, 1). The set V in Eq. (1) will be decomposed into balls with radius kwlk per layer. Optimization. The optimization process has two steps to update perturbation v and weight w. (1) v-step: To consider the constraint in (2), we need to use a projection. Note that v is layer-wisely updated, we need a projection function \u21e7(\u00b7) that projects all perturbations vl that violate constraint (Eq. (2)) back to the surface of the perturbation ball with radius kwlk. To achieve this goal, we define \u21e7 in Eq. (3) (Wu et al., 2020):\n\u21e7 (vl) =\n8 <\n: kwlk kvlk vl if kvlk > kwlk\nvl otherwise (3)\nWith the projection, the computation of the perturbation v in Eq. (1) is given by v \u21e7 \u21e3 v + \u23181\nrvLper(w+v,b) krvLper(w+v,b)kkwk\n\u2318 , where \u23181 is the learning rate.\n(2) w-step: With the updated perturbation v, the weight of the perturbed model \u2713 can be updated using w w \u23182rw+vLper(w + v, b), where \u23182 is the learning rate."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we conduct comprehensive experiments to evaluate the effectiveness of the proposed watermark injection method. Datasets. We use CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009) and GTSRB (Stallkamp et al.,\n2012) for model utility evaluation. Both CIFAR-10 and CIFAR-100 contain 32\u21e5 32 with 10 and 100 classes, respectively. The GTSRB consists of sign images in 43 classes. All images in GTSRB are reshaped as 32\u21e5 32. Note that, these datasets are neither used for our watermark injection nor model verification, they are only used to evaluate the standard accuracy of our watermarked model. OoD image. OoD image is used for watermark injection and ownership verification. We use three different OoD images as our candidate source image to inject watermarks, denoted as \u201cCity\u201d1, \u201cAnimals\u201d2, and \u201cBridge\u201d3. We use \u201cCity\u201d by default unless otherwise mentioned. Evaluation metrics. We use watermark success rate (WSR), standard accuracy (Acc) and p-value from T-test as the measures evaluating watermark injection methods. Acc is the classification accuracy measured on a clean i.i.d. test set. IDWSR is the portion of watermarked i.i.d. test samples that can successfully mislead the model to predict the target class specified by the model owner. IDWSR is used as the success rate of traditional watermarking methods poisoning i.i.d. data and used as a reference for our method. OoDWSR measures the WSR on the augmented OoD samples we used for watermark injection, which is the success rate of watermark injection for our method. T-test takes the output logits of the non-watermarked model and suspect model Ms as input, and the null hypothesis is the logits distribution of the suspect model is identical to that of a non-watermarked model. If the p-value of the T-test is smaller than the threshold 0.05, then we can reject the null hypothesis and statistically verify that Ms differs significantly from the non-watermarked model, so the ownership of Ms can be claimed (Jia et al., 2021). Higher OoDWSR with a p-value smaller than the threshold and meanwhile a larger Acc indicate a successful watermark injection. Trigger patterns. To attain the best model with the highest watermark success rate, we use the OoDWAR to choose triggers from 6 different backdoor patterns: BadNets with grid (badnet_grid) (Gu et al., 2019), l0-invisible (l0_inv) (Li et al., 2020), smooth (Zeng et al., 2021), Trojan Square 3\u21e53 (trojan_3\u21e53), Trojan Square 8\u21e58 (trojan_8\u21e58), and Trojan watermark (trojan_wm) (Liu et al., 2018a).\nDataset Class num DNN architecture Acc CIFAR-10 10 WRN-16-2 (Zagoruyko & Komodakis, 2016) 0.9400\nCIFAR-100 100 WRN-16-2 (Zagoruyko & Komodakis, 2016) 0.7234 GTSRB 43 ResNet18 (He et al., 2015) 0.9366\nTable 1: Pre-trained models.\nPre-training models. The detailed information of the pretrained models is shown in Table 1. All the models are pretrained on clean samples until convergence, with a learning rate\nof 0.1, SGD optimizer, and batch size 128. We follow public resources to conduct the training such that the performance is close to state-of-the-art results. Watermark removal attacks. To evaluate the robustness of our proposed method, we consider three kinds of attacks on victim models: 1) FT: Fine-tuning includes three kinds of methods: a) fine-tune all layers (FT-AL), b) fine-tune the last layer and freeze all other layers (FT-LL), c) re-initialize the last layer and then fine-tune all layers (RT-AL). 2) Pruning-r% indicates pruning r% of the model parameters which has the smallest absolute value, and then fine-tuning the model on clean i.i.d. samples to restore accuracy. 3) Model Extraction: We use knockoff (Orekondy et al., 2019) as an example of the model extraction attack, which queries the model to get the predictions of an auxiliary dataset (ImagenetDS (Chrabaszcz et al., 2017) is used in our experiments), and then clones the behavior of a victim model by re-training the model with queried image-prediction pairs. Assume the adversary obtains 10% of the training data of the pre-trained models for fine-tuning and pruning. Fine-tuning and pruning are conducted for 50 epochs. Model extraction is conducted for 100 epochs."
        },
        {
            "heading": "4.1 WATERMARK INJECTION",
            "text": "The poisoning ratio of the generated surrogate dataset is 10%. For CIFAR-10 and GTSRB, we finetune the pre-trained model for 20 epochs (first 5 epochs are with WP). For CIFAR-100, we fine-tune the pre-trained model for 30 epochs (first 15 epochs are with WP). The perturbation constraint in Eq. (2) is fixed at 0.1 for CIFAR-10 and GTSRB, and 0.05 for CIFAR-100. The trade-off parameter in Eq. (1) is fixed at 6 for all the datasets. The watermark injection process of CIFAR-10 is shown in Fig. 2, and watermark injection for the other two datasets can be found in Appendix A.1. We observe that the injection process is efficient, it takes only 10 epochs for CIFAR-10 to achieve stable high standard accuracy and OoDWSR. The highest OoDWSR for CIFAR-10 is 95.66% with standard accuracy degradation of less than 3%. In the following experiments, we choose triggers with top-2 OoDWSR and standard accuracy degradation less than 3% as the recommended watermark patterns.\n1https://pixabay.com/photos/japan-ueno-japanese-street-sign-217883/ 2https://www.teahub.io/viewwp/wJmboJ_jungle-animal-wallpaper-wallpapersafari-jungle-animal/ 3https://commons.wikimedia.org/wiki/File:GG-ftpoint-bridge-2.jpg"
        },
        {
            "heading": "4.2 DEFENDING AGAINST FINE-TUNING & PRUNING",
            "text": "We evaluate the robustness of our proposed method against fine-tuning and pruning in Table 2, where victim models are watermarked models, and suspect models are stolen copies of victim models using watermark removal attacks. OoDWSR of the pre-trained model in Table 1 is the probability that a non-watermarked model classifies the verification samples as the target label. If the OoDWSR of a suspect model far exceeds that of the non-watermarked model, the suspect model can be justified as a copy of the victim model (Jia et al., 2021).\nFT-AL and pruning maintain the performance of the main classification task with an accuracy degradation of less than 6%, but OoDWSR remains high for all the datasets. Compared with FT-AL, FT-LL will significantly bring down the standard accuracy by over 15% for all the datasets. Even with the large sacrifice of standard accuracy, FT-LL still cannot wash out the injected watermark, and the OoDWSR even increases for some of the datasets. RT-AL loses 4.50%, 16.63%, and 5.47% (mean value for two triggers) standard accuracy respectively for three datasets. Yet, OoDWSR in RT-AL is larger than the one of the random guess and non-watermarked models. To statistically verify the ownership, we conduct a T-test between the non-watermarked model and the watermarked model. The p-value is the probability that the two models behave similarly. p-values for all the datasets are close to 0. The low p-values indicate that the suspect models have significantly different behaviors compared with non-watermarked models in probability, at least 95%. Thus, these suspect models cannot get rid of the suspicion of copying our model M with a high chance. IDWSR is also used here as a reference, although we do not use i.i.d. data for verification of the ownership of our model. We observe that even though watermark can be successfully injected into\nboth our generated OoD dataset and i.i.d. samples (refer to IDWSR and OoDWSR for victim model), they differ in their robustness against these two watermark removal attacks. For instance, for smooth of GTSRB, after fine-tuning or pruning, IDWSR drops under 1%, which is below the random guess, however, OoDWSR remains over 67%. This phenomenon is also observed for other triggers and datasets. Watermarks injected in OoD samples are much harder to be washed out compared with watermarks injected into i.i.d. samples. Due to different distributions, fine-tuning or pruning will have a smaller impact on OoD samples compared with i.i.d. samples.\nTo further verify our intuition, we also compare our method (OoD) with traditional backdoor-based methods using i.i.d. data (ID) for data poisoning on CIFAR-10. We use RT-AL which is the strongest attack in Table 2 as an example. The results are shown in Table 3. Note that ID poison and the proposed OoD poison adopt IDWSR and OoDWSR as the success rate for the injection watermark, respectively. Clean refers to the pre-trained model without watermark injection. With only one single OoD image for watermark injection, we can achieve comparable results as ID poisoning which utilizes the entire ID training set. After RT-AL, the watermark success rate drops to 4.13% and 3.42%, respectively for ID poison, while drops to 57.52% and 24.19% for OoD poison, which verifies that our proposed method is also much more robust against watermark removal attacks."
        },
        {
            "heading": "4.3 DEFENDING AGAINST MODEL EXTRACTION",
            "text": "We evaluate the robustness of our proposed method against model extraction in Table 4. By conducting model extraction, the standard accuracy drops 6% on the model pre-trained on CIFAR-10, and drops more than 10% on the other two datasets. Re-training from scratch makes it hard for the suspect model to resume the original model\u2019s utility using an OoD dataset and soft labels querying from the watermarked model. OoDWSR is still over 90% and 76% for CIFAR-10 and GTSRB, respectively. Although OoDWSR is 6.22% for l0_inv, it is still well above 0.02%, which is observed for the non-watermarked model. All the datasets also have a p-value close to 0. All the above observations indicate that the re-training-based extracted model has a high probability of being a copy of our model. One possible reason for these re-training models still extracting the watermark is that during re-training, the backdoor information hidden in the soft label queried by the IP infringers can also embed the watermark in the extracted model. The extracted model will behave more similarly to the victim model as its decision boundary gradually approaches that of the victim model."
        },
        {
            "heading": "4.4 QUALITATIVE STUDIES",
            "text": "Distribution of generated OoD samples and ID samples. We first augment an unlabeled OoD dataset, and then assign predicted labels to them using the model pre-trained on clean CIFAR-10 data. According to the distribution of OoD and ID samples before and after our watermark fine-tuning as shown in Fig. 3, we can observe that the OoD data drawn from one image lies close to ID data with a small gap. After a few epochs of fine-tuning, some of the OoD data is drawn closer to ID,\nbut still maintains no overlap. This can help us successfully implant watermarks to the pre-trained model while maintaining the difference between ID and OoD data. In this way, when our model is fine-tuned with clean ID data by attackers, the WSR on the OoD data will not be easily erased.\nEffects of different OoD images for watermark injection. In Table 5, we use different source images to generate surrogate datasets and inject watermarks into a pre-trained model. The model is pre-trained on CIFAR-10. From these results, we observe that the choice of the OoD image for injection is also important. Dense images such as \u201cCity\" and \u201cAnimals\" can produce higher OoDWSR than the sparse image \u201cBridge\", since more knowledge is included in the visual representations of dense source images. Thus, dense images perform better for backdoor-based watermark injection. This observation is also consistent with some previous arts (Asano & Saeed, 2023; Asano et al., 2019) about single image representations, which found that dense images perform better for model distillation or self-supervised learning.\nEffects of backdoor weight perturbation. We show the results in Fig. 4. The initial model is WideResNet pre-trained on CIFAR-10, and the fine-tuned model is the model fine-tuning using our proposed method. If the OoD data is directly utilized to fine-tune the pre-trained models with only a few epochs, the weight distribution is almost identical for pre-trained and fine-tuned models (left figure). According to Garg et al. (2020), if the parameter perturbations are small, the backdoor-based watermark can be easily removed by fine-tuning or adding random noise to the model\u2019s parameters. Our proposed watermark injection WP (right figure) can shift the fine-tuned model parameters from the pre-trained models in a reasonable scale compared with the left one, while still maintaining high standard accuracy and watermark success rate as shown in Table 6. Besides, the weight distribution of the perturbed model still follows a normal distribution as the unperturbed model, performing statistical analysis over the model parameters distributions will not be able to erase our watermark.\nTo show the effects of WP, we conduct the attack RT-AL on CIFAR-10 as an example. From Table 6, we observe that WP does not affect the model utility, and at the same time, it will become more robust against stealing threats, since OoDWSR increases from 19.94% and 12.81% to 57.52% and 24.19%, respectively, for two triggers. More results for WP can be referred to Appendix A.2."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we proposed a novel and practical watermark injection method that does not require training data and utilizes a single out-of-distribution image in a sample-efficient and time-efficient manner. We designed a robust weight perturbation method to defend against watermark removal attacks. Our extensive experiments on three benchmarks showed that our method efficiently injected watermarks and was robust against three watermark removal threats. Our approach has various real-world applications, such as protecting purchased models by encoding verifiable identity and implanting server-side watermarks in distributed learning when ID data is not available."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This material is based in part upon work supported by the National Science Foundation under Grant IIS-2212174, IIS-1749940, Office of Naval Research N00014-20-1-2382, N00014-24-1-2168, and National Institute on Aging (NIA) RF1AG072449. The work of Z. Wang is in part supported by the National Science Foundation under Grant IIS2212176."
        }
    ],
    "year": 2024
}