{
    "abstractText": "This work studies the problem of panoptic symbol spotting, which is to spot and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from computer-aided design (CAD) drawings. Existing methods typically involve either rasterizing the vector graphics into images and using image-based methods for symbol spotting, or directly building graphs and using graph neural networks for symbol recognition. In this paper, we take a different approach, which treats graphic primitives as a set of 2D points that are locally connected and use point cloud segmentation methods to tackle it. Specifically, we utilize a point transformer to extract the primitive features and append a mask2former-like spotting head to predict the final output. To better use the local connection information of primitives and enhance their discriminability, we further propose the attention with connection module (ACM) and contrastive connection learning scheme (CCL). Finally, we propose a KNN interpolation mechanism for the mask attention module of the spotting head to better handle primitive mask downsampling, which is primitive-level in contrast to pixel-level for the image. Our approach, named SymPoint, is simple yet effective, outperforming recent state-of-the-art method GAT-CADNet by an absolute increase of 9.6% PQ and 10.4% RQ on the FloorPlanCAD dataset. The source code and models will be available at https://github. com/nicehuster/SymPoint.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenlong Liu"
        },
        {
            "affiliations": [],
            "name": "Tianyu Yang"
        },
        {
            "affiliations": [],
            "name": "Yuhan Wang"
        },
        {
            "affiliations": [],
            "name": "Qizhi Yu"
        },
        {
            "affiliations": [],
            "name": "Lei Zhang"
        }
    ],
    "id": "SP:6079c0e8dd4f9a08b453c454457bcd217e6d972f",
    "references": [
        {
            "authors": [
                "Alexey Bochkovskiy",
                "Chien-Yao Wang",
                "Hong-Yuan Mark Liao"
            ],
            "title": "Yolov4: Optimal speed and accuracy of object detection",
            "venue": "arXiv preprint arXiv:2004.10934,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre Carlier",
                "Martin Danelljan",
                "Alexandre Alahi",
                "Radu Timofte"
            ],
            "title": "Deepsvg: A hierarchical generative network for vector graphics animation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Cheng",
                "Alex Schwing",
                "Alexander Kirillov"
            ],
            "title": "Per-pixel classification is not all you need for semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Masked-attention mask transformer for universal image segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yizong Cheng"
            ],
            "title": "Mean shift, mode seeking, and clustering",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 1995
        },
        {
            "authors": [
                "Ruoxi Deng",
                "Chunhua Shen",
                "Shengjun Liu",
                "Huibing Wang",
                "Xinru Liu"
            ],
            "title": "Learning to predict crisp boundaries",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Zhiwen Fan",
                "Lingjie Zhu",
                "Honghua Li",
                "Xiaohao Chen",
                "Siyu Zhu",
                "Ping Tan"
            ],
            "title": "Floorplancad: A large-scale cad drawing dataset for panoptic symbol spotting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiwen Fan",
                "Tianlong Chen",
                "Peihao Wang",
                "Zhangyang Wang"
            ],
            "title": "Cadtransformer: Panoptic symbol spotting transformer for cad drawings",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Frosst",
                "Nicolas Papernot",
                "Geoffrey Hinton"
            ],
            "title": "Analyzing and improving representations with the soft nearest neighbor loss",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Gutmann",
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Hui Zhou",
                "Xinge Zhu",
                "Hongsheng Li",
                "Ziwei Liu"
            ],
            "title": "Lidar-based panoptic segmentation via dynamic shifting network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyang Jiang",
                "Lu Liu",
                "Caihua Shan",
                "Yifei Shen",
                "Xuanyi Dong",
                "Dongsheng Li"
            ],
            "title": "Recognizing vector graphics without rasterization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Kaiming He",
                "Ross Girshick",
                "Carsten Rother",
                "Piotr Doll\u00e1r"
            ],
            "title": "Panoptic segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jinke Li",
                "Xiao He",
                "Yang Wen",
                "Yuan Gao",
                "Xiaoqiang Cheng",
                "Dan Zhang"
            ],
            "title": "Panoptic-phnet: Towards real-time and high-precision lidar panoptic segmentation via clustering pseudo heatmap",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Tuan Duc Ngo",
                "Binh-Son Hua",
                "Khoi Nguyen"
            ],
            "title": "Isbnet: a 3d point cloud instance segmentation network with instance-aware sampling and box-aware dynamic convolution",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Thi-Oanh Nguyen",
                "Salvatore Tabbone",
                "Alain Boucher"
            ],
            "title": "A symbol spotting approach based on the vector model and a visual vocabulary",
            "venue": "In 2009 10th International Conference on Document Analysis and Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Charles R Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pradyumna Reddy",
                "Michael Gharbi",
                "Michal Lukac",
                "Niloy J Mitra"
            ],
            "title": "Im2vec: Synthesizing vector graphics without vector supervision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph Redmon",
                "Ali Farhadi"
            ],
            "title": "Yolov3: An incremental improvement",
            "venue": "arXiv preprint arXiv:1804.02767,",
            "year": 2018
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Alireza Rezvanifar",
                "Melissa Cote",
                "Alexandra Branzan Albu"
            ],
            "title": "Symbol spotting for architectural drawings: state-of-the-art and new industry-driven developments",
            "venue": "IPSJ Transactions on Computer Vision and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Alireza Rezvanifar",
                "Melissa Cote",
                "Alexandra Branzan Albu"
            ],
            "title": "Symbol spotting on digital architectural floor plans using a deep learning-based framework",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Schult",
                "Francis Engelmann",
                "Alexander Hermans",
                "Or Litany",
                "Siyu Tang",
                "Bastian Leibe"
            ],
            "title": "Mask3d: Mask transformer for 3d semantic instance segmentation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Ruoxi Shi",
                "Xinyang Jiang",
                "Caihua Shan",
                "Yansen Wang",
                "Dongsheng Li"
            ],
            "title": "Rendnet: Unified 2d/3d recognizer with latent space rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ke Sun",
                "Bin Xiao",
                "Dong Liu",
                "Jingdong Wang"
            ],
            "title": "Deep high-resolution representation learning for human pose estimation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhi Tian",
                "Chunhua Shen",
                "Hao Chen",
                "Tong He"
            ],
            "title": "Fcos: Fully convolutional one-stage object detection",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Zeqi Xiao",
                "Wenwei Zhang",
                "Tai Wang",
                "Chen Change Loy",
                "Dahua Lin",
                "Jiangmiao Pang"
            ],
            "title": "Position-guided point cloud panoptic segmentation transformer",
            "venue": "arXiv preprint arXiv:2303.13509,",
            "year": 2023
        },
        {
            "authors": [
                "Bingchen Yang",
                "Haiyong Jiang",
                "Hao Pan",
                "Jun Xiao"
            ],
            "title": "Vectorfloorseg: Two-stream graph attention network for vectorized roughcast floorplan segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Hao Zhang",
                "Feng Li",
                "Shilong Liu",
                "Lei Zhang",
                "Hang Su",
                "Jun Zhu",
                "Lionel M Ni",
                "HeungYeung Shum. Dino"
            ],
            "title": "Detr with improved denoising anchor boxes for end-to-end object detection",
            "venue": "arXiv preprint arXiv:2203.03605,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaohua Zheng",
                "Jianfang Li",
                "Lingjie Zhu",
                "Honghua Li",
                "Frank Petzold",
                "Ping Tan"
            ],
            "title": "Gatcadnet: Graph attention network for panoptic symbol spotting in cad drawings",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zixiang Zhou",
                "Yang Zhang",
                "Hassan Foroosh"
            ],
            "title": "Panoptic-polarnet: Proposal-free lidar point cloud panoptic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Shi"
            ],
            "title": "Vector Graphics Recognition Dataset",
            "venue": "Similar to (Jiang et al.,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "This work studies the problem of panoptic symbol spotting, which is to spot and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from computer-aided design (CAD) drawings. Existing methods typically involve either rasterizing the vector graphics into images and using image-based methods for symbol spotting, or directly building graphs and using graph neural networks for symbol recognition. In this paper, we take a different approach, which treats graphic primitives as a set of 2D points that are locally connected and use point cloud segmentation methods to tackle it. Specifically, we utilize a point transformer to extract the primitive features and append a mask2former-like spotting head to predict the final output. To better use the local connection information of primitives and enhance their discriminability, we further propose the attention with connection module (ACM) and contrastive connection learning scheme (CCL). Finally, we propose a KNN interpolation mechanism for the mask attention module of the spotting head to better handle primitive mask downsampling, which is primitive-level in contrast to pixel-level for the image. Our approach, named SymPoint, is simple yet effective, outperforming recent state-of-the-art method GAT-CADNet by an absolute increase of 9.6% PQ and 10.4% RQ on the FloorPlanCAD dataset. The source code and models will be available at https://github. com/nicehuster/SymPoint."
        },
        {
            "heading": "1 Introduction",
            "text": "Vector graphics (VG), renowned for their ability to be scaled arbitrarily without succumbing to issues like blurring or aliasing of details, have become a staple in industrial designs. This includes their prevalent use in graphic designs(Reddy et al., 2021), 2D interfaces(Carlier et al., 2020), and Computer-aided design (CAD)(Fan et al., 2021). Specifically, CAD drawings, consisting of geometric primitives(e.g., arc, circle, polyline, etc.), have established themselves as the preferred data representation method in the realms of interior design, indoor construction, and property development, promoting a higher standard of precision and innovation in these fields. Symbol spotting (Rezvanifar et al., 2019; 2020; Fan et al., 2021; 2022; Zheng et al., 2022) refers to spotting and recognizing symbols from CAD drawings, which serves as a foundational task for reviewing the error of design drawing and 3D building information modeling (BIM). Spotting each symbol, a grouping of graphical primitives, within a CAD drawing poses a significant challenge due to the existence of obstacles such as occlusion, clustering, variations in appearances, and a significant imbalance in the distribution of different categories. Traditional symbol spotting usually deals with instance symbols representing countable things (Rezvanifar et al., 2019), like table, sofa, and bed. Fan et al. (2021) further extend it to panoptic symbol spotting which performs both the spotting of countable instances (e.g., a single door, a window, a table, etc.) and the recognition of uncountable stuff (e.g., wall, railing, etc.). Typical approaches (Fan et al., 2021; 2022) addressing the panoptic symbol spotting task involve first converting CAD drawings to raster graphics(RG) and then processing it with\npowerful image-based detection or segmentation methods (Ren et al., 2015; Sun et al., 2019). Another line of previous works (Jiang et al., 2021; Zheng et al., 2022; Yang et al., 2023) abandons the raster procedure and directly processes vector graphics for recognition with graph convolutions networks. Instead of rastering CAD drawings to images or modeling the graphical primitives with GCN/GAT, which can be computationally expensive, especially for large CAD graphs, we propose a new paradigm that has the potential to shed novel insight rather than merely delivering incremental advancements in performance. Upon analyzing the data characteristics of CAD drawings, we can find that CAD drawing has three main properties: 1). irregularity and disorderliness. Unlike regular pixel arrays in raster graphics/images, CAD drawing consists of geometric primitives(e.g., arc, circle, polyline, etc.) without specific order. 2). local interaction among graphical primitives. Each graphical primitive is not isolated but locally connected with neighboring primitives, forming a symbol. 3). invariance under transformations. Each symbol is invariant to certain transformations. For example, rotating and translating symbols do not modify the symbol\u2019s category. These properties are almost identical to point clouds. Hence, we treat CAD drawing as sets of points (graphical primitives) and utilize methodologies from point cloud analysis (Qi et al., 2017a;b; Zhao et al., 2021) for symbol spotting. In this work, we first consider each graphic primitive as an 8-dimensional data point with the information of position and primitive\u2019s properties (type, length, etc.). We then utilize methodologies from point cloud analysis for graphic primitive representation learning. Different from point clouds, these graphical primitives are locally connected. We therefore propose contrastive connectivity learning mechanism to utilize those local connections. Finally, we borrow the idea of Mask2Former(Cheng et al., 2021; 2022) and construct a maskedattention transformer decoder to perform the panoptic symbol spotting task. Besides, rather than using bilinear interpolation for mask attention downsampling as in (Cheng et al., 2022), which could cause information loss due to the sparsity of graphical primitives, we propose KNN interpolation, which fuses the nearest neighboring primitives, for mask attention downsampling. We conduct extensive experiments on the FloorPlanCAD dataset and our SymPoint achieves 83.3% PQ and 91.1% RQ under the panoptic symbol spotting setting, which outperforms the recent state-of-the-art method GAT-CADNet (Zheng et al., 2022) with a large margin."
        },
        {
            "heading": "2 Related Work",
            "text": "Vector Graphics Recognition Vector graphics are widely used in 2D CAD designs, urban designs, graphic designs, and circuit designs, to facilitate resolution-free precision geometric modeling. Considering their wide applications and great importance, many works are devoted to recognition tasks on vector graphics. Jiang et al. (2021) explores vectorized object detection and achieves a superior accuracy to detection methods (Bochkovskiy et al., 2020; Lin et al., 2017) working on raster graphics while enjoying faster inference time and less training parameters. Shi et al. (2022) propose a unified vector graphics recognition framework that leverages the merits of both vector graphics and raster graphics. Panoptic Symbol Spotting Traditional symbol spotting usually deals with instance symbols representing countable things (Rezvanifar et al., 2019), like table, sofa, and bed. Following the idea in (Kirillov et al., 2019), Fan et al. (2021) extended the definition by recognizing semantic of uncountable stuff, and named it panoptic symbol spotting. Therefore, all components in a CAD drawing are covered in one task altogether. For example, the wall represented by a group of parallel lines was properly handled by (Fan et al., 2021), which however was treated as background by (Jiang et al., 2021; Shi et al., 2022; Nguyen et al., 2009) in Vector graphics recognition. Meanwhile, the first large-scale real-world FloorPlanCAD dataset in the form of vector graphics was published by (Fan et al., 2021). Fan et al. (2022) propose CADTransformer, which modifies existing vision transformer (ViT) backbones for the panoptic symbol spotting task. Zheng et al. (2022) propose GAT-CADNet, which formulates the instance symbol spotting task as a subgraph detection problem and solves it by predicting the adjacency matrix. Point Cloud Segmentation Point cloud segmentation aims to map the points into multiple homogeneous groups. Unlike 2D images, which are characterized by regularly\narranged dense pixels, point clouds are constituted of unordered and irregular point sets. This makes the direct application of image processing methods to point cloud segmentation an impracticable approach. However, in recent years, the integration of neural networks has significantly enhanced the effectiveness of point cloud segmentation across a range of applications, including semantic segmentation (Qi et al., 2017a;a; Zhao et al., 2021), instance segmentation (Ngo et al., 2023; Schult et al., 2023) and panoptic segmentation (Zhou et al., 2021; Li et al., 2022; Hong et al., 2021; Xiao et al., 2023), etc."
        },
        {
            "heading": "3 Method",
            "text": "Our methods forgo the raster image or GCN in favor of a point-based representation for graphical primitives. Compared to image-based representations, it reduces the complexity of models due to the sparsity of primitive CAD drawings. In this section, we first describe how to form the point-based representation using the graphical primitives of CAD drawings. Then we illustrate a baseline framework for panoptic symbol spotting. Finally, we thoroughly explain three key techniques, attention with local connection, contrastive connection learning, and KNN interpolation, to adapt this baseline framework to better handle CAD data."
        },
        {
            "heading": "3.1 From Symbol to Points",
            "text": "Given vector graphics represented by a set of graphical primitives {pk}, we treat it as a collection of points {pk | (xk, fk)}, and each point contains both primitive position {xk} and primitive feature {fk} information; hence, the points set could be unordered and disorganized. Primitive position. Given a graphical primitive, the coordinates of the starting point and the ending point are (x1, y1) and (x1, y2), respectively. The primitive position xk \u2208 R2 is defined as : xk = [(x1 + x2)/2, (y1 + y2)/2] , (1) We take its center as the primitive position for a closed graphical primitive(circle, ellipse). as shown in Fig. 1a. Primitive feature. We define the primitive features fk \u2208 R6 as:\nfk = [\u03b1k, lk, onehot(tk)] , (2) where \u03b1k is the clockwise angle from the x positive axis to xk, and lk represents the distance between v1 and v2 for linear primitives, as shown in Fig. 1b. For circular primitives like circles and ellipses, lk is defined as the circumference. We encode the primitive type tk(line, arc, circle, or ellipse) into a one-hot vector to make up the missing information of segment approximations."
        },
        {
            "heading": "3.2 Panoptic Symbol Spotting via Point-based Representation",
            "text": "The baseline framework primarily comprises two components: the backbone and the symbol spotting head. The backbone converts raw points into points features, while the symbol\nspotting head predicts the symbol mask through learnable queries (Cheng et al., 2021; 2022). Fig. 2 illustrates the the whole framework.\nBackbone. We choose Point Transformer (Zhao et al., 2021) with a symmetrical encoder and decoder as our backbone for feature extraction due to its good generalization capability in panoptic symbol spotting. The backbone takes primitive points as input, and performs vector attention between each point and its adjacent points to explore local relationships. Given a point pi and its adjacent points M(pi), we project them into query feature qi, key feature kj and value feature vj , and obtain the vector attention as follows:\nwij = \u03c9(\u03b3(qi, kj)), fattni = \u2211\npj\u2208M(pi)\nSoftmax(Wi)j \u2299 vj , (3)\nwhere \u03b3 serves as a relational function, such as subtraction. \u03c9 is a learnable weight encoding that calculates the attention vectors. \u2299 is Hadamard product.\nSymbol Spotting Head. We follow Mask2Former (Cheng et al., 2022) to use hierarchical multi-resolution primitive features Fr \u2208 RNr\u00d7D from the decoder of backbone as the input to the symbol spotting predition head, where Nr is the number of feature tokens in resolution r and D is the feature dimension. This head consists of L layers of masked attention modules which progressively upscales low-resolution features from the backbone to produce high-resolution per-pixel embeddings for mask prediction. There are two key components in the masked attention module: query updating and mask predicting. For each layer l, query updating involves interacting with different resolution primitive features Fr to update query features. This process can be formulated as,\nXl = softmax(Al\u22121 + QlKTl )Vl + Xl\u22121, (4) where Xl \u2208 RO\u00d7D is the query features. O is the number of query features. Ql = fQ(Xl\u22121), Kl = fK(Fr) and Vl = fV (Fr) are query, key and value features projected by MLP layers. Al\u22121 is the attention mask, which is computed by,\nAl\u22121(v) = {\n0 if Ml\u22121(v) > 0.5, \u2212\u221e otherwise. (5)\nwhere v is the position of feature point and Ml\u22121 is the mask predicted from mask predicting part. Note that we need to downsample the high-resolution attention mask to adopt the query updating on low-resolution features. In practice, we utilize four coarse-level primitive features from the decoder of backbone and perform query updating from coarse to fine.\nDuring mask predicting process, we obtain the object mask Ml \u2208 RO\u00d7N0 and its corresponding category Yl \u2208 RO\u00d7C by projecting the query features using two MLP layers fY and fM , where C is the category number and N0 is the number of points. The process is as follows:\nYl = fY (Xl), Ml = fM (Xl)F T0 , (6) The outputs of final layer, YL and ML, are the predicted results."
        },
        {
            "heading": "3.3 Attention with Connection Module",
            "text": "The simple and unified framework rewards excellent generalization ability by offering a fresh perspective of CAD drawing, a set of points. It can obtain competitive results compared to previous methods. However, it ignores the widespread presence of primitive connections in CAD drawings. It is precisely because of these connections that scattered, unrelated graphical elements come together to form symbols with special semantics. In order to utilize these connections between each primitive, we propose Attention with Connection Module (ACM), the details are shown below. It is considered that these two graphical primitives(pi, pj) are interconnected if the minimum distance dij between the endpoints (vi, vj) of two graphical primitives (pi, pj) is below a certain threshold \u03f5, where:\ndij = minvi\u2208pi,vj\u2208pj \u2225vi \u2212 vj\u2225 < \u03f5. (7) To keep the complexity low, at most K connections are allowed for every graphical primitive by random dropping. Fig. 3a demonstrates the connection construction around the wall symbol, the gray line is the connection between two primitives. In practice, we set \u03f5 to 1.0px. The attention mechanism in (Zhao et al., 2021) directly performs local attention between each point and its adjacent points to explore the relationship. The original attention mechanism interacts only with neighboring points within a spherical region, as shown in Fig. 3b. Our ACM additionally introduces the interaction with locally connected primitive points during attention (pink points), essentially enlarging the radius of the spherical region. Note that we experimentally found that crudely increasing the radius of the spherical region without considering the local connections of primitive points does not result in performance improvement. This may be explained by that enlarging the receptive field also introduces additional noise at the same time. Specifically, we extend the adjacent points set M(pi) in Eq. (3) to A(pi) = M(pi) \u222a C(pi), where C(pi) = {pj |dij < \u03f5}, yielding,\nfattni = \u2211\npj\u2208A(pi)\nSoftmax(Wi)j \u2299 vj , (8)\nIn practice, since we cannot directly obtain the connection relationships of the points in the intermediate layers of the backbone, we integrate this module into the first stage of the backbone to replace the original local attention, as shown in Fig. 2."
        },
        {
            "heading": "3.4 Contrastive Connection Learning.",
            "text": "Although the information of primitive connection are considered when calculating attention of the encoder transformer, locally connected primitives may not belong to the same instance,\nin other words, noisy connections could be introduced while take primitive connections into consideration, as shown in Fig. 3c. Therefore, in order to more effectively utilize connection information with category consistency, we follow the widely used InfoNCE loss (Oord et al., 2018) and its generalization (Frosst et al., 2019; Gutmann & Hyv\u00e4rinen, 2010) to define the contrastive learning objective on the final output feature of backbone. We encourage learned representations more similar to its connected points from the same category and more distinguished from other connected points from different categories. Additionally, we also take neighbor points M(pi) into consideration, yielding,\nLCCL = \u2212 log \u2211\npj\u2208A(pi)\u2227lj=li exp(\u2212d(fi, fj)/\u03c4)\u2211 pk\u2208A(pi) exp(\u2212d(fi, fk)/\u03c4)\n(9)\nwhere fi is the backbone feature of pi, d(\u00b7, \u00b7) is a distance measurement, \u03c4 is the temperature in contrastive learning. we set the \u03c4 = 1 by default."
        },
        {
            "heading": "3.5 KNN Interpolation",
            "text": "During the process of query updating in symbol spotting head Eq. (4) & Eq. (5), we need to convert high-resolution mask predictions to low-resolution for attention masks computation as shown in Fig. 2 (AMD on the right). Mask2Former (Cheng et al., 2022) employs the bilinear interpolation on the pixel-level mask for downsampling. However, the masks of CAD drawings are primitive-level, making it infeasible to directly apply the bilinear interpolation on them. To this end, we propose the KNN interpolation for downsampling the attention masks by fusing the nearest neighbor points. A straightforward operation is max pooling or average pooling. We instead utilize distance-based interpolation. For simplicity, we omit layer index l in A,\nAr(pj) = \u2211 pj\u2208K(pi) A 0(pj)/d(pi, pj)\u2211\npj\u2208K(pi) 1/d(pi, pj) (10)\nwhere, A0 and Ar are the full-resolution attention mask and the r-resolution attention mask repectively. d(\u00b7, \u00b7) is a distance measurement. K(pi) is the set of K nearest neighbors, In practice, we set K = 4r in our experiments."
        },
        {
            "heading": "3.6 Training and Inference",
            "text": "Throughout the training phase, we adopt bipartite matching and set prediction loss to assign ground truth to predictions with the smallest matching cost. The full loss function L can be formulated as L = \u03bbBCELBCE + \u03bbdiceLdice + \u03bbclsLcls + \u03bbCCLLCCL, while LBCE is the binary cross-entropy loss (over the foreground and background of that mask), Ldice is the Dice loss (Deng et al., 2018), Lcls is the default multi-class cross-entropy loss to supervise the queries classification, LCCL is contrastive connection loss. In our experiments, we empirically set \u03bbBCE : \u03bbdice : \u03bbcls : \u03bbCCL = 5 : 5 : 2 : 8. For inference, we simply use argmax to determine the final panoptic results."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we present the experimental setting and benchmark results on the public CAD drawing dataset FloorPlanCAD (Fan et al., 2021). Following previous works (Fan et al., 2021; Zheng et al., 2022; Fan et al., 2022), we also compare our method with typical image-based instance detection (Ren et al., 2015; Redmon & Farhadi, 2018; Tian et al., 2019; Zhang et al., 2022). Besides, we also compare with point cloud semantic segmentation methods (Zhao et al., 2021), Extensive ablation studies are conducted to validate the effectiveness of the proposed techniques. In addition, we have also validated the generalizability of our method on other datasets beyond floorplanCAD, with detailed results available in the Appendix A."
        },
        {
            "heading": "4.1 Experimental Setting",
            "text": "Dataset and Metrics. FloorPlanCAD dataset has 11,602 CAD drawings of various floor plans with segment-grained panoptic annotation and covering 30 things and 5 stuff classes.\nFollowing (Fan et al., 2021; Zheng et al., 2022; Fan et al., 2022), we use the panoptic quality (PQ) defined on vector graphics as our main metric to evaluate the performance of panoptic symbol spotting. By denoting a graphical primitive e = (l, z) with a semantic label l and an instance index z, PQ is defined as the multiplication of segmentation quality (SQ) and recognition quality (RQ), which is formulated as\nPQ = RQ \u00d7 SQ (11)\n= |TP | |TP | + 12 |FP | + 1 2 |FN |\n\u00d7 \u2211\n(sp,sg)\u2208T P IoU(sp, sg) |TP |\n(12)\n= \u2211\n(sp,sg)\u2208T P IoU(sp, sg) |TP | + 12 |FP | + 1 2 |FN | . (13)\nwhere, sp = (lp, zp) is the predicted symbol, sg = (lg, zg) is the ground truth symbol. |TP |, |FP |, |FN | indicate true positive, false positive and false negative respectively. A certain predicted symbol is considered as matched if it finds a ground truth symbol, with lp = lg and IoU(sp, sg) > 0.5, where the IoU is computed by:\nIoU(sp, sg) = \u03a3ei\u2208sp\u2229sg log(1 + L(ei)) \u03a3ej\u2208sp\u222asg log(1 + L(ej)) . (14)\nImplementation Details. We implement SymPoint with Pytorch. We use PointT (Zhao et al., 2021) with double channels as the backbone and stack L = 3 layers for the symbol spotting head. For data augmentation, we adopt rotation, flip, scale, shift, and cutmix augmentation. We choose AdamW (Loshchilov & Hutter, 2017) as the optimizer with a default weight decay of 0.001, the initial learning rate is 0.0001, and we train the model for 1000 epochs with a batch size of 2 per GPU on 8 NVIDIA A100 GPUs."
        },
        {
            "heading": "4.2 Benchmark Results",
            "text": "Semantic symbol spotting. We compare our methods with point cloud segmentation methods (Zhao et al., 2021), and symbol spotting methods (Fan et al., 2021; 2022; Zheng et al., 2022). The main test results are summarized in Tab. 1, Our algorithm surpasses all previous methods in the task of semantic symbol spotting. More importantly, compared to GAT-CADNet (Zheng et al., 2022), we achieves an absolute improvement of 1.8% F1. and 3.2% wF1 respectively. For the PointT\u2021, we use our proposed point-based representation in Section 3.1 to convert the CAD drawing to a collection of points as input. It is worth noting that PointT\u2021 has already achieved comparable results to GAT-CADNet (Zheng et al., 2022), which demonstrates the effectiveness of the proposed point-based representation for CAD symbol spotting. Instance Symbol Spotting. We compare our method with various image detection methods, including FasterRCNN (Ren et al., 2015), YOLOv3 (Redmon & Farhadi, 2018),\nMethod Data Format PQ SQ RQ #Params Speed PanCADNet (Fan et al., 2021) VG + RG 55.3 83.8 66.0 >42M >1.2s CADTransformer (Fan et al., 2022) VG + RG 68.9 88.3 73.3 >65M >1.2s GAT-CADNet (Zheng et al., 2022) VG 73.7 91.4 80.7 - - PointT\u2021Cluster(Zhao et al., 2021) VG 49.8 85.6 58.2 31M 80ms SymPoint(ours, 300epoch) VG 79.6 89.4 89.0 35M 66ms SymPoint(ours, 500epoch) VG 81.9 90.6 90.4 35M 66ms SymPoint(ours, 1000epoch) VG 83.3 91.4 91.1 35M 66ms\n(c) Ablation studies on architecture design. BS: Backbone size. SW: share weights. L: layer number of spotting head. O: query number. D: feature dimension. \u2713 in the share weights column means whether share weights for head layers.\nTable 4: Ablation Stuides on different techniques, attention mask downsampling, and architecture desgin.\nFCOS (Tian et al., 2019), and recent DINO (Zhang et al., 2022). For a fair comparison, we post-process the predicted mask to produce a bounding box for metric computation. The main comparison results are listed in Tab. 2. Although our framework is not trained to output a bounding box, it still achieves the best results.\nPanoptic Symbol Spotting. To verify the effectiveness of the symbol spotting head, we also design a variant method without this head, named PointT\u2021Cluster, which predicts an offset vector per graphic entity to gather the instance entities around a common instance centroid and performs class-wise clustering (e.g. meanshift (Cheng, 1995)) to get instance labels as in CADTransformer (Fan et al., 2022). The final results are listed in Tab. 3. Our SymPoint trained with 300epoch outperforms both PointT\u2021Cluster and the recent SOTA method GAT-CADNet(Zheng et al., 2022) substantially, demonstrate the effectiveness of the proposed method. Our method also benefits from longer training and achieves further performance improvement. What\u2019s more, our method runs much faster during the inference phase than previous methods. For image-based methods, it takes approximately 1.2s to render a vector graphic into an image while our method does not need this process. The qualitative results are shown in Fig. 4."
        },
        {
            "heading": "4.3 Ablation Studies",
            "text": "In this section, we carry out a series of comprehensive ablation studies to clearly illustrate the potency and intricate details of the SymPoint framework. All ablations are conducted under 300 epoch training.\nEffects of Techniques. We conduct various controlled experiments to verify different techniques that improve the performance of SymPoint in Tab. 4a. Here the baseline means the method described in Sec. 3.2. When we only introduce ACM (Attention with Connection Module), the performance drops a bit due to the noisy connections. But when we combine it with CCL (Contrastive Connection Learning), the performance improves to 74.3 of PQ. Note that applying CCL alone could only improve the performance marginally. Furthermore, KNN Interpolation boosts the performance significantly, reaching 77.3 of PQ.\nKNN Interpolation. In Tab. 4b, we ablate different ways of downsampling attention mask: 1) linear interpolation, 2) KNN average pooling, 3) KNN max pooling, 4) KNN interpolation. KNN average pooling and KNN max pooling means using the averaged value or max value of the K nearest neighboring points as output instead of the one defined in Eq. (10). We can see that the proposed KNN interpolation achieves the best performance.\nArchitecture Design. We analyze the effect of varying model architecture design, like channel number of backbone and whether share weights for the L layers of symbol spotting head. As shown in Tab. 4c, we can see that enlarging the backbone, the query number and the feature channels of the symbol spotting head could further improve the performance. Sharing weights for spotting head not only saves model parameters but also achieves better performance compared with the one that does not share weights."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "This work introduces a novel perspective for panoptic symbol spotting. We treat CAD drawings as sets of points and utilize methodologies from point cloud analysis for symbol spotting. Our method SymPoint is simple yet effective and outperforms previous works. One limitation is that our method needs a long training epoch to get promising performance. Thus accelerating model convergence is an important direction for future work."
        },
        {
            "heading": "A Appendix",
            "text": "Due to space constraints in the paper,additional techniques analysis, additional quantitative results, qualitative results, and other dataset results can be found in the supplementary materials.\nA.1 Additional Techniques Analysis\nEffects of Attention with Connection Module. We conduct additional experiments in SESYD-floorplans dataset that is smaller than floorplanCAD. ACM can significantly promote performance and accelerate model convergence. We present the convergence curves without/with ACM in Fig. 5.\nExplanation and Visualization of KNN interpolation Technique. While bilinear interpolation, as utilized in Mask2Former, is tailored for regular data, such as image, but it is unsuitable for irregular sparse primitive points. Here, we provided some visualizations of point masks for KNN interpolation and bilinear interpolation as shown in Fig. 6. Note that these point masks are soft masks (ranging from 0 to 1) predicted by intermediate layers. After downsampling the point mask to 4x and 16x, we can clearly find that KNN interpolation well preserves the original mask information, while bilinear interpolation causes a significant information loss, which could harm the final performance.\nA.2 Additional Quantitative Evaluations\nWe present a detailed evaluation of panoptic quality(PQ), segmentation quality(SQ), and recognition quality(RQ) in Tab. 5. Here, we provide the class-wise evaluations of different variants of our methods.\nA.3 Additional Datasets\nTo demonstrate the generality of our SymPoint, we conducted experiments on other datasets beyond floorplanCAD.\nPrivate Dataset. We have also collected a dataset of floorplan CAD drawings with 14,700 from our partners. We\u2019ve meticulously annotated the dataset at the primitive level. Due to privacy concerns, this dataset is currently not publicly available. we randomly selected 10,200 as the training set and the remaining 4,500 as the test set. We conduct ablation studies of the proposed three techniques on this dataset, and the results are shown in Tab. 6. Different from the main paper, we also utilize the color information during constructing the connections, i.e., locally connected primitives with the same color are considered as\nvalid connections. We do not use color information in the floorCAD dataset because their color information is not consistent for the same category while ours is consistent. It can be seen that applying ACM does not lead to a decline in performance. In fact, there\u2019s an approximate 3% improvement in the PQ.\nVector Graphics Recognition Dataset. Similar to (Jiang et al., 2021; Shi et al., 2022), we evaluate our method on SESYD, a public dataset comprising various types of vector graphic documents. This database is equipped with object detection ground truth. For our experiments, we specifically focused on the floorplans and diagrams collections. The results are presented in Tab. 7 We achieved results on par with YOLaT(Jiang et al., 2021) and RendNet(Shi et al., 2022), which are specifically tailored for detection tasks. The aforementioned results further underscore the robust generalizability of our method. Some comparison visualized results with YOLaT are shown in Fig. 7.\nA.4 Additional Qualitative Evaluations\nThe results of additional cases are visually represented in this section, you can zoom in on each picture to capture more details, primitives belonging to different classes are represented in distinct colors. The color representations for each category can be referenced in Fig. 8. Some visualized results are shown in Fig. 9, Fig. 10 and Fig. 11 ."
        }
    ],
    "title": "Symbol as Points: Panoptic Symbol Spotting via Point-based Representation",
    "year": 2024
}