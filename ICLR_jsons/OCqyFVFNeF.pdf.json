{
    "abstractText": "Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.",
    "authors": [],
    "id": "SP:2ac799ddbfa691097681dfbd280bac23e87e380f",
    "references": [
        {
            "authors": [
                "David Bau",
                "Bolei Zhou",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Network dissection: Quantifying interpretability of deep visual representations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Huiqi Deng",
                "Qihan Ren",
                "Hao Zhang",
                "Quanshi Zhang"
            ],
            "title": "Discovering and explaining the representation bottleneck of dnns",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Huiqi Deng",
                "Na Zou",
                "Mengnan Du",
                "Weifu Chen",
                "Guocan Feng",
                "Ziwei Yang",
                "Zheyang Li",
                "Quanshi Zhang"
            ],
            "title": "Understanding and unifying fourteen attribution methods with taylor interactions",
            "venue": "arXiv preprint arXiv:2303.01506,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Amil Dravid",
                "Yossi Gandelsman",
                "Alexei A. Efros",
                "Assaf Shocher"
            ],
            "title": "Rosetta neurons: Mining the common units in a model zoo",
            "venue": "IEEE International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "John C Harsanyi"
            ],
            "title": "A simplified bargaining model for the n-person cooperative game",
            "venue": "International Economic Review,",
            "year": 1963
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas"
            ],
            "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "The mnist database of handwritten digits. http://yann",
            "venue": "lecun. com/exdb/mnist/,",
            "year": 1998
        },
        {
            "authors": [
                "Mingjie Li",
                "Quanshi Zhang"
            ],
            "title": "Technical note: Defining and quantifying and-or interactions for faithful and concise explanation of dnns",
            "venue": "arXiv preprint arXiv:2304.13312,",
            "year": 2023
        },
        {
            "authors": [
                "Mingjie Li",
                "Quanshi Zhang"
            ],
            "title": "Does a neural network really encode symbolic concept",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang"
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250,",
            "year": 2016
        },
        {
            "authors": [
                "Jie Ren",
                "Zhanpeng Zhou",
                "Qirui Chen",
                "Quanshi Zhang"
            ],
            "title": "Can we faithfully represent absence states to compute shapley values on a dnn",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Ren",
                "Mingjie Li",
                "Qirui Chen",
                "Huiqi Deng",
                "Quanshi Zhang"
            ],
            "title": "Defining and quantifying the emergence of sparse concepts in dnns",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Qihan Ren",
                "Huiqi Deng",
                "Yunuo Chen",
                "Siyu Lou",
                "Quanshi Zhang"
            ],
            "title": "Bayesian neural networks tend to ignore complex and sensitive concepts",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Qihan Ren",
                "Jiayang Gao",
                "Wen Shen",
                "Quanshi Zhang"
            ],
            "title": "Where we have arrived in proving the emergence of sparse symbolic concepts in ai models",
            "venue": "arXiv preprint arXiv:2305.01939,",
            "year": 2023
        },
        {
            "authors": [
                "Wen Shen",
                "Lei Cheng",
                "Yuxiao Yang",
                "Mingjie Li",
                "Quanshi Zhang"
            ],
            "title": "Can the inference logic of large language models be disentangled into symbolic concepts",
            "venue": "arXiv preprint arXiv:2304.01083,",
            "year": 2023
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "venue": "arXiv preprint arXiv:1312.6034,",
            "year": 2013
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing,",
            "year": 2013
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Anh Nguyen",
                "Thomas Fuchs",
                "Hod Lipson"
            ],
            "title": "Understanding neural networks through deep visualization",
            "venue": "International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Quanshi Zhang",
                "Xin Wang",
                "Jie Ren",
                "Xu Cheng",
                "Shuyun Lin",
                "Yisen Wang",
                "Xiangming Zhu"
            ],
            "title": "Proving common mechanisms shared by twelve methods of boosting adversarial transferability",
            "venue": "arXiv preprint arXiv:2207.11694,",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "More crucially",
                "Ren"
            ],
            "title": "2023c) have derived several theorems as mathematical evidence of considering such interactions as primitive inference patterns encoded by a DNN. Specifically, Ren et al. (2023c) proved that DNNs usually only encoded a small number of interactions, under some common conditions1",
            "year": 2023
        },
        {
            "authors": [
                "samples. Li",
                "Zhang"
            ],
            "title": "2023b) further discovered the discriminative power of certain interactions",
            "venue": "Ren et al",
            "year": 2022
        },
        {
            "authors": [
                "values. Deng"
            ],
            "title": "DNNs it was difficult to learn interactions with median number of input variables, and Ren et al. (2023b) discovered that Bayesian neural networks were unlikely to model complex interactions with many input variables. B THE CONDITIONS FOR UNIVERSAL MATCHING OF THE DNN",
            "venue": "OUTPUT Ren et al",
            "year": 2023
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "2023b) to extract interactions between a set of randomly selected input variables N",
            "venue": "t}, (t < n),",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Explaining and quantifying the exact knowledge encoded by a deep neural network (DNN) presents a new challenge in explainable AI. Previous studies mainly visualized patterns encoded by DNNs (Bau et al., 2017; Kim et al., 2018) and estimated a saliency map on input variables (Simonyan et al., 2013; R. Selvaraju et al., 2017). However, a new question is that can we formulate the implicit knowledge encoded by the DNN as explicit and symbolic primitive patterns? In fact, we hope these primitive patterns serve as elementary units for inference, just like concepts in human cognition.\nHowever, there is no widely accepted way to define the concept encoded by a DNN, because we cannot mathematically define/formulate the exact concept in human cognition. Nevertheless, if we ignore cognitive issues, Ren et al. (2023c); Li & Zhang (2023b) have derived a series of theorems as convincing evidence to take interactions as symbolic primitives encoded by a DNN. Specifically, an interaction captures the intricate nonlinear relationship encoded by the DNN. For instance, when a DNN processes a sentence \u201cIt is raining cats and dogs!\u201d, the DNN may encode the interaction between a set of input variables S = {raining, cats, and, dogs} \u2286 N . When all words in S are present, an interactive effect I(S) emerges, and pushes the DNN\u2019s inference towards the semantic meaning of \u201cheavy rain.\u201d However, if any word in S is masked, the effect will be removed.\nRen et al. (2023c) have mainly proven two theorems to justify the convincingness of considering above interactions as primitive inference patterns encoded by the DNN. First, it is proven that under some common conditions1, a well-trained DNN usually just encodes a limited number of interactions w.r.t. a few sets of input variables. More crucially, let us randomly mask an input sample x in different ways to generate an exponential number of masked samples. It is proven that people can use just a few interactions to accurately approximate the DNN\u2019s outputs on all these masked samples. Thus, these few interactions are referred to as interaction primitives.\nDespite the aforementioned theorems, this study does not yet deem the above interactions as faithful primitives of DNNs. The core problem is that existing interaction extraction methods cannot theoretically guarantee the generalization (transferability) of the interactions, e.g., ensuring to extract common interactions shared by different AI models. Interactions, which are not shared by different DNNs, may be perceived as out-of-distribution signals without clear meanings.\nTherefore, in this study, we revisit the generalization of interactions. Specifically, we identify a clear mechanism that makes the existing method extract different interactions from the same DNN under different initialization states, which hurts the generalization power of interactions.\n1Please see Appendix B for details.\nThus, to address the generalization issue, we propose a new method for extracting generalizable interactions. A generalizable interaction is defined as Figure 1 shows. Given multiple DNNs trained for the same task and an input sample, if an interaction can be extracted from all these DNNs, then we consider it generalizable. Our method is designed to extract interactions with maximum generalization power. This approach ensures that if an interaction exhibits a significant impact on the output score for one DNN, it usually demonstrates noteworthy influence for other DNNs. We conducted experiments on various dataset. Experiments showed that our proposed method significantly improved the generalization power of the extracted interactions across different DNNs."
        },
        {
            "heading": "2 GENERALIZABLE INTERACTION PRIMITIVES ACROSS DNNS",
            "text": ""
        },
        {
            "heading": "2.1 PRELIMINARIES: EXPLAINING THE NETWORK OUTPUT WITH INTERACTION PRIMITIVES",
            "text": "Although there is no theory to guarantee that how to define concepts that fit well with human cognition, Li & Zhang (2023a) and Ren et al. (2023c) still provided mathematical supports to explain why we can still use interactions between input variables as the primitives or concepts encoded by the DNN. Specifically, there are two types of interactions, i.e., AND interactions and OR interactions.\nAND interactions. Given a function v : Rn \u2192 R, let us consider an input sample x = [x1, x2, \u00b7 \u00b7 \u00b7 , xn]\u22ba with n input variables indexed by N = {1, 2, ..., n}. Here, v(x) \u2208 R denotes the function output on x2. Then, Ren et al. (2022) have used the Harsanyi dividend (Harsanyi, 1963) Iand(S|x) to quantify the numerical effect of the AND relationship between input variables in S \u2286 N , which is encoded by the function v. We consider this interaction as an AND interaction.\nIand(S|x) := \u2211\nT\u2286S (\u22121)|S|\u2212|T |v(xT ). (1)\nwhere Iand(\u2205|x) = v(x\u2205), and xT denotes a sample whose input variables in N \\ T are masked3. Each AND interaction Iand(S|x) reveals the AND relationship between all variables in S. For instance, let us consider the slang term S = {x3, x4, x5, x6} in the sentence \u201cx1 = It, x2 = is, x3 = raining, x4 = cats, x5 = and, x6 = dogs!\u201d as a toy example. The co-occurrence of four words forms the semantic concept of \u201cheavy rain\u201d and contributes a numerical effect Iand(S|x) to the function output. Otherwise, the masking of any word xi \u2208 S invalidates the semantic concept and eliminates the interaction effect, i.e., obtaining Iand(S|xmasked) = 0 on the masked sample. OR interactions. Analogously, we can also use the OR interaction to explain the function v : Rn \u2192 R. To this end, (Li & Zhang, 2023a) have defined the following OR interaction effect Ior(S|x) to measure the OR interaction encoded by v. In particular, Ior(\u2205|x) = v(x\u2205).\nIor(S|x) := \u2212 \u2211\nT\u2286S (\u22121)|S|\u2212|T |v(xN\\T ). (2)\n2If the target function/model/network has a vectorized output, e.g., a DNN for multi-category classification, we may set v(x) = log p(y=y\ntruth|x) 1\u2212p(y=ytruth|x) by following (Deng et al., 2021).\n3We followed (Li & Zhang, 2023a) to obtain two discrete states for each input variable, i.e., the masked and unmasked states. We simply masked each input variable i \u2208 N\\S using baseline values. Please see Appendix C for details.\nEach OR interaction Ior(S|x) describes the OR relationship between all variables in S. Let us consider an input sentence \u201cx1 = This, x2 = movie, x3 = is, x4 = boring, x5 = and, x6 = disappointing\u201d for sentiment classification. Let us set S = {x4, x6}. The presence of any word in S will contribute a negative sentiment effect Ior(S|x) to the function output. Sparsity of interactions. Theoretically, according to Equation (1), a function can encode at most 2n different AND interactions w.r.t. all 2n subsets \u2200S, S \u2286 N . However, Ren et al. (2023c) have proved that under some common conditions1, most well-trained DNNs only encode a small set of AND interactions, denoted by \u2126, i.e., only a few interactions S \u2208 \u2126 have considerable effects Iand(S|x). All other interactions have almost zero effects, i.e., Iand(S|x) \u2248 0, which can be regarded as a set of negligible noise patterns.\nIt is worth noting that an OR interaction can be regarded as a specific AND interaction, if we inverse the definition of the masked state and the unmasked state of an input variable4. Thus, the proven sparsity of AND interactions can also indicate the conclusion that well-trained DNNs tend to encode a small number of OR interactions.\nDefinition of interaction primitives. Considering the above proven sparsity of interactions, we define an interaction primitive as a salient interaction. Formally, given a threshold \u03c4 , the set of interaction primitives are defined as \u2126 = {S \u2286 N : |I(S|x)| > \u03c4}. Theorem 1 (Universal matching theorem, proved by (Ren et al., 2023c)). As the corollary of the proven sparsity in Ren et al. (2023c), the function\u2019s output on all 2n masked samples {xS |S \u2286 N} could be universally explained by the interaction primitives in \u2126, s.t., |\u2126| \u226a 2n, i.e., \u2200S \u2286 N, v(xS) = \u2211 T\u2286S Iand(T |x) \u2248 \u2211 T\u2286S:T\u2208\u2126 Iand(T |x).\nIn particular, Theorem 1 shows that if we arbitrarily mask the input sample x, we can get 2n different masked samples3, \u2200S, S \u2286 N . Then, we can universally match the output of the function v(xS) on all 2n masked samples using only a few interaction primitives in \u2126.\nTheorem 2 (proved by Harsanyi (1963)). The Shapley value \u03d5(i) of an input variable i can be explained as a uniform allocation of the AND interactions, i.e., \u03d5(i) = \u2211 S\u2286N :S\u220bi 1 |S|Iand(S|x)."
        },
        {
            "heading": "2.2 FAITHFULNESS PROBLEM WITH INTERACTION-BASED EXPLANATIONS",
            "text": "Basic setting of using AND-OR interactions to explain a DNN. In this section, we consider to employ both AND interactions and OR interactions to explain the DNN\u2019s output. This is because the complexity of the representations in DNNs makes it difficult to rely solely on either AND interactions or OR interactions to faithfully explain true inference primitives encoded by the DNN.\nTo this end, we need to decompose the output of the DNN into two terms v(x) = vand(x) + vor(x), so that we can use AND interactions to explain the term vand(x) and use OR interactions to explain the term vor(x). In this way, the first challenge is how to learn an appropriate decomposition of vand(x) and vor(x) that reveals intrinsic primitive interactions encoded by the DNN. We will discuss this challenge later.\nNevertheless, no matter how we randomly decompose v(x) = vand(x) + vor(x), Theorem 3 demonstrates that we can still use interactions to fit the DNN\u2019s outputs on 2n randomly masked samples {xT |T \u2286 N}. Furthermore, according to the sparsity of interaction primitives in Section 2.1, we can obtain Proposition 1, i.e., the 2n network outputs on all masked samples can usually be approximated by a small number of AND interaction primitives in \u2126and and OR interaction primitives in \u2126or, s.t., |\u2126and|, |\u2126or| \u226a 2n.\nTheorem 3 (Universal matching theorem, proof in Appendix D). Let us be given a DNN v and an input sample x. For each randomly masked sample xT , T \u2286 N , we obtain\nv(xT ) = vand(xT ) + vor(xT ) = \u2211\nS\u2286T Iand(S|xT ) + \u2211 S\u2208{S:S\u2229T \u0338=\u2205}\u222a{\u2205} Ior(S|xT ). (3)\n4To compute Iand(S|x), we use a baseline value bi and set xi = bi to represent its masked state. If we consider bi variable as the presence of the variable, and consider the original value xi as its masked state (i.e., using v(bT ) to represent v(xN\\T ) in Equation (2), then Ior(S|x) in Equation (2) can be formulated the same as the AND interaction in Equation (1). Please see Appendix C for details.\nProposition 1. The output of a well-trained DNN on all 2n masked samples {xT |T \u2286 N} could be universally approximated by the interaction primitives in \u2126and and \u2126or, s.t., |\u2126and|, |\u2126or| \u226a 2n, i.e., \u2200T \u2286 N, v(xT ) = \u2211 S\u2286T Iand(S|xT ) + \u2211 S\u2208{S:S\u2229T \u0338=\u2205}\u222a{\u2205} Ior(S|xT ) \u2248\nv(x\u2205)+ \u2211 \u2205\u0338=S\u2286T :S\u2208\u2126and Iand(S|xT )+ \u2211 S\u2229T \u0338=\u2205:S\u2208\u2126or Ior(S|xT ), where v(x\u2205) = vand(x\u2205)+vor(x\u2205).\nProblems with the faithfulness of interactions. Although the universal matching capacity proven in Theorem 3 is a quite significant advantage of AND-OR interactions, it is still not the ultimate guarantee for the faithfulness of the extracted interactions. To be precise, there is still no standard way to faithfully decompose the vand(x) term and the vor(x) term that reveal intrinsic primitive interactions encoded by the DNN, considering the following two challenges.\n\u2022Challenge 1, ambiguous decomposition of vand(x) and vor(x) usually brings in considerable uncertainty in the extraction of interactions. Let us take the following toy Boolean function as an example to illustrate the diversity of interactions, f(x) = x1 \u2227x2 \u2227x3+x2 \u2227x3+x3 \u2227x4+x4 \u2228x5, where x = [x1, x2, x3, x4, x5]\n\u22ba and xi \u2208 {0, 1}. We have two ways to decompose f(x). First, we can simply decompose vand(x) = x1 \u2227 x2 \u2227 x3 + x2 \u2227 x3 + x3 \u2227 x4 and vor(x) = x4 \u2228 x5, then to explain f(x) with an OR interaction Ior(S = {4, 5}) and three AND interactions Iand(S = {1, 2, 3}), Iand(S = {2, 3}) and Iand(S = {3, 4}). Alternatively, we can also use exclusively AND interactions to explain f(x). Specifically, we can rewrite vand(x) = x1\u2227x2\u2227x3+x2\u2227x3+x3\u2227x4+x4\u2228x5 = x1 \u2227 x2 \u2227 x3 + x2 \u2227 x3 + x3 \u2227 x4 + (x4 + x5 \u2212 x4 \u2227 x5) and vor(x) = 0, w.r.t xi \u2208 {0, 1}, please see Appendix E for the proof. Thus, the vand term can be explained by a total of six AND interaction primitives. This is a typical case for diverse strategy of extracting interactions that are generated by different decompositions.\nThe aforementioned f(x) is just an exceedingly simple function. In real-world applications, DNNs usually encode intricate AND-OR relationships among input variables, making it exceptionally challenging to formulate an explicit expression for the DNN function or to establish a definitive groundtruth decomposition of vand(x) and vor(x). Consequently, the diversity issue with interactions are ubiquitous and unavoidable.\n\u2022 Challenge 2, how to ensure the interaction primitives are generalizable. It is commonly considered that generalizable primitives are usually transferable over different models trained for the same task, instead of being over-fitted by a single model. Thus, if an interaction primitive can be consistently extracted from different DNNs, then it can be considered as a faithful concept. Otherwise, non-generalizable (non-transferable) interactions appear as mathematical manipulations, rather than faithful concepts, even though they still satisfy the criteria of sparsity and universal matching in Theorem 3.\nDefinition 1 (Transferability of interaction primitives). Given m different DNNs trained for the same task, v(1), v(2), . . . , v(m), we use AND-OR interactions to explain the output score v(i)(x) of each DNN v(i) on the input sample x. Let \u2126and,(i) = {S \u2286 N : |I(i)and(S|x)| > \u03c4\n(i)} and \u2126or,(i) = {S \u2286 N : |I(i)or (S|x)| > \u03c4 (i)} denote a set of sparse AND interaction primitives and a set of sparse OR interaction primitives, respectively. Then, the set of generalizable AND and the set of generalizable OR interaction primitives for the i-th DNN, are defined as \u2126andshared = \u22c2m i=1 \u2126 and,(i)\nand \u2126orshared = \u22c2m i=1 \u2126 or,(i), respectively. The generalization power of AND and OR interactions of the i-th DNN, can be measured by s(i)and = |\u2126 and shared|/|\u2126and,(i)| and s (i) or = |\u2126orshared|/|\u2126or,(i)|, respectively.\nDefinition 1 introduces the generalization power of interaction primitives. A larger value signifies higher transferability and, consequently, more generalizable interactive primitives."
        },
        {
            "heading": "2.3 EXTRACTING GENERALIZABLE INTERACTION PRIMITIVES",
            "text": "Neither of the aforementioned two challenges has been adequately tackled in previous interaction studies. In essense, interactions are determined by the decomposition of the network output v(x) = vand(x)+vor(x). Thus, if we rewrite the decomposition as vand(xT ) = 0.5v(xT )+\u03b3T and vor(xT ) = 0.5v(xT )\u2212 \u03b3T , then the learning of the best decomposition is equivalent to learning a set of {\u03b3T }. Here, the parameter \u03b3T \u2208 R for a subset T \u2286 N determines a specific decomposition between vand(xT ) and vor(xT ). Therefore, our goal is to learn the appropriate parameters {\u03b3T } that reduce the aforementioned uncertainty of interaction primitives and boost their generalization power.\nTo this end, to alleviate the uncertainty of the interactions, the most intuitive approach is to learn the sparsest interactions, considering the principle of Occam\u2019s Razor, as follows. It is because the sparsest (or simplest) explanation is usually considered as the most faithful explanation.\nmin{\u03b3T } \u2225Iand\u22251 + \u2225Ior\u22251, (4)\nwhere Iand = [Iand(T1|x), . . . , Iand(T2n |x)]\u22ba, Ior = [Ior(T1|x), . . . , Ior(T2n |x)]\u22ba \u2208 R2 n\n, Tk \u2286 N . The above \u21131 norm loss promotes the sparsity of both AND interactions and OR interactions."
        },
        {
            "heading": "2.3.1 ONLY ACHIEVING SPARSITY IS NOT ENOUGH",
            "text": "Although the sparsity can be used to reduce the uncertainty of interactions, the sparsity of interactions w.r.t. each single input sample obtained in Equation (4) does not fully solve the above two challenges. First, Ren et al. (2023b) have found that the extraction of high-order interactions is usually sensitive to small noises in input variables, where the order is defined as the number of input variables in S, i.e., order(S) = |S|. It means that when different noises are added to the input samples, the algorithm may extract fully different high-order interactions5. Similarly, this will also hurt the generalization power of interaction primitives over different samples, when these samples contain similar sets of input variables.\nSecond, optimizing the loss in Equation (4) may lead to diverse solutions. Given different initial states, the loss in Equation (4) may learn two different sets of parameters {\u03b3T } as two local minima with similar loss values, while the two sets of parameters {\u03b3T } generate two different sets of interactions. We conducted experiments to illustrate this point. Given a pre-trained BERT model (Devlin et al., 2018) and an input sentence x on the SST-2 dataset for sentiment classification, we learned the parameters {\u03b3T } to extract sparse interaction primitives3. In this experiment, we repeatedly extracted two sets of AND-OR interactions by applying two different sets of initialized parameters {\u03b3T }, which are denoted by (Aand, Aor) and (Band, Bor). Aand = {S \u2286 N : |Iand(S|x)| > \u03c4Aand)} denotes the set of AND interaction primitives extracted by a certain initialization of {\u03b3T }, where the parameter \u03c4Aand was determined to ensure that each set selected the most salient K = 100 interactions. We used the transferability of interaction primitives in Definition 1, Sand = |Aand \u22c2 Band|/|Aand| and Sor = |Aor \u22c2 Bor|/|Aor|, to measure the diversity of interactions caused by the different parameter initializations. Table 2 shows that given different initial states, optimizing the loss in Equation (4) usually extracted two dramatically different sets of AND-OR interactions with only 21% overlap. Figure 12 further shows the top 5 AND-OR interaction primitives extracted from BERT model on the same input sentence, which illustrates that given different initial states, the loss in Equation (4) would learn different AND-OR interactions.\nThird, prioritizing sparsity cannot guarantee high generalization power across different models. Since a DNN may simultaneously learn common knowledge shared by different DNNs and be overfitted to some out-of-the-distribution patterns, different DNNs may only share partial interaction primitives. We believe that the shared common interactions are more faithful, so the transferability is another way to guarantee the generalization power of interaction primitives. Therefore, we hope to formulate and extract common interactions that are generalizable through different DNNs.\nWe conducted experiments to illustrate the difference between interactions extracted from two DNNs by using Equation (4). We used BERTBASE vbase and BERTLARGE vlarge (Devlin et al., 2018) for the task of sentiment classification. Specifically, given an input sentence x, we learned two sets of parameters {\u03b3baseT } and {\u03b3 large T } for the BERT-base model and the BERT-large model, respectively. Then we extracted two sets of AND-OR interactive concepts (\u2126and,base, \u2126or,base) and (\u2126and,large, \u2126or,large), respectively. Subsequently, we computed the transferability of the extracted interaction primitives according to Definition 1. Figure 3(a) shows that the transferability of the extracted ANDOR interaction primitives was much lower than interactions proposed in this study."
        },
        {
            "heading": "2.3.2 EXTRACTING GENERALIZABLE INTERACTIONS",
            "text": "As discussed above, the sparsity alone is not enough to tackle the aforementioned challenges. Therefore, in this study, we propose to use the generalization power as a straightforward purpose, to boost the faithfulness of interactions. Meanwhile, the sparsity of interactions is also supposed to be\n5Please see Appendix F for details.\nguaranteed. Given a total of m DNNsv(1),v(2),. . ., v(m) trained for the same task, the objective of extracting generalizable interactions shared by the m DNNs is revised from Equation (4), as follows.\nmin{\u03b3(1)T ,...,\u03b3 (m) T }\n\u2225rowmax(Iand)\u22251 + \u2225rowmax(Ior)\u22251, (5) where Iand = [ I (1) and I (2) and . . . I (m) and ] \u2208 R2n\u00d7m and Ior = [ I (1) or I (2) or . . . I (m) or ] \u2208 R2n\u00d7m. I(i)and =\n[I (i) and(T1|x), . . . , I (i) and(T2n |x)]\u22ba \u2208 R2 n and I(i)or represent the all 2n AND-OR interactions extracted from the i-th DNN, Tk \u2286 N . The matrix operator rowmax() computes the \u2113\u221e norm of each row within the matrix, i.e., rowmax(Iand) = [\u2225Iand[1, :]\u2225\u221e, . . . , \u2225Iand[2n, :]\u2225\u221e]\u22ba \u2208 R2 n\n. For each specific subset of variables Tk \u2286 N , the rowmax() operation returns the most salient interaction strength over all m interactions from the m DNNs. Please see Appendix F for more discussion on the matrix Iand.\nUnlike Equation (4), the revised loss in Equation (5) only penalizes the most salient interactions over all m interactions extracted from m DNNs, with respect to each subset Tk \u2286 N . This loss function ensures that if a DNN encodes a strong interaction w.r.t. the set Tk, then we can also extract the same interaction w.r.t. Tk from the other m \u2212 1 DNNs without a penalty. The \u21131 norm also makes that the m DNNs share similar sets of sparse interactions. Considering the sparsity of interactions, for most subset Tk, the effect I (i) and/or(Tk|x) is supposed to keep almost zero on all m DNNs.\nJust like in Equation (4), we decompose the output of the i-th DNN as v(i)and(xT )=0.5v (i)(xT )+\u03b3 (i) T and v(i)or (xT )=0.5v(i)(xT )\u2212\u03b3(i)T to compute two vectors of AND-OR interactions, I (i) and and I (i) or .\nRedundancy of interactions. However, it is important to emphasize that only penalizing the largest interaction among the m DNNs in Equation (5) still faces the redundancy problem. Specifically, for each i-th DNN, we compute a total of 2n AND interactions and 2n OR interactions w.r.t. different subsets T \u2286 N . Some of these 2n+1 interactions, denoted by the set \u2126(i)max, are selected by the loss in Equation (5) as the most salient interactions over m DNNs, while the set of other unselected interactions are denoted by \u2126(i)others = {T \u2286 N} \\ \u2126 (i) max. Then, the redundancy problem is caused by a short-cut solution to the loss minimization in Equation (5), i.e., using unselected not-so-salient interactions in \u2126(i)others to represent numerical effects of selected interactions \u2126 (i) max, as discussed in Challenge 1 in Section 2.2. As a short-cut solution to Equation (5), this may also reduces the strength of the penalized salient interactions in Equation (5), but generates lots of redundant interacitons.\nTherefore, we revise the loss in Equation (5) to add penalties on unselected interactions to avoid the short-cut solution with a coefficient of \u03b1, as follows6.\nmin{\u03b3(1)T ,...,\u03b3 (m) T }\n(\u2225rowmax(Iand)\u22251 + \u2225rowmax(Ior)\u22251) + \u03b1 (\u2225Iand\u22251 + \u2225Ior\u22251) , (6)\nwhere \u03b1 \u2208 [0, 1] is a positive scalar. We extend the notation of the \u21131 norm \u2225 \u00b7 \u22251 to represent the sum of the absolute values of all elements in a given vector or matrix. It is worth noting that the generalization power of interactions is guaranteed by the rowmax() function in Equation (5), which assigns much higher penalties to non-generalizable interactions than generalizable interactions.\nSharing decomposition between DNNs. Optimizing Equation (6) is challenging7. To address this challenge, we introduce a set of strategies to facilitate the optimization process. We assume that when all m DNNs are sufficiently trained, these DNNs tend to have similar decompositions of AND interactions and OR interactions, i.e., obtaining similar parameters, \u2200T \u2286 N, \u03b3(1)T \u2248 \u03b3 (2) T \u2248 \u00b7 \u00b7 \u00b7 \u2248 \u03b3 (m) T . To achieve this, we introduce two types of parameters for \u03b3 (i) T , \u03b3 (i) T = \u03b3\u0304T + \u03b3\u0302 (i) T , where \u03b3\u0304T represents the common decomposition shared by all DNNs, and \u03b3\u0302(i)T represents the decomposition specific to each i-th DNN. We constrain the significance of the unshared decomposition by using a bound |\u03b3\u0302(i)T | < \u03c4 (i) \u03b3 , where \u03c4 (i) \u03b3 = 0.5 \u00b7 Ex[|v(i)(x) \u2212 v(i)(x\u2205)|]. During the training process, if |\u03b3\u0302(i)T | > \u03c4 (i) \u03b3 , then we set \u03b3\u0302 (i) T = \u03c4 (i) \u03b3 \u00b7 sign(\u03b3\u0302(i)T ).\nModeling noises. Furthermore, we have identified a potential limitation in the definition of the interactions, i.e., the sensitivity to noise. Let us assume that the output of the i-th DNN has\n6Please see Appendix F for a detailed explanation of Equation (6). 7Note that regardless of whether Equation (6) is optimized to the optimal solution, theoretically, the ex-\ntracted AND-OR interactions can still satisfy the property of universal matching in Theorem 3.\na small noise. We represent such noises by adding a small Gaussian noise \u03f5T \u223c N (0, \u03c32) to the network output v\u2032(i)and (xT ) = v (i) and(xT ) + \u03f5 (i) T . In this case, we can derive that I \u2032(i) and (T ) = I (i) and(T ) + \u2211 T \u2032\u2286T (\u22121)|T |\u2212|T \u2032|\u03f5 (i) T \u2032 . We prove that the variance of I \u2032(i) and (T ) caused by the Gaussian noises is E\u03f5T\u223cN (0,\u03c32)[I \u2032(i) and (T ) \u2212 E\u2200S,\u03f5S\u223cN (0,\u03c32)I \u2032(i) and (S)]\n2 = 2|T |\u03c32 (please see Appendix F for details). Similarly, the variance of I \u2032(i)or (T ) is also 2|T |\u03c32 for OR interactions. It means that the variance/instability of interactions increases exponentially with the order of the interaction |T |.\nTherefore, we propose to directly learn the error term \u03f5(i)T based on Equation (6) to remove tiny noisy signals, which are unavoidable in real data but cannot be modeled as AND-OR interactions, i.e., setting v(i)(xT ) = v (i) and(xT ) + v (i) or (xT ) + \u03f5 (i) T , in order to enhance the robustness of our interaction extraction process. The error term is constrained to a small range |\u03f5(i)T | < \u03c4 (i) \u03f5 , subject to \u03c4 (i) \u03f5 = 0.02\u00b7 |v(i)(x)\u2212v(i)(x\u2205)|. During the training process, if |\u03f5(i)| > \u03c4 (i) \u03f5 , then we set |\u03f5(i)| = \u03c4 (i)\u03f5 \u00b7 sign(\u03f5(i)T ).\nThen, we conducted experiments to examine whether the extracted AND-OR interactions could still accurately explain the network output, when we removed the error term. We followed experimental settings in Section 3 to extract interactions on both BERTBASE model and the BERTLARGE model. We computed the matching error e(xT ) = |v(xT ) \u2212 vapprox(xT )| , where vapprox(xT ) was the network output approximated by all interactions based on Theorem 3. Figure 13 shows matching errors of all masked samples w.r.t. all subsets T \u2286 N , when we sorted the network outputs for all 2n masked samples in a descending order. It shows that the real network output was well approximated by interactions."
        },
        {
            "heading": "3 EXPERIMENT",
            "text": "In this section, we conducted experiments to verify the sparsity and generalization power of the interaction primitives extracted by our proposed method on the following three tasks.\nTask1: sentiment classification with language models. We jointly extracted two sets of AND-OR interaction primitives from the BERTBASE model and the BERTLARGE model (Devlin et al., 2018) by following Equation (6). We finetuned the pre-trained BERTBASE model and the BERTLARGE model on the SST-2 dataset (Socher et al., 2013) for sentiment classification. For each input sentence x containing n tokens8, we analyzed the log-odds output of the ground-truth label, i.e., v(x) = log p(y=y\ntruth|x) 1\u2212p(y=ytruth|x) by following (Deng et al., 2021).\nTask2: dialogue task with large language models. We extracted two sets of AND-OR interaction primitives from the pre-trained LLaMA model (Touvron et al., 2023) and OPT-1.3B model (Zhang et al., 2022b). We explained the DNNs\u2019 outputs on the SQuAD dataset (Rajpurkar et al., 2016). We took the first several words of each document in the dataset as the input of a DNN, and let the DNN predict the next word. For each input sentence x containing n words8, we analyzed the log-odds output of the (n + 1)-th word that was associated with the highest probability by the DNN, ymax, i.e., v(x) = log p(y=y\nmax|x) 1\u2212p(y=ymax|x) .\nTask3: image classification task with vision models. We extracted two sets of AND-OR interaction primitives from the ResNet-20 model (He et al., 2016) and the VGG-16 model (Simonyan & Zisserman, 2014), which were trained on the MNIST dataset (LeCun, 1998). These models were trained to classify the digit \u201c3\u201d from other digits. In practice, considering the 2n computational complexity, we have followed settings in (Li & Zhang, 2023b), who labeled a few important input patches in the image as input variables. For each input image x containing n patches8, we analyzed the scalar output before the softmax layer corresponding to the digit \u201c3.\u201d\nSparsity of the extracted primitives. We aggregated all AND-OR interactions from various samples, and draw their strength in a descending order in Figure 2. This figure compares the curve of interaction strength |I(S)|, S \u2286 N between our extracted interactions, the traditional interactions (Li & Zhang, 2023b)9(namely, Traditional) and the original Harsanyi interactions (Ren et al., 2023a)\n8Please see Appendix Q for details. 9In the implementation of the competing method, we also learned an additional error term \u03f5(i)T to remove\nsmall noises, just like in Section 2.3.2, to enable fair comparisons.\n(namely, Harsanyi). The competing method (Li & Zhang, 2023b) (Traditional in Figure 2) extracts the sparest interactions according to Equation (4), and the original Harsanyi interactions (Ren et al., 2023a) (Harsanyi in Figure 2) extracts interactions according to Equation (1)10. We found that most of the interactions had negligible effect. Although the proposed method reduced the sparsity a bit, the extracted interactions were still sparse enough to be considered as primitive inference patterns. For each DNN, we further set a threshold \u03c4 (i) to collect a set of salient interactions from this DNN as interaction primitives, i.e., \u03c4 (i) = 0.05 \u00b7maxS |I(S|x)|. Generalization power of the extracted interaction primitives. We took the most salient k interactions from each i-th DNN as the set of AND-OR interaction primitives, i.e., |\u2126and, (i)| = |\u2126or, (i)| = k, i \u2208 {1, 2}. We used the metric sand and sor in Definition 1 to measure the generalization power of interactions extracted from two DNNs. Figure 3 shows the generalization power of interactions when we computed sand and sor based on different numbers k of most salient interactions. We found that the set of AND-OR interactions extracted from the proposed method exhibited higher generalization power than interactions extracted from the traditional method.\nLow-order interaction primitives are more stable. Furthermore, we compared the ratio of shared interactions of different orders, i.e., order(S) = |S|. For interactions of each order o, we computed the overall strength of all positive interactions and that of all negative interactions of the i-th DNN, which were shared by other DNNs, as Shared+,(i)(o) = \u2211 op\u2208{and,or} \u2211 S\u2208\u2126op,(i)shared ,|S|=o max(0, I (i) op (S|x))\nand Shared\u2212,(i)(o) = \u2211 op\u2208{and,or} \u2211\nS\u2208\u2126op,(i)shared ,|S|=o min(0, I\n(i) op (S|x)), respectively. Besides, All+,(i)(o) = \u2211 op\u2208{and,or} \u2211\nS\u2208\u2126op,(i),|S|=o max(0, I (i) op (S|x)) and All\u2212,(i)(o) =\u2211 op\u2208{and,or} \u2211 S\u2208\u2126op,(i),|S|=o min(0, I (i) op (S|x)) denote the overall strength of salient positive interactions and that of salient negative interactions, respectively. In this way, Figure 4 reports (Shared+,(i), Shared\u2212,(i)) and (All+,(i),All\u2212,(i)) for different orders within the three tasks. It shows that low-order interactions were more likely to be shared by different DNNs than high-order interactions. Besides, Figure 4 further shows a higher ratio of interactions extracted by the proposed method were shared by different DNNs than interactions extracted by the traditional method. In particular, the high similarity of interactions between ResNet-20 and VGG-16 shows that although two DNNs for the same tasks had fully different architectures, there probably existed a set of ultimate interactions for a task, and different well-optimized DNNs were likely to converge to such interactions.\nVisualization of the shared interaction primitives across different DNNs. We also visualize the shared and distinctive interaction primitives in Figure 5. This figure shows that generalizable interactions shared by different models can be regarded as more reliable concepts, which consistently contribute salient interaction effects to the output of different DNNs. In comparison, non-generalizable\n10Please see Appendix J for more details.\ninteractions, which are sometimes over-fitted by a single model, may appear as out-of-distribution features. From this pespective, we consider generalizable interactions as relatively faithful concepts that often have a significant impact on the inference of DNNs. Figure 5 further shows that, our method extracted much more shared interactions than the traditional interaction-extraction method, which shows that our method could obtain more stable explanation of the inference logic of a DNN. It is because the interactions shared by different DNNs were usually considered more faithful."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "In this paper, we proposed a method to extract generalizable interaction primitives. The sparsity and universal-matching property of interactions provide lots of evidence to faithfully explain DNNs with interactions. Thus, in this paper, we propose to further improve the generalization power of interactions, which adds the last piece of the puzzle of interaction primitives. Compared to traditional interactions, interactions shared by different DNNs are more likely to be the underlying primitives that shape the DNN\u2019s output. Furthermore, the extraction of interaction primitives also contributes to real applications. For example, it can assist in learning optimal baseline values for Shapley values (Ren et al., 2022) and explaining the representation limits of Bayesian networks (Ren et al., 2023b). In addition, the extraction of generalizable interaction primitives shared by different DNNs provide a new perspective to formulating the out-of-distribution (OOD) features. Previous studies usually treated an entire sample as an OOD sample, whereas our work redefines the OOD problem at the level of detailed interactions, i.e., unshared interactions can be regarded as OOD information.\nETHICS STATEMENT\nThis paper aims to extract generalizable interaction primitives that are shared by different DNNs. This paper utilizes publicly released datasets which have been widely accepted by the machine learning community. This paper does not involve human subjects and does not include potentially harmful insights, methods, or applications. The paper also does not involve discrimination/bias/fairness issues, as well as privacy and security issues. There are no ethical issues with this paper.\nREPRODUCIBILITY STATEMENT\nWe provide proofs for the theoretical results of this study in Appendix C to F. We also provide experimental details in Section 3 and Appendix Q. In addition, we will release the code when the paper is accepted."
        },
        {
            "heading": "A PREVIOUS LITERATURE OF USING INTERACTIONS TO EXPLAIN DNNS",
            "text": "We will move this section back to the main paper in the additional page, if the paper is accepted.\nExplaining the knowledge encoded by DNNs is one of the ultimate goals of explainable AI but presents significant challenges. For instance, some studies employed visualization techniques to show the patterns learned by a DNN (Simonyan et al., 2013; Yosinski et al., 2015), and some focused on extracting feature vectors that may be associated with semantic concepts (Simonyan et al., 2013), while some studies learned feature vectors potentially related to concepts (Kim et al., 2018). Dravid et al. (2023) identified convolutional kernels in different DNNs that expressed similar concepts.\nHowever, theoretically, whether the knowledge or the complex inference logic of a DNN can be faithfully represented as symbolic primitive inference patterns still presents significant challenges. Up to now, there is still no universally accepted definition of the knowledge, as it encompasses various aspects of cognitive science, neuroscience, and mathematics. However, if we ignore cognitive and neuroscience aspects, Ren et al. (2023a) have proposed to quantify interactions between input variables encoded by the DNN, to explain the knowledge in the DNN. More crucially, Ren et al. (2023c) have derived several theorems as mathematical evidence of considering such interactions as primitive inference patterns encoded by a DNN. Specifically, Ren et al. (2023c) proved that DNNs usually only encoded a small number of interactions, under some common conditions1. Besides, these interactions can universally explain the DNN\u2019s output score on any arbitrary masked input samples. Li & Zhang (2023b) further discovered the discriminative power of certain interactions.\nBesides, Deng et al. (2023) found that different attribution scores estimated by different attribution methods could all be represented as a combination of different interactions. Zhang et al. (2022a) used interactions to explain the mechanism of different methods of boosting adversarial transferability. Ren et al. (2022) used interactions to define the optimal baseline value for computing Shapley values. Deng et al. (2021) found that for most DNNs it was difficult to learn interactions with median number of input variables, and Ren et al. (2023b) discovered that Bayesian neural networks were unlikely to model complex interactions with many input variables."
        },
        {
            "heading": "B THE CONDITIONS FOR UNIVERSAL MATCHING OF THE DNN OUTPUT",
            "text": "Ren et al. (2023c) have proved that a well-trained DNN usually just encodes a limited number of interactions. More importantly, one can just use a few interactions to approximate the DNN\u2019s outputs on all 2n masked samples {xS |S \u2286 N}, under the following three common assumptions. (1) The high order derivatives of the DNN output with respect to the input variables are all zero. (2) The DNN works well on the masked samples, and yield higher confidence when the input sample is less masked. (3) The confidence of the DNN does not drop significantly on the masked samples. With these natural assumptions, a well-trained DNN\u2019s output on all 2n masked samples can be universally approximated by the sum of just a few salient interactions in \u2126, s.t., |\u2126| \u226a 2n."
        },
        {
            "heading": "C THE RELATIONSHIP BETWEEN AND INTERACTIONS AND OR INTERACTIONS",
            "text": "In Section 2.1, we mentioned that an OR interaction can be regarded as a specific AND interaction, if we inverse the definition of the masked state and the unmasked state of an input variable. In this section, we further discuss how to understand the relationship between AND and OR interactions.\nGiven a DNN v : Rn \u2192 R and an input sample x \u2208 Rn, if we arbitrarily mask the input sample, we can get 2n different masked samples xS ,\u2200S \u2286 N . Specifically, let us use baseline values b \u2208 Rn to represent the masked state of a masked sample xS , i.e.,\n(xS)i = { xi, i \u2208 S bi, i /\u2208 S\n(7)\nConversely, if we inverse the definition of the masked state and the unmasked state of an input variable, i.e., we consider b as the input sample, and consider the original value x as the masked\nstate, then the masked sample bS can be defined as follows.\n(bS)i = { bi, i \u2208 S xi, i /\u2208 S\n(8)\nAccording to the definition of a masked sample in Equations (7) and (8), we can get xN\\S = bS . To simply the analysis, if we assume that vand(xT ) = vor(xT ) = 0.5v(xT ), then the OR interaction Ior(S|x) in Equation (2) can be regarded as a specific AND interaction Iand(S|b) as follows.\nIor(S|x) = \u2212 \u2211\nT\u2286S (\u22121)|S|\u2212|T |vor(xN\\T ), = \u2212 \u2211\nT\u2286S (\u22121)|S|\u2212|T |vor(bT ), = \u2212 \u2211\nT\u2286S (\u22121)|S|\u2212|T |vand(bT ),\n= \u2212Iand(S|b).\n(9)"
        },
        {
            "heading": "D PROOF OF THEOREM 3",
            "text": "Theorem 3 (Universal matching theorem) Let us be given a DNN v and an input sample x. For each randomly masked sample xT , T \u2286 N , we obtain\nv(xT ) = vand(xT ) + vor(xT ) = \u2211\nS\u2286T Iand(S|xT ) + \u2211 S\u2208{S:S\u2229T \u0338=\u2205}\u222a{\u2205} Ior(S|xT ). (10)\nProof. (1) Universal matching theorem of AND interactions.\nRen et al. (2023a) have used the Haranyi dividend (Harsanyi, 1963) Iand(S|x) to state the universal matching theorem of AND interactions. The output of a well-trained DNN on all 2n masked samples {xT |T \u2286 N} could be universally explained by the all interaction primitives in T \u2286 N , i.e., \u2200T \u2286 N, vand(xT ) = \u2211 S\u2286T Iand(S|x).\nSpecifically, the AND interaction is defined as Iand(S|x) := \u2211\nL\u2286S(\u22121)|S|\u2212|L|vand(xL) in Equation (1). To compute the sum of AND interactions \u2200T \u2286 N, \u2211 S\u2286T Iand(S|x) =\u2211\nS\u2286T \u2211\nL\u2286S(\u22121)|S|\u2212|L|vand(xL), we first exchange the order of summation of the set L \u2286 S \u2286 T and the set S \u2287 L. That is, we compute all linear combinations of all sets S containing L with respect to the model outputs vand(xL) given a set of input variables L, i.e.,\u2211\nS:L\u2286S\u2286T (\u22121)|S|\u2212|L|vand(xL). Then, we compute all summations over the set L \u2286 T .\nIn this way, we can compute them separately for different cases of L \u2286 S \u2286 T . In the following, we consider the cases (1) L = S = T , and (2) L \u2286 S \u2286 T, L \u0338= T , respectively. (1) When L = S = T , the linear combination of all subsets S containing L with respect to the model output vand(xL) is (\u22121)|T |\u2212|T |vand(xL) = vand(xL). (2) When L \u2286 S \u2286 T, L \u0338= T , the linear combination of all subsets S containing L with respect to the model output vand(xL) is \u2211 S:L\u2286S\u2286T (\u22121)|S|\u2212|L|vand(xL). For all sets S : T \u2287 S \u2287 L, let us consider the linear combinations of all sets S with number |S| for the model output vand(xL), respectively. Let m := |S|\u2212|L|, (0 \u2264 m \u2264 |T |\u2212|L|), then there are a total of Cm|T |\u2212|L| combinations of all sets S of order |S|. Thus, given L, accumulating the model outputs vand(xL) corresponding to all S \u2287 L, then \u2211 S:L\u2286S\u2286T (\u22121)|S|\u2212|L|vand(xL) = vand(xL) \u00b7 \u2211|T |\u2212|L| m=0\nCm|T |\u2212|L|(\u22121) m\ufe38 \ufe37\ufe37 \ufe38\n=0\n= 0.\nPlease see the complete derivation of the following formula.\n\u2211 S\u2286T Iand(S|xT ) = \u2211 S\u2286T \u2211 L\u2286S (\u22121)|S|\u2212|L|vand(xL)\n= \u2211\nL\u2286T \u2211 S:L\u2286S\u2286T (\u22121)|S|\u2212|L|vand(xL)\n= vand(xT )\ufe38 \ufe37\ufe37 \ufe38 L=T\n+ \u2211\nL\u2286T,L \u0338=T vand(xL) \u00b7 \u2211|T |\u2212|L| m=0\nCm|T |\u2212|L|(\u22121) m\ufe38 \ufe37\ufe37 \ufe38\n=0\n= vand(xT ).\n(11)\n(2) Universal matching theorem of OR interactions. According to the definition of OR interactions in Section 2.1, we will derive that \u2200T \u2286 N, vor(xT ) =\u2211 S\u2208{S:S\u2229T \u0338=\u2205}\u222a{\u2205} Ior(S|xT ) = Ior(\u2205|xT ) + \u2211 S:S\u2229T \u0338=\u2205 Ior(S|xT ), s.t., Ior(\u2205|xT ) = vor(x\u2205).\nSpecifically, the OR interaction is defined as Ior(S|x) := \u2212 \u2211\nL\u2286S(\u22121)|S|\u2212|L|vor(xN\\L) in Equation (2). Similar to the above derivation of the universal matching theorem of AND interactions, to compute the sum of OR interactions \u2200T \u2286 N, \u2211 S:S\u2229T \u0338=\u2205 Ior(S|xT ) =\u2211\nS:S\u2229T \u0338=\u2205\n[ \u2212 \u2211 L\u2286S(\u22121)|S|\u2212|L|vor(xN\\L) ] , we first exchange the order of summation of the set\nL \u2286 S \u2286 N and the set S : S \u2229 T \u0338= \u2205. That is, we compute all linear combinations of all sets S containing L with respect to the model outputs vor(xN\\L) given a set of input variables L, i.e.,\u2211\nS:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L). Then, we compute all summations over the set L \u2286 N .\nIn this way, we can compute them separately for different cases of L \u2286 S \u2286 N,S \u2229 T \u0338= \u2205. In the following, we consider the cases (1) L = N \\ T , (2) L = N , (3) L \u2229 T \u0338= \u2205, L \u0338= N , and (4) L \u2229 T = \u2205, L \u0338= N \\ T , respectively. (1) When L = N \\T , the linear combination of all subsets S containing L with respect to the model output vor(xN\\L) is \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L) = \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xT ). For all sets S : S \u2287 L, S \u2229 T \u0338= \u2205 (then S \u0338= N \\ T, S \u0338= L), let us consider the linear combinations of all sets S with number |S| for the model output vor(xT ), respectively. Let |S\u2032| := |S| \u2212 |L|, (1 \u2264 |S\u2032| \u2264 |T |), then there are a total of C |S\n\u2032| |T | combinations of all sets S of\norder |S|. Thus, given L, accumulating the model outputs vor(xT ) corresponding to all S \u2287 L, then\u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L) = vor(xT ) \u00b7 \u2211|T | |S\u2032|=1 C |S\u2032| |T | (\u22121)\n|S\u2032|\ufe38 \ufe37\ufe37 \ufe38 =\u22121 = \u2212vor(xT ).\n(2) When L = N (then S = N ), the linear combination of all subsets S containing L with respect to the model output vor(xN\\L) is \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L) = (\u22121)|N |\u2212|N |vor(x\u2205) = vor(x\u2205).\n(3) When L \u2229 T \u0338= \u2205, L \u0338= N , the linear combination of all subsets S containing L with respect to the model output vor(xN\\L) is \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L). For all sets S : S \u2287 L, S \u2229 T \u0338= \u2205, let us consider the linear combinations of all sets S with number |S| for the model output vor(xT ), respectively. Let us split |S| \u2212 |L| into |S\u2032| and |S\u2032\u2032|, i.e.,|S| \u2212 |L| = |S\u2032|+ |S\u2032\u2032|, where S\u2032 = {i|i \u2208 S, i /\u2208 L, i \u2208 N \\ T}, S\u2032\u2032 = {i|i \u2208 S, i /\u2208 L, i \u2208 T} (then 0 \u2264 |S\u2032\u2032| \u2264 |T | \u2212 |T \u2229 L|) and S\u2032 + S\u2032\u2032 + L = S. In this way, there are a total of C\n|S\u2032\u2032| |T |\u2212|T\u2229L| combinations of all sets S\n\u2032\u2032 of order |S\u2032\u2032|. Thus, given L, accumulating the model outputs vor(xN\\L) corresponding to all S \u2287 L, then \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L) =\nvor(xN\\L) \u00b7 \u2211\nS\u2032\u2286N\\T\\L \u2211|T |\u2212|T\u2229L| |S\u2032\u2032|=0 C |S\u2032\u2032| |T |\u2212|T\u2229L|(\u22121)\n|S\u2032|+|S\u2032\u2032|\ufe38 \ufe37\ufe37 \ufe38 =0 = 0.\n(4) When L \u2229 T = \u2205, L \u0338= N \\ T , the linear combination of all subsets S containing L with respect to the model output vor(xN\\L) is \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L). Similarly, let us split |S|\u2212|L| into |S\u2032| and |S\u2032\u2032|, i.e.,|S|\u2212|L| = |S\u2032|+|S\u2032\u2032|, where S\u2032 = {i|i \u2208 S, i /\u2208 L, i \u2208 N \\T},\nS\u2032\u2032 = {i|i \u2208 S, i \u2208 T} (then 0 \u2264 |S\u2032\u2032| \u2264 |T |) and S\u2032 + S\u2032\u2032 + L = S. In this way, there are a total of C |S\n\u2032\u2032| |T | combinations of all sets S\n\u2032\u2032 of order |S\u2032\u2032|. Thus, given L, accumulating the model outputs vor(xN\\L) corresponding to all S \u2287 L, then \u2211 S:S\u2229T \u0338=\u2205,S\u2287L(\u22121)|S|\u2212|L|vor(xN\\L) = vor(xN\\L) \u00b7\u2211\nS\u2032\u2286N\\T\\L \u2211|T | |S\u2032\u2032|=0 C |S\u2032\u2032| |T | (\u22121)\n|S\u2032|+|S\u2032\u2032|\ufe38 \ufe37\ufe37 \ufe38 =0 = 0.\nPlease see the complete derivation of the following formula. \u2211 S:S\u2229T \u0338=\u2205 Ior(S|xT ) = \u2211 S:S\u2229T \u0338=\u2205 [ \u2212 \u2211 L\u2286S (\u22121)|S|\u2212|L|vor(xN\\L) ] = \u2212 \u2211 L\u2286N \u2211 S:S\u2229T \u0338=\u2205,S\u2287L (\u22121)|S|\u2212|L|vor(xN\\L)\n= \u2212  |T |\u2211 |S\u2032|=1 C |S\u2032| |T | (\u22121) |S\u2032|  \u00b7 vor(xT )\ufe38 \ufe37\ufe37 \ufe38 L=N\\T \u2212 vor(x\u2205)\ufe38 \ufe37\ufe37 \ufe38 L=N\n\u2212 \u2211\nL\u2229T \u0338=\u2205,L \u0338=N  \u2211 S\u2032\u2286N\\T\\L |T |\u2212|T\u2229L|\u2211 |S\u2032\u2032|=0 C |S\u2032\u2032| |T |\u2212|T\u2229L|(\u22121) |S\u2032|+|S\u2032\u2032|  \u00b7 vor(xN\\L) \u2212\n\u2211 L\u2229T=\u2205,L \u0338=N\\T  \u2211 S\u2032\u2286N\\T\\L  |T |\u2211 |S\u2032\u2032|=0 C |S\u2032\u2032| |T | (\u22121) |S\u2032|+|S\u2032\u2032|  \u00b7 vor(xN\\L) = \u2212(\u22121) \u00b7 vor(xT )\u2212 vor(x\u2205)\u2212\n\u2211 L\u2229T \u0338=\u2205,L \u0338=N  \u2211 S\u2032\u2286N\\T\\L 0  \u00b7 vor(xN\\L) \u2212\n\u2211 L\u2229T=\u2205,L \u0338=N\\T  \u2211 S\u2032\u2286N\\T\\L 0  \u00b7 vor(xN\\L) = vor(xT )\u2212 vor(x\u2205)\n(12)\n(3) Universal matching theorem of AND-OR interactions.\nWith the universal matching theorem of AND interactions and the universal matching theorem of OR interactions, we can easily get v(xT ) = vand(xT ) + vor(xT ) = \u2211 S\u2286T Iand(S|xT ) +\u2211\nS\u2208{S:S\u2229T \u0338=\u2205}\u222a{\u2205} Ior(S|xT ), thus, we obtain the universal matching theorem of AND-OR interactions.\nE PROOF OF DIVERSE DECOMPOSITION OF FUNCTION f(x)\nAs discussed in Challenge 1 in Section 2.2, we stated that the function f(x) = x1 \u2227 x2 \u2227 x3 + x2 \u2227 x3 + x3 \u2227 x4 + x4 \u2228 x5, where x = [x1, x2, x3, x4, x5]\u22ba and xi \u2208 {0, 1}, can be rewritten as exclusively AND interactions. Specifically, the function f(x) = vand(x)+vor(x) can be decomposed as vand(x) = x1 \u2227 x2 \u2227 x3 + x2 \u2227 x3 + x3 \u2227 x4 + x4 \u2228 x5 and vor(x) = 0. This is because we can use the truth table to prove that x4 \u2228 x5 = x4 + x5 \u2212 x4 \u2227 x5, thus, we can get vand(x) = x1\u2227x2\u2227x3+x2\u2227x3+x3\u2227x4+x4\u2228x5 = x1\u2227x2\u2227x3+x2\u2227x3+x3\u2227x4+(x4+x5\u2212x4\u2227x5), which uses exclusively AND interactions to explain the function f(x)."
        },
        {
            "heading": "F PROOF OF THE VARIANCE OF AND AND OR INTERACTIONS",
            "text": "In this section, we prove that the variance of I \u2032(i)and (T ) caused by the Gaussian noises \u03f5 (i) T \u223c N (0, \u03c32) is E\u03f5T\u223cN (0,\u03c32)[I \u2032(i) and (T )\u2212 E\u2200S,\u03f5S\u223cN (0,\u03c32)I \u2032(i) and (S)] 2 = 2|T |\u03c32 in Section 2.3.2.\nProof. Given I \u2032(i)and (T ) = I (i) and(T ) + \u2211 T \u2032\u2286T (\u22121)|T |\u2212|T \u2032|\u03f5 (i) T \u2032 , the variance of I \u2032(i) and (T ) is Var(I \u2032(i)and (T )) = Var(I (i) and(T ) + \u2211 T \u2032\u2286T (\u22121)|T |\u2212|T \u2032|\u03f5 (i) T \u2032 ). As the AND interaction I (i) and(T ) and the Gaussian noise \u03f5(i)T are independent of each other, then the variance of I \u2032(i) and (T ) can be decomposed to Var(I \u2032(i)and (T )) = Var(I (i) and(T )) + Var( \u2211 T \u2032\u2286T (\u22121)|T |\u2212|T \u2032|\u03f5 (i) T \u2032 ) = Var( \u2211 T \u2032\u2286T (\u22121)|T |\u2212|T \u2032|\u03f5 (i) T \u2032 ), this is because here I(i)and(T ) can be regarded as a constant.\nSince each Gaussian noise \u03f5(i)T \u223c N (0, \u03c32),\u2200T \u2286 N is independent and identically distributed, then the variance is Var(I \u2032(i)and (T )) = Var( \u2211 T \u2032\u2286T (\u22121)|T |\u2212|T \u2032|\u03f5 (i) T \u2032 ) = Var(\u03f5 (i) T \u20321 ) + Var(\u03f5(i)T \u20322 ) + \u00b7 \u00b7 \u00b7 + Var(\u03f5(i)T \u2032 2|T | ) = 2|T | \u00b7 \u03c32 (there are a total of 2|T | subsets for T \u2032 \u2286 T )."
        },
        {
            "heading": "G THE FAITHFULNESS OF THE SPARSITY AND UNIVERSAL-MATCHING.",
            "text": "This section further demonstrates the faithfulness of the sparsity and universal-matching theorem of AND-OR interactions, both theoretically and experimentally.\nTheoretically, the faithfulness of the sparsity and universal-matching theorem of AND-OR interactions means that, given an input sample with n variables, we must prove that (1) a well-trained DNN usually just encodes a small number of salient interactions \u2126 (Ren et al., 2023c), comparing with all 2n potential combinations of the input variables in a given input sample, i.e., |\u2126| \u226a 2n, and that (2) the network output v(xS) on all 2n randomly masked samples {xS |S \u2286 N} can be well matched by a few interactions in \u2126 = {S \u2286 N : |I(S|x)| > \u03c4} as defined in the Definition of interaction primitives in Section 2.1. These two terms have been proved in Theorem 1, Theorem 3 and Proposition 1.\nThen, in practice, considering the 2n computational complexity, we have followed settings in (Li & Zhang, 2023b) to extract interactions between a set of randomly selected input variables N = {1, 2, . . . , t}, (t < n), while other unselected (n\u2212 t) input variables remain unmasked, leaving the original state unchanged. In this case, faithfulness does not mean that our interactions explain all the inference logic encoded between all n input variables for a given sample x in the pre-trained DNN. Instead, it only means that the extracted interactions can also accurately match the inference logic encoded between the selected t input variables for a given sample x in the DNN.\nExperimental verification. In addition, we have further conducted an experiment to verify the faithfulness when we explain interactions between all input variables. To this end, for the sentiment classification task on the SST-2 dataset in BERTBASE, we selected sentences containing 15 tokens to verify the faithfulness of the sparsity in Proposition 1 and the universal-matching property in Theorem 3. All tokens are selected as input variables. We focused on the matching errors for all masked samples xT ,\u2200T \u2286 N . Specifically, we observed whether the real network output on the masked sample v(xT ),\u2200T \u2286 N can be well approximated by interactions. We have verified that the extracted salient interactions in \u2126 faithfully explain the network output, i.e., \u2200T \u2286 N, v(xT ) \u2248 v(x\u2205) + \u2211 \u2205\u0338=S\u2286T :S\u2208\u2126and Iand(S|xT ) + \u2211 S\u2229T \u0338=\u2205:S\u2208\u2126or Ior(S|xT ). Figure 6 illustrates that network output v(xT ),\u2200T \u2286 N on all 2n randomly masked samples can be well fitted by interactions."
        },
        {
            "heading": "H DETAILED EXPLANATION OF EQUATION (6)",
            "text": "This section uses a simple example to illustrate Equation (6) more clearly. Let us illustrate how the interaction matrices are formatted in a toy example. Let us consider two pre-trained DNNs v(1), v(2) and an input sample x with N = {1, 2} variables, and take the AND interaction matrix Iand in Equation (6) as an example (the OR interaction matrix Ior can be obtained similarly). First, we get all 22 AND interactions extracted from the i-th model I(i)and = [Iand(T1|x), Iand(T2|x), Iand(T3|x), Iand(T4|x)]\u22ba \u2208 R2 2\n, according to the description of Equation (4). Here, each interaction value Iand(Tk|x) \u2208 R, Tk \u2286 N denotes the interaction value of\neach masked sample xTk , which is computed according to Equation (1). Second, the AND interaction matrix Iand = [ I (1) and, I (2) and ] \u2208 R22\u00d72 represents the interaction values corresponding to the 22 masked samples for each of the two models.\nLet us illustrate how to learn the parameter \u03b3(i)T in Equation (6). The loss in Equation (6) in the above example can be represented as the function of {\u03b3T }, as follows.\nLoss = min{\u03b3(1) Tk ,\u03b3 (2) Tk } (\u2225rowmax(Iand)\u22251 + \u2225rowmax(Ior)\u22251) + \u03b1 (\u2225Iand\u22251 + \u2225Ior\u22251)\n= min{\u03b3(1) Tk ,\u03b3 (2) Tk } \u2211 Tk\u2286N |max( \u2211 T\u2286S (\u22121)|S|\u2212|T |[0.5 \u00b7 v(1)(xTk ) + \u03b3 (1) Tk ], \u2211 T\u2286S (\u22121)|S|\u2212|T |[0.5 \u00b7 v(2)(xTk ) + \u03b3 (2) Tk ])|\n+ \u2211\nTk\u2286N\n|max(\u2212 \u2211\nT\u2286S (\u22121)|S|\u2212|T |[0.5 \u00b7 v(1)(xN\\Tk )\u2212 \u03b3 (1) Tk\n],\u2212 \u2211\nT\u2286S (\u22121)|S|\u2212|T |[0.5 \u00b7 v(2)(xN\\Tk )\u2212 \u03b3 (2) Tk ])|\n+ \u03b1 \u00b7 \u2211\nTk\u2286N\n2\u2211 i=1 | \u2211 T\u2286S (\u22121)|S|\u2212|T |[0.5 \u00b7 v(i)(xTk ) + \u03b3 (i) Tk ]|\n+ \u03b1 \u00b7 \u2211\nTk\u2286N\n2\u2211 i=1 | \u2212 \u2211 T\u2286S (\u22121)|S|\u2212|T |[0.5 \u00b7 v(i)(xN\\Tk )\u2212 \u03b3 (i) Tk ])|.\n(13)\nTherefore, we only need to optimize {\u03b3(1)Tk , \u03b3 (2) Tk } via gradient descent to reduce the loss in Equation (6)."
        },
        {
            "heading": "I THE RUN-TIME COMPLEXITY",
            "text": "This section explores the run-time complexity of extracting AND-OR interactions on different tasks. Theoretically, given an input sample with n input variables, the time complexity is 2n, and we need to generate masked samples for model inference. Fortunately, the variable number is not too large by following the settings in (Li & Zhang, 2023b) and (Shen et al., 2023). In real applications, the average running time for a sentence in the SST-2 dataset on the BERTLARGE model is 45.14 seconds. Table 1 further shows the average running time of each input sample for different tasks."
        },
        {
            "heading": "J MORE BASELINE TO VERIFY THE EFFECTIVENESS OF EQUATION (6)",
            "text": "To further verify the effectiveness of the interactions extracted from Equation (6), we conducted experiments on more baseline, namely the original Harsanyi interaction (Ren et al., 2023a). We compared the sparsity and the generalization power of the AND interactions extracted from different DNNs. Figure 7 shows that the interactions extracted by our method exhibit higher sparsity and generalization power compared to the original AND interactions.\nTo enable fair comparisons, we double the number of Harsanyi interactions extracted from a sample by setting I \u2032(S|x) = I(S|x). Thus, we congregate both sets of interactions (a total of 2 \u00b7 2n interactions) to draw a curve in Figure 2 and Figure 3. In this way, we can compare the same number of interactions over different methods.\nBERTBASE BERTLARGE Task 1: Sentiment Classification\n# of salient interactions (a) (b)\nIn te ra ct io n St re ng th In lo g sp ac e\n\ud835\udc25\ud835\udc28 \ud835\udc20 |\ud835\udc70 (\ud835\udc7a |\ud835\udc99 )|\nge ne\nra liz\nat io\nn po\nw er\n(ra tio\no f s\nha re\nd in\nte ra\nct io\nns )\n(m ea\nsu re\nd by\n\ud835\udc46 !\"\n# )\n18:5\n1 20 % max$ \ud835\udc3c(\ud835\udc46|\ud835\udc65)\nFigure 7: Comparing the sparsity and generalization power of the extracted interactions between the original Harsanyi interactions and our proposed method.\nK ABLATION STUDY OF THE PARAMETER \u03b1 IN EQUATION (6)\nTo explore the effect of \u03b1 on the AND-OR interactions extracted from Equation (6), we conducted an ablation study on \u03b1. Specifically, we jointly extracted two sets of AND-OR interactions from the BERTBASE and BERTLARGE models, which were trained by (Devlin et al., 2018) and further finetuned by us for sentiment classification on the SST-2 dataset. For comparison, we set the value of \u03b1 to [0, 0.2, 0.4, 0.6, 0.8, 1.0], respectively. Here, when \u03b1 = 0, Equation (6) degenerated into Equation (5), indicating that only the largest interactions among the m DNNs were penalized. As \u03b1 increases, the effects of not-so-salient interactions in other models were taken into account. Figure 8 shows that as the value of \u03b1 increases, the sparsity of the extracted interactions did not increase too much, but the generalization power of the extracted interactions decreased. This shows the effectiveness of the penalty in Equation (5) in boosting the generalization power without significantly hurting the sparsity."
        },
        {
            "heading": "L EXTRACTING GENERALIZABLE INTERACTIONS FROM NEW LLMS",
            "text": "In this section, we conducted more experiments to show the generalizable interactions between different DNNs. Specifically, given the LLaMA model (Touvron et al., 2023) and the Aquila-7B model (BAAI, 2023), we extracted AND-OR interaction primitives on a given input sentence. We followed the settings in Task 2 in Section 3. Figure 9 shows that the new pair of LLMs (LLaMA and Aquila-7B) shared a large ratio of interactions.\nM VISUALIZATION OF MORE SHARED AND DISTINCTIVE INTERACTION PRIMITIVES FOR FIGURE 5\nIn this section, we visualized all shared and distinctive interaction primitives across different DNNs in Figure 5. Specifically, we randomly selected 10 tokens in the given sentence, labeling these 10 tokens in red and the other unselected tokens in black. Here, n = 10 input variables denote the embeddings corresponding to these 10 tokens. Since each input variable has two states, masked and unmasked, a total of 210 masked samples are generated. Then, according to Equations (1) and (2), a total of 2 \u00b7 210 AND interactions and OR interactions are finally obtained. Then, we extracted the most salient k = 50 interactions from a total of 2 \u00b7 210 AND interactions and OR interactions as the set of AND-OR interaction primitives in each DNN, respectively. Therefore, in our proposed method, 25 shared interactions were extracted from both models, and 25 distinctive interactions were extracted from the BERTBASE and BERTLARGE models, respectively. In contrast, in the traditional method, 16 shared interactions were extracted from both models, and 34 distinctive interactions were extracted from the BERTBASE and BERTLARGE models, respectively.\nIn addition, Figure 10 shows the strength of the interaction value |I(S|x)| for each salient interaction. Specifically, we show the strength of the interaction value for each distinctive interaction in each DNN. We also show the strengths of two interaction values for each shared interaction extracted from both DNNs, where the strength of the interaction value on the BERTBASE model is on the left and the strength of the interaction value on the BERTLARGE model is on the right. Figure 10 illustrates that, our method extracted more shared interactions compared to the traditional interaction-extraction method."
        },
        {
            "heading": "N DISCUSSION ON THE DIFFERENCES IN THE SIMILARITY OF INTERACTIONS",
            "text": "This section compares the consistency of interactions between different types of DNNs. Since LLaMA (Touvron et al., 2023) and OPT-1.3B (Zhang et al., 2022b) have much more parameters than BERTBASE and BERTLARGE (Devlin et al., 2018), it is often widely believed that these two LLMs can better converge to the true language knowledge in the training data.\nWe can roughly understand this phenomenon by clarifying the following two phenomena. First, through extensive experiments, the authors have found that the learning of a DNN usually has two phases, i.e., the learning of new interactions and the forgotten of incorrectly learned interactions (this is our on-going research). In most cases, after the forgetting phase, the remained interactions of an LLM are usually shared by other LLMs. The high similarity of interactions between LLMs has also been observed in (Shen et al., 2023).\nSecond, compared to LLMs, relatively small models are less powerful to remove all incorrectly learned interactions. For example, a simple model is usually less powerful to regress a complex function. The less powerful models (BERTBASE and BERTLARGE) are not powerful enough to accurately encode the potentially complex interactions.\nTherefore, small models are more likely to represent various incorrect interactions to approximate the true target of the task. This may partially explains the high difficulty of extracting common interactions from two BERT models, as well as why the proposed method shows more improvements in the generalization power of interactions on BERT models.\nExperimental verification. To this end, we have further conducted a new experiment to compare the interactions extracted from two LLMs, i.e., LLaMA (Touvron et al., 2023) and Aquila-7B (BAAI, 2023). As Figure 11 shows, two LLMs usually encode much more similar interactions than two not-so-large models. In fact, we have also observed similar interactions in anther pair of LLMs, i.e., LLaMA and OPT-1.3B (Zhang et al., 2022b) (please see Figure 4). This partially explains the reason why the performance improvement of these two LLMs is similar to that of LLaMA and OPT-1.3B."
        },
        {
            "heading": "O A CONCRETE EXAMPLE TO ILLUSTRATE THE PROCEDURE",
            "text": "Let us use a concrete input sentence with six tokens to illustrate the experimental procedure from beginning to end.\nStep 1: Given the sentence \u201cA stitch in time saves nine\u201d with six tokens, the six input variables are x1 = embedding of token \u201cA\u201d, x2 = embedding of token \u201cstitch\u201d, x3 = embedding of token \u201cin\u201d, x4 = embedding of token \u201ctime\u201d, x5 = embedding of token \u201csaves\u201d, and x6 = embedding of token \u201cnine\u201d. Although the dimensions of the embedding for the BERTBASE and BERTLARGE models are different, we can still compute the interactions between the embeddings corresponding to the same token. Specifically, the embedding xi of a token in the BERTBASE model is xi \u2208 R768, and the embedding xi of a token in the BERTLARGE model is xi \u2208 R1024. Step 2: The baseline value for each input variable is b1 = b2 = b3 = b4 = b5 = b6 = special embedding of a masked token, where the special embedding of the masked token is encoded by the BERTBASE and BERTLARGE model, respectively. This special embedding can use the embedding of a special token, e.g., the embedding of the [CLS] token. Specifically, the baseline bi in the BERTBASE model is bi \u2208 R768, and the baseline bi in the BERTLARGE model is bi \u2208 R1024. Step 3: In this case, there are a total of 26 masked samples xT0 ,xT1 ,xT2 ,xT3 , \u00b7 \u00b7 \u00b7 ,xT62 ,xT63 used for the model inference. Specifically, the first masked sample is xT0 = {b1, b2, b3, b4, b5, b6}, where each of its input variables (embedding) is replaced with the corresponding baseline value (embedding of a special token), i.e., xi = bi, i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 6}. The second masked sample is xT1 = {b1, b2, b3, b4, b5, x6}, where its input variable x6 is kept as the original embedding, and the other five input variables are replaced with the corresponding baseline values, i.e., x1 = b1, x2 = b2, x3 = b3, x4 = b4, x5 = b5. The third masked sample is xT2 = {b1, b2, b3, b4, x5, b6}, where its input variable x5 is kept as the original embedding, and the other five input variables are replaced with the corresponding baseline values, i.e., x1 = b1, x2 = b2, x3 = b3, x4 = b4, x6 = b6. The fourth masked sample is xT3 = {b1, b2, b3, b4, x5, x6}, where its input variables x5 and x6 are kept as the original embedding, respectively, and the other four input variables are replaced with the corresponding baseline values. Similarly, the 63rd masked sample is xT62 = {x1, x2, x3, x4, x5, b6}, where its input variables x1, x2, x3, x4, x5 are kept as the original embedding, respectively, and x6 = b6. The 64th masked sample is xT63 = {x1, x2, x3, x4, x5, x6}, where all input variables are kept unchanged from the original embedding.\nStep 4: For each masked sample xTj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63}, we computed the log-odds output of the ground-truth label v(xTj ) = log p(y=y truth|xTj )\n1\u2212p(y=y truth|xTj ) \u2208 R as the model output. In this way, feeding all\nmasked samples xTj into the BERTBASE model produced a total of 2 6 model outputs vBASE(xT0), vBASE(xT1), \u00b7 \u00b7 \u00b7 ,vBASE(xT63). Feeding all masked samples xTj into the BERTLARGE model produced a total of 26 model outputs vLARGE(xT0), v\nLARGE(xT1), \u00b7 \u00b7 \u00b7 ,vLARGE(xT63). Step 5: Computed the AND outputs vBASEand (xTj ) = 0.5vBASE(xTj ) + \u03b3BASETj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} and the OR outputs vBASEor (xTj ) = 0.5v\nBASE(xTj ) \u2212 \u03b3BASETj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} for the BERTBASE model, respectively. Computed the AND outputs vLARGEand (xTj ) = 0.5v LARGE(xTj ) + \u03b3 LARGE Tj\n, j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} and the OR outputs vLARGEor (xTj ) = 0.5vLARGE(xTj ) \u2212 \u03b3LARGETj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} for the BERTLARGE model, respectively.\nStep 6: Computed the AND interactions IBASEand (Tj |x) = \u2211 T\u2286Tj (\u22121) |Tj |\u2212|T |vBASEand (xT ), j \u2208\n{0, 1, \u00b7 \u00b7 \u00b7 , 63} and the OR interactions IBASEor (Tj |x) = \u2212 \u2211 T\u2286Tj (\u22121) |Tj |\u2212|T |vBASEor (xN\\T ), j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} for the BERTBASE model, respectively. Computed the AND interactions ILARGEand (Tj |x) = \u2211 T\u2286Tj (\u22121)\n|Tj |\u2212|T |vLARGEand (xT ), j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} and the OR interactions ILARGEor (Tj |x) = \u2212 \u2211 T\u2286Tj (\u22121)\n|Tj |\u2212|T |vLARGEor (xN\\T ), j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} for the BERTLARGE model, respectively.\nStep 7: Learned the parameters \u03b3BASETj and \u03b3 LARGE Tj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} using the loss in Equation (6). Went back to Step 5 and repeated the iterations until the loss converges.\nStep 8: Computed the final AND interactions IBASEand (Tj |x) and the final OR interactions IBASEor (Tj |x), j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} for the BERTBASE model using the learnable parameters \u03b3BASETj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63}. The final salient interactions of the BERTBASE model are obtained from all 26 interactions, where the final salient interactions are those with interaction values greater than the threshold \u03c4BASE, \u2126BASE = {Tj \u2286 N : |IBASEand/or(Tj |x)| > \u03c4BASE}.\nComputed the final AND interactions ILARGEand (Tj |x) and the final OR interactions ILARGEor (Tj |x), j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63} for the BERTLARGE model using the learnable parameters \u03b3LARGETj , j \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 63}. The final salient interactions of the BERTLARGE model are obtained from all 26 interactions, where the final salient interactions are those with interaction values greater than the threshold \u03c4LARGE, \u2126LARGE = {Tj \u2286 N : |ILARGEand/or (Tj |x)| > \u03c4LARGE}."
        },
        {
            "heading": "P MORE EXPERIMENTS",
            "text": "P.1 OPTIMIZING THE LOSS IN EQUATION (4) MAY LEAD TO DIVERSE SOLUTIONS\nIn Section 2.3.1, we mentioned that optimizing the loss in Equation (4) may lead to diverse solutions. Given different initial states, optimizing the loss in Equation (4) usually extracted two different sets of AND-OR interactions. In particular, as shown in Table 2, when we set the initial state of two sets of parameters {\u03b3T } as \u03b3T \u223c N (0, 1), the loss in Equation (4) would learned dramatically different interactions with only 21% overlap. Figure 12 further shows the top 5 AND-OR interaction primitives extracted from BERTLARGE model on an input sentence. Both experiments illustrate that given different initial states, the loss in Equation (4) may learn different AND-OR interactions.\nP.2 EXAMINING WHETHER THE INTERACTIONS CAN EXPLAIN THE NETWORK OUTPUT\nIn Section 2.3.2, we mentioned to conduct experiments to examine whether the extracted AND-OR interactions could still accurately explain the network output, when we removed the error term. Figure 13 shows matching errors of all masked samples for all subsets T \u2286 N , when we sorted the network outputs for all 2n masked samples in a descending order. It shows that the real network output was well approximated by interactions."
        },
        {
            "heading": "Q EXPERIMENTAL DETAILS",
            "text": "Q.1 PERFORMANCE OF DNNS\nIn Section 3, we conducted experiments using several DNNs trained on different types of datasets, including the language and image datasets. In the sentiment classification task, we fintuned the pretrained models, BERTBASE and BERTLARGE, using the SST-2 dataset. For image classification task, we trained ResNet-20 and VGG-16 with the MNIST-3 dataset. Table 3 reports the classification accuracy of the aforementioned DNNs. For the dialogue task, we used the pretrained models, the LLaMA model and OPT-1.3B model, directly.\nQ.2 THE SELECTION OF INPUT VARIABLES FOR INTERACTION EXTRACTION\nThis section discusses the selection of input variables for extracting interactions. As mentioned in Section 2.2, given an input sample x with n input variables, we can extracted at most 2n interactions. Therefore, the computational cost for extracting interactions increases exponentially with the number of input variables. For example, if we take a word in a sentence (or a pixel in an image) as an input variable, the computation is usually inapplicable. To alleviate this issue, we followed (Shen et al., 2023) to select a set of words as input variables and leave other words as the constant background to compute interactions between them. Specifically, we selected 8-10 input variables for each sample in the three tasks. We only extracted the interactions between the selected variables, leaving the rest of the unselected input variables unchanged as background.\n\u2022 For sentences in the SST-2 dataset, we first tokenized the input sentences and selected tokens as input variables for 200 samples. For each sentence in this dataset, some words have no clear semantics for sentiment classification, i.e., stop words containing dummy words and pronouns, and consequently, there is little interaction within their corresponding tokens. Therefore, we bypassed\nthese tokens without clear semantics, and only selected tokens among the remaining semantic tokens. To facilitate analysis, we randomly selected n = 10 tokens as input variables for sentences with more than 10 semantic tokens. The masking of input variables was performed at the embedding level.\n\u2022 For sentences in the SQuAD dataset, we take first several words in each document as the input sample to the DNN. Specifically, we selected the first 30 words in each document as the input sample and the 31st word as the target word, provided that the following conditions were met: 1) The 31st word possessed clear semantic meaning, which means that it did not belong to the category of stop words. 2) The five words immediately preceding the target word did not constitute sentence-ending punctuation marks, such as a full stop. If either of these conditions was not satisfied, we extended the initial 30 words until all requirements were fulfilled, and selected the next word as the target word. When extracting interactions, we randomly selected a set of n = 10 words which have semantic meanings as input variables. It\u2019s worth noting that a single word can correspond to multiple tokens, so when we masked a specific word, we masked all of its corresponding tokens. The masking of input variables was performed at the embedding level.\n\u2022 For images in the MNIST-3 dataset, we manually labeled the semantic part for 100 positive samples (digit 3) by following (Li & Zhang, 2023b). For each image in this dataset, most of the pixels are black background pixels with no semantic information, and consequently, there is no interaction within these black pixels. Thus, we considered interactions only within foreground pixels. Specifically, we divided a whole image into small patches of size 3 \u00d7 3 and selected n = 8 patches in the foreground of an image as input variables. Following (Li & Zhang, 2023b), we used the zero patches as the baseline values to mask the unselected patches i \u2208 N \\ T in the sample."
        }
    ],
    "year": 2023
}