{
    "abstractText": "Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in highstakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. In particular, currently used bottom-up approaches that focus on the values of the time series at specific time points (usually regularly spaced) do not provide a holistic understanding of the entire time series. This limits the applicability of ML in many critical areas. To open up these domains for ML, we propose a top-down framework of bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. Applying this framework, we develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. Through a series of experiments, we demonstrate the efficacy and interpretability of our approach, paving the way for more transparent and reliable applications of ML in various domains.",
    "authors": [],
    "id": "SP:333f5a1b61b975a26bb46bcc5247936c9404e1cd",
    "references": [
        {
            "authors": [
                "B.S. Aakash",
                "JohnPatrick Connors",
                "Michael D. Shields"
            ],
            "title": "Stress-strain data for aluminum 6061T651 from 9 lots at 6 temperatures under uniaxial and plane strain tension",
            "venue": "Data in Brief,",
            "year": 2019
        },
        {
            "authors": [
                "Takuya Akiba",
                "Shotaro Sano",
                "Toshihiko Yanase",
                "Takeru Ohta",
                "Masanori Koyama"
            ],
            "title": "Optuna: A next-generation hyperparameter optimization framework",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "Ahmed M Alaa",
                "Mihaela van der Schaar"
            ],
            "title": "Attentive State-Space Modeling of Disease Progression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Elaine Angelino",
                "Nicholas Larus-Stone",
                "Daniel Alabi",
                "Margo Seltzer",
                "Cynthia Rudin"
            ],
            "title": "Learning Certifiably Optimal Rule Lists for Categorical Data",
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2018
        },
        {
            "authors": [
                "Alejandro Barredo Arrieta",
                "Natalia D\u0131\u0301az-Rodr\u0131\u0301guez",
                "Javier Del Ser",
                "Adrien Bennetot",
                "Siham Tabik",
                "Alberto Barbado",
                "Salvador Garcia",
                "Sergio Gil-Lopez",
                "Daniel Molina",
                "Richard Benjamins",
                "Raja Chatila",
                "Francisco Herrera"
            ],
            "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
            "venue": "Information Fusion,",
            "year": 2020
        },
        {
            "authors": [
                "L. Biggio",
                "T. Bendinelli",
                "A. Neitz",
                "A. Lucchi",
                "G. Parascandolo"
            ],
            "title": "Neural Symbolic Regression that Scales",
            "venue": "In 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Paul Blanchard",
                "Robert L. Devaney",
                "Glen R. Hall"
            ],
            "title": "Differential Equations",
            "venue": "Cengage Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Thomas F. Brooks",
                "D. Stuart Pope",
                "Michael A. Marcolini"
            ],
            "title": "Airfoil self-noise and prediction",
            "year": 1989
        },
        {
            "authors": [
                "Steven L. Brunton",
                "Joshua L. Proctor",
                "J. Nathan Kutz"
            ],
            "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "C. Melody Carswell",
                "Cathy Emery",
                "Andrea M. Lonon"
            ],
            "title": "Stimulus complexity and information integration in the spontaneous interpretations of line graphs",
            "venue": "Applied Cognitive Psychology,",
            "year": 1993
        },
        {
            "authors": [
                "Jiahui Chen",
                "Yuan Wan",
                "Xiaoyu Wang",
                "Yinglv Xuan"
            ],
            "title": "Learning-based shapelets discovery by feature selection for time series classification",
            "venue": "Applied Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "XGBoost: A Scalable Tree Boosting System",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
            "year": 2016
        },
        {
            "authors": [
                "Erin Chung",
                "Jonathan Sen",
                "Priya Patel",
                "Winnie Seto"
            ],
            "title": "Population Pharmacokinetic Models of Vancomycin in Paediatric Patients: A Systematic Review",
            "venue": "Clinical Pharmacokinetics,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Crabb\u00e9",
                "Mihaela Van Der Schaar"
            ],
            "title": "Explaining Time Series Predictions with Dynamic Masks",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Crabbe",
                "Zhaozhi Qian",
                "Fergus Imrie",
                "Mihaela van der Schaar"
            ],
            "title": "Explaining Latent Representations with a Corpus of Examples",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Miles Cranmer"
            ],
            "title": "PySR: Fast & parallelized symbolic regression in Python/Julia",
            "venue": "Zenodo,",
            "year": 2020
        },
        {
            "authors": [
                "St\u00e9phane D\u2019Ascoli",
                "Pierre-Alexandre Kamienny",
                "Guillaume Lample",
                "Francois Charton"
            ],
            "title": "Deep symbolic regression for recurrence prediction",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Carl De Boor"
            ],
            "title": "A Practical Guide to Splines, volume 27",
            "venue": "springer-verlag New York,",
            "year": 1978
        },
        {
            "authors": [
                "Brian de Silva",
                "Kathleen Champion",
                "Markus Quade",
                "Jean-Christophe Loiseau",
                "J. Kutz",
                "Steven Brunton"
            ],
            "title": "PySINDy: A Python package for the sparse identification of nonlinear dynamical systems from data",
            "venue": "Journal of Open Source Software,",
            "year": 2020
        },
        {
            "authors": [
                "Angela Dispenzieri",
                "Jerry A. Katzmann",
                "Robert A. Kyle",
                "Dirk R. Larson",
                "Terry M. Therneau",
                "Colin L. Colby",
                "Raynell J. Clark",
                "Graham P. Mead",
                "Shaji Kumar",
                "L. Joseph Melton",
                "S. Vincent Rajkumar"
            ],
            "title": "Use of Nonclonal Serum Immunoglobulin Free Light Chains to Predict Overall Survival in the General Population",
            "venue": "Mayo Clinic Proceedings,",
            "year": 2012
        },
        {
            "authors": [
                "Adeline Fermanian",
                "Terry Lyons",
                "James Morrill",
                "Cristopher Salvi"
            ],
            "title": "New Directions in the Applications of Rough Path Theory",
            "venue": "IEEE BITS the Information Theory Magazine, pp",
            "year": 2023
        },
        {
            "authors": [
                "Penglei Gao",
                "Xi Yang",
                "Rui Zhang",
                "Kaizhu Huang",
                "John Y. Goulermas"
            ],
            "title": "Explainable Tensorized Neural Ordinary Differential Equations for Arbitrary-Step Time Series Prediction",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
            "venue": "International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Bryce Goodman",
                "Seth Flaxman"
            ],
            "title": "European Union Regulations on Algorithmic Decision-Making and a \u201cRight to Explanation",
            "venue": "AI Magazine,",
            "year": 2017
        },
        {
            "authors": [
                "Yi Rang Han",
                "Ping I. Lee",
                "K. Sandy Pang"
            ],
            "title": "Finding Tmax and Cmax in Multicompartmental Models",
            "venue": "Drug Metabolism and Disposition,",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Hao",
                "Huiping Cao"
            ],
            "title": "A New Attention Mechanism to Classify Multivariate Time Series",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani"
            ],
            "title": "Generalized additive models",
            "venue": "Statistical Science,",
            "year": 1986
        },
        {
            "authors": [
                "Samuel Holt",
                "Zhaozhi Qian",
                "Mihaela van der Schaar"
            ],
            "title": "Deep Generative Symbolic Regression",
            "venue": "The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Xiyang Hu",
                "Cynthia Rudin",
                "Margo Seltzer"
            ],
            "title": "Optimal Sparse Decision Trees",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Kadierdan Kaheman",
                "J. Nathan Kutz",
                "Steven L. Brunton"
            ],
            "title": "SINDy-PI: A robust algorithm for parallel implicit sparse identification of nonlinear dynamics",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Alan A. Kaptanoglu",
                "Brian M. de Silva",
                "Urban Fasel",
                "Kadierdan Kaheman",
                "Andy J. Goldschmidt",
                "Jared Callaham",
                "Charles B. Delahunt",
                "Zachary G. Nicolaou",
                "Kathleen Champion",
                "Jean-Christophe Loiseau",
                "J. Nathan Kutz",
                "Steven L. Brunton"
            ],
            "title": "PySINDy: A comprehensive Python package for robust sparse system identification",
            "venue": "Journal of Open Source Software,",
            "year": 2022
        },
        {
            "authors": [
                "Amir-Hossein Karimi",
                "Gilles Barthe",
                "Borja Balle",
                "Isabel Valera"
            ],
            "title": "Model-Agnostic Counterfactual Explanations for Consequential Decisions",
            "venue": "International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "TieYan Liu"
            ],
            "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas",
                "Rory Sayres"
            ],
            "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "William La Cava",
                "Patryk Orzechowski",
                "Bogdan Burlacu",
                "Fabr\u0131\u0301cio Olivetti de Fran\u00e7a",
                "Marco Virgolin",
                "Ying Jin",
                "Michael Kommenda",
                "Jason H. Moore"
            ],
            "title": "Contemporary Symbolic Regression Methods and their Relative Performance",
            "venue": "In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks,",
            "year": 2021
        },
        {
            "authors": [
                "Kin Kwan Leung",
                "Clayton Rooke",
                "Jonathan Smith",
                "Saba Zuberi",
                "Maksims Volkovs"
            ],
            "title": "Temporal Dependencies in Feature Importance for Time Series Prediction",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Q. Vera Liao",
                "Daniel Gruen",
                "Sarah Miller"
            ],
            "title": "Questioning the AI: Informing Design Practices for Explainable AI User Experiences",
            "venue": "In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bryan Lim",
                "Arik Sercan",
                "Nicolas Loeff",
                "Tomas Pfister"
            ],
            "title": "Temporal Fusion Transformers for interpretable multi-horizon time series forecasting",
            "venue": "International Journal of Forecasting,",
            "year": 2021
        },
        {
            "authors": [
                "Zichao Long",
                "Yiping Lu",
                "Bin Dong"
            ],
            "title": "PDE-Net 2.0: Learning PDEs from data with a numericsymbolic hybrid deep network",
            "venue": "Journal of Computational Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Yin Lou",
                "Rich Caruana",
                "Johannes Gehrke"
            ],
            "title": "Intelligible models for classification and regression",
            "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Terry Lyons"
            ],
            "title": "Rough paths, Signatures and the modelling of functions on streams",
            "year": 2014
        },
        {
            "authors": [
                "Daniel A. Messenger",
                "David M. Bortz"
            ],
            "title": "Weak SINDy: Galerkin-Based Data-Driven Model Selection",
            "venue": "Multiscale Modeling & Simulation,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel A. Messenger",
                "David M. Bortz"
            ],
            "title": "Weak SINDy for partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Dr Mould",
                "Rn Upton"
            ],
            "title": "Basic Concepts in Population Modeling, Simulation, and Model-Based Drug Development",
            "venue": "CPT: Pharmacometrics & Systems Pharmacology,",
            "year": 2012
        },
        {
            "authors": [
                "Harsha Nori",
                "Samuel Jenkins",
                "Paul Koch",
                "Rich Caruana"
            ],
            "title": "InterpretML: A unified framework for machine learning interpretability",
            "venue": "arXiv preprint arXiv:1909.09223,",
            "year": 2019
        },
        {
            "authors": [
                "Qingyi Pan",
                "Wenbo Hu",
                "Jun Zhu"
            ],
            "title": "Series Saliency: Temporal Interpretation for Multivariate Time Series Forecasting",
            "year": 2020
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine Learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Sebastian P\u00f6lsterl"
            ],
            "title": "Scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikitlearn",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Kristopher J. Preacher",
                "Gregory R. Hancock"
            ],
            "title": "Meaningful aspects of change as novel random coefficients: A general method for reparameterizing longitudinal models",
            "venue": "Psychological Methods,",
            "year": 2015
        },
        {
            "authors": [
                "Liudmila Prokhorenkova",
                "Gleb Gusev",
                "Aleksandr Vorobev",
                "Anna Veronika Dorogush",
                "Andrey Gulin"
            ],
            "title": "CatBoost: Unbiased boosting with categorical features",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Garima Pruthi",
                "Frederick Liu",
                "Satyen Kale",
                "Mukund Sundararajan"
            ],
            "title": "Estimating Training Data Influence by Tracing Gradient Descent",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhaozhi Qian",
                "Krzysztof Kacprzyk",
                "Mihaela van der Schaar"
            ],
            "title": "D-CODE: Discovering Closedform ODEs from Observed Trajectories",
            "venue": "The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "L. Rabiner",
                "B. Juang"
            ],
            "title": "An introduction to hidden Markov models",
            "venue": "IEEE ASSP Magazine,",
            "year": 1986
        },
        {
            "authors": [
                "Maziar Raissi",
                "George Em Karniadakis"
            ],
            "title": "Hidden physics models: Machine learning of nonlinear partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Cynthia Rudin"
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Samuel H. Rudy",
                "Steven L. Brunton",
                "Joshua L. Proctor",
                "J. Nathan Kutz"
            ],
            "title": "Data-driven discovery of partial differential equations",
            "venue": "Science Advances,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Schmidt",
                "Hod Lipson"
            ],
            "title": "Distilling Free-Form Natural Laws from Experimental Data",
            "year": 2009
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje"
            ],
            "title": "Learning Important Features Through Propagating Activation Differences",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Shoaib Ahmed Siddiqui",
                "Dominique Mercier",
                "Mohsin Munir",
                "Andreas Dengel",
                "Sheraz Ahmed"
            ],
            "title": "TSViz: Demystification of Deep Learning Models for Time-Series Analysis",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
            "venue": "The journal of machine learning research,",
            "year": 1929
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic Attribution for Deep Networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Sahar Torkamani",
                "Volker Lohweg"
            ],
            "title": "Survey on time series motif discovery",
            "venue": "WIREs Data Mining and Knowledge Discovery,",
            "year": 2017
        },
        {
            "authors": [
                "Silviu-Marian Udrescu",
                "Max Tegmark"
            ],
            "title": "AI Feynman: A physics-inspired method for symbolic regression",
            "venue": "Science Advances,",
            "year": 2020
        },
        {
            "authors": [
                "Berk Ustun",
                "Cynthia Rudin"
            ],
            "title": "Supersparse linear integer models for optimized medical scoring systems",
            "venue": "Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Charles R. Harris",
                "Anne M. Archibald",
                "Ant\u00f4nio H. Ribeiro",
                "Fabian Pedregosa",
                "Paul van"
            ],
            "title": "Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in python",
            "venue": "Nature Methods,",
            "year": 2020
        },
        {
            "authors": [
                "Julia Wilkerson",
                "Kald Abdallah",
                "Charles Hugh-Jones",
                "Greg Curt",
                "Mace Rothenberg",
                "Ronit Simantov",
                "Martin Murphy",
                "Joseph Morrell",
                "Joel Beetsch",
                "Daniel J Sargent",
                "Howard I Scher",
                "Peter Lebowitz",
                "Richard Simon",
                "Wilfred D Stein",
                "Susan E Bates",
                "Tito Fojo"
            ],
            "title": "Estimation of tumour regression and growth rates during treatment in patients with advanced prostate cancer: A retrospective analysis",
            "venue": "ISSN 1470-2045",
            "year": 2017
        },
        {
            "authors": [
                "Jean-Baptiste Woillard",
                "Brenda C.M. de Winter",
                "Nassim Kamar",
                "Pierre Marquet",
                "Lionel Rostaing",
                "Annick Rousseau"
            ],
            "title": "Population pharmacokinetic model and Bayesian estimator for two tacrolimus formulations\u2013twice daily Prograf and once daily Advagraf",
            "venue": "British Journal of Clinical Pharmacology,",
            "year": 2011
        },
        {
            "authors": [
                "Xiaoming Xi"
            ],
            "title": "Aspects of performance on line graph description tasks: Influenced by graph familiarity and different task features",
            "venue": "Language Testing,",
            "year": 2010
        },
        {
            "authors": [
                "Lexiang Ye",
                "Eamonn Keogh"
            ],
            "title": "Time series shapelets: A new primitive for data mining",
            "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2009
        },
        {
            "authors": [
                "\u00c7a\u011fatay Y\u0131ld\u0131z",
                "Markus Heinonen",
                "Harri L\u00e4hdesm\u00e4ki"
            ],
            "title": "Continuous-Time Model-Based Reinforcement Learning, June 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Zacks",
                "Barbara Tversky"
            ],
            "title": "Bars and lines: A study of graphic communication",
            "venue": "Memory & Cognition,",
            "year": 1999
        },
        {
            "authors": [
                "Matthew D. Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and Understanding Convolutional Networks",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Hui Zou",
                "Trevor Hastie"
            ],
            "title": "Regularization and Variable Selection Via the Elastic Net",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2005
        },
        {
            "authors": [
                "Ghorbani",
                "Zou",
                "Pruthi et al",
                "Crabbe"
            ],
            "title": "2021) that identify important training samples, and concept-based explanations",
            "venue": "(Kim et al.,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Why do we need transparent models? eXplainable Artificial Intelligence (XAI) methods are broadly divided into transparent models and post-hoc explanation techniques (Barredo Arrieta et al., 2020). Transparent (also called glass box) models are crucial in many settings involving high-stakes decisions such as healthcare or credit scoring (Rudin, 2019). As these models are interpretable by design, they are by themselves understandable. Such understanding, apart from being mandated by certain regulatory bodies (Goodman & Flaxman, 2017), is needed, for instance, to improve robustness, detect biases, evoke trust, or certify model compliance with legislation (Barredo Arrieta et al., 2020). Many transparent machine learning models that issue static predictions have been proposed. That includes Linear/Logistic Regression, Generalized Additive Models (Hastie & Tibshirani, 1986), and Decision Trees (Hu et al., 2019). By definition, these methods are not directly applicable to time series forecasting\u2014when we want to predict a whole trajectory rather than a single label. Although they can be adapted, they exhibit poor performance (see Section 7).\nChallenges of time series forecasting: limitations of a bottom-up approach. In contrast to a single-label output (as in classification or regression), understanding the change in the trajectory is more complicated as it is an entire function (described by many values). As time series forecasting remains an under-studied field of XAI (Barredo Arrieta et al., 2020), current techniques usually resolve to a bottom-up approach. This means they focus on the values of the trajectory at individual time points (usually regularly spaced). For instance, the importance scores in saliency methods are calculated for different prediction horizons (Leung et al., 2023). This may be sufficient when we are interested in a particular time point (e.g., 5-year survival rate), but we often want to comprehend the whole trajectory at once. For instance, when administering a drug, we may be less interested in the concentration of the drug every few hours but rather in understanding the entire curve, including properties like the peak plasma concentration and the time when it is achieved (Han et al., 2018).\nBi-level transparency for time series forecasting: a top-down approach. We propose a top-down approach to trajectory comprehension and consequently two levels of transparency for time series forecasting: (level 1) understanding how the trend (the general shape of the trajectory) changes as we modify the input, and (level 2) understanding how the properties of the current trend (e.g., minimum value) change as we modify the input. To illustrate this, let us consider the following example.\nForecasting a tumor volume trajectory from patient\u2019s baseline covariates and drug dose\nUnderstanding a model like this may include answering questions such as: (Liao et al., 2020)\n\u2022 What if: \u201cWhat would happen to the model\u2019s prediction if a specific covariate changes?\u201d \u2022 How to be that: \u201cHow should the covariates be modified to get a different prediction?\u201d \u2022 How to still be this: \u201cWhat range of drug dose values keeps the prediction the same?\u201d\nWe characterize the difference between predicted trajectories on two levels, which enables us to answer concrete questions about each level such as\nLevel 1 (trends) Level 2 (properties)\n\u201cWould the predicted tumor volume keep decreasing if we adjusted the treatment?\u201d\n\u201cWhat feature changes would lower the minimum tumor volume?\u201d\nWe explain trends and properties in detail in Section 2 and then formalize them in Section 4. It is worth noting that answering such questions with the current bottom-up approaches may often be futile since the notion of a \u201cdifferent prediction\u201d (based on individual time points or norms such as Lp) may be non-interpretable (see Section 2) or simplistic. We demonstrate how our framework can enable answering such questions in Figure 1 and more thoroughly in Appendix E.1.\nTime series forecasting based on static features. Our work focuses on understanding the change in the predicted trajectory. However, time series models take many types of inputs, including time series and exogenous features. To provide a clear exposition of our framework, we focus on one specific input type: static features. Time series forecasting based on static features has applications in many domains ranging from finance through medicine and pharmacology to physics (see Section 3). In Section 5, we introduce TIMEVIEW\u2014a transparent ML model for time series forecasting based on static features. As with many transparent models (e.g., GAMs, decision trees), model visualization is crucial for its interpretability. In Section 7, we demonstrate a visualization tool based on interactive plots that allows for the examination of both the higher and lower-level features of the predicted trajectories and how they change based on the input (Figure 1).\nContributions. We introduce bi-level transparency, a novel top-down framework for time series forecasting that allows for a holistic understanding of the entire trajectory through trends and properties (Section 2). We formalize it by introducing the notions of motifs and compositions (Section 4). Based on the new formalism, we develop TIMEVIEW, Time series Interpretable Model with Effective VIsualization (Section 5). We demonstrate how its visualization aids in model comprehension while exhibiting only a minor performance drop compared to black-box models (Section 7)."
        },
        {
            "heading": "2 TRANSPARENCY FOR TIME SERIES FORECASTING",
            "text": ""
        },
        {
            "heading": "2.1 SETUP",
            "text": "Time series forecasting. A general ML model is a function f mapping samples from the input space X to the output space Y . We say that f issues static predictions when Y is a subset of R (a regression model) or a finite set of labels {1, . . . ,K} (a classification model).1 In contrast, we define f to be a time series forecasting model (or just forecasting model) when Y is a space of trajectories. A trajectory is a function y : T \u2192 R, where T \u2282 R is a set of time points. Although the conceptual framework in Section 2 is agnostic to the nature of T , our work focuses on settings where T is an interval [0, T ] \u2282 R, where T \u2208 R is a time horizon, and the underlying trajectory is continuous. Note, in practice, we only observe discrete samples of y, which may be irregular and noisy.\nTransparency. We assume the following general definition of transparency: ML model is transparent if we can understand the impact of the inputs on the prediction. In particular, how changing one (or a few) of the features would impact the output. This is crucial for counterfactual reasoning (e.g., \u201cWhat would the model predict if the patient was female?\u201d) or detecting anomalies (e.g., \u201cWhy does the model assign a significantly higher risk score if the patient\u2019s age is changed from 64 to 65?\u201d).\nComprehending the change in the output. As discussed in Section 1, understanding the change in the output is crucial for answering important questions about the model (e.g., \u201cWhat would happen to the model\u2019s prediction if a specific feature changes?\u201d). In a static setting, when the prediction is a single label, understanding the change in the output is relatively straightforward as only a few things can happen. In regression, the target variable can decrease, increase, or remain constant. In classification, the target variable can change from one option to another among a finite number of classes. In time series forecasting, when the prediction is a trajectory, understanding the change in the output is challenging because there are numerous ways a function can change (discussed further in Appendix E). Moreover, these changes need to be interpretable for humans."
        },
        {
            "heading": "2.2 BOTTOM-UP: CURRENT XAI APPROACH TO TRAJECTORY COMPREHENSION",
            "text": "As trajectory is a function y : T \u2192 R, current XAI techniques for time series forecasting focus on understanding the impact of the inputs on y(t) for a particular t \u2208 T . For instance, the values in saliency methods are calculated independently for different prediction horizons (Leung et al., 2023) (and might be later aggregated). Inspired by the motivations of rough path theory (Lyons, 2014; Fermanian et al., 2023), we call the current comprehending strategy bottom-up. It means the trajectory is understood by looking at its values at individual time points, and subsequently, more information is gained by looking at more points2. However, we argue that this strategy for trajectory understanding is not optimal in many scenarios. In particular, it is not a natural way for people to understand trajectories, and it is challenging to convey time-varying trends and global features by simply looking at individual time points in isolation.\nInconsistent with the natural way people understand trajectories. Standard representation of a trajectory y : [0, T ] \u2192 R is a line graph. Research on graph comprehension (Zacks & Tversky, 1999; Xi, 2010) suggests that people understand line graphs in terms of trends rather than individual values. For instance, \u201cwhen x increases, y also increases\u201d. They also tend to focus on the minimum and maximum values and trend reversals (Carswell et al., 1993). Thus, understanding a (continuous) trajectory by individual values is unnatural for humans. See Appendix E for more details.\nIncreased cognitive load. As mentioned above, the bottom-up approach requires an increasing number of values to understand the trajectory better. This becomes problematic when we want to understand any change in the trajectory, as it places the cognitive burden on the human interpreter to piece together changes in trends from changes at individual time steps.\nUnsuitable for global features. A bottom-up approach may be sufficient when we are interested in a particular time point (e.g., 5-year survival rate) or when there are only a few time points of interest. However, we often want to comprehend the whole trajectory at once. For instance, when administering a drug, we are interested in understanding the entire drug concentration curve, including properties like peak plasma concentration and the time when it is achieved (Han et al., 2018).\n1Another example of a model issuing static predictions is a multi-output regression where Y \u2282 RK . 2Note, \u201dbottom-up\u201d refers to how the trajectory is comprehended, not how the prediction is generated"
        },
        {
            "heading": "2.3 TOP-DOWN: NEW APPROACH TO TRAJECTORY COMPREHENSION",
            "text": "To address the shortcomings of the bottom-up approach, we propose a top-down approach to understanding a trajectory. It is motivated by the fact that humans tend to describe trajectories by referring to the trends and properties it exhibits rather than just the values it attains (Carswell et al., 1993). Consider the natural language descriptions of trajectories presented in Table 1. In all these examples, we have a trend\u2014the general shape of the function (e.g., \u201cincreasing\u201d, \u201cstays below\u201d), and properties\u2014the details of the particular trend (e.g., \u201cfor the last 10 years\u201d, \u201cbelow 100mg/dl\u201d).\nThe top-down approach addresses shortcomings of the bottom-up approach, i.e., it is more consistent with the natural way people understand trajectories and conveys time-varying trends and global features in an interpretable way. Moreover, it is also compatible with the scientific approach to analyzing various trajectories. For instance, while studying dynamical systems, we are often interested in understanding bifurcations\u2014a qualitative change in the behavior of a system as the parameter changes (Blanchard et al., 2012). This corresponds to understanding the inputs where the trend of the trajectory changes.\nBi-level transparency: understanding how the trends and properties change. By using the topdown approach above, we do not need all the trajectory values to understand it. Instead, we can focus on the trends and properties of the trajectory and only access the exact values when necessary. This is how we can achieve an interpretable model: instead of tracking the individual values of the trajectory (as in bottom-up approaches), we track how the trends and properties of the trajectory change as we vary the input. Thus, we refine the definition of transparency and adapt it specifically for time series forecasting. We call it bi-level transparency.\nA time series forecasting model is (bi-level) transparent if the following holds.\n\u2022 (Level 1) We can understand the impact of the input on the trends of the trajectory. \u2022 (Level 2) We can understand the impact of the input on the properties of a given trend."
        },
        {
            "heading": "3 TIME SERIES FORECASTING FROM STATIC FEATURES",
            "text": "Bi-level transparency unweaves the \u201coutput\u201d part of transparency into two separate objects: trends and properties. Thus, it provides a concrete answer to the question: what does it mean to understand the change of the output? However, time series models may take many types of inputs, including static features, information about the future (e.g., upcoming holiday dates), and other exogenous time series (Lim et al., 2021). To provide a clear exposition of our framework, develop formalism, and demonstrate a practical implementation, we focus on settings where inputs are static features.\nReal life settings. Time series forecasting from static features is frequently encountered in medicine and pharmacology, where we are interested in predicting the disease progression or the drug concentration based on the patient\u2019s covariates. Static features can also include the dosage/strength of the treatment or even the time and type of intervention. If necessary, one or a few initial observations at pre-specified times can also be considered to be static features. More examples of such scenarios can be found in finance (predicting stock values from the company\u2019s static data), time-to-event problems (predicting the survival or the hazard function), or modeling any 1D dynamical system from\nits initial conditions. In some scientific or engineering domains, time can be even replaced by other continuous variables. For instance, when modeling stress-strain or current-voltage curves.\nProblem formulation. Let T \u2208 R be the time horizon. Each sample consists of static features x(d) \u2208 RM , whereM \u2208 N is the number of features, and a discretely sampled trajectory y(d) \u2208 RNd at times t(d) \u2208 RNd , where Nd \u2208 N is the number of measurements for the dth sample. We assume that y(d) consists of noisy samples of some true underlying continuous trajectory y(d)\u2217 : [0, T ]\u2192 R. Given a dataset {x(d),y(d), t(d)}Dd=1, the task is to find a model that matches static covariates x \u2208 RM to a trajectory y\u0302 : [0, T ]\u2192 R such that y\u0302 minimizes the expected value of 1T \u222b T 0 (y\u0302(t)\u2212y\u2217(t))2dt for all test samples. We denote the class of predicted trajectories as Y\u0302 ."
        },
        {
            "heading": "4 MOTIFS AND COMPOSITIONS",
            "text": "In this section, we propose a way to formalize the notion of a trend by defining the composition of a trajectory. The composition is a sequence of motifs where each motif describes the current \u201cshape\u201d of the trajectory at a specific interval. For instance, we can choose a set of three motifs: \u201cincreasing\u201d (i), \u201cdecreasing\u201d (d), and \u201cconstant\u201d (c). Then, we can divide a trajectory into a few segments, so each can be classified as being in one of these motifs throughout the interval. Thus, we can assign a sequence of motifs to this trajectory - a composition. For instance, a ReLU function on [-1,1] has a composition (\u201cconstant\u201d, \u201cincreasing\u201d) or just (c, i), whereas a sin on the interval [0, 2\u03c0] has a composition (i, d, i). The motifs can be chosen based on the application and the required granularity. The points between motifs are called transition points, and their coordinates can be mapped to the properties of a trend (see Figure 2)\nNotation. We say I is an interval (of R) if it is an open interval, closed interval, or half-closed interval. The interval has to contain more than one point. We denote the set of all intervals on R as I. Let c \u2208 R, we denote the shifted interval as I + c = {x + c | x \u2208 I}. Let I \u2282 R be any interval, we call any function f : I \u2192 R an interval function and we denote its domain as dom(f). We denote the set of all interval functions as F . Definition 1 (Motif). A motif s is a binary relation between the set of interval functions F and the set of intervals I (i.e., s \u2282 F \u00d7 I = {(f, I) | f \u2208 F , I \u2208 I}). We denote (f, I) \u2208 s as f |I \u223c s and read it as \u201cf on I has a motif s\u201d. Each motif s needs to be:\n\u2022 well-defined, i.e., for any f \u2208 F , and any I \u2208 I, f |I \u223c s =\u21d2 I \u2286 dom(f) (1)\n\u2022 translation-invariant, i.e., for any I \u2208 I, and any f \u2208 F , f |I \u223c s \u21d0\u21d2 f \u25e6 (x\u2212 c)|(I + c) \u223c s \u2200c \u2208 R (2)\nNow, we would like to assign a minimal sequence of motifs to a given trajectory: a composition. Definition 2 (Composition). Let f : I \u2192 R be an interval function and S be a set of motifs. A motif sequence of f in S is a finite sequence of motifs (s1, . . . , sd), such that there exists an interval partition3 (I1, . . . , Id) of I such that f |Ij \u223c sj \u2200j \u2208 [d]. A composition of f in S is the shortest motif sequence of f in S. The points between the intervals are called the transition points. The set of all compositions for a given set of motifs S is denoted by CS . A set of motifs S is called compatible with a subset F \u2032 \u2282 F if for every f \u2208 F \u2032 there exists a unique composition, denoted CS [f ]\nCompatibility between the set of motifs and the set of trajectories is crucial for an ML model that employs bi-level transparency as we want to assign a composition to every possible prediction unambiguously and, in turn, to every feature vector. We call this assignment a composition map.\n3For a definition of interval partition, see Appendix A\nDefinition 3 (Composition map). Let a set of motifs S be compatible with some subset F \u2032 \u2282 F . Let g : RM \u2192 F \u2032 be an ML model for time series forecasting, where M \u2208 N is the number of static features. A composition map is denotedMS : RM \u2192 CS defined byMS(x) = CS [g(x)].\nTo understand a model g, it is crucial to understand its composition map with respect to some meaningful set of motifs. We discuss examples of motifs and when they can be helpful in Appendix A. We define a particular set of motifs that we call dynamical motifs (see Table 2). They encode information about the trajectory\u2019s first and second derivatives. Moreover, the transition points between these motifs correspond to local minima, maxima, and inflection points. These are the exact properties used in a standard mathematical exercise of function sketching whose goal is precisely to understand the function. These motifs form a backbone of TIMEVIEW introduced in Section 5. Dynamical motifs are depicted in Table 2 and defined formally in Example 5 in Appendix A."
        },
        {
            "heading": "5 TIMEVIEW",
            "text": "Based on our formalism in Section 4, we introduce Time series Interpretable Model with Effective VIsualization (TIMEVIEW). This framework consists of two parts: the predictive model based on B-Spline basis functions, and an algorithm for calculating the composition map. This map aims to facilitate model visualization that complements our framework and is demonstrated in Section 7.\nRealizing bi-level transparency through dynamical motifs. To realize bi-level transparency through dynamical motifs, we need to (1) understand the relation between the feature vectors x and the compositions of the predicted trajectories, and (2) understand the relation between the feature vectors x and the transition points of a given composition. To fulfill these conditions, we need to find a space of trajectories Y\u0302 satisfying the following criteria.\n1. The set of dynamical motifs S is compatible with the class of predicted trajectories Y\u0302 2. For every y\u0302 \u2208 Y\u0302 we can calculate its composition CS [y\u0302] Cubic splines are a class of functions that satisfies both criteria mentioned above. We demonstrate that dynamical motifs are compatible with cubic splines in Appendix B. Moreover, it is easy to calculate the dynamical composition of a cubic spline as it is a piece-wise function consisting of cubic polynomials connected at knots. We describe the exact procedure below and in Appendix C.\nB-Spline basis functions. We describe cubic splines as linear combinations of B-Spline (De Boor, 1978) basis functions. Let \u03d5b : [0, T ] \u2192 R be a bth B-Spline basis function of degree 3. Given a set of B basis functions {\u03d5b}b\u2208[B], we can express a cubic spline as a linear combination y\u0302(t) =\u2211B\nb=1 cb\u03d5b(t), where cb \u2208 R \u2200b \u2208 [B]. Thus, each spline is described by a latent vector c \u2208 RB .\nArchitecture. To match a feature vector x \u2208 RM to a vector c \u2208 RB describing a time series, we use an encoder h : RM \u2192 RB . Ultimately, we define our model g : RM \u2192 Y\u0302 as\ng(x)(t) = y\u0302x(t) = B\u2211 b=1 h(x)b\u03d5b(t) (3)\nImplementation. We implement the encoder h as a fully-connected neural network. We choose a set of knots for the B-Spline basis functions based on the training dataset using a heuristic algorithm described in Appendix C (note, the number of knots controls explainability-performance trade-off). The values of \u03d5b at times td (\u2200b \u2208 [B] \u2200d \u2208 [D]) can be efficiently precomputed before the training using scipy library\u2019s BSpline class. We want to minimize the MSE loss between the predicted values of the trajectory y\u0302 at points td and the ground truth yd. We also add L2 regularization loss LL2, so that the B-Spline coefficients (and thus the compositions) do not change too abruptly. The final objective is:\nL = 1 D D\u2211 d=1  1 Nd Nd\u2211 j=1 ( ydj \u2212 B\u2211 b=1 h(xd)b\u03d5b(t d j ) )2+ \u03b1LL2(g) (4) We minimize it using gradient descent. The block diagram describing the training procedure can be seen in Figure 3. Implementation details, including the pseudocode, can be found in Appendix C.\nAs with many transparent models (e.g., GAMs, Decision Trees), model visualization is crucial for its interpretability. After TIMEVIEW is trained, we compute the composition map (see Definition 3) and demonstrate how we can visualize it (or a part of it) in Section 7. To compute the composition map, we need to perform composition extraction from a predicted trajectory, i.e., calculate CS [y\u0302]. Composition extraction. As described earlier, each trajectory is described by a latent vector c \u2208 RB and defined as a linear combination of B-Splines, y\u0302(t) = \u2211B b=1 cb\u03d5b(t). Each \u03d5b is a piece-wise polynomial defined over the intervals determined by the internal knots (t1, . . . , tB\u22122)4 chosen by our heuristic algorithm (Appendix C). We can associate a cubic in a monomial basis (t3, t2, t, 1) with each of these intervals for each basis function (this can be precomputed). We call these cubics \u03c8b,k, where k ranges from 1 toB\u22123 (the number of intervals). Given a vector c, we can now calculate the cubic in a monomial basis for each interval. The kth interval is just \u2211B b=1 cb\u03c8b,k. As it is just a cubic polynomial, we can readily calculate its first and second derivatives and thus assign a composition to the kth interval. We repeat this process for every other interval, connect all the compositions, and merge some neighboring motifs if they are the same. Ultimately, we get a global composition for the whole y\u0302. See Appendix C for the pseudocode and the block diagram description."
        },
        {
            "heading": "6 RELATED WORKS",
            "text": "We explain how our work intersects with related areas of ML. Refer to Appendix F for more details.\nTransparent models for static predictions. Standard transparent methods for static predictions include linear/logistic regression, scoring systems (Ustun & Rudin, 2016), decision trees/rule lists\n4For B-Splines of degree 3, B \u2212 2 knots produce B basis functions.\n(Angelino et al., 2018; Hu et al., 2019), and generalized additive models (GAMs) (Hastie & Tibshirani, 1986; Lou et al., 2012). Such methods can often be used for time series forecasting by passing the time t as an additional feature. They often satisfy bi-level transparency but have poor performance. In particular, all trajectories predicted by linear regression and GAMs are parallel; thus, they cannot model different trends (Section 7). Decision Trees capture non-additive interactions, enabling flexible forecasting models. However, they require many splits to approximate the ground truth, leading to poor performance or incomprehensibility (Section 7).\nClosed-form expressions. Symbolic Regression (Schmidt & Lipson, 2009; La Cava et al., 2021) aims to fit closed-form expressions to data, i.e., mathematical formulas composed of a finite number of variables, binary operators (+,\u2212,\u00d7,\u00f7), well-known functions (e.g., sin, exp, log), and constants. For instance, sin(x2) \u2212 e2.1y . Differential equations represent another category of mathematical expressions that draw significant interest in the scientific community. Numerous algorithms have been proposed for discovering Ordinary Differential Equations (ODEs) (Brunton et al., 2016; Qian et al., 2022) and Partial Differential Equations (Rudy et al., 2017; Long et al., 2019). Mathematical expressions may not always satisfy bi-level transparency. In fact, the reparametrization of equations to reflect key theoretical quantities is an active area of research (Preacher & Hancock, 2015).\nFeature importance for time series. While our research focuses on transparent models, many saliency (or feature importance) methods have been developed to highlight which features the model is sensitive to (Ribeiro et al., 2016; Lundberg & Lee, 2017). Although these methods have been extended to time series inputs (Crabbe\u0301 & Schaar, 2021; Leung et al., 2023), limited work has been done to extend them specifically to time series outputs. Current XAI techniques either assume the output is a scalar (Siddiqui et al., 2019) (e.g., time series classification (Hao & Cao, 2020)), treat the trajectory as a single object (Gao et al., 2023)\u2014thus do not show how a feature changes the trajectory\u2014or show a saliency map at each predicted point separately (Pan et al., 2020), thus allowing only for a bottom-up understanding of the predicted trajectory. The last category also includes many recently proposed methods with attention mechanisms (Alaa & van der Schaar, 2019; Lim et al., 2021). We contrast our framework with feature importance techniques in Appendix E.\nShapelets and motifs. As our method discusses the shape of the trajectory, it may seem related to shapelet-based methods (Ye & Keogh, 2009). However, these methods are usually used for data mining and classification tasks. They aim to find subsequences of a time series that represent the most important patterns of each class and thus can be used to distinguish between them (Chen et al., 2022). Similarly, motif discovery identifies short repeating patterns in the time series (Torkamani & Lohweg, 2017) usually for insights into the problem or classification tasks."
        },
        {
            "heading": "7 TIMEVIEW IN ACTION",
            "text": "Answering questions. Our interactive visualization tool for TIMEVIEW allows for answering questions such as \u201cWhat If \u201d, \u201cHow to be that\u201d, and \u201cHow to still be this\u201d from the XAI Question Bank (Liao et al., 2020) discussed in Section 1. As explained earlier, answering such questions with the current bottom-up approaches may often be futile since the notion of a \u201cdifferent prediction\u201d may be non-interpretable or simplistic. In contrast, TIMEVIEW allows the analysis of a trajectory change at two levels, i.e., the composition of the trajectory or the coordinate of the transition point.\nVisualizing perturbations. We can visualize the effect of perturbing one or two features at a time using colorful bands (as in Figure 1) and colorful 2D contour plots (Figure 4). In the left panel of Figure 1, we have a movable slider for each feature that changes the predicted trajectory in the center. The colors on the band below the slider signify the composition of the trajectory if the feature is in the corresponding range. This allows us to understand how the trend of the trajectory changes as we change each feature (level 1 of bi-level transparency). To understand the properties of this trend (level 2), we choose any of the transition points in the central plot, and we can analyze its position with respect to the chosen feature on the plot on the right. It currently shows how the y-coordinate of\nthe second transition point (local minimum) increases as initial tumor volume increases. (Figure 4)\nshows how we can visualize the effect of changing two features at a time. Each color in the contour plot corresponds to a different composition, so it is clear how changing the features influences the composition of the trajectory. Please see Appendix E for a more in-depth discussion.\nComparison with other methods. In the absence of time series methods fulfilling bi-level transparency, we adapt static transparent methods, such as linear regression, decision trees, and GAMs (Lou et al., 2012; Nori et al., 2019) to time series forecasting by treating time as a feature, denoted as Linear-T, DecisionTree-T, and GAM-T. We also compare with methods discovering closed-form expressions for trajectories, such as PySR for symbolic regression (Cranmer, 2020), and SINDy for ODE discovery (Brunton et al., 2016). We also include black-box models RNN, \u2206t-RNN, and stateof-the-art tree-based models adapted to time series forecasting (XGB-T (Chen & Guestrin, 2016), LGBM-T (Ke et al., 2017), CatBoost-T (Prokhorenkova et al., 2018)).\nExperiments were conducted on four real-world datasets (Airfoil (Brooks et al., 1989), flchain (Dispenzieri et al., 2012), Stress-Strain (Aakash et al., 2019), and Tacrolimus (Woillard et al., 2011)) and three synthetic ones (Sine, Beta, and Tumor, the latter based on a model from (Wilkerson et al., 2017)). The synthetic datasets are constructed to contain trajectories exhibiting many different trends. Figure 1, Figure 4, Figure 1 show TIMEVIEW fitted to Sine, Beta, and Tumor datasets. As shown in Table 3, TIMEVIEW outperforms the transparent methods and closed-form expression on most datasets and achieves comparable performance to the black boxes. Details about the experiments can be found in Appendix D."
        },
        {
            "heading": "Black boxes",
            "text": ""
        },
        {
            "heading": "Closed-form expressions",
            "text": ""
        },
        {
            "heading": "Transparent models",
            "text": ""
        },
        {
            "heading": "8 DISCUSSION AND CONCLUSION",
            "text": "Applications. We believe bi-level transparency and our mathematical framework can inspire future XAI methods. For instance, note, that models adhering to our framework (like TIMEVIEW) provide additional output next to the standard forecasted trajectory: the current composition and the coordinates of the transition points. Traditional XAI techniques for regression and classification can be applied to these additional outputs, instead of individual trajectory points, to gain more meaningful explanations. Thus, techniques such as feature importance methods (Lundberg & Lee, 2017), local surrogates (Ribeiro et al., 2016), and counterfactual explanations (Karimi et al., 2020) can now be extended to time series forecasting settings. These, in turn, can open domains where the applicability of ML has been limited due to transparency concerns, including medicine, finance, and science.\nLimitations and open challenges. TIMEVIEW is a particular application of bi-level transparency for time series forecasting from static features. We hope future works will extend it to settings where the input may contain the previous part of the trajectory or other exogenous time series (further discussion in Appendix E).\nEthics Statement. In this paper, we present a novel conceptual framework for enhancing transparency in the domain of time series forecasting, accompanied by its practical implementation known as TIMEVIEW. A better understanding of machine learning models serves critical purposes such as model debugging and identifying and mitigating potential harmful biases. However, XAI techniques can also be misused to foster unwarranted trust in models or to merely achieve surfacelevel compliance with regulatory standards. As highlighted in our paper, domains such as medicine and pharmacology involve high-stakes scenarios. Therefore, prior to deploying our model in such contexts, a rigorous examination is imperative to ensure it does not endorse decisions that could prove detrimental to individuals\u2019 well-being.\nReproducibility Statement. All mathematical definitions are provided in Section 4 and Appendix A. The proofs of theoretical results are shown in Appendix B. The implementation, including block diagrams and pseudocode, is discussed in Section 5 and in Appendix C. The experiment settings are discussed in Section 7 and Appendix D. The code to reproduce the results and for the visualization tool can be found in supplementary materials."
        },
        {
            "heading": "TABLE OF SUPPLEMENTARY MATERIALS",
            "text": "1. Appendix A: notation and definitions\n2. Appendix B: theoretical results\n3. Appendix C: implementation details\n4. Appendix D: experimental details and additional results\n5. Appendix E: additional discussion\n6. Appendix F: extended related works"
        },
        {
            "heading": "A NOTATION AND DEFINITIONS",
            "text": ""
        },
        {
            "heading": "A.1 NOTATION",
            "text": "We present the symbols used in this work and their meanings in Table 4."
        },
        {
            "heading": "A.2 GLOSSARY OF TERMS",
            "text": "In Table 5, we present a glossary of important words and where they can be found in the paper."
        },
        {
            "heading": "A.3 DEFINITIONS",
            "text": "Definition 4 (Interval). We say I is an interval (of R) if it is an open interval, closed interval, or halfclosed interval. The interval has to contain more than one point. We denote the set of all intervals on R as I. Let c \u2208 R, we denote the shifted interval as I + c = {x+ c | x \u2208 I}. Definition 5 (Interval partition). Interval partition of an interval I is a sequence of intervals (I1, . . . , In) such that \u22c3n i=1 Ii = I , Ii \u2229 Ii+1 = \u2205 \u2200i \u2208 [n \u2212 1] and the right boundary of Ii should be no greater than the left boundary of Ii+1 for all i \u2208 [n\u2212 1]. Definition 6 (Interval function). Let I \u2282 R be any interval, we call any function f : I \u2192 R an interval function and we denote its domain as dom(f). We denote the set of all interval functions as F .\nDefinition 7 (Motif). A motif s is a binary relation between the set of interval functions F and the set of intervals I (i.e., s \u2208 F \u00d7I = {(f, I) | f \u2208 F , I \u2208 I}). We denote (f, I) \u2208 s as f |I \u223c s and read it as \u201cf on I has a motif s\u201d. Each motif s needs to be:\n\u2022 well-defined, i.e., for any f \u2208 F , and any I \u2208 I,\nf |I \u223c s =\u21d2 I \u2286 dom(f) (5)\n\u2022 translation-invariant, i.e., for any I \u2208 I, and any f \u2208 F ,\nf |I \u223c s \u21d0\u21d2 f \u25e6 (x\u2212 c)|(I + c) \u223c s \u2200c \u2208 R (6) Definition 8 (Composition). Let f : I \u2192 R be an interval function and S be a set of motifs. A motif sequence of f in S is a finite sequence of motifs (s1, . . . , sd), such that there exists an interval partition (I1, . . . , Id) of I such that f |Ij \u223c sj \u2200j \u2208 [d]. A composition of f in S is the shortest motif sequence of f in S. The points between the intervals are called the transition points. The set of all compositions for a given set of motifs S is denoted by CS . A set of motifs S is called compatible with a subset F \u2032 \u2282 F if for every f \u2208 F \u2032 there exists a unique composition, denoted CS [f ] Definition 9 (Compatibility). Let S be a set of motifs and F \u2032 \u2282 F be a subset of interval functions. S is called compatible with F \u2032 if for every f \u2208 F \u2032 there exists a unique composition, CS [f ] and a unique sequence of the transition points associated with this composition.\nDefinition 10 (Composition map). Let a set of motifs S be compatible with some subset F \u2032 \u2282 F . Let g : RM \u2192 F \u2032 be a machine learning model for time series forecasting. A composition map is denotedMS : RM \u2192 CS defined byMS(x) = CS [g(x)]. Definition 11 (Inclusive motifs). We say that a motif s is inclusive if for any function f \u2208 F and any interval I \u2208 I, we have f |I \u223c s =\u21d2 f |I0 \u223c s \u2200I0 \u2282 I (7) Definition 12 (Exclusive motifs). We say that a finite set of motifs S is exclusive if for any function f \u2208 F and any interval I \u2208 I, we have\nf |I \u223c s =\u21d2 f |I \u0338\u223c s\u2032 \u2200s \u0338= s\u2032 (8)\nfor all s, s\u2032 \u2208 S. In other words, if f is in a particular motif on the interval I then it cannot be in any other motif on this interval."
        },
        {
            "heading": "A.4 MOTIF EXAMPLES",
            "text": "Example 1 (Range motifs). Let a, b \u2208 R such that a < b. We define range motifs s<, s=, s> as:\n\u2022 f |I \u223c s< if \u2200x \u2208 I f(x) < a \u2022 f |I \u223c s= if \u2200x \u2208 I a \u2264 f(x) \u2264 b \u2022 f |I \u223c s> if \u2200x \u2208 I b < f(x)\nThese motifs are beneficial when our interest lies not in the trajectory\u2019s exact shape, but in whether its value falls within a specific range [a, b] and at what point in time. This can be particularly useful in control problems where different policies apply based on whether a variable is within or outside the operating range [a, b]. More motif examples can be found in Appendix A.\nExample 2 (Threshold motifs). Range motifs (Example 1) can be modified by choosing a = b. Then we end up with only two motifs:\n\u2022 f |I \u223c s< if \u2200x \u2208 I f(x) < a \u2022 f |I \u223c s> if \u2200x \u2208 I a < f(x)\nThese motifs are useful for monitoring whether the trajectory is above or below a certain threshold.\nExample 3 (Monotonic motifs). Let us define monotonic motifs s+s\u2212, s0 which are defined as:\n\u2022 f |I \u223c s+ if f is strictly increasing on I (\u2200x, y \u2208 I x < y =\u21d2 f(x) < f(y)) \u2022 f |I \u223c s\u2212 if f is strictly decreasing on I (\u2200x, y \u2208 I x < y =\u21d2 f(x) > f(y)) \u2022 f |I \u223c s0 if f is constant on I (\u2200x, y \u2208 I f(x) = f(y))\nExample 4 (Derivative range motifs). Let a, b \u2208 R and a < b. We define derivative range motifs as:\n\u2022 f |I \u223c s< if f |I \u2208 C1 and \u2200x \u2208 I f \u2032(x) < a \u2022 f |I \u223c s= if f |I \u2208 C1 and \u2200x \u2208 I a \u2264 f \u2032(x) \u2264 b \u2022 f |I \u223c s> if f |I \u2208 C1 and \u2200x \u2208 I b \u2264 f \u2032(x)\nExample 5 (Dynamical motifs). We define seven dynamical motifs: s0+, s0\u2212, s00, s++, s+\u2212, s\u2212+, s\u2212\u2212 as follows (also visualized in Table 2).\n\u2022 f |I \u223c s0+ if \u2203a > 0 \u2203b \u2208 R \u2200x \u2208 I f(x) = ax+ b \u2022 f |I \u223c s0\u2212 if \u2203a < 0 \u2203b \u2208 R \u2200x \u2208 I f(x) = ax+ b \u2022 f |I \u223c s00 if \u2203b \u2208 R \u2200x \u2208 I f(x) = b \u2022 f |I \u223c s++ if f |I \u2208 C2 and \u2200x \u2208 int(I) f \u2032(x) > 0, f \u2032\u2032(x) > 0 \u2022 f |I \u223c s+\u2212 if f |I \u2208 C2 and \u2200x \u2208 int(I) f \u2032(x) > 0, f \u2032\u2032(x) < 0 \u2022 f |I \u223c s\u2212+ if f |I \u2208 C2 and \u2200x \u2208 int(I) f \u2032(x) < 0, f \u2032\u2032(x) > 0 \u2022 f |I \u223c s\u2212\u2212 if f |I \u2208 C2 and \u2200x \u2208 int(I) f \u2032(x) < 0, f \u2032\u2032(x) < 0"
        },
        {
            "heading": "B THEORETICAL RESULTS",
            "text": "Theorem 1 (Dynamical motifs are compatible with cubic splines). Let S be the set of dynamical motifs and let F \u2032 be the set of cubic splines on some interval [a, b]. Then S is compatible with F \u2032\nBefore we prove this theorem, we will prove two propositions about dynamical motifs, and two helpful lemmas.\nProposition 1. Dynamical motifs are inclusive (as defined in Definition 11.\nProof. Observe that s+0, s\u22120, and s00 can be equivalently defined as:\n\u2022 f |I \u223c s+0 if f |I \u2208 C2 and \u2200x \u2208 int(I)f \u2032(x) > 0, f \u2032\u2032(x) = 0 \u2022 f |I \u223c s\u22120 if f |I \u2208 C2 and \u2200x \u2208 int(I)f \u2032(x) < 0, f \u2032\u2032(x) = 0 \u2022 f |I \u223c s00 if f |I \u2208 C2 and \u2200x \u2208 int(I)f \u2032(x) = 0, f \u2032\u2032(x) = 0\nThis (with the rest of the motifs defined in Example 5) shows that each motif is uniquely defined by the signs of its first and second derivatives.\nIf sign(f \u2032) on an interval I is constant then sign(f \u2032|I0) = sign(f \u2032) for any interval I0 \u2282 I . The same holds for the second derivative. This proves that dynamical motifs are inclusive.\nProposition 2. Dynamical motifs are exclusive (as defined in Definition 12.\nProof. As we shown in the proof of Proposition 1, each dynamical motif is uniquely defined by the signs of its first and second derivatives. If sign(f \u2032) on an interval I is constant then it cannot be equal to anything else. The same holds for the second derivative. This proves that dynamical motifs are exclusive.\nLemma 1 (Uniqueness of dynamical motif sequences). Let S be the set of dynamical motifs and let f be an interval function. If there exists a motif sequence for f , (s1, . . . , sd), satisfying si \u2208 S\u2200i \u2208 [d], si \u0338= si+1 \u2200i \u2208 [d\u2212 1], then this motif sequence is unique.\nProof. Consider two motif sequences (s1, . . . , sd), (p1, . . . , pe) such that for both of them every two consecutive motifs are different. We show that these sequences need to be the same.\nLet us compare p1 and s1. Consider the intervals described by s1 and p1 and denote them I1 and J1 respectively. By inclusivity of dynamical motifs (Proposition 1), f |(I1 \u2229 J1) \u223c s1 and f |(I1 \u2229 J1) \u223c p1. That implies, by exclusivity (Proposition 2), that s1 = p1. That also shows that J1 cannot be bigger than I1. If that were the case, it would overlap with I2 (described by s2). But s2 \u0338= s1 = p1. So, J1 \u2282 I1. If J1 is smaller than I1 then J2 \u2282 (I1 \\ J1), where J2 is described by p2, and p2 = s1 = p1. But we assumed that p1 \u0338= p2. Thus, J1 = I1. Now, we can apply exactly the same reasoning to the next pair of motifs (s2, p2). We continue this procedure and in the end, we get these two motif sequences are the same. Note, it is also possible that I1 \\ J1 is just a single point. That is why the intervals corresponding to the motif sequence are unique up to the boundaries of the interval, i.e., the intervals might differ by single points.\nLemma 2 (Dynamic motifs are compatible with cubics). Let S be the set of dynamical motifs and let Q be the set of polynomials up to a third degree on R. Then S is compatible with Q.\nProof. Consider any polynomial f \u2208 Q. Let\u2019s write f as f(t) = at3+bt2+ct+d. We can calculate its first and second derivatives. Respectively, f \u2032(t) = 3at2 + 2bt + c, f \u2032\u2032(t) = 6at + 2b. f \u2032 and f \u2032\u2032 divide R into intervals where on each interval f \u2032 (or f \u2032\u2032 respectively) is either positive, negative, or equal to zero. f \u2032 divides R into at most 3 intervals, and f \u2032\u2032 divides R into at most two intervals. There might be some isolated points that do not belong to any of these intervals. For instance, if f \u2032\u2032(t) = t, we have two intervals: (\u2212\u221e, 0) where f \u2032\u2032 is negative, (0,+\u221e) where f \u2032\u2032 is positive, and an isolated point at t = 0. For f \u2032\u2032 we have at most one isolated point, and for f \u2032 we have at most two isolated points. We can now take the intersection of every interval defined by f \u2032 with every other interval defined by f \u2032\u2032. This gives us at most 4 intervals where each interval belongs to one of 9 different configurations that we denote by two symbols. The first symbol denotes the sign of the first derivative (+,\u2212, 0), and the second sign denotes the sign of the second derivative. For instance, an interval might be described as \u2212+ if it was created by intersecting an interval that has a negative first derivative with an interval that has a positive second derivative. We observe that two configurations are impossible, 0+ and 0\u2212, so we end up with 7 possible configurations. We observe that if an interval I is described by +\u2212 then by Example 5 f |I \u223c s+\u2212. Similarly for ++, \u2212+, and \u2212\u2212. If an interval I is denoted by 00 then it means that 6at + 2b = 0 \u2200t \u2208 I , and 3at2 + 2bt+ c = 0 \u2200t \u2208 I . From the first equation, we get that a = 0, b = 0, and from the second one we get c = 0. That means that f |I \u223c s00. Similarly, an interval I is described by +0 then a = 0, b = 0 and c > 0. That means f is given by ct+ d, where c > 0, so f |I \u223c s+0. Analogously with \u22120. The isolated points mentioned above are exactly the transition points between different motifs.\nThis shows how for every polynomial up to a third degree, there exists a sequence of dynamical motifs and we showed explicitly how to construct such a sequence. Let us call this sequence (s1, . . . , sd). Observe that the shortest motif sequence for f cannot have two identical consecutive motifs. If that were the case, they could have been combined into a shorter motif sequence. As every two consecutive motifs of (s1, . . . , sd) are different then, by Lemma 1, this is the only such sequence and thus it is the shortest. So (s1, . . . , sd) is a composition. That shows that for every f \u2208 Q there exists a unique composition and thus S is compatible with Q.\nNow, we can prove Theorem 1.\nProof of theorem Theorem 1. Let f be a cubic spline defined on internal knots t1, . . . , tn. Let us denote fi the polynomial defined on (ti, ti+1). To construct a composition for f , we construct compositions for each of the polynomials fi (as described in Lemma 2). Then we look at the neighboring motifs on both sides of every knot. If the motifs are the same, then we combine them and remove the transition point. This procedure gives us a motif sequence for f that we denote as (s1, . . . , sd).\nObserve that the shortest motif sequence for f cannot have two identical consecutive motifs. If that were the case, they could have been combined into a shorter motif sequence (this is true because cubic splines have continuous first and second derivatives at the knots). As every two consecutive motifs of (s1, . . . , sd) are different then, by Lemma 1, this is the only such sequence and thus it is the shortest. So (s1, . . . , sd) is a composition. That shows that for every cubic spline, there exists a unique composition, and thus S is compatible with cubic splines.\nC IMPLEMENTATION"
        },
        {
            "heading": "C.1 MODEL",
            "text": "Pseudocode. The pseudocode of the model training in TIMEVIEW is shown in Algorithm 1.\nAlgorithm 1 TIMEVIEW Model training Input: Static features X \u2208 RD\u00d7M Input: Time series {t(d)}Dd=1, {y(d)}Dd=1, t(d) \u2208 RNd , y(d) \u2208 RNd Input: Number of basis functions B Input: Gradient-based optimization algorithm O Output: Trained model G : RM \u2192 Y\u0302 t1, . . . , tB\u22122 \u2190 SELECTKNOTS({t(d)}Dd=1, {y(d)}Dd=1) \u25b7 Appendix C.3 {\u03d5b}Bb=1 \u2190 B-Spline basis functions for knots t1, . . . , tB\u22122 Initialize matrices {\u03a6(d)}Dd=1, \u03a6(d) \u2208 RNd\u00d7B\n\u03a6 (d) jb \u2190 \u03d5b(t (d) j ) procedure LOSS(h) y\u0302(d) \u2190 \u03a6(d)h(x(d))\nL \u2190 1D \u2211D d=1 ( 1 Nd \u2211Nd j=1 ( y\u0302 (d) j \u2212 y (d) j )2) + \u03b1LL2\nreturn L end procedure h = O(LOSS) \u25b7 Training procedure G(x \u2208 RM , t \u2208 RN )\nInitialize \u03a6 \u2208 RN\u00d7B \u03a6jb \u2190 \u03d5b(tj) y\u0302 \u2190 \u03a6h(x) return y\u0302\nend procedure return G\nArchitecture of the encoder. We implement encoder h as a fully connected neural network with 3 hidden layers. We also include dropout (Srivastava et al., 2014) and batch normalization (Ioffe & Szegedy, 2015). The sizes of the hidden layers, activation functions, and dropout probability are fine-tuned.\nDynamic bias. We add an additional constant basis function (equal to 1 everywhere) as a \u201cbias\u201d that is adjusted for each sample.\nHyperparameters. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 6. We fix the number of basis functions to be 9 for all real datasets and Tumor, and 5 for Sine and Beta datasets. We found that 9 usually gives the best or nearly the best performance while maintaining compositions that are still short. We chose 5 for Sine and Beta datasets because the compositions\nof trajectories in these datasets are very short, so there was no need for a bigger number of basis functions. In practice, this parameter can be fine-tuned but numbers between 5 and 9 work well by default."
        },
        {
            "heading": "C.2 COMPOSITION EXTRACTION",
            "text": "Block diagram. The composition extraction procedure in TIMEVIEW is depicted in Algorithm 2.\nPseudocode. The pseudocode of composition extraction implemented in TIMEVIEW is shown in Algorithm 2."
        },
        {
            "heading": "C.3 ALGORITHM FOR KNOT SELECTION",
            "text": "Each B-Spline basis function is determined by its knots\u2014places where two polynomials meet. The challenge of knot selection in TIMEVIEW comes from the necessity of having fixed knots for all trajectories. As each trajectory may have a different optimal knot placement, we aim to find knots that may not be optimal but are nevertheless chosen based on the trajectories in the dataset. For instance, there is no need to have many knots in parts of the domain where the trajectories vary little. But it might be beneficial to have more knots in parts of the domain where the trajectories do vary. To achieve this goal we propose the following heuristic algorithm based on the UnivariateSpline function in scipy library (Virtanen et al., 2020).\nAlgorithm 2 TIMEVIEW Composition Extraction Input: Static features x \u2208 RM Input: Encoder h Input: Knots t1, . . . , tB\u22122 Input: Method for calculating the composition of a cubic COMPOSITIONCUBIC Output: Composition C {\u03d5b}Bb=1 \u2190 B-Spline basis functions for knots t1, . . . , tB\u22122 {\u03c8bk}b\u2208[B],k\u2208[B\u22123] \u2190 \u03d5b on interval [tk, tk+1] is given by a cubic \u03c8bk c = h(x) Clist \u2190 empty list for k = 1 to B \u2212 3 do Clist \u2190 append COMPOSITIONCUBIC (\u2211B b=1 cb\u03d5bk, tk, tk+1\n) end for C \u2190 concatenate Clist for i = 1 to len(C)\u22121 do\nif Ci == Ci+1 then remove Ci\nend if end for return C\nUnivariateSpline function takes the trajectory (both t and y), a smoothing parameter s5, calculates the optimal number of knots, and returns their placement. For each trajectory in our training dataset, we test different values of s to find the one that gives the exact number of knots we want (using a method similar to binary search). Then we take the positions of these knots and add them to our set of found knots. We repeat this procedure for every trajectory. Then we use the K-means clustering algorithm from scikit-learn library (Pedregosa et al., 2011) with the number of clusters equal to the desired number of knots. The clusters returned by the algorithm are the selected knots."
        },
        {
            "heading": "D EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "D.1 DATASETS",
            "text": "All datasets are split into training, validation, and testing sets with ratios (0.7 : 0.15 : 0.15)."
        },
        {
            "heading": "D.1.1 REAL DATASETS",
            "text": "Airfoil. Airfoil Self-Noise dataset (Brooks et al., 1989) is a UCI dataset, obtained from a series of aerodynamics and acoustic tests of airfoil blade sections conducted in a wind tunnel. The static features are the angle of attack, chord length, free-stream velocity, and suction side displacement thickness. The output is a sound pressure level at specified frequencies (obtained by a Fourier transform of the raw signal). We treat the sound pressure level with respect to the frequency as a time series, i.e., t\u2014frequency, y\u2014sound pressure level. As the frequencies are uniformly chosen on a logarithmic scale, we take the log transform of t. The processed dataset contains 106 samples, each having on average 14 measurements.\nflchain. Flchain is a dataset of the subjects from a study of the relationship between serum-free light chain and mortality (Dispenzieri et al., 2012), often used in time-to-event problems. We train a Random Survival Forest using scikit-survival library (Po\u0308lsterl, 2020). Then for each sample from the dataset, we generate a survival function and sample it at 20 predetermined points. The static features are the same as in the original dataset, i.e., age, serum creatinine, the FLC group, kappa portion of serum FLC, lambda portion of FLC, monoclonal gammapothy (MGUS), and sex. The time series is described by y\u2014the probability of survival up to this point, t\u2014time in days. We subsample the dataset to get 1000 samples , each with 20 measurements.\n5Note, do not confuse with s denoting motifs in our work\nStress-Strain. We take the stress-strain curves of aluminum 6061-T651 obtained by Aakash et al. (2019). The aluminum samples are sourced from 9 different lots at a few temperatures between 20\u25e6C and 300\u25e6C. W treat the temperature and lot as static features and the time series is defined by t\u2014strain, y\u2014experienced stress. After preprocessing, we end up with 100 samples, each with 212 measurements on average.\nTacrolimus. We take the drug concentration curves of two tacrolimus formulations obtained by Woillard et al. (2011). The static features include sex, weight, hematocrit, hemoglobin, creatinine, dose, CYP3A5 genotype, and formulation. We also supplement it with the initial concentration of the drug in the blood. After preprocessing, we end up with 90 samples, each with 10 measurements on average."
        },
        {
            "heading": "D.1.2 SYNTHETIC DATASETS",
            "text": "Tumor. We take the tumor growth model proposed in Wilkerson et al. (2017). This model is described by the following equation:\ny(t) = \u03d5 exp(\u2212dt) + (1\u2212 \u03d5) exp(gt)\u2212 1 (9)\nWhere \u03d5, d, g are free parameters corresponding to the proportion of tumor cells that undergo cell death due to treatment, decay of the tumor, and the growth of the tumor. This equation has been shown to capture many different trends of tumor volume over time and that is why we decided to use it in our experiments. To make the model more realistic we multiply y by the initial tumor volume, and we add the following static features: age, weight, and drug dosage. We describe the parameters from Equation (9) using the following relationships:\ng = g0 \u2217 (age/20.0)0.5\nd = d0 \u2217 dosage/weight \u03d5 = 1/(1 + exp(\u2212dosage \u2217 \u03d50))\n(10)\nThese relationships are inspired by covariate models often used in PKPD models (Mould & Upton, 2012; Chung et al., 2021).\nWe choose the following parameters to get trajectories of different and realistic shapes.\ng0 = 2.0\nd0 = 180\n\u03d50 = 10\n(11)\nTo generate the data, we create 2000 samples. For each sample, we draw static features from uniform distributions described below.\n\u2022 age \u223c Uniform(20, 80) \u2022 weight \u223c Uniform(40, 100) \u2022 initial tumor volume \u223c Uniform(0.1, 0.5) \u2022 dosage \u223c Uniform(0.0, 1.0)\nFor each sample, we evaluate the tumor volume function y at 20 equally spaced time points on [0, 1].\nSine. This synthetic dataset was created to show how our model can be used when we have one static feature that nevertheless renders trajectories with different trends. We use the following equation:\ny(t) = sin\n( 2t\u03c0\nx\n) (12)\nIn this equation, x is the static covariate. As we show in Figure 1, it can generate trajectories with different compositions. To create the dataset we generate 200 values for x uniformly on [0, 2.5], and for each x we generate 20 uniform time points from 0 to 1 and evaluate y on them.\nBeta. This synthetic dataset was created to show how our model can be used when we have two static features that render trajectories with different trends. As the probability density functions of the beta\ndistribution are known for their variety of shapes, we decided to use them in our experiments. In particular, we define our trajectory by the following equation\ny(t) = 1\nB(\u03b1, \u03b2) t\u03b1\u22121(1\u2212 t)\u03b2\u22121 (13)\nwhere B is the beta function. We treat \u03b1 and \u03b2 as static features. To create the datasets, we create a grid of (\u03b1, \u03b2) pairs where \u03b1 and \u03b2 are created by taking 30 values uniformly spread over [1.0, 4.0]. This gives us 900 samples in total. For each sample, we generate 20 uniform measurements on [0, 1]."
        },
        {
            "heading": "D.2 BASELINES",
            "text": "Linear-T. We implement the linear model with elastic net regularization (Zou & Hastie, 2005) using scikit-learn package. We adapt them for time series forecasting by passing time as a feature. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 7\nDecisionTree-T. We implement decision tree regressor using scikit-learn package. We adapt them for time series forecasting by passing time as a feature. To keep the decision trees transparent, we restrict their depth to 5. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 8\nGAM-T. We implement GAM using the Explainable Boosting Machines (EBMs) (Lou et al., 2012) available in the InterpretML package (Nori et al., 2019). We adapt them for time series forecasting by passing time as a feature. By doing so, GAM finds a \u201cbaseline\u201d trajectory which is the shape function associated with the t variable. Then every prediction consists of the same baseline trajectory but shifted vertically depending on the values of other shape functions. Such model is very transparent but it is likely to underperform in settings where the trajectories have different trends for different features. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 9\nSINDy. SINDYy (Brunton et al., 2016) is an ODE discovery algorithm that produces closed-form ODEs. We use the implementation in PySINDy library (de Silva et al., 2020; Kaptanoglu et al., 2022). We adapt SINDy to work with static features by treating static features as a constant control input. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We tune the optimizer threshold by considering values from 0.001 to 0.1 (on a logarithmic scale), and we tune the derivative estimation method. We consider three different kinds of differentiation algorithms: finite difference, spline, and trend filtered. We show their corresponding hyperparameter ranges in Table 10\nPySR. PySR (Cranmer, 2020) is a symbolic regression library that uses genetic programming. Genetic programming algorithms highly depend on the time they are allowed to run for (as they can explore more possibilities). We put a time constraint for PySR that is equal to the time it took TIMEVIEW to run, including the hyperparameter tuning. Thus we do not perform hyperparameter tuning for PySR (we run with default parameters). Instead, after each run we look at the best equations of each length and we try each of them on the validation set. We then report the loss of the best-found equation on the test set. We choose the maximum length of the expression to be a maximum of 20 and the number of features times 3. We choose 20 as this is a reasonable length of expression to comprehend, and we choose 3 times the number of features as that is the length of a GAM that is usually considered interpretable. The parameters of PySR are listed in Table 11\nRNN. As a Recurrent Neural Network baseline, we use a multi-layer long short-term memory (LSTM) RNN as implemented in pytorch. We implement the encoder as multi-layer neural networks. We also include dropout (Srivastava et al., 2014) and batch normalization (Ioffe & Szegedy, 2015). The sizes of the hidden layers, activation functions, and dropout probability are fine-tuned. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 12.\n\u2206t-RNN. We adapt RNN described above to a setting with irregular measurements by passing the difference between time points as an additional feature (Y\u0131ld\u0131z et al., 2021).\nXGB-T. We implement XGBoost (Chen & Guestrin, 2016) using py-xgboost package. We adapt it for time series forecasting by passing time as a feature. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 13\nCatBoost-T. We implement CatBoost (Prokhorenkova et al., 2018) using catboost package. We adapt it for time series forecasting by passing time as a feature. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 14\nLGBM-T. We implement LightGBM (Ke et al., 2017) using lightgbm package. We adapt it for time series forecasting by passing time as a feature. We perform hyperparameter tuning using Optuna (Akiba et al., 2019) and run it for 100 trials. We describe the hyperparameters we tune and their ranges in Table 15"
        },
        {
            "heading": "D.2.1 SENSITIVITY ANALYSIS",
            "text": "In this section, we investigate how sensitive TIMEVIEW is to the number of basis functions B. We perform our experiments for different numbers of basis functions (from 5 to 16) on two real\ndatasets\u2014Airfoil and Tacrolimus. The results can be seen in Figure 6. We observe that TIMEVIEW is not very sensitive to the number of basis functions and achieves good performance even when the number of basis functions is very low. That means it can have a good predictive power even if the compositions of the predicted trajectories are very short\u2014and thus easy to understand."
        },
        {
            "heading": "D.3 COMPUTATION TIME",
            "text": "It is important to note that training TIMEVIEW is basically the same as training a standard neural network for multi-output regression. Thanks to the preprocessing of the B-Spline basis functions there is no significant overhead due to time series forecasting. In fact, TIMEVIEW uniquely requires a single pass through the network to predict the whole trajectory no matter the time horizon. This may make it faster than methods that forecast trajectory by generating outputs sequentially. The visualization tool is applied only after the model is trained and works in real-time. The computation times (in seconds) can be seen in the Table 16 below. The run times are for all experiments for a given dataset. That includes 100 runs of hyperparameter tuning and 10 runs to get standard errors.\nThe experiments were performed on 12th Gen Intel(R) Core i7-12700H with 64 GB of RAM and NVIDIA GeForce RTX 3050 Ti Laptop GPU as well as on the 10th Gen Intel Core i9-10980XE with 60 GB of RAM and NVIDIA RTX A4000. All experiments take around 23.6h to run."
        },
        {
            "heading": "D.4 LICENSES",
            "text": "The licenses of the software used in this work are presented in Table 17"
        },
        {
            "heading": "E ADDITIONAL DISCUSSION",
            "text": ""
        },
        {
            "heading": "E.1 ANSWERING QUESTIONS ABOUT TIMEVIEW: TUMOR EXAMPLE",
            "text": "In this section, we demonstrate how we can use the visualization tool to understand TIMEVIEW predictions. In particular, we show how our tool allows us to answer the questions described in Section 1. We train TIMEVIEW on the synthetic tumor dataset (details in Appendix D).\nQ: Would the predicted tumor volume keep decreasing if we adjusted the treatment?\nThis is a question on level 1\u2014about the trend of the trajectory. Figure 7 shows that the current trajectory is decreasing. The colorful band under the dosage slider shows the compositions for different drug dosages. We can see that the trajectory is increasing for very low dosages (orange). For higher dosage (blue), the trajectory is first decreasing and then increasing. Finally, higher dosage (green and pink) corresponds to decreasing trajectories. To answer the question, it is enough to check at which points the bands change color. If the drug\u2019s dose is kept above 0.46, the trajectory will keep decreasing. The secondary plot in the right panel shows how the y-coordinate of the first transition point changes as the initial tumor volume changes. As expected, the plotted function is close to the identity. It is a good sanity check to confirm that our model learned that one of the static features corresponds to the initial measurement."
        },
        {
            "heading": "Q: What feature changes would lower the minimum tumor volume?",
            "text": "This is a question on level 2\u2014about the property of a particular trend. Let us consider a lower drug dosage where the trajectory decreases and then increases (Figure 8). We observe that the minimum of the tumor corresponds to the y-coordinate of the second transition point. We can use the tool on the right to plot how the minimum depends on the dosage (or any other feature). We can see that the minimum volume decreases with an increased drug dosage."
        },
        {
            "heading": "Q: How feature changes would impact the time this minimum is achieved?",
            "text": "Similarly, we can consider the time when this minimum is achieved. We can easily do that by just switching from the y-coordinate of the transition point to its t-coordinate (Figure 9). We can see how the time depends on the dose of the drug. Crucially, we can see how by lowering the dose of the drug, we bring this time closer to 0 where it \u201cmerges\u201d with the first transition point, and the composition of the trajectory changes. The trajectory becomes increasing (Figure 10)."
        },
        {
            "heading": "E.2 CLOSED-FORM EXPRESSIONS",
            "text": "Whether a closed-form expression guarantees bi-level transparency or not is highly dependent on the exact for of the expression. For instance, a trajectory described by y = sin(2\u03c0xt) is transparent. We can understand the trend of the trajectory (the characteristic sinusoidal shape) and we can understand the impact of the covariate (x) on the property of this trend (its frequency). However, some even very\nsimple expressions do not have this property. A good example is the expression for the probability density function of the beta distribution used in Section 7, i.e., y(t) = 1B(\u03b1,\u03b2) t\n\u03b1\u22121(1\u2212 t)\u03b2\u22121, where B is the beta function, Even though the expression is very compact it is very challenging to infer the trend of the trajectory based on the features \u03b1 and \u03b2. It is even harder to understand the impact of the features on the properties of these trends. That is why, we believe, the Beta dataset is a very interesting example for TIMEVIEW."
        },
        {
            "heading": "E.3 CONTRAST WITH FEATURE IMPORTANCE METHODS",
            "text": "As we explain throughout the paper, current feature importance techniques are bottom-up, i.e., they calculate the importance scores with respect to individual prediction horizons. Below we show the results of applying SHAP (Lundberg & Lee, 2017) on top of LightGBM (adapted for time series forecasting) to the Sine (Figure 11), Beta (Figure 12), and Tacrolimus (Figure 13) datasets.\nSHAP values assign an importance score to each feature. Features with positive scores impact the prediction positively, while those with negative SHAP values impact the prediction negatively. As we are interested in time series forecasting, the output of the algorithm is not a single outcome, but rather a whole trajectory. That is why we apply SHAP separately to a set of predictions at discrete time points.\nWhile SHAP offers insights into the importance of specific features at discrete time points, TIMEVIEW takes this analysis a step further by providing a comprehensive understanding of feature impacts on the entire trajectory. It shows exactly how the prediction would change if the value of the feature is changed (i.e., how the trend of the trajectory and its properties would change).\nMoreover, SHAP indicates feature importance at pre-specified time steps that may lack broader significance. In contrast, TIMEVIEW, captures the essence of the trajectory\u2019s behavior by demonstrating the impact on critical properties such as local maxima, minima, and inflection points even as their time coordinates vary.\nEven in complex, high-dimensional scenarios, TIMEVIEW remains adept at addressing counterfactual queries. For instance, it can address the question of how the maximum of a trajectory changes if one of the variables is adjusted. This is of critical importance in pharmacology, where the trajectory is the drug concentration curve and the variable of interest is the drug dose. It also provides a clear understanding of how changes to the feature vector influence the trajectory\u2019s direction, for instance, transitioning it from ascending to descending. It is impossible to answer such queries using SHAP values.\nIn the table below we summarize examples of questions that can be asked about the model and whether they can be answered by SHAP and TIMEVIEW. We also observe that SHAP can always be used on top of TIMEVIEW to get additional insights. However, we cannot get TIMEVIEW-type explanations from just any black box model."
        },
        {
            "heading": "E.4 ADDITIONAL SNAPSHOTS OF THE INTERFACE",
            "text": "Tacrolimus. We apply our model to the Tacrolimus dataset (Woillard et al., 2011) that contains measurements of drug concentration in blood, and we present a snapshot of the trained model in Figure 14. The band under each slider shows the values of the composition map when we change the value of the corresponding slider but all other features are fixed. This allows us to answer questions such as \u201cWhat If \u201d, \u201cHow to be that\u201d, and \u201cHow to still be this\u201d from the XAI Question Bank (Liao et al., 2020) discussed in Section 1. For instance, in Figure 14, we choose the second transition point that corresponds to the maximum concentration of the drug. On the secondary plot, on the right, we can see how the position of this maximum changes as we vary any of the covariates. Currently shown plot indeed shows that if the dose of the drug is decreased then the maximum concentration decreases as well. We can also see that it is not an entirely linear relationship."
        },
        {
            "heading": "Current feature values",
            "text": "Sine. We also show a snapshot of our interface when TIMEVIEW is applied to the Sine dataset\u2014 Figure 15.\nBeta. Sometimes, we may be interested in the effect of perturbing two features at the same time. We visualize it using a colorful 2D contour plot. Figure 16 shows TIMEVIEW applied to the Beta dataset.\nE.5 INSIGHT INTO RESULTS\nSome methods perform better on real datasets than synthetic ones. As we explain in Section 7, we create the two synthetic datasets (Sine and Beta) to be challenging in the following way: the trend of the trajectory changes significantly for different features. The same is true for the Tumor datasets. The tumor growth model we use was proposed to fit many different cancer trajectories (Wilkerson et al., 2017). That includes exponential increase, exponential decay, decrease and then increase. Although real data can be noisy, the bigger variability of trends makes it harder for some methods.\nGAM-T struggles on the Tumor dataset. As we mention in Section 6, GAM-T\u2019s predicted trajectories are all parallel. They are equal to the shape function ft(t) shifted vertically by f1(x1) + . . . ,+fM (xM ) for static features x. That means it can model only one trend. As the focus of the synthetic datasets was on different trends, it is not surprising that GAM-T performs very poorly on them. In particular, the Tumor dataset contains trajectories of different trends (e.g., exponential increase, exponential decay, decrease and then increase)."
        },
        {
            "heading": "E.6 LIMITATIONS AND FUTURE WORKS",
            "text": "Static inputs. TIMEVIEW is a particular application of bi-level transparency for time series forecasting from static features. We hope future works will extend it to settings where the input may contain the previous part of the trajectory or other exogenous time series.\nModelling the trajectory as a cubic spline. We chose to model the trajectory as a cubic spline because cubic splines are flexible, and this representation allows for a straightforward composition extraction (see Section 5 and Appendix C). However, not all trajectories can be effectively approximated by a cubic spline. Future works should investigate other ways to represent the trajectory that would still make it amenable to composition extraction.\nRegularization. Although we use the L2 penalty to constrain how quickly the compositions change as we change the input features, future research could explore constraining the learned composition map for improved interpretability or designing better penalties for overly long and abruptly changing compositions.\nLikelihood of a sample. The visualization interface allows for showing all combinations of input variables, even ones that do not appear in reality. In the future, the interface can be enhanced to display information about the likelihood of a particular sample (or a whole set of samples).\nConfidence bounds. We could apply a technique like Deep Ensembles to get uncertainty estimates on the coefficients of the B-Spline basis functions which we could then transform into uncertainty estimated for the whole trajectory. Another interesting approach would be to adapt uncertainty estimation to our conceptual framework of trends and properties, i.e., how uncertain we are about a particular trend.\nModeling seasonality. Our framework could accommodate seasonality by modeling seasonal changes as a sine function where the neural network determines the frequency, magnitude, and offset (the same way as the B-Spline coefficients). Our model would be defined as:\ng(x)(t) = B\u2211 k=1 h(x)b\u03d5b(t) +A(x) sin(\u03c9(x)t+ \u03c8(x)) (14)\nwhere A : RM \u2192 R, \u03c9 : RM \u2192 R, \u03c8 : RM \u2192 R are either separate neural networks or they share weights with the encoder h.\nThen, we can treat the compositions in the same way as the current implementation of TIMEVIEW but with three additional properties to monitor (in addition to transition points). This allows us to answer questions like \u201dHow would the variability increase if I decrease this particular feature?\u201d.\nAlternatives to cubic splines. As we describe in Section 5, we chose cubic splines because we needed a space of trajectories that satisfies the following two criteria.\n\u2022 The set of dynamical motifs S is compatible with the class of predicted trajectories Y\u0302 . \u2022 For every y\u0302 \u2208 Y\u0302 we can calculate its composition CS [y\u0302]\nWe could use many other basis functions, such as a sine basis or some types of wavelets. They could satisfy the first condition, but the composition would likely need to be calculated numerically and not analytically as it is done now. In case we are interested in a different set of motifs (please see Appendix A.4 for examples), other choices may also be possible. For instance, for range motifs or monotonic motifs, Haar wavelets may be a good choice, and we suspect the compositions can be calculated efficiently.\nApplications to traditional XAI techniques for static predictions. Adhering to our framework provides additional output next to the standard forecasted trajectory: the current composition and the coordinates of the transition points. Traditional XAI techniques for regression and classification can be applied to these additional outputs instead of individual trajectory points to gain more meaningful explanations. Thus, techniques such as dimensionality reduction, feature importance methods, local surrogates, and counterfactual explanations can now be extended to time series forecasting. This opens up numerous potential extensions and applications of our approach.\nE.7 INHERENTLY DISCRETE TRAJECTORIES\nModeling a phenomenon as a continuous system is an established practice in sciences and engineering. That includes settings mentioned in our paper, such as disease modeling or drug concentration in blood. Of course, in practice, we only observe discrete measurements, which may be irregular and noisy. However, sometimes the phenomenon cannot be modeled as a continuous system. This happens where the set of time points is inherently discrete, and there is no notion of time \u201cin between the time points\u201d. For instance, the number of sunshine hours for each day would be an inherently discrete trajectory as there is no meaningful \u201cnumber of sunshine hours\u201d between two consecutive days. We note that our conceptual work in Section 2 still accommodates such scenarios. However, the formalism in Section 4 must be adapted."
        },
        {
            "heading": "E.8 TRAJECTORY CHANGES",
            "text": "In Section 2 we claim that although a prediction of a regression algorithm can change in only 3 ways (increase, decrease, remain constant), a trajectory can change in numerous ways. To see that, let us consider a set of time points T = {1, 2, . . . , 9, 10}. The function can increase, decrease, or remain constant for each time step. Thus, the function described by these ten values can change in 310 = 59049 ways (which is much bigger than 3\u2014the number of ways an output of a regression model can change). We describe this in the paragraph on \u201dincreased cognitive load\u201d in Section 2. It is infeasible for a human to reason about all these different possibilities, which motivates our proposal of a top-down approach. The situation becomes even more complex where the trajectory is not defined over ten time points but over a whole real interval\u2014which is the focus of our work."
        },
        {
            "heading": "F EXTENDED RELATED WORKS",
            "text": "Transparent models for static predictions. Standard transparent methods for static predictions include linear/logistic regression, scoring systems (Ustun & Rudin, 2016), decision trees/rule lists (Angelino et al., 2018; Hu et al., 2019), and generalized additive models (GAMs) (Hastie & Tibshirani, 1986; Lou et al., 2012). Such methods can often be used for time series forecasting by passing the time t as an additional feature. They often satisfy bi-level transparency but have poor performance. For instance, all trajectories predicted by linear regression and GAMs are parallel; thus, they cannot model different trends (Section 7). Decision Trees capture non-additive interactions, enabling flexible forecasting models. However, they require many splits to approximate the ground truth, leading to poor performance or incomprehensibility (Section 7).\nClosed-form expressions. Closed-form expressions are mathematical formulas composed of a finite number of variables, binary operators (+,\u2212,\u00d7,\u00f7), well-known functions (e.g., sin, exp, log), and constants. For instance, sin(x2) \u2212 e2.1y . A machine learning area that aims to find such expressions fitting the data is called Symbolic Regression (La Cava et al., 2021). This area originated from attempts to unearth equations describing physical systems through machine learning (Schmidt & Lipson, 2009; Udrescu & Tegmark, 2020; Holt et al., 2023; Biggio et al., 2021; D\u2019Ascoli et al., 2022). Differential equations represent another category of mathematical expressions that draw significant interest in the scientific community. Numerous algorithms have been proposed for discovering Ordinary Differential Equations (ODEs) (Brunton et al., 2016; Qian et al., 2022; Kaheman et al., 2020; Messenger & Bortz, 2021a) and Partial Differential Equations (Rudy et al., 2017; Long et al., 2019; Raissi & Karniadakis, 2018; Messenger & Bortz, 2021b). Although some mathematical expressions may satisfy bi-level transparency, this is not guaranteed as it depends on the actual form of the found equation. In fact, reparametrization of equations so that their parameters reflect quantities of key theoretical interest is an active area of research (Preacher & Hancock, 2015). We delve into this subject in greater detail and provide examples in Appendix E.\nOverview of XAI techniques. While our research focuses on transparent models, the landscape of post-hoc explainability methods has experienced significant growth. Such methods are used to explain the predictions of a black box model. These include feature importance methods (Ribeiro et al., 2016; Lundberg & Lee, 2017) (also called saliency methods) that highlight which features the model is sensitive to, example importance methods (Ghorbani & Zou, 2019; Pruthi et al., 2020; Crabbe et al., 2021) that identify important training samples, and concept-based explanations (Kim et al., 2018). Relatively little attention has been devoted to time series (Barredo Arrieta et al., 2020), but a few recent methods aim to extend feature importance to this setting (Crabbe\u0301 & Schaar, 2021; Leung et al., 2023).\nFeature importance for time series. While our research focuses on transparent models, many saliency (or feature importance) methods have been developed to highlight which features the model is sensitive to (Ribeiro et al., 2016; Lundberg & Lee, 2017). Although these methods have been extended to time series inputs (Crabbe\u0301 & Schaar, 2021; Leung et al., 2023), limited work has been done to extend them specifically to time series outputs. Current XAI techniques either assume the output is a scalar (Siddiqui et al., 2019) (e.g., time series classification (Hao & Cao, 2020)), treat the trajectory as a single object (Gao et al., 2023)\u2014thus do not show how a feature changes the trajectory\u2014or show a saliency map at each predicted point separately (Pan et al., 2020), thus allowing only for a bottom-up understanding of the predicted trajectory. Saliency methods can be broadly divided into Gradient-based (Sundararajan et al., 2017; Shrikumar et al., 2017), Perturbation-based\n(Zeiler & Fergus, 2014), and Attention-based methods (Vaswani et al., 2017; Alaa & van der Schaar, 2019; Lim et al., 2021). Other important examples include SHAP (Lundberg & Lee, 2017) based on Shapley values and LIME (Ribeiro et al., 2016) that fits a local linear model.\nShapelets and motifs. As our method discusses the shape of the trajectory, it may seem related to shapelet-based methods (Ye & Keogh, 2009). However, these methods are usually used for data mining and classification tasks. They aim to find subsequences of a time series that represent the most important patterns of each class and thus can be used to distinguish between them (Chen et al., 2022). Similarly, motif discovery identifies short repeating patterns in the time series (Torkamani & Lohweg, 2017) usually for insights into the problem or classification tasks.\nStatistical methods. Although methods like ARIMA or Hidden Markov Models (Rabiner & Juang, 1986) have potential for compact state transition equations, these models generally fail to meet bilevel transparency standards due to the complexity of understanding how the input feature changes influence the entire trajectory\u2014they do not allow for top-down understanding as the trajectory is constructed sequentially."
        }
    ],
    "title": "TOWARDS TRANSPARENT TIME SERIES FORECASTING",
    "year": 2023
}