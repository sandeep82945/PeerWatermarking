{
    "abstractText": "This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with an off-the-shelf detector with limited performance, our approach explicitly learns an image tagger using tags parsed from image-paired text and thus provides a strong semantic guidance to vision-language models. In this way, Tag2Text can utilize largescale annotation-free image tags in accordance with image-text pairs, and provides more diverse tag categories beyond objects. As a result, Tag2Text demonstrates the ability of a foundational image tagging model, with superior zero-shot performance even comparable to fully supervised models. Moreover, by leveraging the tagging guidance, Tag2Text effectively enhances the performance of visionlanguage models on both generation-based and alignment-based tasks. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art results with similar model sizes and data scales, demonstrating the efficacy of the proposed tagging guidance. Codes, demo and pre-trained models are available at https://github.com/xinyu1205/recognize-anything.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinyu Huang"
        },
        {
            "affiliations": [],
            "name": "Youcai Zhang"
        },
        {
            "affiliations": [],
            "name": "Jinyu Ma"
        },
        {
            "affiliations": [],
            "name": "Weiwei Tian"
        },
        {
            "affiliations": [],
            "name": "Rui Feng"
        },
        {
            "affiliations": [],
            "name": "Yuejie Zhang"
        },
        {
            "affiliations": [],
            "name": "Yaqian Li"
        },
        {
            "affiliations": [],
            "name": "Yandong Guo"
        },
        {
            "affiliations": [],
            "name": "Lei Zhang"
        }
    ],
    "id": "SP:81b98201665b04804bd218b5c98340d558b8faec",
    "references": [
        {
            "authors": [
                "Harsh Agrawal",
                "Karan Desai",
                "Yufei Wang",
                "Xinlei Chen",
                "Rishabh Jain",
                "Mark Johnson",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee",
                "Peter Anderson"
            ],
            "title": "Nocaps: Novel object captioning at scale",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Hangbo Bao",
                "Wenhui Wang",
                "Li Dong",
                "Qiang Liu",
                "Owais Khan Mohammed",
                "Kriti Aggarwal",
                "Subhojit Som",
                "Furu Wei"
            ],
            "title": "Vlmo: Unified vision-language pre-training with mixture-of-modalityexperts",
            "venue": "arXiv preprint arXiv:2111.02358,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut"
            ],
            "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Wei Chao",
                "Zhan Wang",
                "Yugeng He",
                "Jiaxuan Wang",
                "Jia Deng"
            ],
            "title": "Hico: A benchmark for recognizing human-object interactions in images",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Xi Chen",
                "Xiao Wang",
                "Soravit Changpinyo",
                "AJ Piergiovanni",
                "Piotr Padlewski",
                "Daniel Salz",
                "Sebastian Goodman",
                "Adam Grycner",
                "Basil Mustafa",
                "Lucas Beyer"
            ],
            "title": "Pali: A jointly-scaled multilingual language-image model",
            "venue": "arXiv preprint arXiv:2209.06794,",
            "year": 2022
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Tat-Seng Chua",
                "Jinhui Tang",
                "Richang Hong",
                "Haojie Li",
                "Zhiping Luo",
                "Yantao Zheng"
            ],
            "title": "Nus-wide: a real-world web image database from national university of singapore",
            "venue": "In Proceedings of the ACM international conference on image and video retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "year": 2021
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Yichong Xu",
                "Zhe Gan",
                "Jianfeng Wang",
                "Shuohang Wang",
                "Lijuan Wang",
                "Chenguang Zhu",
                "Pengchuan Zhang",
                "Lu Yuan",
                "Nanyun Peng"
            ],
            "title": "An empirical study of training end-to-end visionand-language transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Everingham",
                "SM Ali Eslami",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes challenge: A retrospective",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Hao Fang",
                "Saurabh Gupta",
                "Forrest Iandola",
                "Rupesh K Srivastava",
                "Li Deng",
                "Piotr Doll\u00e1r",
                "Jianfeng Gao",
                "Xiaodong He",
                "Margaret Mitchell",
                "John C Platt"
            ],
            "title": "From captions to visual concepts and back",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Zhiyuan Fang",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Lijuan Wang",
                "Yezhou Yang",
                "Zicheng Liu"
            ],
            "title": "Compressing visual-linguistic model via knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyuan Fang",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Lin Liang",
                "Zhe Gan",
                "Lijuan Wang",
                "Yezhou Yang",
                "Zicheng Liu"
            ],
            "title": "Injecting semantic concepts into end-to-end image captioning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhe Gan",
                "Yen-Chun Chen",
                "Linjie Li",
                "Chen Zhu",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Large-scale adversarial training for vision-and-language representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "Lvis: A dataset for large vocabulary instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Saurabh Gupta",
                "Jitendra Malik"
            ],
            "title": "Visual semantic role labeling",
            "venue": "arXiv preprint arXiv:1505.04474,",
            "year": 2015
        },
        {
            "authors": [
                "Sunan He",
                "Taian Guo",
                "Tao Dai",
                "Ruizhi Qiao",
                "Bo Ren",
                "Shu-Tao Xia"
            ],
            "title": "Open-vocabulary multilabel classification via multi-modal knowledge transfer",
            "venue": "CoRR, abs/2207.01887,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaowei Hu",
                "Xi Yin",
                "Kevin Lin",
                "Lei Zhang",
                "Jianfeng Gao",
                "Lijuan Wang",
                "Zicheng Liu"
            ],
            "title": "Vivo: Visual vocabulary pre-training for novel object captioning",
            "venue": "In proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaowei Hu",
                "Zhe Gan",
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Zicheng Liu",
                "Yumao Lu",
                "Lijuan Wang"
            ],
            "title": "Scaling up vision-language pre-training for image captioning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Huang",
                "Youcai Zhang",
                "Ying Cheng",
                "Weiwei Tian",
                "Ruiwei Zhao",
                "Rui Feng",
                "Yuejie Zhang",
                "Yaqian Li",
                "Yandong Guo",
                "Xiaobo Zhang"
            ],
            "title": "Idea: Increasing text diversity via online multilabel recognition for vision-language pre-training",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim"
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International journal of computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Alina Kuznetsova",
                "Hassan Rom",
                "Neil Alldrin",
                "Jasper Uijlings",
                "Ivan Krasin",
                "Jordi Pont-Tuset",
                "Shahab Kamali",
                "Stefan Popov",
                "Matteo Malloci",
                "Alexander Kolesnikov"
            ],
            "title": "The open images dataset v4",
            "year": 1956
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Wei Li",
                "Can Gao",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Hao Liu",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
            "venue": "arXiv preprint arXiv:2012.15409,",
            "year": 2020
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Yue Liao",
                "Si Liu",
                "Fei Wang",
                "Yanjie Chen",
                "Chen Qian",
                "Jiashi Feng"
            ],
            "title": "Ppdm: Parallel point detection and matching for real-time human-object interaction detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Shilong Liu",
                "Lei Zhang",
                "Xiao Yang",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Query2label: A simple transformer way to multi-label classification",
            "venue": "arXiv preprint arXiv:2107.10834,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012\u201310022,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik"
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Filip Radenovic",
                "Abhimanyu Dubey",
                "Abhishek Kadian",
                "Todor Mihaylov",
                "Simon Vandenhende",
                "Yash Patel",
                "Yi Wen",
                "Vignesh Ramanathan",
                "Dhruv Mahajan"
            ],
            "title": "Filtering, distillation, and hard negatives for vision-language pre-training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Steven J Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Tal Ridnik",
                "Emanuel Ben-Baruch",
                "Nadav Zamir",
                "Asaf Noy",
                "Itamar Friedman",
                "Matan Protter",
                "Lihi Zelnik-Manor"
            ],
            "title": "Asymmetric loss for multi-label classification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Tal Ridnik",
                "Gilad Sharir",
                "Avi Ben-Cohen",
                "Emanuel Ben-Baruch",
                "Asaf Noy"
            ],
            "title": "Ml-decoder: Scalable and versatile classification head",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
            "venue": "arXiv preprint arXiv:2111.02114,",
            "year": 2021
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Ronghang Hu",
                "Vedanuj Goswami",
                "Guillaume Couairon",
                "Wojciech Galuba",
                "Marcus Rohrbach",
                "Douwe Kiela"
            ],
            "title": "Flava: A foundational language and vision alignment model",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Pengchuan Zhang",
                "Xiujun Li",
                "Lijuan Wang",
                "Lei Zhang",
                "Jianfeng Gao",
                "Zicheng Liu"
            ],
            "title": "Minivlm: A smaller and faster vision-language model",
            "year": 2012
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Xiyang Dai",
                "Zicheng Liu",
                "Yumao Lu",
                "Lijuan Wang"
            ],
            "title": "Ufo: A unified transformer for vision-language representation learning",
            "venue": "arXiv preprint arXiv:2111.10023,",
            "year": 2021
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Xiaowei Hu",
                "Linjie Li",
                "Kevin Lin",
                "Zhe Gan",
                "Zicheng Liu",
                "Ce Liu",
                "Lijuan Wang"
            ],
            "title": "Git: A generative image-to-text transformer for vision and language",
            "venue": "arXiv preprint arXiv:2205.14100,",
            "year": 2022
        },
        {
            "authors": [
                "Junke Wang",
                "Dongdong Chen",
                "Zuxuan Wu",
                "Chong Luo",
                "Luowei Zhou",
                "Yucheng Zhao",
                "Yujia Xie",
                "Ce Liu",
                "Yu-Gang Jiang",
                "Lu Yuan"
            ],
            "title": "Omnivl: One foundation model for image-language and video-language tasks",
            "venue": "arXiv preprint arXiv:2209.07526,",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang"
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Johan Bjorck",
                "Zhiliang Peng",
                "Qiang Liu",
                "Kriti Aggarwal",
                "Owais Khan Mohammed",
                "Saksham Singhal",
                "Subhojit Som"
            ],
            "title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
            "venue": "arXiv preprint arXiv:2208.10442,",
            "year": 2022
        },
        {
            "authors": [
                "Zirui Wang",
                "Jiahui Yu",
                "Adams Wei Yu",
                "Zihang Dai",
                "Yulia Tsvetkov",
                "Yuan Cao"
            ],
            "title": "Simvlm: Simple visual language model pretraining with weak supervision",
            "venue": "arXiv preprint arXiv:2108.10904,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Wu",
                "Jiayuan Mao",
                "Yufeng Zhang",
                "Yuning Jiang",
                "Lei Li",
                "Weiwei Sun",
                "Wei-Ying Ma"
            ],
            "title": "Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Legg Yeung",
                "Mojtaba Seyedhosseini",
                "Yonghui Wu"
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "year": 1917
        },
        {
            "authors": [
                "Yan Zeng",
                "Xinsong Zhang",
                "Hang Li"
            ],
            "title": "Multi-grained vision language pre-training: Aligning texts with visual concepts",
            "venue": "arXiv preprint arXiv:2111.08276,",
            "year": 2021
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao"
            ],
            "title": "Vinvl: Revisiting visual representations in vision-language models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Youcai Zhang",
                "Yuhao Cheng",
                "Xinyu Huang",
                "Fei Wen",
                "Rui Feng",
                "Yaqian Li",
                "Yandong Guo"
            ],
            "title": "Simple and robust loss design for multi-label learning with missing labels",
            "venue": "arXiv preprint arXiv:2112.07368,",
            "year": 2021
        },
        {
            "authors": [
                "Yue Zheng",
                "Yali Li",
                "Shengjin Wang"
            ],
            "title": "Intention oriented image captions with guiding objects",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Rohit Girdhar",
                "Armand Joulin",
                "Philipp Kr\u00e4henb\u00fchl",
                "Ishan Misra"
            ],
            "title": "Detecting twenty-thousand classes using image-level supervision",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "transformers (Vaswani"
            ],
            "title": "The generation decoder and alignment encoder share parameters with the cross-attention layers. The tag recognition",
            "venue": "BERTBase (Devlin et al.,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Vision language pre-training (VLP) has shown an effective approach for learning a generic multimodal representation and improving vision-language (VL) tasks including generation-based (e.g., image captioning) and alignment-based (e.g., image-text retrieval). As large-scale datasets of imagetext pairs (Sharma et al., 2018; Changpinyo et al., 2021; Schuhmann et al., 2021; Radford et al., 2021; Jia et al., 2021) become available, recent works mainly focus on using transformer-based models to perform contrastive (Radford et al., 2021; Jia et al., 2021; Li et al., 2021; Bao et al., 2021; Li et al., 2022; Yu et al., 2022) or generative learning (Wang et al., 2021b; Li et al., 2022; Wang et al., 2022a; Chen et al., 2022; Yu et al., 2022; Wang et al., 2022c;d; Li et al., 2023) from massive image-text pairs. While great progress has been made, such studies normally rely on brute force pre-training manners that involve direct interaction with different modality features, modeling weakly-supervised learning due to the lack of explicit alignment supervision between image and text (Li et al., 2020b; Hu et al., 2021; Zeng et al., 2021).\nPrior approaches (e.g., OSCAR (Li et al., 2020b), VIVO (Hu et al., 2021), X-VLM (Zeng et al., 2021)) introduce the use of object tags as anchor points to ease the learning of semantic alignments between images and texts. However, these approaches rely on obsolete detector-based VLP frameworks, which employ off-the-shelf object detectors (e.g., Faster RCNN (Ren et al., 2015)) to extract image features (as shown in Figure 1 1\u20dd). The primary limitation of detector-based models is that the used object detectors are normally not perfect but have to be kept frozen during VLP to maintain detection ability, thereby restricting the capacity of vision-language models (Li et al., 2021; Dou et al., 2022; Huang et al., 2022). Moreover, utilizing object detectors leads to a substantial increase in model parameters and running time (Li et al., 2021; Kim et al., 2021). Consequently, more recent works (Li et al., 2021; 2022; Dou et al., 2022; Li et al., 2023) primarily utilize detector-free VL\n\u2217Corresponding author.\nmodels to address these limitations, resulting in the discarding of valuable tags (as shown in Figure 1 2\u20dd).\nIn this work, as shown in Figure 1 3\u20dd, we re-introduce tag guidance into detector-free VL models via the novel approach of image tagging. We demonstrate that integrating image tagging with other VLP tasks in a multi-task manner is a natural and effective approach from two crucial perspectives. 1) Data: The pre-training image tags are obtained through automatically text semantic parsing, enabling large-scale annotation-free image tags in accordance with image-text pairs can be utilized, without the requirement of expensive grounding annotations which are necessary for object detectors. Image tagging also provides a better bridge between image and text, given that the parsed tag categories are more diverse and beyond objects, such as scenes, attributes, actions, etc. 2) Architecture: Image tagging merely necessitates adding a recognition head followed by the original image encoder, ensuring efficient end-to-end pre-training and resulting in fewer parameters and improved efficiency. Figure 1(b) provides a comprehensive comparison between image tagging and object detection.\nConcretely, we present Tag2Text, a VLP framework which introduces image tagging into visionlanguage models to guide the learning of visual-linguistic features. For image tagging, previous approaches primarily rely on limited manually annotated datasets (Lin et al., 2014; Everingham et al., 2015), resulting in a poor generalization capability. In contrast, Tag2Text utilizes largescale image-text pairs, achieving an exceptional tag recognition capability of 3,429 commonly human-used categories. Remarkably, Tag2Text demonstrates a foundational image tagging capability with superior zero-shot performance, which significantly outperforms other state-of-the-art (SOTA) vision-language models such as CLIP (Radford et al., 2021), BLIP (Li et al., 2022), and BLIP-2 (Li et al., 2023) and is even comparable to fully supervised models (Ridnik et al., 2023).\nMoreover, Tag2Text effectively leverages tagging guidance to enhance the performance of visionlanguage models. For generation-based tasks, we design the training task as image-tag-text generation which empowers the model to produce text descriptions based on the image features in accordance with assigned tags. As depicted in Figure 2, Tag2Text generates more comprehensive text descriptions with the guidance of comprehensively recognized tags. Additionally, Tag2Text permits users to input desired tags, providing the flexibility in composing corresponding texts (Zheng et al., 2019). For alignment-based tasks, while previous models rely on the alignment of multimodal features which are considered as black-box approaches, Tag2Text augments these methods by incorporating tags as visible alignment indicators.\nOur key contributions can be summarized as follows:\n\u2022 For the first time, Tag2Text demonstrates the potential of a foundational image tagging model by utilizing large-scale annotation-free image tags parsed from image-paired text, exhibiting zero-shot capabilities rivalling full supervision manners.\nmore comprehensive text descriptions (see Table 2 for quantitative results). Tag2Text also allows users to input specified tags to generate corresponding captions, offers a way of controlling caption generation through the use of input tags.\n\u2022 Tag2Text re-introduces tag guidance into detector-free vision-language models by seamlessly integrating image tagging, effectively enhancing the performance of both generationbased tasks and alignment-based tasks.\n\u2022 A wide range of downstream benchmarks, along with a series of qualitative results, demonstrate the superior tagging ability of Tag2Text and the efficacy of incorporating tagging guidance information into vision-language models."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Vision-Language Models consist of generation-based and alignment-based models. Generationbased models involve generating text related to an input image. The initial approach of generationbased models relies on a two-stage process of recognizing tags from an image and then using them to compose a caption (Fang et al., 2015). Notably, image features do not participate in the text generation stage. With remarkable progress of language models (Devlin et al., 2018; Brown et al., 2020; Ouyang et al., 2022), language modeling gradually becomes a dominant pre-training objective in vision-language generation-based models (Wang et al., 2021b; Li et al., 2022; Wang et al., 2022c; Chen et al., 2022; Wang et al., 2022a;d). Such an approach endows vision-language models with the capability to generate expressive captions conditioned on visual information. Distinguished from existing works, our proposed approach is a novel scheme of image-tag-text generation, enabling our model to effectively regulate the content and quality of the generated text based on assigned tags.\nAlignment-based models involve determining whether an image and a text are matched. Previous models perform either image-text contrastive learning (Radford et al., 2021; Jia et al., 2021; Li et al., 2021; Bao et al., 2021; Li et al., 2022; Huang et al., 2022) with the dual-encoder architecture or image-text matching (Li et al., 2020b; 2021; Bao et al., 2021; Dou et al., 2022; Li et al., 2022) with the fusion-encoder architecture. IDEA (Huang et al., 2022) introduces identified tags as additional text supervision only enhancing image classification accuracy. These models predominantly rely on the alignment of multi-modal features which are considered as black-box approaches for retrieval task. Tag2Text augments these methods by incorporating tags as visible alignment indicators, leading to further performance improvements.\nImage Tagging, also known as multi-label image recognition, is a fundamental computer vision task that involves identifying multiple tags for a given image. Traditional approaches rely on a fully connected classifier and Binary Cross-Entropy loss (BCE) for optimization. Recent studies propose transformer-based classifiers (Liu et al., 2021a; Ridnik et al., 2023) to better leverage visual features, as well as robust loss functions (Ridnik et al., 2021; Zhang et al., 2021b) to address the issues of missing samples and unbalanced positive-negative samples. Most existing multi-label datasets (Lin et al., 2014; Everingham et al., 2015) rely on manual annotations, which are laborintensive and difficult to scale up. Our study employs text semantic parsing to efficiently obtain\nimage tags and constructs a large-scale image tagging dataset comprising 3,429 commonly used categories, resulting in a superior tag recognition ability."
        },
        {
            "heading": "3 APPROACH",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW FRAMEWORK",
            "text": "We present Tag2Text, a VLP framework that enhances the performance of vision-language models by incorporating tagging guidance. Figure 3 shows the framework of Tag2Text. With large-scale image-text pairs, the core of Tag2Text lies in the utilization of image tags from texts. Initially, the image tags are extracted through text semantic parsing, providing a large-scale of tags without expensive manual annotations. Afterward, the parsed tags can serve as ground-truth labels for image tag recognition tasks. Moreover, we design a novel scheme of image-tag-text generation, enabling the model to effectively regulate the content and quality of the generated text with the guidance of recognized tags. Furthermore, Tag2Text encompasses image-text alignment and leverages tags as visible alignment indicators."
        },
        {
            "heading": "3.2 MINING TAGS FROM TEXTS",
            "text": "Text Semantic Parser is adopted to parse text into image tags. The parser (Wu et al., 2019) first identifies entities (= head +modifier) and relationships from the input sentence based on the rules of the dependency tree, which is a grammatical structure that maps syntactic relationships within a sentence. Subsequently, we obtain the tags (including objects, scenes, attributes, and actions) of the image based on the contrast maps from head \u2192 object/scene, modifier \u2192 attribute, and relationship \u2192 action. For instance, given the sentence \u201cA red alarm clock is on a wooden desk\u201d, the parser automatically parse this as: \u201chead\u201d: [\u2019alarm clock\u2019, \u2019desk\u2019], \u201cmodifier\u201d: [\u2019red\u2019, \u2019wooden\u2019], \u201crelation\u201d: [\u2019on\u2019].\nTag Category System Construction is based on the principle that tags with higher frequency are considered more significant since they reflect common elements in the image descriptions. By employing the semantic parser, we process 4 million open-source image-text pairs and select the 5,000 most frequently occurring tags. Further filtering by human annotation results in the selection of the most commonly human-used 3,429 categories of tags (e.g., synonyms such as \u201cperson\u201d and \u201chuamn\u201d are merged). More statistics and details are presented in Appendix B."
        },
        {
            "heading": "3.3 TAG2TEXT PRE-TRAINING",
            "text": "With triplet image-tag-text as inputs, Tag2Text employs a multi-task pre-training approach, which consists of Tagging, Generation, and Alignment. Both generation-based and alignment-based task\nutilize the guidance from image tagging to improve their performance. Concretely, the shared visual features obtained from the image encoder are interacted with various pre-training tasks through cross-attention.\nImage Tagging aims to associate image features with the corresponding tags. We apply the imagetag recognition decoder (Liu et al., 2021a) with the robust alignment loss function for optimization. Compared to CLIP, which relies on the alignment of global image features with text via dot product interaction. Tag2Text introduces a more fine-grained alignment of visual spatial features with tags (parsed from texts) through an efficient recognition decoder. This approach is particularly effective for multi-tag recognition, since the tags often correspond to multiple image regions and reside at the token level within the texts.\nImage-Tag-Text Generation aims to generate texts based on the image features in accordance with assigned tags. To achieve image-tag-text generation, Tag2Text employs the transformer encoder-decoder (Vaswani et al., 2017) architecture. The [BOS] token is prepended to the beginning of the text to indicate the start of a sequence. To eliminate positional bias, the image tags are rearranged prior to processing. Both tags and text are transformed into embeddings through tokenization and a word embedding matrix. The tag embeddings are integrated with image features in the image-tag interaction encoder and subsequently forwarded to the image-tag-text generation decoder for text generation. The text embeddings are utilized as ground truths to optimize the model via Language Modeling Loss (LM).\nThe distinction between image-tag-text generation and other generation approaches is illustrated in Figure 4. Our image-tag-text generation incorpo-\nrates tags as a bridge to guide image features for text generation in an end-to-end manner. This approach enables the model to generate more comprehensive and controllable captions, provided that many accurate tags are given as the guidance signal.\nImage-Text Alignment aims to determine whether a given pair of image and text is aligned. Following Li et al. (2022), Tag2Text leverages an additional image-text alignment encoder. The text is converted into embeddings via tokenization and word embedding. Then the text embeddings pass through the encoder and undergo coarse-grained Image-Text Contrastive Loss (ITC) with image features. Subsequently, the text embeddings undergo fine-grained Image-Text Matching Loss (ITM) with image features through cross-attention. The negative samples with higher ITC similarity will be selected for ITM with greater probability for hard mining."
        },
        {
            "heading": "3.4 TAG-GUIDED V+L TASKS",
            "text": "Image Tagging, also known as multi-label image recognition, demands the model to recognize all relevant tags of an image. Image tagging can serve as an effective indicator of the model\u2019s recognition abilities. As illustrated in Figure 3(a), Tag2Text achieves this task by directly utilizing the image-tag recognition decoder.\nImage Captioning entails the model to generate a textual description for a given image. Figure 3(b) shows that the same components for image-tag-text generation pre-training are utilized during finetuning. Previous image-text generation models are challenging to control the content of the generated description. By incorporating comprehensive tags recognized by the image-tag recognition decoder, our approach effectively improves the performance over the generated text. Furthermore, users can also input alternate guidance tags to generate descriptions highlighting specific aspects of the image.\nImage-Text Retrieval encompasses both image-to-text and text-to-image retrieval. Previous methods match image-text pairs based solely on the features of different modalities, resulting in a lack of control and interpretability. Our approach, as depicted in Figure 3(d), augments these methods by incorporating tags as visible alignment indicators (Image Recognize\u2212\u2192 Tag, Text Parse\u2212\u2192 Tag). By weighing the number of matching tags with feature similarity, Tag2Text boosts the retrieval results. Besides, real-world applications frequently involve users\u2019 searching for images using several keywords rather than a sentence, highlighting the advantages of our approach. Figure 5 presents some examples of visible alignment indicators that enable effective alignment between image and text."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Following Li et al. (2021; 2022), we pre-train our model on two widely used dataset settings, including a 4 million image dataset and a 14 million image dataset, respectively. The 4M image dataset setting includes two human-annotated datasets (COCO Lin et al. (2014) and VG Krishna et al. (2017)) and two web datasets (SBU Captions Ordonez et al. (2011) and CC-3M Sharma et al. (2018)). The 14M image dataset setting builds upon the 4M setting by adding more noisy web dataset CC-12M Changpinyo et al. (2021). We adopt two most widely used backbones pre-trained on ImageNet (Deng et al., 2009) as the image encoder: ViTBase (Dosovitskiy et al., 2021) and SwinBase (Liu et al., 2021b). Unless illustrated with subscript, the default model vision refers to SwinBase as the image encoder. More implementation details are provided in Appendix A."
        },
        {
            "heading": "4.2 EVALUATION ON IMAGE TAGGING",
            "text": "To assess the tagging capability of Tag2Text, we conduct evaluation on two multi-label recognition tasks: COCO and OpenImages (Kuznetsova et al., 2020). Given the significant number of rare categories with missing labels in OpenImages, we curated a subset that encompasses common categories with high quality labels. We also employed an internal high-quality annotated test set, known as OPPO, to provide a comprehensive evaluation of tagging performance. Our model is fine-tuned on the COCO training dataset using texts and tags parsed from the texts provided in COCO caption annotations, since the original COCO multi-label annotations encompass only 80 categories of tags. More details can be found in Appendix C. Addition zero-shot evaluations on NUS-WIDE (Chua et al., 2009) are provided in Appendix E.\nThese tagging benchmarks serve to gauge the recognition capabilities of image recognition models for prevalent categories. The comparison of Tag2Text with other SOTA recognition models (both classification models and vision-language models) is shown in Table 2. With regard to classification models, Tag2Text demonstrates superior zero-shot recognition capabilities, even comparable to full supervision manners of ML-Decoder. With regard to vision-language models, for alignment vision-language models, we calculate the similarity between an image and all tag categories with\nthresholding to obtain image tags. For captioning vision-language models, we parse the caption and classify them into synonyms to obtain image tags. Remarkably, both the tagging and captioning capabilities of Tag2Text significantly exceeds other SOTA vision-language models (including CLIP, BLIP, BLIP-2) in common category recognition.\nto zero-shot performance; Green refers to fully supervised learning; Yellow denotes that the model has seen the corresponding training images, but not the annotations. Notably, Tag2Text\u2019s zero-shot generalization on OpenImages is even comparable with ML-Decoder\u2019s full supervision."
        },
        {
            "heading": "4.3 EVALUATION ON IMAGE CAPTIONING",
            "text": "In Section 4.2, we provide a novel captioning evaluation paradigm based on image tagging benchmarks, which effectively gauges the caption recognition capability for prevalent categories. In this section, we evaluate Tag2Text on two established Image Captioning benchmarks: COCO Captions (Karpathy & Fei-Fei, 2015) and NoCaps (Agrawal et al., 2019), with the latter focusing more on recognizing novel objects. The comparison of Tag2Text with other SOTA generation models can be found in Table 3. To ensure fairness, we compare the results with base version of all methods without utilizing the CIDEr optimization (Rennie et al., 2017).\nThe experimental results demonstrate Tag2Text outperforms other methods across all metrics on both benchmarks with similar model size and data scale. Furthermore, Tag2Text surpasses most metrics of BLIP+Bootstrap, which employs a dataset bootstrapping approach, as well as LEMON and SIMVLM, which are pre-trained on 200 million and 1.8 billion images, respectively. Notably, due to the dual capability for both generation and alignment of Tag2Text, the performance of Tag2Text can also be further enhanced through Bootstrap, which we aim to accomplish in our future work."
        },
        {
            "heading": "4.4 EVALUATION ON IMAGE-TEXT RETRIEVAL",
            "text": "The Image-Text Retrieval task is evaluated on two benchmarks: COCO and Flickr30K (Plummer et al., 2015), for both image-to-text retrieval (I2T) and text-to-image retrieval (T2I). The performance comparsion with other methods are shown in Table 4. Under equivalent pre-training data and image encoder configurations, Tag2Text demonstrates comparable or superior performance compared to ALBEF, VLMO, and BLIP. Tag2Text-Swin leads to a further substantial improvement in performance. More importantly, the integration of tag alignment in Tag2Text makes it well-suited for practical search scenarios, where users search through the query of several keywords."
        },
        {
            "heading": "4.5 ANALYSIS OF TAGGING GUIDANCE",
            "text": "In this section, we present a detailed analysis to investigate the effectiveness of tagging guidance.\nEvaluation of Tagging Guidance. In Table 5, we verify the superiority of incorporating tagging guidance on a wide range of downstream benchmarks, including four generation benchmarks, two retrieval benchmarks, and two recognition benchmarks.\nControllability Analysis. We provide the analysis of the controllability of tagging guidance for image captioning. We manipulate the threshold of the tagging head to obtain tagging guidance of varying quality. As depicted in Figure 6, the captioning performance (evaluation on COCO) declines when the precision or recall of tagging (evaluation on OpenImages) is low. These results effectively establish that tagging guidance exerts significant control over image captioning.\nBetter Bridge between Image and Text. In order to highlight the superiority of Tag2Text in tag recognition, we compare the recognized tags with other SOTA open-source models on multi-label recognition and object detection. For multi-label recognition, we employ the ML-Decoder (Ridnik et al., 2023) model based on OpenImages (Kuznetsova et al., 2020) of 9,600 categories. For object detection, we employ the Detic (Zhou et al., 2022) model based on LVIS (Gupta et al., 2019) of 1,203 categories. The comparison results are illustrated in Figure 7, ML-Decoder recognizes many tags which are not frequently used and lacks many obvious common tags. On the other hand, Detic is limited to only recognizing object categories. In contrast, Tag2Text provides a more comprehensive and widely used set of tags, including objects, scenes, attributes, and actions.\nAblation Study. Despite the presence of noise for tags parsed from the texts, our model design enables Tag2Text to leverage tags with noise and achieve exceptional image tagging performance. As demonstrated in Table 6, the integration of vision-language pre-training tasks into the model also improves the tag recognition ability. Furthermore, Table 6 highlights two-stage \u201cpre-training + finetuning\u201d paradigm in the context of multi-label recognition. The model, trained solely on the limited COCO dataset, fails to generalize well on the OpenImages dataset, attaining an mAP score of 57.5.\nHowever, when pre-trained on a large dataset, our model exhibits remarkable performance, even in the absence of any exposure to the training images from the OpenImages dataset, achieving an mAP score of 83.4, which is comparable to the fully supervised performance of 85.8 mAP."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper has presented Tag2Text, a vision-language pre-training framework, which introduces image tagging into vision-language models. Tag2Text achieves superior image tag recognition ability by exploiting fine-grained text information. Moreover, Tag2Text leverages tagging guidance and effectively enhances the performance and controllability of vision-language models. On a wide range of vision-language tasks, Tag2Text demonstrates the value of tag as a bridge between image and text to infuse structure and knowledge information into vision-language models."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62172101), the Science and Technology Commission of Shanghai Municipality (No.22DZ1100101, No. 21511100500), and the OPPO Research Foundation."
        },
        {
            "heading": "A PRE-TRAINING DETAILS",
            "text": "A.1 IMPLEMENTATION DETAILS.\nThe encoder-decoder used for text generation and encoder for image-text alignment are 12-layer transformers (Vaswani et al., 2017) initialized from BERTBase (Devlin et al., 2018). The generation decoder and alignment encoder share parameters with the cross-attention layers. The tag recognition decoder is a 2-layer transformer initialized from BERTBase and shares parameters with the lowest 2-layer of the interaction encoder. The models are pre-trained for 20 epochs with the batch size of 960 on 8 NVIDIA A100 GPUs. The optimizer is the AdamW (Loshchilov & Hutter, 2017) with a weight decay of 0.05. The learning rate is warmed-up to 1e\u22124 in the first 3,000 iterations, and then follows linear decay with a rate of 0.9. The input images are resized to 224\u00d7 224 uniformly during the pre-training stage. Due to the presence of missing labels in the parsed tags and an imbalanced distribution of positive and negative samples, we employ Asymmetric Loss (ASL) (Ridnik et al., 2021) for image tagging optimization.\nA.2 PRE-TRAINING OBJECTIVES.\nImage Tagging. Image tagging is generally decomposed into multiple binary classification with binary cross-entropy loss (BCE) to optimize:\nLTagging = \u2212Ey\u223cD [BCE(y, P (y))]\n= \u2212Ey\u223cD\n[ C\u2211\ni=1\nyi logP (yi) + (1\u2212 yi) log(1\u2212 P (yi)) ] (1)\nwhere yi represents the label for the i-th category and C denotes the total number of categories. We employ Asymmetric Loss (ASL) (Ridnik et al., 2021) for optimization instead of BCE.\nLTagging = \u2212Ey\u223cD [ASL(y, P (y))]\n= \u2212Ey\u223cD [ C\u2211\ni=1\nyi(1\u2212 P (yi))\u03b3+ logP (yi) (2)\n+ (1\u2212 yi)P (yi)\u03b3\u2212 log(1\u2212 P (yi)) ]\nImage-Tag-Text Generation. The pre-training objective for image-tag-text generation is Language Modeling Loss (LM) (Brown et al., 2020) to maximize the likelihood of the text in an autoregressive manner:\nLLM = \u2212Ex\u223cD [CE(x, P (x))]\n= \u2212Ex\u223cD [ N\u2211 i=1 logP (xi | x<i) ] (3)\nwhere xi represents the i-th token in the text and N denotes the total number of text tokens. Compared with the bidirectional Masked Language Modeling (MLM) (Devlin et al., 2018), the unidirectional LM Loss is also gradually widely used in recent VLP studies (Wang et al., 2021b; 2022a; Chen et al., 2022; Li et al., 2022), as it enables seamless transfer of the model to text generation tasks.\nImage-Text Alignment. The pre-training objectives for alignment utilizes Image-Text Contrastive Loss (ITC) based on multi-modal feature cos similarity (Radford et al., 2021; Li et al., 2021; Bao et al., 2021; Li et al., 2022) and Image-Text Matching Loss (ITM) based on multi-modal feature fusion (Li et al., 2021; Bao et al., 2021; Li et al., 2022).\nLITC = \u2212EI,T\u223cD [CE(y(I), P (I)) + CE(y(T), P (T))] (4)\nLITM = \u2212EI,T\u223cD [BCE(y, P (I,T))] (5)"
        },
        {
            "heading": "B TAG CATEGORY DETAILS",
            "text": "Pre-training Dataset. The pre-training dataset statistics, including the number of texts and tags parsed from texts, are presented in Table 7.\nTag Category System. We obtain annotation-free image tags parsed from its paired text. We construct our tag category system based on the principle that tags with a higher frequency are more significant, as they represent common elements in image descriptions. To this end, we process 4 million open-source image-text pairs (COCO, VG, SBU, CC-3M), and select the 5,000 most frequently occurring tags. Further filtering by human annotation results in the selection of the most commonly human-used 3,429 categories of tags (including objects, scenes, attributes, actions). The tag category statistics are presented in Table 8. An illustration of the tag categories is provided in Figure 8, where the size of each word is proportional to the frequency of the category in the open-source image-text pairs.\nComparison with Public Datasets. This section provides the statistics of the overlap between our tag categories and other widely used public datasets. Table 9 shows the statistics for object/scene categories with other datasets including OpenImages (Kuznetsova et al., 2020), COCO (Lin et al., 2014), ImageNet (Deng et al., 2009), CIFAR100 (Krizhevsky et al., 2009). Table 10 presents\nthe statistics for action categories with other datasets including HICO (Chao et al., 2015), VCOCO (Gupta & Malik, 2015), HOI-W (Liao et al., 2020). To the best of our knowledge, we do not find appropriate public datasets for the recognition of attribute tag categories.\nImpact on Vocabulary Set Size. In Table 11, we expand the vocabulary set from 3,429 to 4,585 categories and compare the performances. Notably, the tagging performance decrease with the larger vocabulary set. We attribute to two possible reasons: 1) The increased complexity in training with more categories. 2) The additional categories leading to more noise, as they lack sufficient training data, thereby impacting the model\u2019s efficiency.\nC IMAGE TAGGING DETAILS\nTuning Dataset. We employ tags parsed from COCO Caption (Lin et al., 2014) for image tagging finetuning. Each image in the COCO Caption dataset is accompanied by five descriptive sentences, offering a comprehensive description of the image. As a result, the tags parsed from these captions are considered to be a close approximation of a complete set of tag labels.\nTest Benchmarks. We respectively take the overlapping categories of Tag2Text with the tagging benchmarks for evaluation. The statistics of the image tagging benchmarks set are shown in Table 12.\nTagging Head Comparison. Table 13 investigates the impact of various tagging recognition heads on the performance of Tag2Text. The results show that transitioning from full connection to tag recognition decoder (Liu et al., 2021a) results in improved performance in image tagging recognition, followed by improvements in caption generation. This indicates that the enhancement of tagging recognition leads to improved text generation. To mitigate the increase in model parameters, we propose sharing the parameters of image-tag recognition decoder and image-tag interaction encoder, reducing the parameters and further boosting the performance.\nControl of Tagging Guidance. During the image tagging inference process, the tagging head outputs logits (ranging from 0 to 1) for each category. These logits are compared to a set threshold to determine the output tags. When the logits exceed this threshold, the corresponding tag category is outputted. Therefore, the tagging guidance can be controlled by adjusting the threshold. For instance, a lower threshold yields more image tags, resulting in higher recall. On the contrary, a higher threshold increases precision.\nD IMAGE CAPTIONING DETAILS\nFinetuning Strategies Comparison. This section discusses two strategies employed by Tag2Text for image captioning finetuning based on input guidance tags, as illustrated in Figure 9 1\u20dd and 2\u20dd. The first strategy, similar to image-tag-text generation in the pre-training stage, involves input guidance tags parsed from the paired text. This enables the model to utilize all available tags to create a comprehensive text description. However, Tag2Text usually recognizes tags with similar meanings (e.g., \u201cman\u201d, \u201cperson\u201d), which may result in redundant sentences (e.g., \u201ca man ..., while a person ...\u201d) with low evaluation metrics.\nThe second strategy involves using the same process as the inference stage, where the input guidance tags for fine-tuning are recognized by the model. This approach allows the model to select guidance tags for generating more precise text generation. In this paper, we utilize a mixed training approach that combines both strategies.\nMore Example Results. In Figure 12, we show more examples of Tag2Text using tag guidance to generate comprehensive descriptions."
        },
        {
            "heading": "E ADDITION ZERO-SHOT EVALUATIONS",
            "text": "In this section, we conduct addition zero-shot evaluations on NUS-WIDE Chua et al. (2009), a well-established tagging benchmark including 81 categories. All images in NUS-WIDE are out-ofdistribution data, since Tag2Text did not utilize any NUS-WDIE training images during its training process. The results are presented in the Table 14. Notably, Tag2Text also demonstrates superior zero-shot performance, exceeding both CLIP and BLIP, while utilizing much less training data."
        },
        {
            "heading": "F EVALUATION ON VISUAL QUESTION ANSWERING",
            "text": "Visual Question Answering aims to predict an answer to a question based on an image. Previous approaches typically treat VQA as a multi-class classification problem with a fixed set of answer choices. In contrast, Tag2Text employs an encoder-decoder architecture for generation, which is suited for generating free-form answers. As shown in Figure 3(c), the question, joint with tags, interacts with image features in the encoder, and then forwards to the decoder to generate a freeform answer.\nWe conduct experiments on the VQA v2 (Goyal et al., 2017) benchmark for Visual Question Answering. Table 11 shows that Tag2Text outperforms or achieves competitive results with other approaches. On the one hand, Tag2Text-ViT also ought not to be inferior to ALBEF, as the structure of Tag2Text degenerates into that of ALBEF without tagging guidance. We attribute the slightly inferior performance to our insufficient resources available for conducting hyper-parameter search.\nOn the other hand, we observe that the VQA v2 dataset is characterized primarily by straightforward questions and answers (e.g., \u201cIs there a big tree behind the clock? Yes.\u201d), which is challenging to directly augment through identified tags. We anticipate that a more strategic utilization of fine-grained positioning information derived from tagging guidance, combined with more complex benchmarks, can effectively highlight the superiority of tagging guidance for VQA tasks. We leave these explorations for future research."
        },
        {
            "heading": "G LIMITATIONS",
            "text": "Hallucinatory Captions. Tag2Text benefits from its powerful tagging capabilities. As depicted in Figure 7, there is a strong correlation between captioning performance and tagging guidance performance. In practical applications, we observe that incorrect user-provided tags may lead to hallucinatory captions.\nSmall Objects. In addition, evaluating a tagging model capable on 3,429 categories is also challenging. Our quantitative comparison and visual validations reveal that Tag2Text efficiently recognizes common objects and scenes, yet struggles in small objects (e.g., spoon or baseball). Our empirical experiments indicates that increasing the resolution during fine-tuning significantly improves performance on these small objects."
        }
    ],
    "title": "TAG2TEXT: GUIDING VISION-LANGUAGE MODEL VIA IMAGE TAGGING",
    "year": 2024
}