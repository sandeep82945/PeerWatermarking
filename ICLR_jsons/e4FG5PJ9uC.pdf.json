{
    "abstractText": "We show how perceptual embeddings of the visual system can be constructed at inference-time with no training data or deep neural network features. Our perceptual embeddings are solutions to a weighted least squares (WLS) problem, defined at the pixel-level, and solved at inference-time, that can capture global and local image characteristics. The distance in embedding space is used to define a perceptual similarity metric which we call LASI: Linear Autoregressive Similarity Index. Experiments on full-reference image quality assessment datasets show LASI performs competitively with learned deep feature based methods like LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020), at a similar computational cost to hand-crafted methods such as MS-SSIM (Wang et al., 2003). We found that increasing the dimensionality of the embedding space consistently reduces the WLS loss while increasing performance on perceptual tasks, at the cost of increasing the computational complexity. LASI is fully differentiable, scales cubically with the number of embedding dimensions, and can be parallelized at the pixel-level. A Maximum Differentiation (MAD) competition (Wang & Simoncelli, 2008) between LASI and LPIPS shows that both methods are capable of finding failure points for the other, suggesting these metrics can be combined.",
    "authors": [],
    "id": "SP:ea4368affed60c5bc7fc8a2eebab850b00bbae26",
    "references": [
        {
            "authors": [
                "Johannes Ball\u00e9",
                "Valero Laparra",
                "Eero P Simoncelli"
            ],
            "title": "End-to-end optimized image compression",
            "venue": "arXiv preprint arXiv:1611.01704,",
            "year": 2016
        },
        {
            "authors": [
                "Johannes Ball\u00e9",
                "David Minnen",
                "Saurabh Singh",
                "Sung Jin Hwang",
                "Nick Johnston"
            ],
            "title": "Variational image compression with a scale hyperprior",
            "venue": "arXiv preprint arXiv:1802.01436,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher J Bates",
                "Robert Jacobs"
            ],
            "title": "Optimal attentional allocation in the presence of capacity constraints in visual search. 2020a",
            "year": 2020
        },
        {
            "authors": [
                "Christopher J Bates",
                "Robert A Jacobs"
            ],
            "title": "Efficient data compression in perception and perceptual memory",
            "venue": "Psychological review,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Bays",
                "Sebastian Schneegans",
                "Wei Ji Ma",
                "Timothy Brady"
            ],
            "title": "Representation and computation in working",
            "year": 2022
        },
        {
            "authors": [
                "Sangnie Bhardwaj",
                "Ian Fischer",
                "Johannes Ball\u00e9",
                "Troy Chinen"
            ],
            "title": "An unsupervised informationtheoretic perceptual quality metric",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern recognition and machine learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs, 2018",
            "venue": "URL http://github.com/google/jax",
            "year": 2018
        },
        {
            "authors": [
                "Timothy F Brady",
                "Talia Konkle",
                "George A Alvarez"
            ],
            "title": "Compression in visual working memory: using statistical regularities to form more efficient memory representations",
            "venue": "Journal of Experimental Psychology: General,",
            "year": 2009
        },
        {
            "authors": [
                "Thomas M Cover"
            ],
            "title": "Elements of information theory",
            "year": 1999
        },
        {
            "authors": [
                "Keyan Ding",
                "Kede Ma",
                "Shiqi Wang",
                "Eero P Simoncelli"
            ],
            "title": "Comparison of full-reference image quality models for optimization of image processing systems",
            "venue": "International Journal of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengfang Duanmu",
                "Wentao Liu",
                "Zhongling Wang",
                "Zhou Wang"
            ],
            "title": "Quantifying visual image quality: A bayesian view",
            "venue": "Annual Review of Vision Science,",
            "year": 2021
        },
        {
            "authors": [
                "Bernd Girod"
            ],
            "title": "What\u2019s wrong with mean squared error?, pp. 207\u2013220",
            "year": 1993
        },
        {
            "authors": [
                "Virginia Klema",
                "Alan Laub"
            ],
            "title": "The singular value decomposition: Its computation and some applications",
            "venue": "IEEE Transactions on automatic control,",
            "year": 1980
        },
        {
            "authors": [
                "Manoj Kumar",
                "Neil Houlsby",
                "Nal Kalchbrenner",
                "Ekin Dogus Cubuk"
            ],
            "title": "Do better imagenet classifiers assess perceptual similarity better",
            "venue": "Transactions of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Pavan C Madhusudana",
                "Neil Birkbeck",
                "Yilin Wang",
                "Balu Adsumilli",
                "Alan C Bovik"
            ],
            "title": "Conviqt: Contrastive video quality estimator",
            "venue": "arXiv preprint arXiv:2206.14713,",
            "year": 2022
        },
        {
            "authors": [
                "Pavan C Madhusudana",
                "Neil Birkbeck",
                "Yilin Wang",
                "Balu Adsumilli",
                "Alan C Bovik"
            ],
            "title": "Image quality assessment using contrastive learning",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Bernd Meyer",
                "Peter E Tischer"
            ],
            "title": "Glicbawls-grey level image compression by adaptive weighted least squares",
            "venue": "In Data Compression Conference,",
            "year": 2001
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Chris R Sims"
            ],
            "title": "Rate\u2013distortion theory and human",
            "venue": "perception. Cognition,",
            "year": 2016
        },
        {
            "authors": [
                "Chris R Sims"
            ],
            "title": "Efficient coding explains the universal law of generalization in human",
            "venue": "perception. Science,",
            "year": 2018
        },
        {
            "authors": [
                "Zhou Wang",
                "Eero P Simoncelli"
            ],
            "title": "Maximum differentiation (mad) competition: A methodology for comparing computational models of perceptual quantities",
            "venue": "Journal of Vision,",
            "year": 2008
        },
        {
            "authors": [
                "Zhou Wang",
                "Eero P Simoncelli",
                "Alan C Bovik"
            ],
            "title": "Multiscale structural similarity for image quality assessment",
            "venue": "In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers,",
            "year": 2003
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing,",
            "year": 2004
        },
        {
            "authors": [
                "Xuekai Wei",
                "Jing Li",
                "Mingliang Zhou",
                "Xianmin Wang"
            ],
            "title": "Contrastive distortion-level learningbased no-reference image-quality assessment",
            "venue": "International Journal of Intelligent Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Hanwei Zhu",
                "Baoliang Chen",
                "Lingyu Zhu",
                "Shiqi Wang"
            ],
            "title": "From distance to dependency: A paradigm shift of full-reference image quality assessment",
            "venue": "arXiv preprint arXiv:2211.04927,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "We show how perceptual embeddings of the visual system can be constructed at inference-time with no training data or deep neural network features. Our perceptual embeddings are solutions to a weighted least squares (WLS) problem, defined at the pixel-level, and solved at inference-time, that can capture global and local image characteristics. The distance in embedding space is used to define a perceptual similarity metric which we call LASI: Linear Autoregressive Similarity Index. Experiments on full-reference image quality assessment datasets show LASI performs competitively with learned deep feature based methods like LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020), at a similar computational cost to hand-crafted methods such as MS-SSIM (Wang et al., 2003). We found that increasing the dimensionality of the embedding space consistently reduces the WLS loss while increasing performance on perceptual tasks, at the cost of increasing the computational complexity. LASI is fully differentiable, scales cubically with the number of embedding dimensions, and can be parallelized at the pixel-level. A Maximum Differentiation (MAD) competition (Wang & Simoncelli, 2008) between LASI and LPIPS shows that both methods are capable of finding failure points for the other, suggesting these metrics can be combined."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The applicability of computer vision in real world applications hinges on how well the loss function aligns with the human visual system. Learning end-to-end solutions for applications such as superresolution and lossy compression (Balle\u0301 et al., 2016; 2018) requires differentiable similarity metrics that correlate well with how humans perceive change in visual stimuli. Unfortunately, widely used metrics such as PSNR/MSE that measure change at the pixel-level, although differentiable, do not satisfy this criterion.\nThe failure of pixel-level metrics in capturing perception has prompted the design of similarity metrics at the patch level, inspired by a subfield of human psychology known as psychophysics. The most successful one to date is the multi-scale structural similarity metric MS-SSIM (Wang et al., 2003; 2004) which models luminance and contrast perception. Despite these efforts, the complexity of the human visual system remains difficult to model by hand; evidenced by the failures of MS-SSIM in predicting human preferences in standardized image quality assessment (IQA) experiments (Zhang et al., 2018).\nTo move away from handcrafting similarity metrics the community has shifted towards using deep features from large pre-trained neural networks. For example, the Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) metric assumes the L2 distance between these deep features can capture human perception. In the same work, the authors introduce the Berkeley Adobe Perceptual Patch Similarity (BAPPS) dataset, which has become widely accepted as a benchmark for measuring perceptual alignment of similarity metrics. LPIPS uses deep features as inputs to a smaller neural network model that is trained on the human annotated data available in BAPPS which indicate human preference of certain images over others with respect to perceived similarity. Collecting this data is expensive as it requires large-scale human trials, and the generalization capabilities of metrics beyond this dataset are not well understood (Kumar et al., 2022).\nTo side-step expensive data collection procedures recent work has attempted to directly learn embeddings inspired by well known phenomena of the human visual system. For example, the Perceptual Information Metric (PIM) (Bhardwaj et al., 2020) optimizes a mutual information (Cover, 1999) based metric and does not use annotated labels. The deep features resulting from this procedure perform competitively with LPIPS on the BAPPS dataset as well as other settings (Bhardwaj et al., 2020). Other methods such as (Wei et al., 2022) define a self-supervised objective where the neural network must predict a label indicating which distortion type, from a predefined set, was used to corrupt the image.\nIn this work, we put into question the necessity of deep features to define similarity metrics aligning with human preference. We take inspiration from recent work in the field of psychology which provide evidence that the visual working memory performs compression of visual stimuli (Bays et al., 2022; Bates & Jacobs, 2020b; Sims, 2018; Brady et al., 2009; Sims, 2016; Bates & Jacobs, 2020a). We employ methods that compress a visual stimuli at inference time with no pre-training or prior knowledge of the data distribution. Applying this procedure at inference-time means we do not require any expensive labelling procedures, nor unlabelled data, as in LPIPS or PIM. Our embeddings are learned at the pixel-level, but can capture patch-level semantics by solving a weighted least squares (WLS) problem from a neighborhood surrounding the pixel, a subcomponent of the lossless compression algorithm developed by Meyer & Tischer (2001).\nOur Linear Autoregressive Similarity Index (LASI) uses the L2 norm of the differences between embeddings of images, averaged over all pixels, to define a perceptual similarity metric. We find that increasing the neighborhood size, which corresponds to the final embeddings dimensionality, consistently improves the WLS loss as well as performance on the tasks in BAPPS. This is in contrast to learned methods like LPIPS, where performance on perceptual tasks can correlate negatively with the classification performance from which the deep features are taken (Kumar et al., 2022).\nAn overview of full-reference image quality assessment (FR-IQA) is provided in Section 2, while Section 3 reviews a representative sample of current state-of-the-art FR-IQA algorithms. Computing the embeddings as well as LASI is discussed, and an algorithm is given, in Section 4. LASI is benchmarked against LPIPS, PIM, and A-DISTS (Zhu et al., 2022) across 6 categories of image quality experiments in Section 5. In Section 5.3, we employ the Maximum Differentiation (MAD) competition (Wang & Simoncelli, 2008) to show that LPIPS and LASI can potentially be combined, as one can be used to find failure modes of the other (see the section for a formal definition)."
        },
        {
            "heading": "2 FULL-REFERENCE IMAGE QUALITY ASSESSMENT (FR-IQA)",
            "text": "FR-IQA is an umbrella term for methods and datasets designed to evaluate the quality of a distorted image, relative to the uncorrupted reference, in a way that correlates with human perception. Correlation is measured through benchmark datasets created by collecting data from psychophysical experiments such as two-alternative forced choice (2-AFC) human trials.\nIn 2-AFC image quality assessment experiments, subjects are forced to decide between two mutually exclusive alternatives relating to the perceived quality of images. For example, in Zhang et al. (2018) subjects are shown 3 images, a reference and 2 alternatives, and must indicate which of the 2 alternatives they perceive as being more similar to the reference. Just-noticeable differences (JND) is another type of 2-AFC experiment where two similar images are shown and subjects must generate a binary label indicating if they perceive the images as being the same or distinct.\nThe response of subjects in 2-AFC trials are taken to be the ground truth. For example, if 2 images in a JND dataset have different pixel values, but are judged to be the same by all subjects, then a perfect FR-IQA algorithm must also decide they are the same (Duanmu et al., 2021). In the case where there is disagreement between subjects on the same pair of images, then the uncertainty is considered inherent to human perception (i.e., aleatoric, not epistemic).\nFR-IQA methods largely ignore perceptual uncertainty and instead attempt to learn a distance function between images that assigns small values to perceptually similar images. Algorithms can be categorized into data-free, unsupervised, and supervised, depending on what training data is needed to learn the distance function. Supervised methods require collecting annotated data from psychophysical experiments using human trials (e.g., LPIPS from Zhang et al. (2018)). Unsupervised methods can learn directly from unlabelled data (e.g., PIM from Bhardwaj et al. (2020)), while data-free methods require no data or training at all (e.g., MS-SSIM of Wang et al. (2003) and our method). These methods are trained and evaluated by performing train\u2013test splits on benchmark 2-AFC datasets such as the Berkeley Adobe Perceptual Patch Similarity (BAPPS) (Zhang et al., 2018)."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "In this section we review closely related literature for data-free and learned (both unsupervised and supervised) full-reference image quality assessment (FR-IQA). For an in-depth survey see Duanmu et al. (2021); Ding et al. (2021).\nData-free distortion metrics operating at the pixel level such as mean squared error (MSE) are commonly used in lossy compression applications (Cover, 1999) but have long been known to correlate poorly with human perception (e.g., Girod, 1993). Patch-level metrics have been shown to correlate better with human judgement on psychophysical tasks. Most notably, the Structural Similarity Index (SSIM), as well as its multi-scale variate MS-SSIM, compare high level patch features such as luminance and contrast to define a distance between images (Wang et al., 2004). SSIM is widely used in commercial television applications, and MS-SSIM is a standard metric for assessing performance on many computer vision tasks. The method presented in this work is also data-free and outperforms MS-SSIM on benchmark datasets.\nMany learned FR-IQA methods are designed mirroring the learned perceptual image patch similarity (LPIPS) method of Zhang et al. (2018), where a neural network is trained on some auxiliary task and the intermediate layers are taken as perceptual representations of an input image. An unsupervised distance between images is defined as the L2 norm of the difference between their representations. A supervised distance uses the representations as inputs to a second model that is trained on human annotated data regarding the perceptual quality of the input images (e.g., labels of 2-AFC datasets discussed in Section 2). Taking representations from neural networks that perform well on their auxiliary task does not guarantee good performance on perceptual tasks (Kumar et al., 2022), making it difficult to decide which existing models will yield perceptually relevant distance functions. In contrast, for the same experimental setup, the performance of our method on perceptual tasks correlated well with performance on the auxiliary task (see Section 5.1).\nSelf-supervision was used by Madhusudana et al. (2022b;a) and Wei et al. (2022) for unsupervised and supervised FR-IQA. Images are corrupted with pre-defined distortion functions and a neural network is trained with a contrastive pairwise loss to predict the distortion type and degree. The unsupervised distance is defined as discussed previously and ridge regression is used to learn a supervised distance function. This method requires training data, while our method requires no training at all."
        },
        {
            "heading": "4 METHOD",
            "text": "Here we present our data-free, self-supervised (at inference time) FR-IQA algorithm called Linear Autoregressive Similarity Index (LASI). We make use of a sub-component of the lossless compression algorithm available in Meyer & Tischer (2001) to define a distance between images, which is described in detail next."
        },
        {
            "heading": "4.1 CONSTRUCTING PERCEPTUAL EMBEDDINGS VIA WEIGHTED LEAST SQUARES",
            "text": "Our method relies on self-supervision (at inference-time) to learn a representation for each pixel that captures global perceptual semantics of the image. The underlying assumption is that, given a representation vector for some pixel, it must successfully predict the value of other pixels in the image in order to capture useful semantics of the image\u2019s structure. Our method acts directly on images x,y to compute a distance d(x,y), similar to other data-free methods like L2 and MS-SSIM (Wang et al., 2003). We describe this formally next.\nWeighted Least Squares Let x = (x1, . . . , xk) \u2208 Rk represent a flattened image with height H , width W , number of channels C, and k = HWC pixels. Our method is autoregressive and uses the previous i\u2212 1 pixels x[1,i) = (x1, . . . , xi\u22121) to predict the value xi of the i-th pixel. The number of previous pixels used will be equal to the dimensionality of the embeddings. Therefore, we restrict the algorithm to use a subset N \u2264 i\u2212 1 of pixels from x[1,i). The subset is made up of the elements in x[1,i) that are closest 1 in the coordinate space of the image to the i-th pixel. We refer to this as the causal neighborhood of pixel i, and represent it as a vector ni \u2208 RN . See Figure 2 for an example. For the i-th pixel of value xi, we find a vector wi(x[1,i)) \u2208 RN that minimizes the weighted least squares objective\nwi(x[1,i)) = argmin w\u2208RN \u2211 j<i \u03c9\u2113ij ( n\u22a4j w \u2212 xj )2 , (1)\nwhere 0 < \u03c9 \u2264 1 is a hyperparameter and \u2113ij the Manhattan distance between coordinates of the i-th and j-th pixels in the image (see Figure 2 for an example). Concatenating wi(x[1,i)) column-wise yields the perceptual embedding matrix W(x) \u2208 RN\u00d7k of image x.\nEquation (1) defines a (weighted) least squares problem with data points {(nj , xj)}i\u22121j=1 extracted from the previous pixels. The weights \u03c9\u2113ij decrease as the distance \u2113ij between coordinates increases,\n1In Manhattan distance.\nbiasing the objective to better predict closer pixels. The value of xi is not used to compute the representation wi(x[1,i)) but is used in the computation of subsequent representations.\nDistance Function The LASI distance between images is defined as the distance between their perceptual embeddings averaged over pixels d(x,y) = 1k \u2211k i=1 \u2225wi(x[1,i))\u2212wi(y[1,i))\u22252.\nDifferentiability All operations, including solving Equation (1), are differentiable which allows us to compute the gradients of d with respect to both arguments. In Section 5.3 we use differentiability to perform the Maximum Differentiation (MAD) Competition (Wang & Simoncelli, 2008) between our method and LPIPS (Zhang et al., 2018).\nPredictions Meyer & Tischer (2001) solve Equation (1) to generate a prediction x\u0302i = wi(x[1,i))\n\u22a4ni of the i-th pixel which is then used for lossless compression of the original image. Figure 2 shows examples of the squared residual image made up of pixels zi = (xi \u2212 x\u0302i)2 for varying sizes of neighborhood size N . In Section 5.1, Figure 4, we show the prediction loss \u2211k i=1 z 2 i has strong correlation with performance on downstream 2-AFC tasks."
        },
        {
            "heading": "4.2 ALGORITHM",
            "text": "Here we describe our implementation which solves (1) in 3 steps. The algorithm is differentiable and most operations can be run in parallel on a GPU. Compute time and memory can be traded-off by, for example, precomputing the rank-one matrices. It is also possible to solve Equation (1) thrice in parallel, once for each channel, and average the results at the expense of some performance on downstream perceptual tasks.\nThe steps of our method are:\n1) Transform For the i-th pixel of value xi, compute a rank-one matrix from the neighborhood, as well as another vector equal to the neighborhood scaled by the pixel itself:\nAi = nin \u22a4 i \u2208 RN\u00d7N , bi = xini \u2208 RN . (2)\n2) Weigh-and-Sum On a second pass, for each pixel, compute a weighted sum of the rank-one matrices of all previous pixels, weighted by \u03c9\u2113ij , where 0 < \u03c9 < 1 is a hyperparameter and \u2113ij the Manhattan distance between locations of pixels xi and xj . Perform a similar procedure for vectors bi:\nA\u0304i = i\u22121\u2211 j=1 \u03c9\u2113ijAj , b\u0304i = i\u22121\u2211 j=1 \u03c9\u2113ijbj . (3)\n3) Solve Finally, for each pixel, solve the least-squares problem with coefficients A\u0304i and target vector b\u0304i by computing the Moore-Penrose pseudo-inverse A\u0304 \u2020 i of A\u0304i,\nwi(x[1,i)) = A\u0304 \u2020 i b\u0304i. (4)\nThe rank-one matrices A\u0304i have dimension N \u00d7 N . In our experiments, we found N = 12 was sufficient to perform competitively with unsupervised methods on images of size 64 \u00d7 64 \u00d7 3. In this regime of small N computing the pseudo-inverse can be done directly using the singular value decomposition (SVD) (Klema & Laub, 1980) (we use jax.numpy.linalg.pinv; Bradbury et al., 2018).\nComputational Complexity Solving Equation (4) requires computing the SVD of A\u0304i which has worst case complexity of O(N3) (Klema & Laub, 1980), where N is the neighborhood size and embedding dimensionality. This must be done for each pixel but can be parallelized at the expense of an increase in memory, with no loss in performance. Figure 3 shows LASI is faster than PIM, which requires forward-passes in a neural network, on the BAPPS dataset."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section we compare our method against state-of-the-art unsupervised FR-IQA algorithms. Our method is data-free but performs competitively with learned methods on experiments of the Berkeley Adobe Perceptual Patch Similarity (BAPPS) dataset (Zhang et al., 2018) (the first row of Figure 5 shows examples from BAPPS). Experiments of Figure 4 indicate the prediction loss and performance on perceptual tasks correlate and improve as the neighborhood size increases.\nIn Table 1, results for PIM as well as LPIPS numbers for BAPPS-JND are taken from Table 1 of Bhardwaj et al. (2020). For LPIPS, we generously took the best models from Table 5 of Zhang et al. (2018) for each 2-AFC category. For our method we used N = 12 across all experiments and the decay parameter in Equation (1) was fixed to \u03c9 = 0.8, the same value used for the lossless compression algorithm of Meyer & Tischer (2001). Images in all categories have dimensions 64\u00d7 64\u00d7 3 which is 1536 times larger than the neighborhood size. The performance consistently improves with larger neighborhood sizes N as can be seen from the solid line in Figure 4. This suggests the parameter can be used to trade off computational complexity and performance and does not require tuning.\nThe last row of Table 1 shows the gap in performance between our data-free method and the best performing unsupervised neural network approach. In the worst case, our method scores only 5.8% less while requiring no learning, data collection, or expensive neural network models."
        },
        {
            "heading": "5.1 TWO-ALTERNATIVE FORCED CHOICE (BAPPS-2AFC)",
            "text": "This section discusses experiments on the BAPPS-2AFC dataset of Zhang et al. (2018). BAPPS2AFC was constructed by collecting data from a 2-AFC psychophysical experiment where subjects are shown reference images r(\u2113) as well as alternatives x(\u2113)0 ,x (\u2113) 1 and must decide which of the two alternatives they perceive as more similar to the reference. The share of subjects that select image x (\u2113) 1 over x (\u2113) 0 is available in the dataset as p (\u2113).\nThe performance of a FR-IQA algorithm, defined by a distance function d between images, on a 2-AFC dataset is measured by\n1\nn n\u2211 \u2113=1 ( p(\u2113) )a(\u2113) ( 1\u2212 p(\u2113) )1\u2212a(\u2113) \u2264 1 n n\u2211 \u2113=1 max{p(\u2113), 1\u2212 p(\u2113)}, (5)\nwhere a(\u2113) = 1 [ d(x\n(\u2113) 1 , r (\u2113)) < d(x (\u2113) 0 , r\n(\u2113)) ] 2. Equality is achieved when the algorithm agrees\nwith the majority of subjects, i.e. a(\u2113) = \u230ap(\u2113)\u2309 3, for all examples in the dataset. Human-level performance is defined as\n1\nn n\u2211 \u2113=1 [( p(\u2113) )2 + ( 1\u2212 p(\u2113) )2] , (6)\nwhich corresponds to the average probability of two randomly chosen subjects agreeing. Majority and human-level performance scores for our 2-AFC experiments are shown in the first rows of Table 1.\nAcross all categories our method scored competitively with the best performing unsupervised method, with the largest gap being 4.5% in the \u201cColorization\u201d category. Our method outperforms MS-SSIM (Wang et al., 2003) on all 2-AFC categories, most notably in \u201cTraditional\u201d where the improvement is 20.6%, and provides perceptual embeddings (Equation 1) for use in downstream computer vision tasks. We highlight the overall improvements with respect to MS-SSIM in the last row of Table 1.\nCorrelation with Self-Supervised Task In an empirical study, Kumar et al. (2022) investigate if deep features from better performing classifiers achieve better perceptual scores on the BAPPS dataset. Surprisingly, their results suggest the correlation can be negative: more accurate models produce embeddings that capture less meaningful perceptual semantics, performing poorly on 2-AFC and JND tasks when used together with LPIPS. We perform a similar study with our method on the prediction task defined in Meyer & Tischer (2001) as a function of neighborhood size. For each reference image r(\u2113), we compute the prediction r\u0302(\u2113), composed of pixels r\u0302(\u2113)i = wi(r[1,i))\n\u22a4ni, and report the residual 1n \u2211n \u2113=1(r (\u2113) i \u2212 r\u0302 (\u2113) i )\n2 alongside the score on the 2-AFC task. Results are shown in Figure 4. The performance on the prediction and perceptual tasks show a strong correlation, and improve consistently, across all 2-AFC categories."
        },
        {
            "heading": "5.2 JUST-NOTICEABLE DIFFERENCES (BAPPS-JND)",
            "text": "In Table 1 we compare our data-free method to recent unsupervised methods on the JND subset of the BAPPS dataset. The BAPPS-JND dataset {(x(i), x\u0303(i), p(i))}ni=1 was created by showing two images\n21[A] = 1 iff statement A is true. 3\u230a\u00b7\u2309 rounds to the nearest integer\nx(i), x\u0303(i) to a group of subjects where the latter is the former but corrupted by one of two different distortion types (identified by the last 2 columns of Table 1). Subjects must indicate if they perceive the images as being the same or not. The share of subjects that judged the images as being the same, p(i), is available in the dataset but not the individual responses. See (Zhang et al., 2018) for more details regarding the images as well as distortion types.\nAs described in Section 2, an FR-IQA algorithm in the context of a JND task will attempt to output a binary response that correlates with p(i). This defines a binary classification task where the distance defined by the FR-IQA algorithm must be thresholded to yield a decision and precision/recall can be traded-off by varying the threshold value (Bishop & Nasrabadi, 2006). We evaluate the JND experiment with an area-under-the-curve score known as mean average precision (mAP), following Zhang et al. (2018); Bhardwaj et al. (2020).\nResults are shown in the last 2 columns of Table 1. For CNN-based distortions our method scores similarly to MS-SSIM (63.8% vs 63.2%). PIM loses to our method (60.1% vs 63.2%) while LPIPS outperforms it only by 5.8%.\nSimilar to the 2-AFC experiments, the subcategory with the largest gap between data-free and unsupervised methods is \u201cTraditional\u201d. The gap is drastically reduced by our method by raising the highest score from 36.2% (achieved by MS-SSIM) to 55.9%, which is within 3.3% of the best scoring unsupervised method (PIM). In this same subcategory our method significantly outperforms LPIPS (55.9% vs 46.9%)."
        },
        {
            "heading": "5.3 MAXIMUM DIFFERENTIATION (MAD) COMPETITION",
            "text": "MAD competition (Wang & Simoncelli, 2008) is a technique to discover failure modes in the perceptual space of differentiable similarity metrics. Failure modes are image pairs (x1,x2) where\none image is clearly closer to a reference r upon inspection, but the metric assigns similar distances d(r,x1) \u2248 d(r,x2). We now describe the MAD competition outlined in Wang & Simoncelli (2008) that uses K steps of projected gradient descent to find a failure mode (x(K)max ,x (K) min ) in d. First, the reference r is corrupted with noise yielding r\u0303 which acts as the starting point (x(0)max,x (0) min) = (r\u0303, r\u0303) for optimization. At the i-th step, the image x(i)max is updated using a gradient step in the direction that maximizes it\u2019s L2 distance to the uncorrupted reference r. However, before updating, the gradient is projected into the space orthogonal to \u2207\nx (i) max d(r,x\n(i) max). This projection step guarantees that the distance to the reference\ndoes not change significantly, i.e., d(r,x(i)max) \u2248 d(r,x(i+1)max ). The same procedure is done for x(K)min , but the gradient step is taken in the opposite direction (minimizing). In practice an extra correction step is required as the projected gradient will be tangent to the set of equidistant images (i.e., the level set). It is common to replace L2 distance with another similarity metric as a way to contrast failure modes and possibly discover ways to combine models (Wang & Simoncelli, 2008).\nWe performed the MAD competition between LPIPS and LASI. Qualitative results are shown in Figure 5. The neighborhood size of LASI was held fixed at N = 16 while deep features from VGG (Simonyan & Zisserman, 2014) were used for LPIPS as in Zhang et al. (2018). Images are parameterized by an unconstrained tensor which is then passed through a sigmoid function to yield an image in the RGB space. Gaussian noise was added in the parameter space (i.e., before the sigmoid is applied) to generate the corrupted reference r\u0303, to guarantee images are valid RGB images.\nResults indicate LASI can find failure points for LPIPS and vice-versa. Each metric fails in different ways. Image x(K)max shows artifacts resembling the structure of the causal neighborhood for LASI while artifacts for LPIPS are smoother.\nThe difference in failure modes suggests LASI and LPIPS can be combined. Merging these models is non-trivial as the embedding dimensions differ in size making it difficult to perform simple aggregation techniques at the embedding level. We leave this as future work."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work we show how perceptual embeddings can be constructed at inference time with no training data or deep neural network features. Our Linear Autoregressive Similarity Index (LASI) metric performs competitively with learned methods such as LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020) on benchmark psychophysical datasets, and outperforms other untrained methods like MS-SSIM (Wang et al., 2003).\nLASI solves a weighted least squares problem at inference time to create embeddings that capture meaningful semantics of the human visual system. Evidence shows increasing the embedding dimensionality improves the overall downstream performance on the tasks present in the BAPPS dataset (Zhang et al., 2018), while improving the WLS loss. This is in strong contrast to learned methods like LPIPS, where the classification performance of deep networks can correlate negatively with perception (Kumar et al., 2022).\nThere are many candidate hypotheses for the unreasonable effectiveness of LASI in FR-IQA, of which we discuss two. First, it is unclear how the performance of an algorithm on BAPPS generalizes to other tasks and datasets; warranting a discussion if BAPPS is indeed a valid benchmark for FR-IQA beyond small image patches. Alternatively, it is possible the performance of LPIPS and LASI are due to different reasons. While LPIPS embeddings are constructed by indirectly compressing data samples during training, LASI embeddings are tasked with compressing a specific image.\nWe conclude with a myriad of open directions to explore. One such direction is to investigate if LASI embeddings, i.e., the solutions to the WLS problem, have useful semantics in computer vision beyond perceptual tasks. The choice of using WLS was inspired by Meyer & Tischer (2001) who use it to perform lossless compression of grayscale images, but there are other small-scale regression tasks that could be used."
        }
    ],
    "year": 2023
}