{
    "abstractText": "Normalization layers are ubiquitous in deep learning, greatly accelerating optimization. However, they also introduce many unexpected phenomena during training, for example, the Fast Equilibrium conjecture proposed by (Li et al., 2020), which states that the scale-invariant normalized network, when trained by SGD with \u03b7 learning rate and \u03bb weight decay, mixes to an equilibrium in \u00d5( 1 \u03b7\u03bb ) steps, as opposed to classical e \u22121) mixing time. Recent works by Wang & Wang (2022); Li et al. (2022c) proved this conjecture under different sets of assumptions. This paper aims to answer the fast equilibrium conjecture in full generality by removing the non-generic assumptions of Wang & Wang (2022); Li et al. (2022c) that the minima are isolated, that the region near minima forms a unique basin, and that the set of minima is an analytic set. Our main technical contribution is to show that with probability close to 1, in exponential time trajectories will not escape the attracting basin containing their initial position.",
    "authors": [
        {
            "affiliations": [],
            "name": "GENERIC SITUATIONS"
        },
        {
            "affiliations": [],
            "name": "Zhiyuan Li"
        },
        {
            "affiliations": [],
            "name": "Yi Wang"
        },
        {
            "affiliations": [],
            "name": "Zhiren Wang"
        }
    ],
    "id": "SP:64e924b75c99855b226ff1e0cd5b037d58a4e2e2",
    "references": [
        {
            "authors": [
                "Ludwig Arnold",
                "Wolfgang Kliemann"
            ],
            "title": "On unique ergodicity for degenerate diffusions",
            "venue": "Stochastics, 21(1):41\u201361,",
            "year": 1987
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Zhiyuan Li",
                "Abhishek Panigrahi"
            ],
            "title": "Understanding gradient descent on the edge of stability in deep learning",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Anton Bovier",
                "Michael Eckhoff",
                "V\u00e9ronique Gayrard",
                "Markus Klein"
            ],
            "title": "Metastability in reversible diffusion processes i: Sharp asymptotics for capacities and exit times",
            "venue": "Journal of the European Mathematical Society,",
            "year": 2004
        },
        {
            "authors": [
                "Yaim Cooper"
            ],
            "title": "Global minima of overparameterized neural networks",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "Berfin \u015eim\u015fek",
                "Fran\u00e7ois Ged",
                "Arthur Jacot",
                "Francesco Spadaro",
                "Clement Hongler",
                "Wulfram Gerstner",
                "Johanni Brea"
            ],
            "title": "Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Damian",
                "Tengyu Ma",
                "Jason Lee"
            ],
            "title": "Label noise sgd provably prefers flat global minimizers",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Amir Dembo",
                "Ofer Zeitouni"
            ],
            "title": "Large deviations techniques and applications, volume 38 of Stochastic Modelling and Applied Probability",
            "venue": "doi: 10.1007/978-3-642-03311-7",
            "year": 2010
        },
        {
            "authors": [
                "Felix Draxler",
                "Kambis Veschgini",
                "Manfred Salmhofer",
                "Fred A. Hamprecht"
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,,",
            "year": 2018
        },
        {
            "authors": [
                "M. Duflo",
                "D. Revuz"
            ],
            "title": "Propri\u00e9t\u00e9s asymptotiques des probabilit\u00e9s de transition des processus de Markov r\u00e9currents",
            "venue": "Ann. Inst. H. Poincare\u0301 Sect. B (N.S.),",
            "year": 1969
        },
        {
            "authors": [
                "K.J. Falconer"
            ],
            "title": "Differentiation of the limit mapping in a dynamical system",
            "venue": "Journal of the London Mathematical Society,",
            "year": 1983
        },
        {
            "authors": [
                "Benjamin Fehrman",
                "Benjamin Gess",
                "Arnulf Jentzen"
            ],
            "title": "Convergence rates for the stochastic gradient descent method for non-convex objective functions",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin"
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mark I. Freidlin",
                "Alexander D. Wentzell"
            ],
            "title": "Random perturbations of dynamical systems, volume 260 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences",
            "year": 2012
        },
        {
            "authors": [
                "Xinran Gu",
                "Kaifeng Lyu",
                "Longbo Huang",
                "Sanjeev Arora"
            ],
            "title": "Why (and when) does local sgd generalize better than sgd",
            "venue": "In The International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Vipul Gupta",
                "Santiago Akle Serrano",
                "DeCoste DeCoste"
            ],
            "title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well",
            "venue": "In The International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Vipul Gupta",
                "Santiago Akle Serrano",
                "Dennis DeCoste"
            ],
            "title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kanji Ichihara",
                "Hiroshi Kunita"
            ],
            "title": "A classification of the second order degenerate elliptic operators and its probabilistic characterization",
            "venue": "Z. Wahrscheinlichkeitstheorie und Verw. Gebiete,",
            "year": 1974
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proceedings of the 32nd International Conference on International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "34th Conference on Uncertainty in Artificial Intelligence 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Gary Shon Katzenberger"
            ],
            "title": "Solutions of a stochastic differential equation forced onto a manifold by a large drift",
            "venue": "The Annals of Probability,",
            "year": 1991
        },
        {
            "authors": [
                "Wolfgang Kliemann"
            ],
            "title": "Recurrence and invariant measures for degenerate diffusions",
            "venue": "Ann. Probab.,",
            "year": 1987
        },
        {
            "authors": [
                "Qianxiao Li",
                "Cheng Tai",
                "Weinan E"
            ],
            "title": "Stochastic modified equations and adaptive stochastic gradient algorithms",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Qianxiao Li",
                "Cheng Tai",
                "Weinan E"
            ],
            "title": "Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Sanjeev Arora"
            ],
            "title": "An exponential learning rate schedule for deep learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Kaifeng Lyu",
                "S. Arora"
            ],
            "title": "Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Tianhao Wang",
                "Sanjeev Arora"
            ],
            "title": "What happens after SGD reaches zero loss? \u2013a mathematical framework",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Tianhao Wang",
                "Sanjeev Arora"
            ],
            "title": "What happens after SGD reaches zero loss? \u2013a mathematical framework",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Tianhao Wang",
                "Dingli Yu"
            ],
            "title": "Fast mixing of stochastic gradient descent with normalization and weight decay",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Diederik P Kingma"
            ],
            "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Bin Shi",
                "Weijie J Su",
                "Michael I Jordan"
            ],
            "title": "On learning rates and schr\u00f6dinger operators",
            "venue": "arXiv preprint arXiv:2004.06977,",
            "year": 2020
        },
        {
            "authors": [
                "Barry Simon"
            ],
            "title": "Semiclassical analysis of low lying eigenvalues. I. Nondegenerate minima: asymptotic expansions",
            "venue": "Ann. Inst. H. Poincare\u0301 Sect. A (N.S.),",
            "year": 1983
        },
        {
            "authors": [
                "Ruosi Wan",
                "Zhanxing Zhu",
                "Xiangyu Zhang",
                "Jian Sun"
            ],
            "title": "Spherical motion dynamics: Learning dynamics of neural network with normalization, weight decay, and sgd",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Wang",
                "Zhiren Wang"
            ],
            "title": "Three-stage evolution and fast equilibrium for SGD with non-degenerate critical points",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "F dist(F(x"
            ],
            "title": "\u03bc\u03b5\u25e6(Fm)\u22121} are exponentially good approximations of another family of probability distributions {\u03bd} on Y",
            "year": 2010
        },
        {
            "authors": [
                "Condition C"
            ],
            "title": "Li et al., 2022a, Lemma B.2) The integrator sequence",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "steps, as opposed to classical eO((\u03b7\u03bb) \u22121) mixing time. Recent works by Wang & Wang (2022); Li et al. (2022c) proved this conjecture under different sets of assumptions. This paper aims to answer the fast equilibrium conjecture in full generality by removing the non-generic assumptions of Wang & Wang (2022); Li et al. (2022c) that the minima are isolated, that the region near minima forms a unique basin, and that the set of minima is an analytic set. Our main technical contribution is to show that with probability close to 1, in exponential time trajectories will not escape the attracting basin containing their initial position."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Normalization layers are ubiquitous and play a fundamental role in modern deep learning, e.g., Batch Normalization (Ioffe & Szegedy, 2015), Group Normalization (Wu & He, 2018), Layer Normalization (Ba et al., 2016), and Weight Normalization (Salimans & Kingma, 2016). Normalization layers not only greatly facilitate optimization and improve trainability, it also brings intriguing new optimization behaviors to neural networks. For example, Li & Arora (2020) showed that normalized networks can be trained by SGD with exponentially increasing learning rates, because training with exponentially increasing learning rates turns out to be equivalent to training with constant learning rates but with weight decay turned on, as shown in (2). Here xk \u2208 Rd is the parameter of a neural network after the k-th step and is updated by\nxk+1 \u2190 (1\u2212 \u03bb)xk \u2212 \u03b7\u2207LBk(xk), (1)\nwhere \u03bb and \u03b7 are respectively the weight decay parameter and the learning rate, and LBk is the loss function evaluated using a randomly chosen mini-batch Bk. The result of Li & Arora (2020) holds not only for normalized networks but more broadly for all scale invariant training losses, which is a popular abstraction of normalized networks in optimization analysis. Mathematically, scale invariance refers to the following property of the loss: LB(cx) = LB(x), \u2200c > 0, x \u2208 Rd and every batch B. Later, Li et al. (2020); Wan et al. (2021) discovered that it is the intrinsic learning rate \u03b7\u03bb that controls the long-term convergence behavior for SGD on scale invariant loss with weight decay, (1). The approach that Li et al. (2020) takes to study (1) is to approximate by the stochastic differential equation (SDE) model Li et al. (2017; 2019), which is quite common in literature.\ndXt = (\u2212\u03b7\u2207L(Xt)\u2212 \u03b7\u03bbXt)dt\u2212 \u03b7\u03c3(Xt)dBKt . (2) \u2217The authors are listed alphabetically.\nHere L is the average 1K \u2211K k=1 LBk over all random batches, \u03c3 = 1\u221a K ( \u2207LBk \u2212\u2207L )K k=1\nis a d\u00d7K matrix, and BKt is the K-dimensional Wiener process. Scale invariance of LB implies that L is scale-invariant and that \u03c3 is (\u22121)-homogeneous, i.e.\nL(cx) = L(x), \u03c3(cx) = c\u22121\u03c3(x),\u2200c > 0, x \u2208 Rd. (3)\nLi et al. (2020) further proposed the following Fast Equilibrium Conjecture for the SDE approximation of SGD. Conjecture 1.1. [Fast Equilibrium Conjecture] (Li et al., 2020) If F (X, input) denotes the output of a neural network NN with parameters X , and Xt denotes the value of SDE (2) at time t, starting from initial parameter X0. Suppose NN has normalization steps so that the F (X, input) is scaleinvariant in X , i.e. F (X, input) = F (cX, input) for all c > 0. Then for all input values input, the probability distribution of F (Xt, input) stabilizes to an equilibrium in O( 1\u03b7\u03bb ) steps of SGD updates.\nExperiments where the empirically observed rates of convergence are polynomial were contained in the original paper Li et al. (2020) where the Fast Equilibrium Conjecture was first asked. The rate O( 1\u03b7\u03bb ) is considered to be fast because according to Langevin dynamics, the time it takes to converge to the Gibbs equilibrium is of exponential order eO((\u03b7\u03bb) \u2212 1\n2 ). This can be done by following a similar analysis to those in (Bovier et al., 2004; Shi et al., 2020). The works by (Bovier et al., 2004) and (Shi et al., 2020) dealt with models without normalization, and the convergence times there are of order eO((\u03b7\u03bb) \u22121). Using the similar method as in (Bovier et al., 2004) and (Shi et al., 2020), when normalization is used the convergence time can be shown to be of order eO((\u03b7\u03bb) \u2212 1\n2 ). This is because Li et al. (2020) proved that the intrinsic learning rate \u03b7\u03bb is replaced by an effective learning rate (\u03b3\u2212 1 2\nt in Li et al. (2020)) for the renormalized parameter vector, which is of order O((\u03b7\u03bb) 1 2 ).\nThe recent paper (Li et al., 2022c), using a mathematical framework from (Li et al., 2022b), established the fast equilibrium conjecture for \u03b7\u03bb \u2192 0 under a mixed set of generic and non-generic assumptions. See (Damian et al., 2021), (Gu et al., 2022) for more work on analyzing the dynamics of SGD near the manifold of minimizers. The goal of the current paper is to remove the non-generic ones and thus provide a general proof in the aforementioned range of parameters."
        },
        {
            "heading": "1.1 NOTATIONS AND ASSUMPTIONS",
            "text": "To introduce assumptions from previous authors as well as our results, we need to set up a few notations first. Let \u0393 \u2286 Rd\\{0} be the set of local minima of L. Notice that by (3), \u0393 is a cone, i.e. x \u2208 \u0393 if and only if cx \u2208 \u0393 for all c > 0. For all r > 0, write \u0393r = {x \u2208 \u0393 : |x| = r}. In particular \u03931 is a subset of the unit sphere Sd\u22121 = {|x| = 1}. In general, \u0393 may have multiple connected components. Decompose \u0393 = \u2294 \u0393i where each \u0393i is a connected cone. We then write \u0393ir = \u0393 i \u2229 {|x| = r}. Then \u0393i1 are the connected components of \u03931. In particular, there are only finitely many \u0393i\u2019s and we index them by i = 1, \u00b7 \u00b7 \u00b7 ,m. In addition to the scaling properties (3) guaranteed by the use of normalization, (Li et al., 2022c) made certain assumptions, which we will need in the following. Assumption 1.2. The functions L and \u03c3 satisfy:\n(i). (Scale invariance) The scaling rules in (3) hold.\n(ii). (Regular critical locus) Each loss function LBk is C4 on Rd\\{0}, the critical points of L form a C2 submanifold \u2126. 1 For all x \u2208 \u2126, \u22072L(x) is of rank d\u2212 dimTx\u2126. (iii). (Controllability) For all x \u2208 \u03931, span{\u2202\u03a6(x)\u03c3k(x)}Kk=1 = Tx\u03931. Here and below, \u03a6(x) = limt\u2192\u221e Xt, with Xt being the solution to the deterministic gradient descent Xt = \u2212\u2207L(Xt) with initial value x.\nBy (Arora et al., 2022, Lemma B.15), under Assumption 1.2.(i) & (ii), \u03a6 is well defined and C2differentiable on a neighborhood of \u0393 as long as L is C4 differentiable. We also note that in general\n1Different connected components of \u2126 are not required to have the same dimension.\nthe noise structure does affect the convergence rate. But as long as Assumption 1.2 (iii) is satisfied, the noise structure won\u2019t affect the asymptotic order of the convergence rate.\nOn the other hand, we will not need the following assumptions.\nAssumption 1.3. \u0393 satisfies:\n(i). (Unique basin) \u03931 is compact and connected;\n(ii). (Analyticity) \u0393 is a real analytic manifold and Tr\u03a3 is a real analytic function on Rd\\{0} where \u03a3 = \u03c3\u03c3\u22a4.\nRestricting to an attracting basin U of \u0393 and assuming both Assumptions 1.2 and 1.3, Li et al. (2022c) proved Conjecture 1.1 when \u03bb\u03b7 \u2192 0 in the natural range of \u03b7 \u2264 O(\u03bb) \u2264 O(1) and the parameter X0 is initialized within U . Note that since U is an attracting basin, \u2126 and \u0393 coincide in U and thus Assumption 1.2 is equivalent to (Li et al., 2022c, Assumption 2.1) for the purpose of that paper. Note that \u0393 is always a submanifold of \u2126.\nRemark 1.4. All three assumptions in Assumption 1.2 are very natural for the following reasons:\n\u2022 As remarked earlier, the scale-invariance (3) is a consequence of the use of normalization steps inside neural networks.\n\u2022 It is a widely used assumption, at least in the case of local minimizers, that the locus is a manifold for overparametrized neural networks, for example in (Fehrman et al., 2020; Arora et al., 2022; Li et al., 2022b). For the locus of global minimizers, this assumption was proved by Cooper (2021). As remarked in (Li et al., 2022b; Cooper, 2021), a main reason for the local minimizers to form manifolds is the overparametrization of modern neural networks. S\u0327ims\u0327ek et al. (2021) further identifies the reason as symmetries arising from overparametrization. In fact, they studied loci of critical points that are not necessarily minima and proved that symmetry-induced critical points form a manifold that satisfies Assumption 1.2.(ii).\n\u2022 The philosophy behind Assumption 1.2.(iii) is that the generation of random batches in training is independent of the aforementioned symmetries in the setup of the neural network, and thus generically should not live in subspaces that are invariant under such symmetries. In particular, the same symmetries are generically capable of move the noises from the random batches to span a tangent space of the same dimension as that of the local manifold of critical points.\nRemark 1.5. On the other hand, both conditions in Assumption 1.3 are non-generic:\n\u2022 Certain evidences from (Draxler et al., 2018) suggests that all local minima appearing in realistic training generically come from connected relatively flat region of small variation in height, so empirically Assumption 1.3.(i) could be a reasonable approximate assumption. However, the experiments in (Draxler et al., 2018, Fig. 5) shows at the same time that in many settings, this region is not completely flat and contains non-trivial saddle points. In particular, there could be multiple disconnected basins. In light of these, it is more reasonable work in the absence of assumption Assumption 1.3.(i).\n\u2022 The analyticity of the \u0393 and Tr\u03a3 depends on that of the activation functions chosen in the neural network. While many popular activation functions are real analytic, one may always choose to use functions that are differentiable but not analytic, in which case Assumption 1.3.(ii) is in general not guaranteed.\nIn this paper, we will give a general proof of Conjecture 1.1 in the same natural range as in (Li et al., 2022c), assuming only the generic conditions from Assumption 1.2. In particular, we will introduce two arguments that respectively remove both hypothesis (Unique basin and Analyticity) in Assumption 1.3.\nHere we would like to provide comments on why Assumption 1.3 are restrictive. Assuming analyticity is restrictive because the regularity of the loss function is decided by that of the activation function. Even though popular activation functions such as Sigmoid are analytic, a priori one could use smooth but not analytic functions. The one basin assumption is restrictive as we do not see empirical evidence of proof that L only has one basin. In fact, the experiments at the end of the paper suggests that there are multiple basins.\nWe would also like to give remarks on why the three assumptions in Assumption 1.2 are essential. (i) is essential because without this assumption, the SDE would not be equivalent to a SDE on the sphere Sd\u22121, which is crucial to our analysis. Without this assumption, similar analysis can probably be formulated on Rd instead of the Sd\u22121 coordinate but there will be new technical obstacles to overcome. Since the original fast equilibrium conjecture was asked for normalized neural nets, we restrict our study to the current setting. (ii) is important because if not a trajectory may stay near a critical point (for example a saddle point) for a very long period of time, it would not be able to converge within a polynomial time. Finally, the reason why we need (iii) is that if the span is not the whole tangent space, but instead a subspace of the tangent space, then the diffusion will be restrained to this subspace, which a priori may be very fractal and existing mathematical theory is not sufficient to guarantee a unique equilibrium in limit."
        },
        {
            "heading": "1.2 MULTIPLE EQUILIBRIA WHEN BASIN IS NOT UNIQUE",
            "text": "It is worthy to explain in more details what happens when the basin is not unique, i.e \u0393 has multiple connected components and Assumption 1.3.(i) fails. In this situation, our analysis generalizes the work of Wang & Wang (2022) and reveals a three-stage equilibrium phenomenon. The most important property of this phenomenon is the mismatching between practical training and theoretical bounds: the equilibrium distribution of network parameters observed in the time window under a realistic budget is both local in space and temporary in time. It is concentrated near the bottom of the same attracting basin containing the initial parameter, and differs from the eventual global Gibbs equilibrium that the distribution of parameters will eventually converge to in exponentially long time. This phenomenon interprets the gap between the empirically based Conjecture 1.1 and the previous theoretical estimate from e.g. (Bovier et al., 2004; Shi et al., 2020). See (Frankle et al., 2020), (Gupta et al., 2019) for more work about the iterates stay in the same basin for a significant amount of time when starting from the same initialization.\nThe major short-come of (Wang & Wang, 2022) is that, while not relying on the uniqueness of the basin, the arguments therein are subject to other non-generic assumptions, namely: (1) all basins are isolated points; (2) the noise \u03c3 is a standard isotropic Gaussian noise.\nOur methods allow to remove these assumption simultaneously together with Assumption 1.3. This is made possible by avoiding using the semi-classical analysis of spectra of differential operators, which was developed by Simon (Simon, 1983) and used in an essential way by previous authors in (Bovier et al., 2004; Shi et al., 2020; Wang & Wang, 2022). Instead, our method is purely probabilistic and predicts that the exiting time from a given basin is exponentially long. This method is based on an adaptation of the large deviation principle of Dembo & Zeitouni (2010)."
        },
        {
            "heading": "2 STATEMENT OF MAIN RESULTS",
            "text": ""
        },
        {
            "heading": "2.1 PRELIMINARIES ON SDE MODEL",
            "text": "A polar coordinate system has been adopted in (Li et al., 2020) to study the SDE model (2). For this purpose, denote by Xt = Xt|Xt| the unit renormalization of Xt, and \u03b3t = |Xt|\n4\u03b7\u22122. By (Li et al., 2020, Theorem 5.1), (2) is equivalent to\ndXt = \u2212\u03b3 \u2212 12 t ( \u2207L(Xt)dt+ \u03c3(Xt)dBKt ) \u2212 1\n2 \u03b3\u22121t Tr\u03a3(Xt)Xtdt; (4)\nd\u03b3t dt = \u22124\u03b7\u03bb\u03b3t + 2Tr\u03a3(Xt). (5)\nRecall that \u03a3 = \u03c3\u03c3\u22a4 is a d\u00d7 d positive semidefinite symmetric matrix. One may view the motion of Xt as an intrinsic one inside the unit sphere Sd\u22121, instead of one inside Rd. From this perspective, (Wang & Wang, 2022, Theorem 3.1) shows that (4) can be rewritten as an intrinsic SDE on Sd\u22121\ndXt = \u2212\u03b3 \u2212 12 t ( \u2207L(Xt)dt+ \u03c3\u0304(Xt)dBKt ) , (6)\nwhere \u03c3\u0304(\u00b7) 12 is a tensor field along Sd\u22121 whose value is given by the restriction of \u03c3(\u00b7) 12 and \u2207 is the gradient operator on Sd\u22121. (See the remark after (Wang & Wang, 2022, Theorem 3.1) for the\nmeaning of being intrinsic. In particular, \u2207L(Xt)dt, \u03c3(Xt)dBKt are viewed as vector fields along Sd\u22121.) Remark 2.1. Instead of the term \u03c3\u0304(Xt)dBKt in (4) and (6), the papers (Li et al., 2020; Wang & Wang, 2022) actually used the restriction of \u03a3(Xt) 1 2 dBdt to Sd\u22121. However, these two expressions are equivalent as Wiener processes because \u03a3 = \u03c3\u03c3\u22a4.\nSince (6) is a perturbation with Brownian noise of the gradient flow dXt = \u2212\u03b3\u2212 1 2\u2207L(Xt) with varying learning rate \u03b3\u2212 1 2\nt , it makes sense to first understand the constant speed gradient flow\nXt = \u2212\u2207L(Xt), (7) which is an ODE on the compact manifold Sd\u22121. Following earlier notation, the local minima of L on Sd\u22121 is \u03931 and has connected components \u0393i1.\nWrite U i1 \u2208 Sd\u22121 for the attracting basin of \u0393i1, i.e. the set of X0 \u2208 Sd\u22121 such that the solution Xt to (7) with initial value X0 satisfies limt\u2192\u221e Xt \u2208 \u0393i1). Lemma 2.2. Under Assumption 1.2.(ii), the U i1\u2019s for i = 1, \u00b7 \u00b7 \u00b7 ,m are disjoint open sets of Sd\u22121. Moreover, their union \u2294m i=1 U i 1 has full volume in Sd\u22121, and Sd\u22121\\ \u2294m i=1 U i 1 is a proper submanifold of Sd\u22121.\nThe proof of Lemma 2.2 is standard and we left it to the reader. The key observation is that the complement Sd\u22121\\ \u2294m i=1 U i 1 is the union of attracting basins of the connected components of critical points that are not local minima. By Assumption 1.2.(i), those critical points are saddle like and their attracting basins are proper submanifolds.\nNote that L is constant on \u0393i1 and \u0393 i, and L(\u0393i1) = L(\u0393 i 1) is the minimum of L inside U i 1. For q > 0, write U i,q1 := {x \u2208 U i1 : L(x) \u2264 L(U i1) + q}.\nDefine cones U i = {x \u2208 Rd\\{0} : x|x| \u2208 U i 1}, U i,q = {x \u2208 Rd\\{0} : x|x| \u2208 U i,q 1 }. Then U i the attracting basin of \u0393i under the gradient flow Xt = \u2212\u2207L(Xt). (8) By (Arora et al., 2022, Lemma B.15), under Assumption 1.2.(i), U i is open and the function \u03a6(x) = limt\u2192\u221e Xt is C2-differentiable on U i."
        },
        {
            "heading": "2.2 MAIN RESULT",
            "text": "Definition 2.3. We define the Lipschitz distance between two probability measures \u00b5, \u03bd on a metric space X as\ndistLip(\u00b5, \u03bd) = sup \u2225\u03c6\u2225Lip\u22641 \u2223\u2223 \u222b \u03c6d\u00b5\u2212 \u222b \u03c6d\u03bd\u2223\u2223, where the Lipschitz norm of a function \u03c6 is given by \u2225\u03c6\u2225Lip := max ( \u2225\u03c6\u2225C0 , supx \u0338=y |\u03c6(x)\u2212\u03c6(y)| d(x,y) ) .\nWe are now able to state our main theorem, which is a mutual reinforcement to both (Li et al., 2022c, Theorem 5.5) and (Wang & Wang, 2022, Theorem 4.6). Theorem 2.4. Under Assumption 1.2, for all \u03f5 > 0 and compact interval [\u03c1\u2212, \u03c1+] \u2282 (0,\u221e), there exist a constant c > 0 and a set \u039b \u2286 \u2294m i=1 U i 1 \u2286 Sd\u22121 of volume volSd\u22121(\u039b) > 1\u2212 \u03f5, such that for all \u03b7 \u2264 O(\u03bb) \u2264 O(1), the following holds: For all initial parameter x0 \u2208 Rd with |x0| \u2208 [\u03c1\u2212, \u03c1+] and x0|x0| \u2208 \u039b, all growth rates K such that K \u2192\u221e as \u03b7\u03bb\u2192 0, and all time values\nt \u2208 [K log(1 + \u03bb\u03b7 )\n\u03b7\u03bb , e\nc\u221a \u03b7\u03bb ] ,\nthe random trajectory to (2) with initial value x0 satisfies dist ( PX0=x0(Xt), \u03bdi ) < \u03f5,\nwhere \u03bdi is a probability measure supported on the attractor \u0393i of the unique attracting basin U i containing x0, and \u03bdi only depend on L, \u03c3 and i.\nHere volSd\u22121 is the renormalized volume on the sphere Sd\u22121 so that the total mass of Sd\u22121 is 1."
        },
        {
            "heading": "3 REMOVAL OF ANALYTICITY ASSUMPTION",
            "text": "In this part, we will prove that Assumption 1.3.(ii) on analyticity (which is (Li et al., 2022c, Assumption 5.3)) is unnecessary for (Li et al., 2022c, Theorem 5.4), and thus (Li et al., 2022c, Theorem 1.2) holds without such an assumption as well. Proposition 3.1. Under Assumption 1.2 and Assumption 1.3.(i), the conclusion of (Li et al., 2022c, Theorem 1.2) hold.\nFor this purpose, we temporarily adopt the setting of (Li et al., 2022c) for now. In other words, Assumption 1.3.(i) is being assumed and \u03931 is a compact connected submanifold of Sd\u22121 and \u0393 = {rx : r > 0;x \u2208 \u03931}. By (Li et al., 2022c, Theorem 4.1) it suffices to prove the (qualitative) mixing of the SDE (Li et al., 2022c, Equation (13)) on \u0393 towards a unique invariant probability measure. Using the notations from (Li et al., 2022c, Chapter E.3), this SDE writes:\ndYt = f0(Yt)dt+ K\u2211\nk=1\nfk(Yt) \u00b7 dBk,t, (9)\nwhere fk are certain vector fields along the radial cone \u0393 \u2286 Rd\\{0} and Bk,t, k = 1, \u00b7 \u00b7 \u00b7K are i.i.d. Wiener processes. Indeed, f1, \u00b7 \u00b7 \u00b7 , fk are orthogonal to the radial direction with\nfk(x) = \u2202\u03a6(x)\u03c3k(x), 1 \u2264 k \u2264 K\nand they span Tx\u0393\u2229x\u2225 = Tx\u0393|x| for all x \u2208 \u0393, where \u0393r = {x \u2208 \u0393 : |x| = r}, and f0 has the form\nf0(x) = 1\n2\n( \u2212 x+ \u22022\u03a6(x)[\u03a3(x)]\u2212 1\n2 K\u2211 i=k \u2202fk(x)fk(x) ) .\nIn the proofs from (Li et al., 2022c), analyticity is only used in Chapter F.4, when Tr\u03a3 is not a constant on \u03931. In this case, it was proved there (without using analyticity) that \u0393\u2217 := {y \u2208 \u0393 : r\u2212 \u2264 |x| \u2264 r+} with\nr\u2212 = min \u03931\n(Tr\u03a3) 1 4 , r+ = max\n\u03931 (Tr\u03a3)\n1 4 (10)\nis the unique invariant control set for the control problem corresponding to (9).\nInstead of using Kliemann\u2019s condition (Kliemann, 1987), which requires the vector field Lie algebra l generated by f0, \u00b7 \u00b7 \u00b7 , fN to be of maximal dimension at all points in \u0393\u2217 and is the reason for the need of analyticity in (Li et al., 2022c), we will use Arnold-Kliemann\u2019s condition (Arnold & Kliemann, 1987), which only requires that the vector field Lie algebra l to be of maximal dimensional at one point in \u0393\u2217. This condition is true because the projection of f0 to the radial direction is not constantly 0 if Tr\u03a3 is not a constant. (Otherwise \u0393\u2217 wouldn\u2019t be the unique invariant set.) Under this condition, (Arnold & Kliemann, 1987, Theorem 5.1) proved that there is a unique invariant probability measure \u03bd supported on \u0393\u2217. The measure \u03bd then has to be ergodic. Moreover, (Arnold & Kliemann, 1987, Theorem 5.2) showed that \u03bd is absolutely continuous with respect to the Riemannian volume on the manifold \u0393. In particular, \u03bd(\u2202\u0393\u2217) = 0 and \u03bd(\u0393\u2217) = 1.\nThe main issue is to prove the convergence of the distribution St,x towards \u03bd as t\u2192\u221e, where St,x denotes the measure of all trajectories of solutions to (9) at time t starting from x \u2208 \u0393\u2217. A priori, such convergence is only known to hold for 1T \u222b T 0 St,xdt by ergodic theorem.\nApplying (Duflo & Revuz, 1969, Theorem II.4) to (\u0393\u2217, \u03bd), it suffices to check two conditions to guarantee St,x \u2192 \u03bd (in total variation distance): (Harris\u2019s recurrence condition) For all sets A with \u03bd(A) > 0 and all x \u2208 \u0393\u2217,\nPX0=x (\u222b \u221e\n0\n1A(Xt)dt =\u221e ) = 1; (11)\n(Regularity condition) The absolutely continuous component of St,x respect to \u03bd, called S0t,x, satisfies limt\u2192\u221e S0t,x(\u0393\u2217) = 1 for all x \u2208 \u0393\u2217.\nLet us first verify (Harris\u2019s recurrence condition). Define D \u2282 \u0393\u2217 by D = {y \u2208 \u0393\u2217 : f0(x) /\u2208 Tx\u0393|x|}.\nThen D is open in \u0393\u2217. For all x \u2208 \u0393\u2217 and A \u2286 \u0393\u2217, we define the random variable \u03c4x,A \u2265 0 to be the first entering time into A for a trajectory of (9) starting at y. Lemma 3.2. P(\u03c4x,D <\u221e) = 1 for all x \u2208 \u0393\u2217.\nProof of (Harris\u2019s recurrence condition). By (Arnold & Kliemann, 1987, Theorem 6.1), (11) holds for all x \u2208 D \u2229 int\u0393\u2217 = D. By Lemma 3.2, it then holds for all x \u2208 \u0393\u2217. This verifies Harris\u2019s recurrence condition.\nWe now verify the (Regularity condition): In addition to the Lie algebra l and set D, define a Lie algebra l0 \u2286 T\u0393 and an open set D0 by\nl0 = span(f1, \u00b7 \u00b7 \u00b7 , fN ) + [l, l], D0 = {y \u2208 \u0393\u2217 : dim l0(x) = dim\u0393\u2217}. It is easy to see that l0 is indeed a Lie algebra and D0 is a relatively open subset in \u0393\u2217 Lemma 3.3. The set D0 has non-empty interior.\nWe postpone the proofs of Lemma 3.2 and 3.3 to Appendix A. Lemma 3.4. For all x \u2208 D0 and t > 0, S0t,x(\u0393\u2217) > 0.\nProof. By (Ichihara & Kunita, 1974, Lemma 2.1), at every x \u2208 intD0, the second order differential operator on the right hand side of (9) is elliptic (non-degenerate) at x. The lemma follows.\nProof of (Regularity condition). By (Harris\u2019s recurrence condition) and Lemma 3.3, for all x \u2208 \u0393\u2217, P(\u03c4x,D0 < \u221e) > 0, and thus there exists t0(x) > 0 such that P(\u03c4x,D0 < t0(x)) > 0. In other words, on a subset \u2126x,D0 \u2282 \u2126 of stochastic incidences \u03c9 with P(\u2126x,D0) > 0, there exists \u03c4 \u2032x,D0 = \u03c4 \u2032 x,D0\n(\u03c9) \u2208 (0, t0(x)) such that the solution starting at Y (0) = x satisfies Y (\u03c4 \u2032x,D0) \u2208 D0. By Lemma 3.4, for all t > t0(x), S0t,x(\u0393\u2217) \u2265 \u222b \u2126x,D S0t\u2212\u03c4 \u2032x,D0 ,Y (\u03c4 \u2032x,D0 ) (\u0393\u2217) > 0. This shows that the statement \u201cFor \u03bd-a.e. x, S0t,x(\u0393\u2217) = 0 for all t > 0\u201d is false. By (Duflo & Revuz, 1969, Proposition, p235), this guarantees (Regularity condition).\nWe have by now completed the proof of the mixing property St,x \u2192 \u03bd under the generic Assumption 1.2, and the Assumption 1.3.(i)."
        },
        {
            "heading": "4 REMOVAL OF UNIQUE BASIN ASSUMPTION",
            "text": "We now stop assuming Assumption 1.3.(i) and decompose \u0393 = \u2294 \u0393i where each \u0393i is a connected\ncone. Write \u0393# = \u0393 \u2229 {|x| \u2208 [R\u2212, R+]} where we fix R\u2212 < (\u03b7min|x|=1 Tr\u03a3(x)\n2\u03bb ) 1 4 and R+ >(\u03b7max|x|=1 Tr\u03a3(x)\n2\u03bb\n) 1 4 near these bounds.\nNote that \u0393 \u2229 {|x| \u2208 [ (\u03b7min|x|=1 Tr\u03a3(x)\n2\u03bb\n) 1 4 , (\u03b7max|x|=1 Tr\u03a3(x)\n2\u03bb\n) 1 4 ] is an invariant control set of (2).\nMoreover, it was proved in (Li et al., 2020) that for a given initial radius |x0|, the radius |Xt| of (2) starting at x0 will be almost surely inside [R\u2212, R+] for t \u2265 O((\u03b7\u03bb)\u22121). Write \u0393ir = \u0393\ni \u2229 {|x| = r} and \u0393i# = \u0393i \u2229 \u0393#. Then \u0393i1 are the connected components of the manifold \u03931. In particular, there are only finitely many \u0393i\u2019s. Write di = dim\u0393i. For s > 0, write U i1,p for the p-neighborhood of \u0393 i 1 in \u03931, U i r,p = rU i 1,p and U i #,p = \u22c3 r\u2208[R\u2212,R+] U i r,p.\nFix from now on a sufficiently small parameter p0 such that U i1,p0 are disjoint for distinct i\u2019s, and \u0393 i 1 is the set of all critical points of L inside U i1,10p0 . This is possible because of Assumption 1.2.(ii). Proposition 4.1. For all sufficiently small p1 (to be determined later), given K > 1, \u03f5 > 0, there exists a subset \u039bK,\u03f5 \u2286 Sd\u22121 with mSd\u22121(\u039bK,\u03f5) > 1 \u2212 \u03f5 such that for all x0 with |x0| \u2208 [ 1K ,K] and x0|x0| \u2208 \u039bK,\u03f5, with probability > 1 \u2212 \u03f5, the trajectory of (2) starting at x0 will remain inside\u22c3\ni U i #,p1 for t \u2208 [0, Cdes(\u03b7\u03bb)\u22121] for some constant Cdes.\nThe proof of the proposition, which we omit, is a simple combination of (Li et al., 2020, Equation (7)) and the proof of (Wang & Wang, 2022, Theorem 4.5).\nThe proposition allows us to assume that our initial point is inside one of finitely many basins U i#,p1 . To prove the main result, it now suffices to make two observations: First, with very high probability, the trajectory will not escape from the basin in exponential time. Second, as long as the trajectory remains in the basin, its distribution always mixes towards a unique probability measure \u03bdi supported at the bottom \u0393i of the basin.\nThe first property is stated as Proposition 4.2 below. Proposition 4.2. There exist C > 0, and sufficiently small p0 > p1 > 0, such that for all i and x0 \u2208 U i#,p1 , in the regime \u03b7 \u2264 O(\u03bb) \u2264 O(1), the solutions Xt to (2) with initial position x0 satisfy\nlim \u03bb\u03b7\u21920\nPX0=x0(Xt remains in U i#,p0 for t \u2208 [0, e C(\u03b7\u03bb)\u2212 1 2 ]) = 1.\nThe convergence is uniform with respect to the inital data x0.\nIt is similar in nature to (Wang & Wang, 2022, Lemma E.6) but requires a more sophisticated proof. This is the main theoretical component of this paper. The argument will be based on the large deviation principle of Dembo-Zeitouni in Dembo & Zeitouni (2010, Chapter 5), which was an adaptation of (Freidlin & Wentzell, 2012, Chapter 6). The reason for which Freidlin-Wentzell\u2019s original theory cannot be applied here like in (Wang & Wang, 2022) is that the diffusion in the SDE system (44), (45) is degenerate. Dembo-Zeitouni\u2019s work allows degenerate diffusions. However, further modifications to (Dembo & Zeitouni, 2010) are needed in our case as the first order drift in (45) depends on the \u03b3t = |Xt|4\u03b7\u22122. We will treat \u03b3t as a control variable. The full proof will be in Appendix B. The second property follows from the main results (Theorem 5.1 & Theorem 6.7) in (Li et al., 2022c)) and is restated as Proposition 4.3 here with an additional emphasis on uniformity. A more detailed discussion can be found in Appendix C. Recall that (Li et al., 2022c)) also assumes Assumption 1.3.(ii), but that can be dropped by the discussion in Chapter 3 above. Proposition 4.3. Under Assumption 1.2 For all K > 0 and sufficiently small p0 > p1 > 0, such that in the regime \u03b7 \u2264 O(\u03bb) \u2264 O(1) and \u03b7\u03bb \u2192 0, the following holds: For each index i and for all initial parameter x0 \u2208 U i#,p1 , the distribution of all trajectories { Xt } 0\u2264t\u2264 K log(1+\u03bb \u03b7 )\n\u03b7\u03bb\nof (2)\nthat do not leave U i#,p1 converges in distribution to the trajectories {X\u0302t} to a fixed SDE model (the Katzenberger model) supported on \u0393i with initial position \u03a6(x0). The convergence is uniform in x0. Moreover, as K \u2192 \u221e the trajectories {X\u0302t} are uniformly mixing (with respect to different x0\u2019s) towards a fixed equilibrium measure \u03bdi.\nOur main theorem, Theorem 2.4, then follows from combining Propositions 4.1, 4.2 and 4.3.\nProof of Theorem 2.4. By Proposition 4.1, after ignoring O( 1\u03b7\u03bb ) time at the beginning, as well as an o(1) portion of stochastic incidences. One may assume x0 \u2208 U i#,p for some i. For t within the range in the statement of Theorem 2.4, again after ignoring an o(1) portion of incidences, one may assume that all trajectories under consideration stay within U i#,p up to time t. Because of the lower bound for t, one may consider a window of length K log(1+\u03bb\u03b7 )\n\u03b7\u03bb that ends at t where K \u2192 \u221e. The distribution of trajectories along this window is the average of distributions over different initial positions. By Proposition 4.3, all such components uniformly mix toward \u03bdi. The proof is completed."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Mixing on local manifold: The key technical observation of this paper (Proposition 4.2) is that the distribution of trajectories with an initial position is trapped locally in the attracting basin containing the initial position during any practical observation windows. Using the method from (Wang & Wang, 2022, Fig. 13), this observation is supported by the experiment below: using a reduced MNIST dataset with only 1280 samples and a small CNN with 1786 parameters (so that the model is still overparametrized), we ran 15 independent instances of SGD, at \u03bb = \u03b7 = 132 , for each of two\nrandomly chosen initial parametrizations. Each instance lasts 0.8 million steps of SGD. A smilar experiment was ran for reduced CIFAR10 dataset with 1280 samples, a CNN model with 2658 parameters, \u03b7 = 11024 , \u03bb = 1 32 and 1.28 million SGD steps. In order to show that the distribution arising from each initial position does stabilize toward an equilibrium and the two equilibria are different, we track the variance within each group, and compare them with the average distance square over all pairs of point s from different groups. Namely, denoting by {Xki,t} the i-th trajectory starting at initial point xk where k = 1, 2, we compute the following quantities:\nV11(t) = Ei\u0338=j |X1i,t \u2212X1j,t|2, V22(t) = Ei\u0338=j |X2i,t \u2212X2j,t|2, V12(t) = Ei,j |X1i,t \u2212X2j,t|2.\nFigures 1, 2 shows that V11 and V22 stabilizes near similar but different values, but V12 stabilizes at a much bigger value. This suggests that the distributions of trajectories with starting point x1 and x2 mixes towards equilibria whose support have similar scales, but these two equilibria are far apart from each other.\nPrediction on the failure of stochastic weight averaging in parallel (SWAP): Our theory predicts that if the local minima manifold of minimizes \u0393i has non-trivial geometry, that is, the average of parameters on the manifold may fall off the manifold, then it might fail to decrease loss, or even increase the loss, once the SGD mixes to the local manifold.\nWe apply stochastic weight average over trajectories (SWAP) to the neural network parameters at each step over the 15 independent instances with the same initial position and compute the loss function at the averaged parameter. SWAP, a variant of stochastic weight average (SWA) from Izmailov et al. (2018), was proposed by Gupta et al. (2020). Figures 3, 4 show that although the loss of the SWAP parameter improves the average loss over the independent instances at the beginning, the improvement quickly breaks after a couple thousands of training steps. This phenomenon verifies our theoretical prediction and also suggests that the support of the equilibrium is not a convex set but rather a manifold of curved shape."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We give rigorous proof of fast equilibrium conjecture in generic situations, removing previous assumptions that there is only one basin and the set of minima is analytic. The main technical contribution is that we justify most of the trajectories of SDE would not escape from one basin within exponential time. Instead of using spectral analysis, we adopt the large deviation principle type of argument. Possible interesting direction may include understanding the dependence of mixing time on dimension, architecture and noise structure."
        },
        {
            "heading": "7 ACKNOWLEDGEMENT",
            "text": "Y.W. and Z.W. acknowledge respectively supports from NSF."
        },
        {
            "heading": "A PROOFS FOR SECTION 3",
            "text": "Proof. of Lemma 3.2. Instead of (9), define\ndY \u2225t = f \u2225 0 (Y \u2225 t )dt+\u2207Ni=1fi(Y \u2225 t ) \u00b7 dBi,t, (12)\nwhere f\u22250 (x) is the projection of f0(x) to Tx\u0393|x|.\nFor all 0 \u2264 t < \u03c4x,D, f0 = f\u22250 at Yt, thus the solutions Y and Y \u2225 to (9) and (12) starting at y coincide up to \u03c4x,D. Moreover, the trajectories of (12) stay in \u0393|x|. So \u03c4x,D is also the first entering time into D\u2229\u0393|x| for (12). We claim that D\u2229\u0393|x| is a non-empty subset of \u0393|x|. Indeed, if this is not true, then the trajectories Y and Y \u2225 coincide forever and stay inside \u0393|x|, making \u0393|x| an invariant control set which contradicts to the uniqueness of \u0393\u2217. Moreover, D \u2229 \u0393|x| is relatively open in the submanifold \u0393|x|. By Assumption 1.2.(iii), (12) has non-degenerate diffusion on the submanifold \u0393|x| = |x|\u03931. Thus Y \u22a5 enters D \u2229 \u0393|x| almost surely. This proves the lemma.\nProof. of Lemma 3.4. The proof is similar to that of (Li et al., 2022c, Lemma F.18). Recall that Tr\u03a3 is (\u22122)-homogeneous and assumed to be non-constant on \u03931. Therefore, there is an open set V1 \u2208 \u03931 and a vector field f\u2217 taking values in span(f1, \u00b7 \u00b7 \u00b7 , fN ) such that \u27e8\u2207Tr\u03a3, f\u2217\u27e9 =\u0338 0 on V1. By homogeneity, this is also true on rV1 \u2286 \u0393r for all r > 0. As Tr\u03a3(x) = \u2212 12 \u27e8x, f0(x) + x 2 \u27e9 and \u27e8x, f\n\u2217\u27e9 = 0, this implies \u27e8\u2207\u27e8x, f0(x)\u27e9, f\u2217\u27e9 =\u0338 0 on rV1. By (Li et al., 2022c, Lemma F.16), \u27e8x, [f0, f\u2217]\u27e9 =\u0338 0 on rV1. Note that [f0, f\u2217] \u2208 l0, and span(f1, \u00b7 \u00b7 \u00b7 , fN ) = T\u0393r on rV1 by controllability. It follows that {rV1 : r \u2208 (r\u2212, r+)} \u2286 D0, which is sufficient to conclude."
        },
        {
            "heading": "B EXITING TIME WITH DEGENERATE DIFFUSION AND EXTRA CONTROL VARIABLE",
            "text": "In this appendix, we establish a probabilistic lower bound to the exiting time of a stochastic process from a basin (Theorem B.21), based on a one-sided large deviation principle (Theorem B.12). Our proofs adapt those from the work of Dembo-Zeitouni (Dembo & Zeitouni, 2010, Chapter 5) to a more general setting. The main differences in the setting are:\n1. The stochastic process in the basin is now governed not only by the current location and an Brownian motion, but also by an extra control variable as stated in equation (17);\n2. The local minima set in the basin is no longer assumed to be a unique isolated fixed point.\nThe diffusion in the stochastic process is allowed to be degenerate, which was the main novelty in the Dembo-Zeitouni theory compared to the earlier work of Freidlin-Wentzell (Freidlin & Wentzell, 2012, Chapter 6).\nThis appendix will solely consist of mathematical analysis. With the exception of \u00a7B.4, all notations are chosen independently from those used in other parts of the current paper.\nB.1 PROPERTIES OF UPPER LARGE DEVIATION PRINCIPLE\nIn this section we define the notion of upper large deviation principle (upper LDP). This is the upper bound part of the large deviation principle defined in (Dembo & Zeitouni, 2010, Chapter 1.2). The principles proved in (Dembo & Zeitouni, 2010, Chapter 4.1 & 4.2), which allows to pass the LDP property between random processes, are still valid for upper LDP because the upper and lower bounds are treated separately in their proofs. The purpose of this section is to briefly list which facts are relevant and justify the survival of their proofs in (Dembo & Zeitouni, 2010) with upper LDP.\nDefinition B.1. A rate function I is a lower semicontinuous mapping I : X \u2192 [0,\u221e] on a metric space X , i.e. I\u22121([0, a]) is closed for all finite a. A good rate function is a rate function I such that I\u22121([0, a]) is compact for all finite a. The effective domain DI if I is I\u22121([0,\u221e)). Definition B.2. A family of Borel probability measures {\u00b5\u03f5} on (X ,B) satisfies upper large deviation principle (upper LDP) with rate function I if for all measurable subsets A of X ,\nlim sup \u03f5\u21920 \u03f5 log\u00b5\u03f5(A) \u2264 \u2212 inf x\u2208A I(x). (13)\nA family of Borel probability measures \u00b5\u03f5 on (X ,B) satisfies weak upper large deviation principle (weak upper LDP) with rate function I if (13) holds for all compact subsets A.\nFor background, recall that {\u00b5\u03f5} is said to satisfy the large deviation principle with rate function I if in addition to (13) it also satisfies the lower bound\nlim inf \u03f5\u21920 \u03f5 log\u00b5\u03f5(A) \u2265 \u2212 inf x\u2208A\u25e6 I(x). (14)\nTheorem B.3. Suppose A is a base for the topology of X . Then a family of probability measures {\u00b5\u03f5} on X satisfies weak upper LDP with rate function\nI(x) := sup A\u2208A,x\u2208A\n( \u2212 lim sup\n\u03f5\u21920 \u03f5 log\u00b5\u03f5(A)\n) .\nTheorem B.4. (Contraction Principle) If F : X \u2192 Y is a continuous map between Hausdorff spaces and I : X \u2192 [0,\u221e] is a good rate function. Then\n(a) The function I \u2032(y) := inf F\u22121({y}) I is a good rate function on Y;\n(b) If a family of probability measures {\u00b5\u03f5} on X satisfies upper LDP with rate function I , then the pushforward measures {\u00b5\u03f5 \u25e6F\u22121} satisfies upper LDP with rate function I \u2032 on Y .\nRemark about the proofs. Theorem B.3 and Theorem B.4 are the upper bound directions of (Dembo & Zeitouni, 2010, Theorem 4.1.11 & 4.2.1). Their proofs are identical to those therein. For Theorem B.3, note that this direction only uses the equality (4.1.14), but not (4.1.12) and (4.1.13), in (Dembo & Zeitouni, 2010).\nDefinition B.5. For families {\u03bd\u03f5,m} and {\u03bd\u03f5} of probability measures on a metric space Y , where m \u2208 N and \u03f5 > 0, we say {\u03bd\u03f5,m} are exponentially good approximations of {\u03bd\u03f5} if there exist probability spaces (\u2126,B\u03f5,P\u03f5,m) and two families of random variables y\u03f5,m, y\u03f5 with joint distribution P\u03f5 and marginal distributions \u03bd\u03f5,m, \u03bd\u03f5 such that for all \u03b4 > 0, the event dist(y\u03f5,m, y\u03f5) > \u03b4 is B\u03f5-measurable and\nlim m\u2192\u221e lim sup \u03f5\u21920\n\u03f5 logP\u03f5 ( dist(y\u03f5,m, y\u03f5) > \u03b4 ) = \u2212\u221e. (15)\nIf in addition \u03bd\u03f5,m = \u03bd\u0303\u03f5 is independent of m, we say {\u03bd\u0303\u03f5} and {\u03bd\u03f5} are exponentially equivalent. Theorem B.6. For families {\u03bd\u03f5,m} and {\u03bd\u03f5} of probability measures on a metric space Y , where m \u2208 N and \u03f5 > 0, and {\u03bd\u03f5,m} are exponentially good approximations of {\u03bd\u03f5}. If for each m, {\u03bd\u03f5,m} satisfies upper LDP with rate function Im, then\n(a) {\u03bd\u0303\u03f5} satisfies weak upper LDP with rate function\nI(y) := sup \u03b4>0 lim sup m\u2192\u221e inf z\u2208B\u03b4(y) Im(z).\n(b) If in addition I is a good rate function and for every closed subset A \u2286 Y ,\ninf Y \u2208A I(Y ) \u2264 lim sup m\u2192\u221e inf Y \u2208A Im(Y ).\nRemark about the proof. The proof is the same as that of (Dembo & Zeitouni, 2010, Theorem 4.2.16). For part (a), by Theorem 3.3 applied to the topological base consisting of all metric balls B\u03b4(y) in Y , {\u03bd\u03f5,m} satisfies weak upper LDP with rate function\nI\u2217(y) := sup \u03b4>0\n( \u2212 lim sup\n\u03f5\u21920 \u03f5 log \u03bd\u03f5(B\u03b4(y))\n) .\nSo it suffices to prove I(y) \u2264 I\u2217(y). This was done in the proof of (Dembo & Zeitouni, 2010, Theorem 4.2.16, part (a)) via the inequality\nlim sup \u03f5\u21920 \u03f5 log \u03bd\u03f5(B\u03b4(y)) \u2264 lim inf m\u2192\u221e\n( \u2212 inf\nz\u2208B2\u03b4(y) Im(z)\n) = \u2212 lim sup\nm\u2192\u221e inf z\u2208B2\u03b4(y) Im(z).\nHence\nI\u2217(y) = sup \u03b4>0\n( \u2212 lim sup\n\u03f5\u21920 \u03f5 log \u03bd\u03f5(B\u03b4(y))\n) \u2265 sup\n\u03b4>0 lim sup m\u2192\u221e inf z\u2208B2\u03b4(y) Im(z) = I(y).\nThe proof of part (b) is verbatim as in (Dembo & Zeitouni, 2010).\nTheorem B.7. Suppose a family of probability measures {\u00b5\u03f5} satisfies upper LDP with a good rate function I on a Hausdorff topological space X . And suppose a sequence of continuous maps {Fm} from X to another Hausdorff topological space Y approximate a measurable maps F in the sense that for all a <\u221e,\nlim sup m\u2192\u221e sup {x:I(x)\u2264a}\ndist(Fm(x), F (x)) = 0.\nFinally, assume the families {\u00b5\u03f5\u25e6(Fm)\u22121} are exponentially good approximations of another family of probability distributions {\u03bd\u03f5} on Y . Then {\u03bd\u03f5} satisfies upper LDP with good rate functions I \u2032(y) := infF\u22121({y}) I .\nRemark about the proof. The theorem is the upper bound part of of (Dembo & Zeitouni, 2010, Theorem 4.2.23). The proof stay the same with (Dembo & Zeitouni, 2010, Theorems 4.2.1 & 4.2.16) replaced by their respective upper bounds direction, namely Theorem B.4 and Theorem B.6.\nB.2 UPPER LDP FOR DEGENERATE DIFFUSION WITH EXTRA CONTROL VARIABLE\nIn this part, we prove a variation of the upper bound direction in the large deviation principle proved in (Dembo & Zeitouni, 2010, Theorem 5.6.7). The notations in this section are self-contained and independent of those from other parts of this paper.\nThroughout this section we will consider the following setting:\n\u2022 U \u2286 Rn is an open domain, \u2022 D \u2286 Rl be a compact set, \u2022 b : U \u2192 Rn, \u03c3 : U \u00d7D \u2192 L(Rd,Rn) and h : R\u00d7U \u00d7D \u2192 Rl are Lipschitz continuous\nfunctions, \u2022 b and \u03c3 are bounded, and h is bounded on [0, \u03f50]\u00d7 U \u00d7D for some \u03f50 > 0. We will fix \u03f50\nand a common upper bound H for b, \u03c3 and h respectively on these domains.\nConsider the following families of stochastic differential equations on (Y, Z) \u2208 U \u00d7 Rl and \u03f5 > 0:\ndY \u03f5t = b(Y \u03f5 t , Z \u03f5 t )dt+ \u221a \u03f5\u03c3(Y \u03f5t , Z \u03f5 t )dB d t ; (16)\ndZ\u03f5t = h(\u03f5, Y \u03f5 t , Z \u03f5 t )dt. (17)\nThe main difference of our setting from that in (Dembo & Zeitouni, 2010) is the existence of the additional control variable Z\u03f5t , whose evolution depends on \u03f5 in a less prescribed way.\nWe will assume throughout this section, in addition to Assumption B.8, that:\nAssumption B.8. For all \u03f5 > 0 and initial values (y0, z0) \u2208 U \u00d7D, the solution (Y \u03f5t , Z\u03f5t ) to (16) and (17) starting at (y0, z0) almost surely remains in U \u00d7D for all t > 0. Definition B.9. Given the functions b, \u03c3, the upper bound H on |h|, and T > 0. The associated path space ST is defined as the family of triples (f, g, u) with f \u2208 C0([0, T ], U), g \u2208 CLipH ([0, T ], D) and u \u2208W 1,2([0, T ],Rd) that\nft = y0 + \u222b t 0 b(fs, gs)ds+ \u222b t 0 \u03c3(fs, gs)dus. (18)\nHere CLipH ([0, T ], D) is the subspace in C 0([0, T ], D) of functions g with Lipschitz constant bounded by H , i.e. that satisfy\nsup s \u0338=t \u2223\u2223\u2223g(s)\u2212 g(t) s\u2212 t \u2223\u2223\u2223 \u2264 H. (19) And W 1,2 is the square integrable Sobolev space of first order differentiability.\nLemma B.10. CLipH ([0, T ], D) is compact in C 0([0.T ], D).\nProof. As C0([0.T ], D) is a metric space, it suffices to show any sequence g(k) in CLipH ([0, T ], D) has a convergent subsequence with limit in CLipH ([0, T ], D)\nBecause D is a bounded domain, the g(k)\u2019s are uniformly bounded. As they are also uniformly Lipschitz with Lipschitz constant bounded by H , by Arzela\u0300-Ascoli Theorem we may assume g(k) converges in C0 to some g \u2208 C0([0, T ], D). Then g would be H-Lipschitz continuous as well.\nFrom now on, CLipH ([0, T ], D) will be equipped with the C 0 topology without further notice.\nDefinition B.11. Given a function f \u2208 C0([0, T ], U), the corresponding energy functional is\n\u03a6T (f) := inf (g,u) such that (f,g,u)\u2208ST\n\u222b T 0 1 2 |u\u0307t|2dt.\nBy convention, \u03a6T (f) =\u221e if ST is empty. Theorem B.12. Given a closed subsetA of the metric spaceA \u2282 C0([0, T ], U) and an initial value y0, for the solution (Y \u03f5t , Z \u03f5 t ) to (16) and (17) with initial value (y0, z0), the following inequality holds: lim sup\n\u03f5\u21920 \u03f5 sup z0\u2208D\nlogP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 t \u2208 A) \u2264 \u2212 inf\nf\u2208A f0=y0\n\u03a6T (f).\nIn order to prove Theorem B.12, we need more notations to study the Y \u03f5t while keeping the path Z \u03f5 t\nfixed. For this purpose, we define some distributions of (Z\u03f5t , \u221a \u03f5Bdt ) and Y \u03f5 t respectively. Definition B.13. Denote by \u00b5\u03f5(y0,z0),T the joint distribution of (Z \u03f5 t , \u221a \u03f5Bdt ) on C Lip H ([0, T ], D) \u00d7 C0([0, T ],Rd), and \u03bd\u03f5(y0,z0),T for the distribution of Y \u03f5 t in C 0([0, T ], U), where (Y \u03f5t , Z \u03f5 t ) is solution to (16), (17) with initial value (y0, z0) at t = 0. Write \u03bb\u03f5 for the distribution of \u221a \u03f5Bdt on C0([0, T ],Rd).\nDefinition B.14. Denote byM\u03f5T the space of all probability distributions \u00b5 on C Lip H ([0, T ], D) \u00d7 C0([0, T ],Rd) whose projection to the second coordinate is \u03bb\u03f5.\nFor all initial values (y0, z0) \u2208 U \u00d7 D, Y \u03f5t , Z\u03f5t are progressively measurable processes. In consequence,\n\u00b5\u03f5(y0,z0),T \u2208M \u03f5 T . (20)\nLemma B.15. The function\nI(g, u) :=  \u222b 1 0 1 2 |u\u0307t|2dt if u \u2208W 1,2([0, 1],Rd)\n\u221e otherwise\nis a good rate function on CLipH ([0, 1], D) \u00d7 C0([0, 1],Rd). Moreover, any family of probability measures {\u00b5\u03f5} on CLipH ([0, 1], D) \u00d7 C0([0, 1],Rd) such that \u00b5\u03f5 \u2208 M\u03f51 satisfies upper LDP with rate function I .\nProof. We first check that I is a rate function. That is, it is lower semicountinuous on CLipH ([0, 1], D) \u00d7 C0([0, 1],Rd). It suffices to show that that I(g, u) \u2264 lim inf\nk\u2192\u221e I(g(k), u(k)) if\n(g(k), u(k))\u2192 (g, u) in C0 norm.\nBy Lemma B.10, g is in CLipH ([0, 1], D), and thus by definition I(g, u) = I0(u) in this case.\nIt was known by Schilder\u2019s Theorem ((Dembo & Zeitouni, 2010, Theorem 5.2.3)) that the function\nI0(u) :=  \u222b 1 0 1 2 |u\u0307t|2dt if u \u2208W 1,2([0, 1],Rd);\n\u221e otherwise.\nis a good rate function, and in particular lower semicountinuous. Thus\nlim inf k\u2192\u221e I(g(k), u(k)) = lim inf k\u2192\u221e I0(u (k)) \u2265 I0(u) = I(g, u).\nThus I is a good rate function and we want to show that it is good, i.e. I\u22121([0, a]) is compact for all finite a. Remark that I\u22121([0, a]) = CLipH ([0, 1], D) \u00d7 I \u22121 0 ([0, a]) and the second factor is compact as I0 is a good rate function. Note that I0(u) = I(g, u). So it suffices to know that C Lip H ([0, 1], D) is a compact space in C0 topology, which is the assertion of Lemma B.10.\nFinally we need to show that (13) holds for {\u00b5\u03f5} and I . The same inequality holds for {\u03bb\u03f5} and I0, i.e. for all A0 \u2286 C0([0, 1],Rd),\nlim sup \u03f5\u21920 \u03f5 log \u03bb\u03f5(A0) \u2264 \u2212 inf u\u2208A0 I0(u).\nFor all measurable set A \u2282 CLipH ([0, 1], D) \u00d7 C0([0, 1],Rd), let A0 denote its projection to C0([0, 1],Rd). Then\nlim sup \u03f5\u21920 \u03f5 log\u00b5\u03f5(A) \u2264 lim sup \u03f5\u21920\n\u03f5 log\u00b5\u03f5 ( CLipH ([0, 1], D)\u00d7A0 ) = lim sup\n\u03f5\u21920 \u03f5 log \u03bb\u03f5(A0)\n\u2264\u2212 inf u\u2208A0 I0(u) \u2264 \u2212 inf (g,u)\u2208A I(g, u).\nIn the last inequality, we used the fact thatA0 contains the projection ofA and that I(g, u) = I0(u). This completes the proof.\nFor all (g, u) \u2208 CLipH ([0, 1], D)\u00d7C0([0, 1],Rd), denote by Y \u03f5(g,u),t the solution on [0, 1], with initial value Y \u03f5(g,u),0 = y0 to the stochastic differential equation\ndY \u03f5(g,u),t = b(Y \u03f5 (g,u),t, gt)dt+ \u03c3(Y \u03f5 (g,u),t, gt)dut. (21)\nIn addition, given an integer m \u2208 N, we also define Y \u03f5,m(g,u),t as the solution on [0, 1], also with initial value y0, to the following stochastic differential equation\ndY \u03f5,m(g,u),t = b(Y \u03f5,m\n(g,u), \u2308mt\u2309 m\n, gt)dt+ \u03c3(Y \u03f5,m\n(g,u), \u2308mt\u2309 m\n, gt)dut. (22)\nWe emphasize that Y \u03f5(g,u),t and Y \u03f5,m (g,u),t are deterministic once the pair (g, u) are given and all randomness comes from \u00b5\u03f5(y0,z0),T .\nLemma B.16. For all any \u03b4 > 0 and all initial values y0 \u2208 D,\nlim m\u2192\u221e lim sup \u03f5\u21920 \u03f5 sup \u00b5\u2208M\u03f51 logP(g,u)\u223c\u00b5( sup t\u2208[0,1] |Y \u03f5,m(g,u),t \u2212 Y \u03f5 (g,u),t| \u2265 \u03b4) = \u2212\u221e.\nProof of Lemma B.16. Fix \u03b4 > 0. Let \u2206\u03f5,m(g,u),t := Y \u03f5,m (g,u),t \u2212 Y \u03f5 (g,u),t , for any \u03c1 > 0, define the stopping time \u03c4 \u03f5,m,\u03c1 := min(inf{t : |Y \u03f5,m(g,u),t \u2212 Y \u03f5,m\n(g,u), [mt] m\n| \u2265 \u03c1}, 1).\n\u03c4 \u03f5,m,\u03c1 depends on (g, u), but we skip it to simplify the notation.\nThe process \u2206\u03f5,n(g,u),t satisfies the SDE\nd\u2206\u03f5,n(g,u),t = b \u2206 t dt+ \u221a \u03f5\u03c3\u2206t dut\nwith coefficients b\u2206t := b(Y \u03f5,m\n(g,u), [mt] m\n, gt)\u2212 b(Y \u03f5(g,u),t, gt),\n\u03c3\u2206t := \u03c3(Y \u03f5,m\n(g,u), [mt] m\n, gt)\u2212 \u03c3(Y \u03f5(g,u),t, gt).\nBy the uniform Lipschitz continuity of b and \u03c3, and the boundedness of gt \u2208 D, there is a constant C such that for all t \u2264 \u03c4 \u03f5,m,\u03c1,\nmax(|b\u2206t |, |\u03c3\u2206t |) \u2264 C(|\u2206 \u03f5,n (g,u),t|+ \u03c1).\nBy (Dembo & Zeitouni, 2010, Lemma 5.6.18), for all \u03f5 \u2208 (0, 1), \u03b4 > 0,\n\u03f5 sup \u00b5\u2208M\u03f51 logP(g,u)\u223c\u00b5( sup t\u2208[0,\u03c4\u03f5,m,\u03c1]\n|\u2206\u03f5,m(g,u),t| \u2265 \u03b4) \u2264 K + log ( \u03c12 \u03c12 + \u03b42 ) ,\nwhere K is a constant independent of \u03b4, \u03f5, \u03c1, \u00b5 and m. Then\nlim \u03c1\u21920 sup m\u22651 lim \u03f5\u21920 \u03f5 sup \u00b5\u2208M\u03f51 logP(g,u)\u223c\u00b5( sup t\u2208[0,\u03c4\u03f5,m,\u03c1] |\u2206\u03f5,m(g,u),t| \u2265 \u03b4) = \u2212\u221e.\nSince the event {supt\u2208[0,1] |\u2206 \u03f5,m (g,u),t| \u2265 \u03b4} is contained in the union\n{\u03c4 \u03f5,m,\u03c1 < 1} \u222a { sup t\u2208[0,\u03c4\u03f5,m,\u03c1] |\u2206\u03f5,m(g,u),t| \u2265 \u03b4},\nthe lemma is proved if we show for all \u03c1 > 0,\nlim m\u2192\u221e lim \u03f5\u21920 \u03f5 sup \u00b5\u2208M\u03f51 logP(g,u)\u223c\u00b5( sup t\u2208[0,1] |Y \u03f5,m(g,u),t \u2212 Y \u03f5,m (g,u), [mt] m | \u2265 \u03c1) = \u2212\u221e. (23)\nTo prove this, recall that |b| and |\u03c3| are bounded by a constant H , and\n|Y \u03f5,m(g,u),t \u2212 Y \u03f5,m\n(g,u), [mt] m | \u2264 H [ 1 m + \u221a \u03f5 max k=0,...,m\u22121\nsup 0\u2264s\u2264 1m\n|us+ km \u2212 u km | ] .\nTherefore, for m > H\u03c1 ,\nsup \u00b5\u2208M\u03f51 logP(g,u)\u223c\u00b5( sup 0\u2264t\u22641 |Y \u03f5,m(g,u),t \u2212 Y \u03f5,m (g,u), [mt] m | \u2265 \u03c1)\n\u2264mP( sup 0\u2264s\u2264 1m\n|us| \u2265 \u03c1\u2212 Hm\u221a\n\u03f5C )\n\u22644dme\u2212m(\u03c1\u2212Hm ) 2/2d\u03f5C2 .\nThis guarantees (23) and proves the lemma.\nProof of Theorem B.12. First of all, notice that one can assume without loss of generality that T = 1 by rescaling the time interval [0, T ] to [0, 1]. To see this, notice that the rescaled Brownian motion BdTt is equivalent to \u221a TBdt on t \u2208 [0, 1] and if we make the change of variable uTt = \u221a Tvt\naccordingly, then \u222b T 0 1 2 |u\u0307t|2dt = 1 T \u222b 1 0 1 2 \u2223\u2223 d dt uTt \u2223\u22232dt = \u222b 1 0 1 2 |v\u0307t|2dt.\nWe will only deal with the [0, 1] interval below.\nDefine maps Fm, F : CLipH ([0, 1], D)\u00d7 C0([0, 1],Rd)\u2192 C0([0, 1], U) as follows. For Fm, the image fm = Fm(g, u) satisfies fm0 = y0 and\nfmt = f m k m + \u222b t k m b(fmk m , gs)ds+ \u222b t k m \u03c3(fmk m , gs)u\u0307sds, (24)\non t \u2208 [ km , k+1 m ], k = 0, ...,m\u2212 1.\nFor F , the image f = F (g, u) instead satisfies f0 = y0 and\nft = f0 + \u222b t 0 b(fs, gs)ds+ \u222b t 0 \u03c3(fs, gs)u\u0307sds (25)\nfor t \u2208 [0, 1]. It is not hard to check by Lipschitz boundedness of b, \u03c3, and the compactnes of D that Fm and F send CLipH ([0, 1], D)\u00d7C0([0, 1],Rd) to C0([0, 1], U). Moreover, (21) and (22) can be reformulated as\nY \u03f5,m(g,u),t = F m(g, u) and Y \u03f5(g,u),t = F (g, u) respectively. (26)\nFor (g, u), (g\u2032, u\u2032) \u2208 CLipH ([0, 1], D)\u00d7 C0([0, 1],Rd), let\n\u0398m := |Fm(g, u)\u2212 Fm(g\u2032, u\u2032)|.\n\u0398m is a function of t, and by the Lipschitz bounds on b and \u03c3,\nsup t\u2208[ km , k+1 m ]\n\u0398m(t) \u2264 C(\u0398m( k m ) + \u2225(g, u)\u2212 (g\u2032, u\u2032)\u2225C0).\nSince \u0398m(0) = 0, we derive the continuity of the operator Fm by iterating the argument with k = 0, 1, ...,m\u2212 1. A similar argument guarantees the continuity of the operator F . By (20) and Lemma B.16, for all family of initial values {z\u03f50 \u2208 D}\u03f5>0,\nlim m\u2192\u221e lim sup \u03f5\u21920 \u03f5 logP(g,u)\u223c\u00b5\u03f5 (y0,z \u03f5 0),1 ( sup t\u2208[0,1] |Y \u03f5,m(g,u),t \u2212 Y \u03f5 (g,u),t| \u2265 \u03b4) = \u2212\u221e.\nIn other words, the families {\u00b5\u03f5(y0,z\u03f50),1 \u25e6 (F m)\u22121} are exponentially good approximations of the distribution of Y \u03f5(g,u),t with (g, u) \u223c \u00b5 \u03f5 (y0,z\u03f50),1 .\nThus if we can prove: for all a <\u221e,\nlim m\u2192\u221e sup (g,u)\u2208CLipH ([0,1],D)\u00d7C\n0([0,1],Rd):\u222b 1 0 1 2 |u\u0307t| 2dt\u2264a\n\u2225Fm(g, u)\u2212 F (g, u)\u2225C0 = 0, (27)\nthen by Lemma B.7 and Lemma B.15, the distribution of Y \u03f5(g,u),t with (g, u) \u223c \u00b5 \u03f5 (y0,z\u03f50),1 satisfies upper LDP with good rate function y 7\u2192 infF (g,u)=y I(g, u), which is exactly \u03a61(y). Because for each different value of \u03f5, z\u03f50 is arbitrarily chosen in D and the distribution of Y \u03f5 (g,u),t for (g, u) \u223c \u00b5\u03f5(y0,z\u03f50),1 coincides with the distribution \u03bd\u03f5(y0,z\u03f50),1 of Y \u03f5 t with initial condition (Y \u03f5 0 , Z \u03f5 0) = (y0, z \u03f5 0) in (16), (17), this exactly yields the statement of the theorem.\nIt remains to show (27). Let \u2206m = |Fm(g, u)\u2212 F (g, u)|. Note that if \u222b 1 0 1 2 |u\u0307t|2dt \u2264 a\nholds, then for fm = Fm(g, u) and f = F (g, u) given in (24), (25),\nsup t\u2208[0,1]\nmax ( |fmt \u2212 fm[mt]\nm\n|, |ft \u2212 f [mt] m | ) \u2264 C0 \u00b7 1 m + \u221a C0a \u00b7 1 m , (28)\nfor some constant C0 by the bound on the coefficients and Cauchy-Schwarz inequality. Write \u03b7m for the right hand side in (28). Then by Lipschitz continuity of b and \u03c3, for some other constant C,\n\u2206mt =|fmt \u2212 ft|\n= \u2223\u2223\u2223\u2223\u2223 \u222b t 0 ( b(fm[ms] m , gs)\u2212 b(fs, gs) ) ds+ \u222b t 0 ( \u03c3(fm[ms] m , gs)\u2212 \u03c3(fs, gs) ) u\u0307sds \u2223\u2223\u2223\u2223\u2223 \u2264 \u222b t 0 C|fm[ms] m \u2212 fs|ds+ \u222b t 0 C|fm[ms] m \u2212 fs| \u00b7 |u\u0307s|ds\n\u2264 \u222b t 0 C(\u2206ms + \u03b7m)(1 + |u\u0307s|)ds\nBy Cauchy-Schwarz inequality, for all t \u2208 [0, 1]\n(\u2206mt ) 2 \u2264C2 \u222b t 0 (\u2206ms + \u03b7m) 2ds \u222b t 0 (1 + |u\u0307s|)2ds\n\u2264C2 \u222b t 0 2 ( (\u2206ms ) 2 + (\u03b7m) 2 ) ds \u222b t 0 2(1 + |u\u0307s|2)ds\n\u22644C2(1 + a) ( \u222b t\n0\n(\u2206ms ) 2ds+ (\u03b7m)2\n) .\nThus by Gronwall\u2019s inequality, (\u2206mt ) 2 \u2264 4C2(1 + a)e4C2(1+a)t(\u03b7m)2. The equality (27) follows by letting m\u2192\u221e, This completes the proof.\nThe following is a strengthen version of Theorem B.12. Theorem B.17. Given a closed subsetA of the metric spaceA \u2282 C0([0, 1], U) and an initial value (y\u2217, z\u2217) \u2208 U \u00d7 V , for solutions (Y \u03f5t , Z\u03f5t ) to (16) and (17), the following inequality holds:\nlim sup \u03f5\u21920\ny0\u2192y\u2217\nsup z0\u2208D\n\u03f5 logP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 t \u2208 A) \u2264 \u2212 inf\nf\u2208A f0=y\u2217\n\u03a6T (f).\nProof of Theorem B.17. As in the proof of Theorem B.12, we can assume T = 1.\nBy Theorem B.6, it suffices to prove that for any family of points y\u03f50 \u2192 y\u2217 as \u03f5 \u2192 0 and arbitrary z\u03f50 \u2208 D, the family of distribution {\u03bd\u03f5(y\u03f50,z\u03f50),1} is exponentially equivalent to {\u03bd \u03f5 (y\u2217,z\u03f50),1\n} as in Definition B.5. Write\n\u2206\u03f5t =\n( Y \u03f5(y\u03f50,z\u03f50),t\n\u2212 Y \u03f5(y\u2217,z\u03f50),t Z\u03f5(y\u03f50,z\u03f50),t \u2212 Z\u03f5(y\u2217,z\u03f50),t\n) .\nThen \u2206\u03f5t satisfies d\u2206\u03f5t = \u03b1 \u03f5 tdt+ \u03b2 \u03f5 tdB d t\nwhere\n\u03b1\u03f5t =\n( b(Y \u03f5(y\u03f50,z\u03f50),t , Z\u03f5(y\u03f50,z\u03f50),t )\u2212 b(Y \u03f5(y\u2217,z\u03f50),t, Z \u03f5 (y\u2217,z\u03f50),t )\nh(Y \u03f5(y\u03f50,z\u03f50),t , Z\u03f5(y\u03f50,z\u03f50),t )\u2212 h(Y \u03f5(y\u2217,z\u03f50),t, Z \u03f5 (y\u2217,z\u03f50),t )\n) ,\nand\n\u03b2\u03f5t =\n( \u03c3(Y \u03f5(y\u03f50,z\u03f50),t , Z\u03f5(y\u03f50,z\u03f50),t )\u2212 \u03c3(Y \u03f5(y\u2217,z\u03f50),t, Z \u03f5 (y\u2217,z\u03f50),t )\n0\n) .\nMoreover,\n\u2206\u03f50 =\n( y\u03f50 \u2212 y\u2217\n0\n) \u2192 0 as \u03f5\u2192 0.\nRemark that the coefficients \u03b1\u03f5t , \u03b2 \u03f5 t are progressively measurable processes with respect to the filter generated by the Brownian motion {Bdt }. Moreover, by Lipschitz continuity of b, \u03c3, h, for some constant C0,\nmax(|\u03b1\u03f5t |, |\u03b2\u03f5t |) \u2264 C0|\u2206\u03f5t|.\nBy applying (Dembo & Zeitouni, 2010, Lemma 5.6.18) with \u03c41 = 1, we know that there is another constant C such that for all \u03c1 > 0, \u03b4 > 0,\n\u03f5 logP( sup t\u2208[0,1]\n|\u2206\u03f5t| \u2265 \u03b4) \u2264 C + log \u03c12 + |y\u03f50 \u2212 y\u2217|2\n\u03c12 + \u03b42 .\nBy letting \u03c1\u2192 0 first and then \u03f5\u2192 0, it follows that\nlim sup \u03f5\u21920 \u03f5 logP( sup t\u2208[0,1] |\u2206\u03f5t| \u2265 \u03b4) = \u2212\u221e.\nThis shows {\u03bd\u03f5(y\u03f50,z\u03f50),1} is exponentially equivalent to {\u03bd \u03f5 (y\u2217,z\u03f50),1 }, which suffices to conclude the proof.\nCorollary B.18. Given a closed subset A of the metric space A \u2282 C0([0, 1], U) and a compact set K \u2286 U , for solutions (Y \u03f5t , Z\u03f5t ) to (16) and (17), the following inequality holds:\nlim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208K\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 t \u2208 A) \u2264 \u2212 inf\nf\u2208A f0\u2208K\n\u03a6T (f).\nProof. Let M \u2208 [0,\u221e) be a finite value strictly less than inf f\u2208A f0\u2208K \u03a6T (f) \u2208 [0,\u221e]. By Theorem B.17, for all y \u2208 K, there is a value \u03f5y such that for all y0 \u2208 B\u03f5y (y) and 0 < \u03f5 < \u03f5y ,\n\u03f5 sup z0\u2208D\n\u03f5 logP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 t \u2208 A) \u2264 \u2212M.\nCover K by finitely many balls of the form B\u03f5yi (yi), then for all 0 < \u03f5 < mini \u03f5yi ,\n\u03f5 sup y\u2208K sup z0\u2208D\n\u03f5 logP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 t \u2208 A) \u2264 \u2212M.\nThe proof is completed by letting M \u2192 inf f\u2208A f0\u2208K \u03a6T (f).\nB.3 EXITING TIME FROM BASIN\nIn this section, it will be assumed, in addition to Assumption B.8, that:\nAssumption B.19. There are a C2 function L : U \u2192 [0,\u221e) and a bounded open set V \u2282 U such that:\n(1) \u2207b(y,z)L(y) \u2264 0 for all (y, z) \u2208 V \u00d7D and the equality holds if and only if L(y) = 0;\n(2) L is strictly positive on \u2202V .\nWrite Vq = L\u22121([0, q)) \u2229 V and choose q0 sufficiently small such that Vq0 lies in the interior of V . In particular, for all q \u2208 [0, q0], L|\u2202Vq \u2261 q. Definition B.20. Suppose 0 < q < Q \u2264 q0. For a solution (Y \u03f5t , Z\u03f5t ) of (16), (17) with initial value in VQ \u00d7D, denote by \u03c4 \u03f5q,Q the first time Y \u03f5t hits Vq \u222a \u2202VQ, and by \u03c4 \u03f5Q the first time Y \u03f5t hits \u2202VQ.\nOur goal is to prove the following main theorem:\nTheorem B.21. Under Assumptions B.8 and B.19, for all 0 < q < Q < q0 there exists IQ > 0 such that for all,\nlim \u03f5\u21920 sup (y0,z0)\u2208Vq\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q > e\nIQ \u03f5 ) = 1.\nThe following lemma is an analogue to (Dembo & Zeitouni, 2010, Lemma 5.7.22)\nLemma B.22. For all 0 < q < Q\u2032 < Q \u2264 q0, the stopping time \u03c4 \u03f5q,Q satisfies\nlim \u03f5\u21920 sup (y0,z0)\u2208VQ\u2032\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 q,Q <\u221e and Y \u03f5\u03c4\u03f5q,Q \u2208 Vq) = 1.\nProof. Consider the solution Y 0t to the deterministic flow\ndY 0t = b(Y 0 t , z0)dt (29)\nstarting at y0. By Assumption B.19, L(Y 0t ) is decreasing and must converge to 0 as t\u2192 0, and Y 0t remains in VQ. Denote by T\u0303 the first moment L(Y 0t ) reaches q 2 . Set\n\u03b4 = \u2225L\u2225\u22121C1 min( q 4 , Q\u2212Q\u2032 2 ) > 0.\nNotice supt\u2208[0,T\u0303 ] dist ( (Y \u03f5t , Z \u03f5 t ), (Y 0 t , z0) ) < \u03b4 implies\nsup t\u2208[0,T\u0303 ]\n|L(Y \u03f5t )\u2212 L(Y 0t )| \u2264 min( 1\n4 q, Q\u2212 L(y0) 2 ).\nSince L(Y 0t ) \u2264 L(Y0) = Q\u2032 and L(Y 0T\u0303 ) = q 2 , this guarantees that\nL(Y \u03f5t ) \u2264 L(Y0) + \u2225L\u2225C1\u03b4 < Q and\nL(Y \u03f5 T\u0303 ) \u2264 q\n2 + \u2225L\u2225C1\u03b4 < q.\nIt would in turn follow that \u03c4 \u03f5q,Q \u2264 T\u0303 <\u221e and Y \u03f5\u03c4\u03f5q,Q \u2208 Vq . Hence it suffices to prove\nlim \u03f5\u21920 P(Y \u03f50 ,Z\u03f50)=(y0,z0) (\nsup t\u2208[0,T\u0303 ] \u2223\u2223(Y \u03f5t , Z\u03f5t ), (Y 0t , z0)\u2223\u2223 < \u03b4) = 1. (30) For simplicity, write J\u03f5t :=\n\u2223\u2223(Y \u03f5t , Z\u03f5t ), (Y 0t , z0)\u2223\u2223. Let M <\u221e be a common upper bound on the C0 and the Lipschitz norms of the maps b, \u03c3 and h over the domains \u03f5 \u2208 [0, 1], y \u2208 Vq0 , z \u2208 D. Then J\u03f50 = 0 and\nJ\u03f5t \u2264 \u222b t 0 2MJ\u03f5sds+ \u221a \u03f5 \u2223\u2223\u2223 \u222b t 0 \u03c3(Y \u03f5s , Z \u03f5 s)dB d s \u2223\u2223\u2223. By Gronwall\u2019s inequality,\nsup t\u2208[0,T\u0303 ]\nJ\u03f5t \u2264 \u221a \u03f5e2MT\u0303 sup\nt\u2208[0,T\u0303 ]\n\u2223\u2223\u2223 \u222b t 0 \u03c3(Y \u03f5s , Z \u03f5 s)dB d s \u2223\u2223\u2223. Therefore, in order to make supt\u2208[0,T\u0303 ] J \u03f5 t \u2265 \u03b4, we must have\nsup t\u2208[0,T\u0303 ] \u2223\u2223\u2223 \u222b t 0 \u03c3(Y \u03f5s , Z \u03f5 s)dB d s \u2223\u2223\u2223 \u2265 \u03b4\u03f5\u2212 12 e\u22122MT\u0303 , and thus\nsup t\u2208[0,T\u0303 ] \u2223\u2223\u2223 \u222b t 0 \u03c3k(Y \u03f5 s , Z \u03f5 s)dB 1 s \u2223\u2223\u2223 \u2265 \u03b4 d \u03f5\u2212 1 2 e\u22122MT\u0303\nmust hold for at least one of the row vectors \u03c3k of \u03c3. The process G\u03f5k,t = \u222b t 0 \u03c3k(Y \u03f5 s , Z \u03f5 s)dB 1 s is a continuous martingale with increasing process \u27e8G\u03f5k\u27e9t \u2264 M2t almost surely.\nRecall \u27e8G\u03f5k\u27e9t is defined as \u222b t 0 \u03c32k(Y \u03f5 s , Z \u03f5 s)ds. By the Burkholder-Davis-Gundy inequality (see e.g. (Dembo & Zeitouni, 2010, Chapter E)),\nE(Y \u03f50 ,Z\u03f50)=(y0,z0) (\nsup t\u2208[0,T\u0303 ]\n|G\u03f5k,t|2 ) \u2264 CE(Y \u03f50 ,Z\u03f50)=(y0,z0)\u27e8G \u03f5 k\u27e9T\u0303 \u2264 CM 2T\u0303\nfor an absolute constant C. And thus by the Chebyshev\u2019s theorem P(Y \u03f50 ,Z\u03f50)=(y0,z0) (\nsup t\u2208[0,T\u0303 ] \u2223\u2223\u2223 \u222b t 0 \u03c3k(Y \u03f5 s , Z \u03f5 s)dB 1 s \u2223\u2223\u2223 \u2265 \u03b4 d \u03f5\u2212 1 2 e\u22122MT\u0303 )\n\u2264CM2T\u0303 ( \u03b4 d \u03f5\u2212 1 2 e\u22122MT\u0303 )\u22122.\nFrom the earlier discussion, after summing over all k\u2019s, we obtain that\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)( sup t\u2208[0,T\u0303 ]\nJ\u03f5t \u2265 \u03b4) \u2264 CM2T\u0303 ( \u03b4\nd \u03f5\u2212 1 2 e\u22122MT\u0303 )\u22122. (31)\nWe deduce (30) from (31) by letting \u03f5\u2192 0, which concludes the proof.\nThe following lemma is the analogue of (Dembo & Zeitouni, 2010, Lemma 5.7.23). Lemma B.23. For all \u03b4, a > 0 and bounded set K \u2282 U , there exists a constant T0 = T0(\u03b4, a,K) > 0 such that\nlim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208K\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)( sup t\u2208[0,T0] |(Y \u03f5t \u2212 y0, Z\u03f5t \u2212 z0)| \u2265 \u03b4) < \u2212a.\nProof. Without loss of generality, assume \u03f5, \u03b4 \u2208 [0, 1] with \u03b4 fixed and \u03f5 varying. Let the stopping time \u03b6\u03f5 be the first time such that |(Y \u03f5t \u2212 y0, Z\u03f5t \u2212 z0)| \u2265 \u03b4. Then for every t \u2208 [0, \u03b6\u03f5],\n|b(Y \u03f5t , Z\u03f5t )| \u2264 max B\u03b4(K)\u00d7D |b|+ \u2225b\u2225Lip\u03b4,\n|\u03c3(Y \u03f5t , Z\u03f5t )| \u2264 max B\u03b4(K)\u00d7D |\u03c3|+ \u2225\u03c3\u2225Lip\u03b4,\n|h(\u03f5, Y \u03f5t , Z\u03f5t )| \u2264 max [0,1]\u00d7B\u03b4(K)\u00d7D |h|+ \u2225h\u2225Lip\u03b4,\nand thus they all are uniformly bounded by a constant M . For all 0 \u2264 t \u2264 min(\u03b6\u03f5, \u03b44M ), |Z \u03f5 t \u2212z0| \u2264 \u03b4 4 , and |Y \u03f5 t \u2212 y0| \u2264 \u03b44 + | \u222b t 0 \u221a \u03f5\u03c3(Y \u03f5s , Z \u03f5 s)dB d s |. Hence for any T0 \u2264 \u03b44M we have\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03b6 \u03f5 \u2265 T0) \u2264P(Y \u03f50 ,Z\u03f50)=(y0,z0) ( \u03b6\u03f5 \u2265 T0 and\n\u221a \u03f5 sup t\u2208[0,T ] \u2223\u2223\u2223 \u222b t 0 \u03c3(Y \u03f5s , Z \u03f5 s)dB d s \u2223\u2223\u2223 \u2265 \u03b4 2 ) .\nAs in the proof of (Dembo & Zeitouni, 2010, Lemma 5.7.23), it suffices to consider each row vector \u03c3k of \u03c3. The stochastic process \u222b t 0 \u03c3k(Y \u03f5 s , Z \u03f5 s)dB 1 s is equivalent to B 1 \u03c4\u03f5k,t by the time change theorem\n(see (Dembo & Zeitouni, 2010, Chapter E.2)) where \u03c4 \u03f5k,t = \u222b t 0 \u03c32k(Y \u03f5 s , Z \u03f5 s)ds. The function \u03c4 \u03f5 k,t is increasing in t and almost surely \u03c4 \u03f5k,t \u2264M2t if T0 \u2264 \u03b6\u03f5 as the C0 and the Lipschitz norms of b, \u03c3, h are bounded by M before the stopping time \u03b6\u03f5. By (Dembo & Zeitouni, 2010, Lemma 5.2.1),\nP(Y \u03f50 ,Z\u03f50)=(y0,z0) ( \u03b6\u03f5 \u2265 T0 and\n\u221a \u03f5 sup t\u2208[0,T0] \u2223\u2223\u2223 \u222b t 0 \u03c3k(Y \u03f5 s , Z \u03f5 s)dB 1 s \u2223\u2223\u2223 \u2265 \u03b4 2d ) \u2264P(Y \u03f50 ,Z\u03f50)=(y0,z0) ( \u03b6\u03f5 \u2265 T0 and\n\u221a \u03f5 sup t\u2208[0,T0] B1\u03c4\u03f5k,t \u2265 \u03b4 2d ) \u2264P(Y \u03f50 ,Z\u03f50)=(y0,z0) ( \u03b6\u03f5 \u2265 T0 and\n\u221a \u03f5 sup \u03c4\u2208[0,M2T0] B1\u03c4 \u2265 \u03b4 2d ) \u2264P(Y \u03f50 ,Z\u03f50)=(y0,z0) (\u221a \u03f5 sup \u03c4\u2208[0,M2T0] B1\u03c4 \u2265 \u03b4 2d\n) \u22644e\u2212( \u03b42d ) 2/(2\u03f5M2T0).\nSumming over different k\u2019s, we conclude from the two inequalities above that for all T0 \u2208 [0, \u03b44M ]\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03b6 \u03f5 \u2265 T0) \u2264 d \u00b7 4e\u2212( \u03b4 2d ) 2/(2\u03f5M2T0) = 4de\u2212( 1 8 \u03b4 2d\u22122M\u22122T\u221210 )/\u03f5.\nIt now suffices to take T0 = min( \u03b44M , \u03b42 8d2M2a ).\nDefinition B.24. For y0, y1 \u2208 U and T > 0, define an energy cost \u03a8T (y0, y1) by \u03a8T (y0, y1) = inf {\u222b T\n0\n1 2 |u\u0307s|2ds : (f, g, u) \u2208 ST , f0 = y0, fT = y1 } = inf { \u03a6(f) : f \u2208 C0([0, T ], U) } .\nThe following lemma claims there is a minimum energy cost required to increase the loss function L between different levels. Lemma B.25. For all 0 < q < Q \u2264 q0,\ninf T\u22650 inf (y0,y1)\u2208Vq\u00d7\u2202VQ \u03a8T (y0, y1) > 0. (32)\nMoreover, there is constant Tq,Q > 0 such that\ninf T\u2265Tq,Q inf y0,y1\u2208VQ\\Vq \u03a8T (y0, y1) > 0. (33)\nProof. Suppose for now y0, y1 \u2208 VQ\\\u2202Vq , and (f, g, u) \u2208 ST with f0 = y0, fT = y1. Then by (18),\ndL(ft) =\u2207\u22a4L(ft)dft = ( \u2207\u22a4L(fs) ) b(fs, gs)ds+ ( \u2207\u22a4L(fs) ) \u03c3(fs, gs)dus.\nBecause q > 0, by Assumption B.19, as ( \u2207\u22a4L(y) ) b(y, z) = \u2207b(y,z)L(y) < 0 for all y \u2208 VQ\\Vq and z \u2208 D. In particular, this also shows \u2207L(y) \u0338= 0 for all y \u2208 VQ\\Vq . Since both VQ\\Vq and D are compact, There exists positive constant \u03ba = \u03ba(q,Q) > 0 and \u03b7 = \u03b7(q,Q) > 0, such that( \u2207\u22a4L(y) ) b(y, z) \u2264 \u2212\u03ba|\u2207L(y)|2 and |\u2207L(y)| \u2265 \u03b7 for (y, z) \u2208 (VQ\\Vq)\u00d7D.\nIntegrating from 0 to T , we get\nL(y1)\u2212 L(y0) =L(fT )\u2212 L(f0) = \u222b T 0 dL(ft)\n= \u222b T 0 (( \u2207\u22a4L(fs) ) b(fs, gs)ds+ ( \u2207\u22a4L(fs) ) \u03c3(fs, gs)dus ) \u2264\u2212 \u03ba\n\u222b T 0 |\u2207L(fs)|2ds\n+ (\u222b T\n0\n|\u2207L(fs)|2ds ) 1 2 (\u222b T\n0\n|\u03c3(fs, gs)u\u0307s|2ds ) 1 2\n\u2264\u2212 \u03ba \u222b T 0 |\u2207L(fs)|2ds\n+ (\u222b T\n0\n|\u2207L(fs)|2ds ) 1 2 \u2225\u03c3\u2225C0(VQ\u00d7D) (\u222b T\n0\n|u\u0307s|2ds ) 1 2 .\n(34)\nIn order to show (32), note that if y0 \u2208 Vq and y1 \u2208 \u2202VQ, then L(y1) \u2212 L(y0) = Q \u2212 q > 0. By (34) and Cauchy-Schwarz inequality,\nQ\u2212 q \u2264\u2212 \u03ba \u222b T 0 |\u2207L(fs)|2ds\n+ (\u222b T\n0\n|\u2207L(fs)|2ds ) 1 2 \u2225\u03c3\u2225C0(VQ\u00d7D) (\u222b T\n0\n|u\u0307s|2ds ) 1 2\n\u2264 1 4\u03ba \u2225\u03c3\u22252 C0(VQ\u00d7D) \u222b T 0 |u\u0307s|2ds.\n(35)\nSince \u03c3 is continuous, \u2225\u03c3\u2225C0(VQ\u00d7D) <\u221e. It follow that\u222b T 0 1 2 |u\u0307s|2ds \u2265 2\u03ba(Q\u2212 q)\u2225\u03c3\u2225\u22122C0(VQ\u00d7D) > 0.\nThis provides a positive lower bound for \u03a8T (y0, y1) that is uniform for T > 0, y0 \u2208 Vq and y1 \u2208 \u2202VQ. This proves (32).\nWe now prove (33). Note that for all y0, y1 \u2208 VQ\\Vq , L(y1)\u2212 L(y0) \u2265 q \u2212Q. By (34),(\u222b T 0 |us|2ds ) 1 2\n\u2265\u2225\u03c3\u2225\u22121 C0(VQ\u00d7D)\n( \u03ba (\u222b T\n0\n|\u2207L(fs)|2ds ) 1 2 \u2212 (Q\u2212 q) (\u222b T\n0\n|\u2207L(fs)|2ds )\u2212 12)\n\u2265\u2225\u03c3\u2225\u22121 C0(VQ\u00d7D) (\u03ba\u03b7T 1 2 \u2212 (Q\u2212 q)\u03b7\u22121T\u2212 12 ).\nThe last expression is uniformly positive for T \u2265 Tq,Q := 2(Q \u2212 q)\u03ba\u22121\u03b7\u22122. Thus \u03a8T (y0, y1) is uniformly positive over the region given by T \u2265 Tq,Q, y0, y1 \u2208 VQ\\\u2202Vq . This proves (33).\nLemma B.26. For all 0 < q < Q \u2264 q0 and initial value (y0, z0) \u2208 VQ \u00d7D, the stopping time \u03c4 \u03f5q,Q satisfies\nlim T\u2192\u221e lim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208VQ\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 q,Q > T ) = \u2212\u221e.\nProof. Note that \u03c4 \u03f5q,Q > T implies Y \u03f5 t \u2208 VQ\\Vq \u2286 VQ\\Vq for all t \u2208 [0, T ]; or in other words, {Y \u03f5t }t\u2208[0,T ] \u2208 C0([0, T ], VQ\\Vq). Therefore, by Corollary B.18,\nlim T\u2192\u221e lim sup \u03f5\u21920 \u03f5log sup (y0,z0)\u2208VQ\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 q,Q > T )\n\u2264\u2212 inf f\u2208C0([0,T ],VQ\\Vq) \u03a6(f);\nand in consequence it suffices to show\nlim T\u2192\u221e inf f\u2208C0([0,T ],VQ\\Vq)\n\u03a6(f) =\u221e. (36)\nAssume, for the sake of contradiction, that (36) is false, then for some M < \u221e and any all k \u2208 N, there exists fk \u2208 C0([0, kTq,Q], VQ\\Vq) with \u03a6(fk) \u2264 M , where Tq,Q is given by Lemma B.25. After breaking fk into k segments on subintervals of length Tq,Q, it follows that there exists f\u2217k \u2208 C0([0, Tq,Q], VQ\\Vq) such that \u03a6(f\u2217k ) \u2264 Mk .\nBy taking limit in C0 norm (which is permitted by Lemma B.10) and using the lower semicontinuity of the good rate function \u03a6, there exists f\u2217 \u2208 C0([0, Tq,Q], VQ\\Vq) with \u03a6(f\u2217) = 0. This implies \u03a8T (y0, y1) = 0 and thus it contradicts to the inequality (33) of Lemma B.25. This completes the proof.\nDenote by Iq,Q > 0 the left hand side in (32). Lemma B.27. The solutions to (16) and (17) satisfy\nlim q\u21920 lim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208V2q\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 \u03c4\u03f5q,Q \u2208 \u2202VQ) \u2264 \u2212 lim q\u21920 Iq,Q. (37)\nProof. Fix an arbitrarily small \u03b4, and let I\u0302Q,\u03b4 := min(limq\u21920 Iq,Q\u2212 \u03b4, 1\u03b4 ). Note that the right hand side in (37) always exists because Iq,Q is a decreasing function in q by construction. In particular, I2q,Q \u2265 I\u0302Q,\u03b4 when q is sufficiently small depending on Q and \u03b4. By Lemma B.26, there exists a large T\u2217 = T\u2217(q,Q, \u03b4) <\u221e such that\nlim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208V2q\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 q,Q > T\u2217) \u2264 \u2212I\u0302Q,\u03b4. (38)\nIn addition, inf\nf\u2208C0([0,T\u2217],U): f0\u2208V2q, supt\u2208[0,T\u2217] L(ft)\u2265Q\n\u03a6(f) \u2265 I2q,Q \u2265 I\u0302Q,\u03b4,\nand thus by Corollary B.18,\nlim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208V2q\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)( sup t\u2208[0,T\u2217] L(Y \u03f5t ) \u2265 Q) \u2264 \u2212I\u0302Q,\u03b4. (39)\nNote that the event {Y \u03f5\u03c4\u03f5q,Q \u2208 \u2202VQ} is contained in the union of the events {\u03c4 \u03f5 q,Q > T\u2217} and {supt\u2208[0,T\u2217] L(Y \u03f5 t ) \u2265 Q}. Therefore, we obtain by combining (38) and (39) that the inequality\nlim sup \u03f5\u21920 \u03f5 log sup (y0,z0)\u2208V2q\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(Y \u03f5 \u03c4\u03f5q,Q \u2208 \u2202VQ) \u2264 \u2212I\u0302Q,\u03b4 (40)\nholds for all sufficiently small q. The lemma then follows by letting \u03b4 \u2192 0.\nWe are now ready to establish Theorem B.21.\nProof of Theorem B.21. Choose q < Q2 to be sufficiently small. Define a sequence of stopping times \u03b80 < \u03c40 < \u03b81 < \u03c41 < \u00b7 \u00b7 \u00b7 recursively by letting\n\u03b80 := 0,\n\u03c4m := inf{t > \u03b8m : Y \u03f5t \u2208 Vq \u222a \u2202VQ}, \u03b8m := inf{t > \u03c4m\u22121 : Y \u03f5t \u2208 \u2202V2q}.\nBy Lemma B.22, all these stopping times are finite almost surely. Write Y\u0303m = Y \u03f5\u03c4m , which is a Markov chain.\nRecall that by Lemma B.25 Iq,Q > 0 for all 0 < q < Q and is an decreasing function in q. Let IQ := 12 limq\u21920 Iq,Q > 0 and fix 0 < \u03b1 < 1 4IQ. By Lemma B.27, if we fix a sufficiently small q, then lim sup\n\u03f5\u21920 \u03f5 log sup (y0,z0)\u2208V2q\u00d7D P(Y \u03f50 ,Z\u03f50)=(y,z)(Y \u03f5 \u03c4\u03f5q,Q \u2208 \u2202VQ) \u2264 \u2212IQ \u2212 3\u03b1.\nBy plugging in (Y \u03f5\u03b8m , Z \u03f5 \u03b8m ) as the the value for (y, z), we deduce that there exists \u03f50 > 0 such that for all 0 < \u03f5 < \u03f50 and m \u2265 1,\nsup (y0,z0)\u2208VQ\u00d7D\n\u03f5 logP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q = \u03c4m) \u2264 \u2212IQ \u2212 2\u03b1. (41)\nOn the other hand, assuming \u03f50 is sufficiently small, applying Lemma B.23 with a = IQ + 2\u03b1, \u03b4 = 12q and K = VQ yields that, for some fixed T0 > 0 depending on q and Q and all \u03f5 \u2208 (0, \u03f50].\nsup (y0,z0)\u2208VQ\u00d7D \u03f5 logP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03b8m \u2212 \u03c4m\u22121 \u2264 T0)\n\u2264 sup (y,z)\u2208VQ\u00d7D \u03f5 logP(Y \u03f50 ,Z\u03f50)=(y,z)( sup t\u2208[0,T0]\n|(Y \u03f5t \u2212 y, Z\u03f5t \u2212 z)| \u2265 1\n2 q)\n<\u2212 IQ \u2212 2\u03b1.\n(42)\nFor a given M , the event {\u03c4 \u03f5Q \u2264 MT0} is contained in the union of the events \u22c3M\nm=0{\u03c4 \u03f5Q = \u03c4m} and \u22c3M m=1{\u03b8m \u2212 \u03c4m\u22121 \u2264 T0}. Combining this fact and the inequalities (41), (42) yield\nsup (y0,z0)\u2208Vq\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q \u2264MT0)\n\u2264 sup (y0,z0)\u2208Vq\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q = \u03c40) + M\u2211 m=1 sup (y0,z0)\u2208Vq\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q = \u03c4m)\n+ M\u2211 m=1 sup (y0,z0)\u2208Vq\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03b8m \u2212 \u03c4m\u22121 \u2264 T0)\n\u2264 sup (y0,z0)\u2208Vq\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q = \u03c40) + 2Me\n\u2212 IQ+2\u03b1\n\u03f5 .\n(43)\nSet M = \u2308T\u221210 e IQ+\u03b1 \u03f5 \u2309, which makes MT0 \u2265 e IQ \u03f5 for all sufficiently small \u03f5. Moreover as \u03f5\u2192 0, sup(y0,z0)\u2208Vq\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q = \u03c40) \u2192 0 by Lemma B.22; and 2Me\u2212 IQ+2\u03b1 \u03f5 \u2192 0 by the choice of M . Thus\nsup (y0,z0)\u2208Vq\u00d7D\nP(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q \u2264 e\nIQ \u03f5 ) \u2264 sup\n(y0,z0)\u2208Vq\u00d7D P(Y \u03f50 ,Z\u03f50)=(y0,z0)(\u03c4 \u03f5 Q \u2264MT0)\nwhich tends to 0 as \u03f5\u2192 0. This completes the proof of Theorem B.21.\nB.4 PROOF OF PROPOSITION 4.2\nRecall that the movement of (2) stays in \u0393#, i.e. |Xt| \u2208 [R\u2212, R+] and can be characterized as the new model (6), (5).\nAfter applying a time change T = (\u03b7\u03bb) 1 2 t\nand writing \u03b3\u0304t = \u03b7\u03bb\u03b3t = \u03bb\u03b7 \u22121|Xt|4,\nwe get the following system of equations on (XT , \u03b3\u0304T ) \u2208 Sd\u22121 \u00d7 R+:\ndXT = \u2212\u03b3\u0304 \u2212 12 T \u2207L(XT )dT + (\u03b7\u03bb) 1 4 \u03b3\u0304 \u2212 12 T \u03c3\u0304(XT )dB d T ; (44)\nd\u03b3\u0304T = (\u03b7\u03bb) 1 2 \u00b7 (\u22124\u03b3\u0304T + 2Tr\u03a3(XT ))dT. (45)\nTo deduce (44), we used the standard fact that for any a > 0, a 1 2Bda\u22121T and B d T are equivalent as Wiener processes.\nNote \u03b3\u0304T stays in the fixed interval [\u03b3\u0304\u2212, \u03b3\u0304+] = [\u03bb\u03b7\u22121R4\u2212, \u03bb\u03b7 \u22121R4+] as long as Xt \u2208 \u0393#.\nAs there are only finitely many basins, we may fix an index i without impact Proposition B.28. In addition, without loss of generality let us assume L = 0 on \u0393i. For our purpose, it might be better to measure the distance to \u0393i on U i#,p0 where p0 > p1 is sufficiently small and in particular U j#,p0 are disjoint for distinct j\u2019s. For q \u2265 0, we will write V i r,q = {x \u2208 U ir,p0 , L(x) < q} and V i#,q = {x \u2208 U i#,p0 , L(x) < q}.\nBy Assumption 1.2.(ii), one may manipulate p0, q0, p1, q1 (p0 > p1, q0 > q1) such that\nU i1,p1 \u2286 V i 1,q1 \u2286 V i 1,q0 \u2286 U i 1,p0 ,\nand thus reformulate Proposition 4.2 as\nProposition B.28. There exists c > 0 such that if q0 > q1 > 0 are fixed, but q1 is sufficiently small compared to q0, then in the regime \u03b7 \u2264 O(\u03bb) \u2264 O(1),\nlim \u03b7\u03bb\u21920 sup (y0,z0)\u2208V i1,q1\u00d7[\u03b3\u0304\u2212,\u03b3\u0304+]\nP(X0,\u03b3\u03040)=(y0,z0) ( XT remains in V i1,q0 for all t \u2208 [0, e c(\u03b7\u03bb)\u2212 1 2 ] ) = 1.\nThe convergence is uniform with respect to (y0, z0).\nProof. In order to apply Theorem B.21 with domain V i1,q0 and control domain D = [\u03b3\u0304\u2212, \u03b3\u0304+], we first make the following remark.\nTheorem B.21 is in the setting where the domain is an open neighborhood in Rn, while our current V iq0 is a neighborhood in the sphere. This is not a problem because unless L is a constant function, \u0393i1 is a proper subset of the sphere Sd\u22121. And V iq is also a proper subset in Sd\u22121 for small q. One can then change coordinates and identify V iq with a subset of the Euclidean space.\nThis converts the problem to the equations (16), (17) with the following dictionary: \u03f5 = (\u03b7\u03bb) 1 2 ; XT and \u03b3\u0304T play the roles of Y \u03f5t , and Z \u03f5 t respectively; b(y, z) = \u2212z\u2212 1 2\u2207(y); \u03c3(y, z) = z\u2212 12 \u03c3\u0304(y); and h(\u03f5, y, z) = \u03f5(\u22124z + 2Tr\u03a3(y)).\nIt remains to check Assumptions B.8 and B.19. We start with the latter. The strictly positivity of L directly follows from the construction of L and the neighborhood V iq0 . The property (1) in Assumption B.19 holds because b is negatively proportional to the gradient \u2207 of L with respect to the spherical coordinates.\nUnfortunately Assumption B.8 doesn\u2019t automatically hold as trajectories may escape from V i1,q0 . In order to adapt to this case, we smoothly modify the value of \u03c3 on V i1,q0 so that it remains unchanged on V i1,q1 and vanishes near \u2202V i 1,q0 . Then near the boundary, the SDEs (16), (17) become deterministic. Because of Assumption B.19.(1), the trajectories do not escape from \u2202V i1,q0 . Moreover, we know that \u03b3\u0304t remains in D = [\u03b3\u0304\u2212, \u03b3+]. This verifies Assumption B.8 for the modified model.\nWe conclude by applying Theorem B.21 that, for some fixed I > 0, for all initial positions in V i1,q1 \u00d7 [\u03b3\u0304\u2212, \u03b3\u0304+], the probability that a trajectory (XT , \u03b3\u0304T ) (with respect to the modified model) leaves V i1,q0 \u00d7 [\u03b3\u0304\u2212, \u03b3\u0304+] before T = e I \u03b7\u03bb = eO((\u03b7\u03bb) \u2212 1 2 ) converges to 0 as \u03b7\u03bb\u2192 0. The convergence is in addition uniform with respect to the initial position.\nSince such modifications only take place outside V i1,q1 \u00d7 [\u03b3\u0304\u2212, \u03b3\u0304+], the same statement also holds for the original model. As V i1,q1 \u2282 V i 0,q0 , we obtain the statement of Proposition B.28 after reparamatrization of variables."
        },
        {
            "heading": "C UNIQUE KATZENBERGER LIMIT INSIDE EACH BASIN",
            "text": "The results from \u00a7B.2, stated in the form of Proposition 4.2, guarantee that, after discarding an exponentially small subset of random incidences, the trajectories of (2) stays inside the basin that contains the initial position for exponentially long time O(eC(\u03b7\u03bb) \u22121 ). We now justify Proposition 4.3\nWe restart (2) from an initial point, still written as x0 by abuse of notation, in some U i#,p1 . We now apply a uniform approximation theorem, which is a stronger version of (Li et al., 2022a, Theorem 4.6). We can prove this uniform approximation result because we can strengthen(Katzenberger, 1991, Theorem 6.3) to be a compactness theorem, uniformly with respect to the initial points of the SDE. To describe this result, let (\u2126n,Fn, {Fnt }t\u22650,P) be a filtered probability space, Zn an Revalued cadlag {Fnt }-semimartingale with Zn(0) = 0 and An a real-valued cadlag {Fnt }-adapted nondecreasing process with An(0) = 0. Let \u03c3n : U \u2192 M(d, e) be continuous with \u03c3n \u2192 \u03c3 uniformly on compact subsets of U . Let Xn be an Rd-valued cadlag {Fnt }-semimartingale satisfying, for all compact K \u2208 U ,\nXn(t) = Xn(0) + \u222b t 0 \u03c3(Xn)dZn + \u222b t 0 \u2212\u2207L(Xn)dAn (46)\nfor all t \u2264 \u03bbn(K) where \u03bbn(K) = inf{t \u2265 0|Xn(t\u2212) /\u2208}K\u030a or Xn(t) /\u2208 K\u030a is the stopping time of Xn leaving K. Theorem C.1. Suppose Xn(0) \u2208 U , Assumption 3.1, 3.2 and Condition B.2, B.3, B.4, B.5 from (Li et al., 2022a) hold. For any compact K \u2282 U define \u00b5n(K) = inf{t \u2265 0|Yn(t\u2212) /\u2208 K\u030a or Yn(t) /\u2208 K\u030a}, then the sequence {(Y \u00b5n(K)n , Z\u00b5n(K)n , \u00b5n(K))} is relatively compact inDRd\u00d7e [0,\u221e)\u00d7 [0,\u221e). If (Y,Z, \u00b5) is a limit point of this sequence under the Skorohod metric, then (Y,Z) is a continuous semimartingale, Y (t) \u2208 \u0393 for every t \u2265 0 a.s., \u00b5 \u2265 inf{t \u2265 0|Y (t) /\u2208 K\u030a} a.s. and Y (t) admits\nY (t) =Y (0) + \u222b t\u2227\u00b5 0 \u2202\u03a6(Y (s))\u03c3(Y (s))dZ(s)\n+ 1\n2 d\u2211 i,j=1 e\u2211 k,l=1 \u222b t\u2227\u00b5 0 \u2202ij\u03a6(Y (s))\u03c3(Y (s))ik\u03c3(Y (s))jld [ Zk, Zl ] (s).\n(47)\nWe will present Assumption B.3, B.4 and Condition B.5, B.6, B.7 and B.8 from (Li et al., 2022a) in Appendix B.\nThe main difference of Theorem C.1 to (Katzenberger, 1991, Theorem 6.3) is that we allow the initial point Xn(0) to vary within U .\nTheorem C.2. Let the manifold \u0393 and its open neighborhood U satisfy Assumption 3.1 and 3.2. Let K \u2282 U be any compact set and xn,0 \u2208 K be a sequence of initial points. Consider the SGD formulated in (46) where X\u03b7n(0) \u2261 xn,0. Define\nY\u03b7n(t) = X\u03b7n(t)\u2212 \u03a6(X\u03b7n(0), A\u03b7n(t)) + \u03a6(X\u03b7n(0))\nand \u00b5\u03b7n(K) = min{t \u2208 N|Y\u03b7n(t) /\u2208 K\u030a}. Then the sequence {Y \u00b5n(K) \u03b7n , Z \u00b5n(K) \u03b7n , \u00b5\u03b7n(K))}n\u22651 is relatively compact in DRd\u00d7e [0,\u221e) \u00d7 [0,\u221e). If (Y, Z, \u00b5) is a limit point of this sequence, it holds that Y (t) \u2208 \u0393 a.s. for all t \u2265 0, \u00b5 \u2265 inf{t \u2265 0|Y (t) /\u2208 K\u030a} and Y (t) admits\nY (t) = \u222b t\u2227\u00b5 0 \u2202\u03a6(Y (s))\u03c3(Y (s))dZ(s)+ 1 2 \u222b t\u2227\u00b5 0 d\u2211 i,j=1 \u2202ij\u03a6(Y (s))(\u03c3(Y (s))\u03c3(Y (s)) \u22a4)ijds (48)\nwhere {W (s)}s\u22650 is the standard Brownian motion.\nProof. The proof of Theorem C.2 follows how (Li et al., 2022a, Theorem B.8) was proved by using (Li et al., 2022a, Lemma B.6) and the standard Katzenberger\u2019s theorem (Katzenberger, 1991, Theorem 6.3). One difference is that here not all trajectories stays inside one basin. However, we claim that the probability that trajectories escape the basin goes to zero when \u03b7\u03bb tends to zero. Once this claim is proved, Theorem C.2 is an immediate consequence of (Li et al., 2022a, Lemma B.6) and Theorem C.1.\nTo prove the claim, we adopt the same idea as in the proof of Theorem 4.2 in Appendix B.4. Although trajectories may escape from level set V i1,q0 , we can smoothly modify the value of \u03c3 on the closure V i1,q1 so that it remains unchanged on V i 1,q0\nand vanishes near the boundary \u2202V i1,q0 . The SDEs become deterministic, and thus the trajectories of the modified model do not escape from the boundary \u2202V i1,q0 . Now, as \u03b7\u03bb tends to zero, by Theorem B.21 the probability of a trajectory of the modified model leaving \u2202V i1,q1\u00d7 [\u03b3\u0304\u2212, \u03b3\u0304+] before a fixed T tends to 0. Since such modifications only take place outside \u2202V i1,q1 \u00d7 [\u03b3\u0304\u2212, \u03b3\u0304+], the same statement holds for the original model. This finishes the proof of the claim.\nProof of Proposition 4.3. The above uniform version of the Katzenberger\u2019s theorem guarantees that, starting from different initial points in the same compact neighborhood of the basin, the distribution of trajectories associated with (2) is still close to that of the Katzenberger\u2019s SDE (47). By Proposition 3.1, the latter is mixing towards a unique equilibrium \u03bdi. Note that even though in Chapter 3, we have only proved it for one basin case, Theorem 4.1 shows that with a large probability, the trajectories do not escape from the basin. For those trajectories, they satisfy a modified SDE equation like before, so that all trajectories do not escape from this basin. At this moment, we can directly apply Proposition 3.1. It follows that within any polynomial time window under consideration, the distribution of trajectories associated with (2) are also mixing towards \u03bdi. This proves Proposition 4.3.\nAssumption C.3. (Li et al., 2022a, Assumption 3.1) Assume that the loss L : Rd \u2192 R is a C3 function, and that \u0393 is a (d\u2212M)-dimensional C2-submanifold of Rd for some integer 0 \u2264M \u2264 d, where for all x \u2208 \u0393, x is a local minimizer of L and rank(\u22072L(x)) = M . Assumption C.4. (Li et al., 2022a, Assumption 3.2) Assume that U is an open neighborhood of \u0393 satisfying that gradient flow starting in U converges to some point in \u03b3, i.e., \u2200x \u2208 U , \u03a6(x) \u2208 \u0393. (Then \u03a6 is C2 on U by (Falconer, 1983).) Condition C.5. (Li et al., 2022a, Lemma B.2) The integrator sequence {An}n\u22651 is asymptotically continuous: supt>0 |An(t)\u2212An(t\u2212)| \u21d2 0 where An(t\u2212) = lims\u2192t\u2212 An(s) is the left limit of An at t.\nCondition C.6. (Li et al., 2022a, Lemma B.3) The integrator sequence {An}n\u22651 increases infinitely fast: \u2200\u03f5 > 0, inft\u22650(An(t+ \u03f5))\u2212An(t))\u21d2\u221e. Condition C.7. ((Katzenberger, 1991, Equation 5.1), (Li et al., 2022a, Lemma B.4)) For every T > 0, as n\u2192\u221e, it holds that\nsup 0<t\u2264T\u2227\u03bbn(K)\n\u2225\u2206Zn(t)\u22252 \u21d2 0.\nCondition C.8. ((Katzenberger, 1991, Condition 4.2), (Li et al., 2022a, Lemma B.5)) For each n \u2265 1, let Yn be a {Fnt }-semimartingale with sample paths in DRd [0,\u221e). Assume that for some \u03b4 > 0 allowing \u03b4 =\u221e and every n \u2265 1 there exist stopping times {\u03c4mn |m \u2265 1} and a decomposition of Yn \u2212 J\u03b4(Yn) into a local martingale Mn plus a finite variation process Fn such that P[\u03c4mn \u2264 m] \u2264 1/m, {[Mn](t \u2227 \u03c4mn ) + Tt\u2227\u03c4mn (Fn)}n\u22651 is uniformly integrable for every t \u2265 0 and m \u2265 1, and\nlim \u03b3\u21920 lim sup n\u2192\u221e\nP [\nsup 0\u2264t\u2264T\n(Tt+\u03b3(Fn)\u2212 Tt(Fn)) > \u03f5 ] = 0,\nfor every \u03f5 > 0 and T > 0, where Tt(\u00b7) denotes total variation on the interval [0, t].\nIt was shown in (Li et al., 2022a, Lemma B.6) that for SGD formulated in (46), the sequences {An}n\u22651 and {Zn}n\u22651 satisfy Condition C.5, C.6, C.7, and C.8. And the landscape of L satisfies Assumption C.3 and C.4. Thus the Katzenberger theorem holds in our case."
        }
    ],
    "year": 2024
}