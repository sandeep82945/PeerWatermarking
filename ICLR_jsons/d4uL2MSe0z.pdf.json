{
    "abstractText": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tamir David"
        },
        {
            "affiliations": [],
            "name": "Lior Wolf"
        }
    ],
    "id": "SP:fabb4418565d0abdc4713a445ccba3c762b90042",
    "references": [
        {
            "authors": [
                "Bowen Baker",
                "Otkrist Gupta",
                "Nikhil Naik",
                "Ramesh Raskar"
            ],
            "title": "Designing neural network architectures using reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "Srinadh Bhojanapalli",
                "Ayan Chakrabarti",
                "Andreas Veit",
                "Michal Lukasik",
                "Himanshu Jain",
                "Frederick Liu",
                "Yin-Wen Chang",
                "Sanjiv Kumar"
            ],
            "title": "Leveraging redundancy in attention with reuse transformers",
            "venue": "arXiv preprint arXiv:2110.06821,",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Han Cai",
                "Ligeng Zhu",
                "Song Han"
            ],
            "title": "Efficient neural architecture search via parameter sharing",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume M Jb Chaslot",
                "Mark HM Winands",
                "H Jaap van den Herik",
                "Jos WHM Uiterwijk",
                "Bruno Bouzy"
            ],
            "title": "Progressive strategies for monte-carlo tree search",
            "venue": "New Mathematics and Natural Computation,",
            "year": 2008
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis for pre-trained BERT networks",
            "venue": "arXiv preprint arXiv:2007.12223,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaohan Chen",
                "Yu Cheng",
                "Shuohang Wang",
                "Zhe Gan",
                "Zhangyang Wang",
                "Jingjing Liu"
            ],
            "title": "Earlybert: Efficient bert training via early-bird lottery tickets",
            "venue": "arXiv preprint arXiv:2101.00063,",
            "year": 2020
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509,",
            "year": 2019
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Franck Dary",
                "Maxime Petit",
                "Alexis Nasr"
            ],
            "title": "Dependency parsing with backtracking using deep reinforcement learning",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Jesse Dodge",
                "Gabriel Ilharco",
                "Roy Schwartz",
                "Ali Farhadi",
                "Hannaneh Hajishirzi",
                "Noah Smith"
            ],
            "title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
            "year": 2002
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Edouard Grave",
                "Armand Joulin"
            ],
            "title": "Reducing transformer depth on demand with structured dropout",
            "venue": "arXiv preprint arXiv:1909.11556,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "arXiv preprint arXiv:1803.03635,",
            "year": 2018
        },
        {
            "authors": [
                "Trevor Gale",
                "Erich Elsen",
                "Sara Hooker"
            ],
            "title": "The state of sparsity in deep neural networks",
            "venue": "arXiv preprint arXiv:1902.09574,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Gao",
                "Hong Yang",
                "Peng Zhang",
                "Chuan Zhou",
                "Yue Hu"
            ],
            "title": "Graphnas: Graph neural architecture search with reinforcement learning",
            "year": 1904
        },
        {
            "authors": [
                "Mozhdeh Gheini",
                "Xiang Ren",
                "Jonathan May"
            ],
            "title": "Cross-attention is all you need: Adapting pretrained transformers for machine translation",
            "venue": "arXiv preprint arXiv:2104.08771,",
            "year": 2021
        },
        {
            "authors": [
                "Song Han",
                "Huizi Mao",
                "William J Dally"
            ],
            "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
            "venue": "arXiv preprint arXiv:1510.00149,",
            "year": 2015
        },
        {
            "authors": [
                "Babak Hassibi",
                "David G Stork",
                "Gregory J Wolff"
            ],
            "title": "Optimal brain surgeon and general network pruning",
            "venue": "In IEEE international conference on neural networks,",
            "year": 1993
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Eldar Kurtic",
                "Daniel Campos",
                "Tuan Nguyen",
                "Elias Frantar",
                "Mark Kurtz",
                "Benjamin Fineran",
                "Michael Goin",
                "Dan Alistarh"
            ],
            "title": "The optimal bert surgeon: Scalable and accurate second-order pruning for large language models",
            "venue": "arXiv preprint arXiv:2203.07259,",
            "year": 2022
        },
        {
            "authors": [
                "Woosuk Kwon",
                "Sehoon Kim",
                "Michael W Mahoney",
                "Joseph Hassoun",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "title": "A fast post-training pruning framework for transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ],
            "title": "Darts: Differentiable architecture search",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Zhuang Liu",
                "Mingjie Sun",
                "Tinghui Zhou",
                "Gao Huang",
                "Trevor Darrell"
            ],
            "title": "Rethinking the value of network pruning",
            "venue": "arXiv preprint arXiv:1810.05270,",
            "year": 2018
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Xiang Kong",
                "Sinong Wang",
                "Chunting Zhou",
                "Jonathan May",
                "Hao Ma",
                "Luke Zettlemoyer"
            ],
            "title": "Luna: Linear unified nested attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Paul Michel",
                "Omer Levy",
                "Graham Neubig"
            ],
            "title": "Are sixteen heads really better than one",
            "venue": "arXiv preprint arXiv:1905.10650,",
            "year": 2019
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing atari with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1312.5602,",
            "year": 2013
        },
        {
            "authors": [
                "Pavlo Molchanov",
                "Stephen Tyree",
                "Tero Karras",
                "Timo Aila",
                "Jan Kautz"
            ],
            "title": "Pruning convolutional neural networks for resource efficient inference",
            "venue": "arXiv preprint arXiv:1611.06440,",
            "year": 2016
        },
        {
            "authors": [
                "Rajiv Movva",
                "Jason Zhao"
            ],
            "title": "Dissecting lottery ticket transformers: Structural and behavioral study of sparse neural machine translation",
            "venue": "In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Prasanna",
                "Anna Rogers",
                "Anna Rumshisky"
            ],
            "title": "When BERT plays the lottery, all tickets are winning",
            "venue": "arXiv preprint arXiv:2005.00561,",
            "year": 2020
        },
        {
            "authors": [
                "Jack W Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "venue": "arXiv preprint arXiv:2112.11446,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Machel Reid",
                "Edison Marrese-Taylor",
                "Yutaka Matsuo"
            ],
            "title": "Subformer: Exploring weight sharing for parameter efficiency in generative transformers",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Ruder"
            ],
            "title": "An overview of gradient descent optimization algorithms",
            "venue": "arXiv preprint arXiv:1609.04747,",
            "year": 2016
        },
        {
            "authors": [
                "Hassan Sajjad",
                "Fahim Dalvi",
                "Nadir Durrani",
                "Preslav Nakov"
            ],
            "title": "On the effect of dropping layers of pre-trained transformer models",
            "venue": "arXiv preprint arXiv:2004.03844,",
            "year": 2020
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Shreyas Subramanian",
                "Vignesh Ganapathiraman",
                "Aly El Gamal"
            ],
            "title": "Learned learning rate schedules for deep neural network training using reinforcement learning, 2023",
            "venue": "URL https: //openreview.net/forum?id=0Zhwu1VaOs",
            "year": 2023
        },
        {
            "authors": [
                "Mingjie Sun",
                "Zhuang Liu",
                "Anna Bair",
                "J Zico Kolter"
            ],
            "title": "A simple and effective pruning approach for large language models",
            "venue": "arXiv preprint arXiv:2306.11695,",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Varun Nair",
                "Colin A Raffel"
            ],
            "title": "Training neural networks with fixed sparse masks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Sho Takase",
                "Shun Kiyono"
            ],
            "title": "Lessons on parameter sharing across layers in transformers",
            "venue": "arXiv preprint arXiv:2104.06022,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Elena Voita",
                "David Talbot",
                "Fedor Moiseev",
                "Rico Sennrich",
                "Ivan Titov"
            ],
            "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
            "year": 1905
        },
        {
            "authors": [
                "Danilo Vucetic",
                "Mohammadreza Tayaranian",
                "Maryam Ziaeefard",
                "James J Clark",
                "Brett H Meyer",
                "Warren J Gross"
            ],
            "title": "Efficient fine-tuning of bert models on the edge",
            "venue": "IEEE International Symposium on Circuits and Systems (ISCAS),",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461,",
            "year": 2018
        },
        {
            "authors": [
                "Tong Xiao",
                "Yinqiao Li",
                "Jingbo Zhu",
                "Zhengtao Yu",
                "Tongran Liu"
            ],
            "title": "Sharing attention weights for fast transformer",
            "venue": "arXiv preprint arXiv:1906.11024,",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Xu",
                "Andrew M Dai",
                "Jonas Kemp",
                "Luke Metz"
            ],
            "title": "Learning an adaptive learning rate schedule",
            "venue": "arXiv preprint arXiv:1909.09712,",
            "year": 2019
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Shauli Ravfogel",
                "Yoav Goldberg"
            ],
            "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "arXiv preprint arXiv:2106.10199,",
            "year": 2021
        },
        {
            "authors": [
                "Weixiong Zhang"
            ],
            "title": "Complete anytime beam search",
            "venue": "In AAAI/IAAI, pp",
            "year": 1998
        },
        {
            "authors": [
                "Michael Zhu",
                "Suyog Gupta"
            ],
            "title": "prune, or not to prune: Exploring the efficacy of pruning for model compression",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Barret Zoph",
                "Quoc Le"
            ],
            "title": "Neural architecture search with reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Barret Zoph",
                "Quoc Le"
            ],
            "title": "Neural architecture search with reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The recent work on large language models is based mostly on the transformer architecture of Vaswani et al. (2017). Such models have become increasingly larger and are trained for 100s of thousands of GPU hours using high-end GPUs (Brown et al., 2020; Chowdhery et al., 2022; Rae et al., 2021; Touvron et al., 2023).\nHowever, it is clear that the Transformer architecture (like other deep architectures) is overparameterized. For example, pruning can be used to reduce the number of FLOPs of transformers during inference time at least by half, with little effect on accuracy (Kurtic et al., 2022; Kwon et al., 2022), attention heads can be removed post-training with little effect on performance (Michel et al., 2019; Voita et al., 2019). The lottery ticket hypothesis holds for transformers (Frankle & Carbin, 2018; Chen et al., 2020a;b; Prasanna et al., 2020; Movva & Zhao, 2020), and, perhaps most relevant to our work, layers can be dropped altogether during inference Fan et al. (2019); Sajjad et al. (2020), and attention scores can be reused Bhojanapalli et al. (2021).\nMotivated by the potential to reuse transformer layers, we conducted a preliminary experiment in which we started with a transformer of L layers and trained only L2 layers by sharing the weights between layers i and layer i+ L2 for i < L 2 . The transformer trained this way achieved the same, or somewhat better performance, as the conventional L layer transformer.\nThis encouraging preliminary finding raises a few questions. First, is there something special about this pattern of repetition? Second, is a factor of two the best we can get? Taken to the extreme, it would be desirable that every layer in the architecture either replicates one of the previous layers or, if needed for the sake of accuracy, have a new set of weights.\nOur method opts to find such a general pattern. Trying to train only once, we view the repetition pattern a as a dynamic action that some driver network Q learns from reinforcement during the training of the primary network T . Every few epochs, a new action vector a is obtained based on the Q-function estimation given by Q. The element ai \u2208 [0, 1, . . . , i] for i = 1, . . . , L indicates from which layer to copy the weights to layer i of T . If ai = i then the weights of this layer are being optimized independently of other layers. If ai < i then the weights of layer ai are used as the weights of layer i. This is transductive: if layer ai = j and aj = k, then both layers i and j share the same weights of layer k.\nAfter a few training iterations of the primary network T , the reward for the driver network Q is computed by considering the loss obtained on a few training batches. Q is then updated, and a new action vector a is recovered. Depending on the dynamics of the driver network, the changes in the replication pattern can be rather rapid. Yet, as we show, the training process is stable.\nOur results indicate that training this way leads to a replication of at least 75% of the transformer layers while maintaining the same level of accuracy, or even slightly better, as the full L layer transformer. This is achieved with a relatively small Q network, which is only applied during training. Our contributions are: (i) Presenting a novel method for dramatically reducing the number of parameters in a transformer architecture. (ii) Establishing the potential of Reinforcement Learning (RL) to serve as a pivotal mechanism for dynamically optimizing the architectural configurations of transformers during training. The impact of RL in this context is considerably more profound than its conventional applications, such as adaptive learning rate tuning Xu et al. (2019). (iii) Demonstrating the use of RL in Neural Architecture Search (NAS) in a single training pass, unlike all previous work we are aware of, which follow Baker et al. (2017); Gao et al. (2019); Zoph & Le (2016) and collect multiple training sessions. (iv) Showing that transformers can be trained effectively, despite rapid changes in architecture during the training process."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Our method changes the architecture of the Transformer network and is, therefore, a Neural Architecture Search (NAS) method. The promise of the field is to discover architectures that would surpass human-designed ones in performance. While most recent contributions rely on techniques such as Differentiable Architecture Search (Liu et al., 2018a), some of the earlier approaches relied on RL. Zoph & Le (2017), employ a recurrent neural network (RNN) to generate architectural descriptions of neural networks and train it with RL. Baker et al. (2017) employ Q-learning to search for optimal CNN architectures. Cai et al. (2018) uses a controller, trained with the policy gradient method, to search for architectures in a more computationally efficient manner. As mentioned, RL NAS methods suggest a fixed architecture and train it from scratch, using the validation score as a reward. The trained network is not changed dynamically during training as we do.\nThe use of RL for dynamically controlling the training of a deep neural network has focused on learning rate optimization. Controlling the learning rate is often done with a fixed schedule, such as a step decay or a cosine decay, which determines the step size for each iteration of the optimization process (Ruder, 2016). Xu et al. (2019) employ proximal policy optimization (PPO) (Schulman et al., 2017) trained across multiple sessions (not a single session as in our method). (Subramanian et al., 2023) also employ PPO, and use a state vector that includes the training loss of the last epoch, the epoch index, and the number of remaining epochs.\nConsiderable effort has been dedicated to making transformers more efficient by reducing the quadratic complexity of the self-attention mechanism, e.g., (Child et al., 2019; Ma et al., 2021). With respect to parameter efficiency, network pruning methods (Molchanov et al., 2016; Hassibi et al., 1993; Frankle & Carbin, 2018; Liu et al., 2018b) including the transformer pruning methods mentioned above (Kurtic et al., 2022; Kwon et al., 2022) reduce the size of the network by removing or shrinking matrices from the network. Such methods often require further re-training, while our method is applied during training, maintaining the training time per epoch and reducing the peak memory consumption. The recent Wanda method (Sun et al., 2023) performs straightforward magnitude-based pruning (Han et al., 2015; Gale et al., 2019; Zhu & Gupta, 2018; Liu et al., 2018b) on the trained transformer. Despite its simplicity, it is shown to outperform other pruning alternatives. In comparison to our method, the sparsity demonstrated is up to 50% of the weights, while our method is shown to reduce 75% to 87% of the parameters. Our approach, which focuses on reuse, and pruning, which attempts to \u201creduce\u201d, are not mutually exclusive and can be combined.\nOther methods that reuse computations or parameters within transformers include the Reuse Transformer (Bhojanapalli et al., 2021) which, unlike our method, uses a specific and fixed pattern of reusing elements and only reuses attention heads. Overall less than 10% of the parameters are shared. Similarly to the Reuse Transformer, the Subformer (Reid et al., 2021) shares the parameters of the middle layers, however, much more extensively, reaching up to 50% reduction in the number of parameters. This requires the addition of auxiliary network elements, which we do not do.\nAlgorithm 1 Q-learning driven dynamic layer tying Require: L the number of layers, K the number of training steps of T , k the number of training\nsteps between the update and evaluation of Q, \u03b3 the discount factor, and \u03f5 initial exploration probability 1: Initialize the primary model T and the Q-network Q 2: Freeze layers 1 to L\u2212 1 in T , such that only layer 0 trains at initialization. 3: Initialize s = a = 0 \u25b7 An all zero vector 4: for step = 0 to K \u2212 1 do 5: Sample a mini-batch B from the dataset 6: Perform a training step with T on B 7: if mod(step,k) == 0 then \u25b7 Every k steps 8: Obtain an action vector a = \u03c0(s) 9: Compute s\u2032 based on a \u25b7 Eq. 1 10: for i = 0 to L\u2212 1 do 11: if s\u2032i \u0338= si then 12: if s\u2032i == i then 13: Untie layer i of T \u25b7 Copy its weights and update it independently of layer si 14: else 15: Replicate all weights of layer s\u2032i of T to layer i of T 16: Tie the weights of layer i to layer s\u2032i 17: end if 18: end if 19: end for 20: Sample a mini-batch B from the data-set 21: rstep = Compute negative PPL score based on T on B 22: rpredicated = Q(s,a) \u25b7 Eq. 3 23: r = rstep + \u03b3 \u2217maxa Q(s\u2032)a 24: L = MSE(rpredicted, r) 25: update Q using L 26: s = s\u2032 27: \u03f5 = max{\u03f5 \u2217 0.95, 0.1} 28: end if 29: end for\nTakase & Kiyono (2021) explore three different fixed patterns of sharing parameters, reusing 50% to 66% of the layers. The differences in performance between the patterns are small, and our last ablation (ablation vii) is similar to the Cycle pattern. Xiao et al. (2019) share attention weights (and not the parameters for computing these), based on the attention similarity. The number of reduced parameters is not reported but the average speedup is 1.3 (23% reduction).\nParameter Efficient Fine-Tuning (PEFT) often target specific layers or modules, e.g., only the top layers (Gheini et al., 2021), only the bias parameters (Zaken et al., 2021), or selecting based on scores (Sung et al., 2021; Vucetic et al., 2022). Additive PEFT methods introduce additional trainable parameters that can be added to the attention and feed-forward layers of transformers (Houlsby et al., 2019). LoRA (Hu et al., 2022) adds low-rank matrices to the weight matrices. PEFT methods substantially reduce the number of trainable parameters, but are applicable for finetuing (after the full model has been trained), while our method is for training from scratch. See Sec. 5 for future work on finetuning."
        },
        {
            "heading": "3 METHOD",
            "text": "We aim to train a transformer T with L layers from scratch. All elements of a transformer layer, including the key, query, and value projections, and the linear layers are considered as a single set of training parameters. The set of parameters for layer i can be either independent from all layers j < i, or tied to the set of parameters of some layer j < i.\nThe state vector s \u2208 NL indicates, at each location i = 0, 1, . . . , L \u2212 1, the layer with the lowest index that has the same tied weights. Therefore, \u2200i \u2208 [0, . . . , L \u2212 1] : 0 \u2264 si \u2264 i. If si = i\nit indicates that layer i does not have its parameters tied with any of the previous layers. By this definition, it always holds that s0 = 0.\nThe action space is similar, except that the action vector a \u2208 NL can point to any previous layer that has its weights tied with layer i, not necessarily the one with the lowest index j \u2264 i. To obtain s from a, one can employ the following recursion\nsi =\n{ i ai == i\nsai Otherwise (1)\nThe Q-function of a Markov Decision Process represents the expected cumulative future reward for taking a particular action a a in a particular state s, while following a certain policy \u03c0 (Sutton & Barto, 2018). Similarly to previous work that employs deep Q-learning(Mnih et al., 2013), we employ an \u03f5\u2212greedy policy obtained interpolating between a random policy and one obtained by maximizing, at a given state, the Q-function over the available actions.\n\u03c0(s) = { argmaxa Q(s,a) at probability 1\u2212 \u03f5 a uniformly sampled a at probability \u03f5 , (2)\nwhere Q is the network we learn in order to approximate the Q-function. Its implementation takes s as input and returns a vector of Q-values for each index i, indicating the Q-value obtained for each action j = 0, 1, . . . , i.\nQ(s,a) := \u2211 i Q(s)[i,ai] , (3)\nwhere indexing occurs first for the vector of Q-values per each index i and then for an element in this vector. Therefore, the input and output domains of the approximated Q-function are Q : RL\u22121 \u2192 R (L+2)(L\u22121) 2 . This reflects the fact that s0 is fixed and that for every layer i = 1, 2, . . . , L \u2212 1 the network Q needs to assign values to i + 1 different actions. The optimal action-value function Q\u2217 obeys an important identity known as the Bellman equation\nQ\u2217(s, a) = Es\u2032 [r + \u03b3max a\u2032 Q\u2217(s\u2032, a\u2032)|s, a] , (4)\nWe run the policy \u03c0 based on Q to obtain a new action a after every k training steps of the primary network T . k is relatively small and such actions are taken frequently. At initialization, only layer i = 0 is trained; all other layers are fixed at their initial values. Then, after k training steps, and every k training steps afterwards, we perform the following set of actions: (i) obtain a new action a = \u03c0(s), (ii) extract the new state s\u2032 based on a, as in Eq. 1, (iii) replicate the weights of each layer i to be the same as si and tie these weights, (iv) compute a reward for T based on the negative perplexity score as computed on a random training batch, (v) update Q based on the expected reward vs. the computed one, using the Bellman equation, (vi) reduce the exploration factor \u03f5 by a fixed factor of 0.95, but always keeping it above a constant of 0.1, and, finally, (vi) run k more training steps for T and repeat. The method is depicted in Alg. 1 and a line-by-line description is provided in Appendix A. A few implementation details are worth noting. First, in line 2, the replication pattern of the first k steps (where k << K) is determined to be such that layer 0 trains and the other layers are kept fixed at their initialization values. Then, every k steps we obtain a new action a, using the \u03f5\u2212greedy policy in Eq. 2, see lines 7-8. Second, we note that a layer that changes state can, based on the condition in line 12, either (i) shift from being untied or tied to one layer to being tied to a new layer, or (ii) shift to being trained independently. In the first type of shift, a new set of weights would be copied, which may change the transformer much more quickly than through gradient steps. In the second type of shift, the weights are not changed immediately. However, they begin to drift between layers that were previously tied. Third, the exact schedule for modifying the value of \u03f5 is given in line 27."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In our experiments, two architectures were used: (i) GPT-2 with 48 decoder blocks, each with 16 attention heads. The hidden dimension for each block was set to 1600, and (ii) BERT, which consists\nof 12 decoder blocks with a hidden size of 768 and 12 attention heads at each layer. In all of our experiments, Q is an MLP with one hidden layer with 128 units and the ReLU activation function. We ran all experiments for K = 300 epochs, a batch size of 16, and k = 15 with a separate validation set used to select the best model. The hyper-parameters used were: the transformer learning rate is set to 0.0001 and Q\u2019s learning rate was set to 0.001, \u03b3 = 0.99, the initial exploration probability is set to \u03f5 = 1.0 (explore), and as depicted in Alg. 1, the \u03f5-decay factor: 0.95, and the minimal \u03f5 value is set to 0.1. Our experiments ran on 2-4 A100 GPUs for the GPT-2 based architecture and 1-4 A6000/A5000 GPUs for the BERT architecture.\nDatasets In this study, we employ four widely used datasets to evaluate the performance of our method for language modeling tasks. All datasets were pre-processed by converting the text into tokens using GPT-2\u2019s tokenizer, which has a vocabulary of 50, 257 tokens. WikiText-2 (Wiki2) is a large language modeling corpus that consists of over 2 million tokens. It is derived from a snapshot of verified Good and Featured articles on Wikipedia. The dataset is widely used for training language models and serves as a standard benchmark for evaluating various NLP algorithms. WikiText-103 (Wiki103) is an extension of the WikiText-2 dataset, containing more than 100 million tokens. It is also sourced from Wikipedia articles and is considered to be one of the most comprehensive datasets for training large-scale language models. LAMBADA is designed to test the capabilities of language models in predicting the final word of a sentence, given all the preceding words in that sentence. The dataset contains approximately 10,000 examples, each a sequence of sentences extracted from books. The task is challenging as it often requires understanding the broader context provided by the preceding sentences. The 1 Billion Words dataset is a corpus of text containing approximately 1 billion tokens, sourced from news articles. It provides a diverse range of vocabulary and sentence structures, making it ideal for training robust language models.\nResults In Table 1, we present a comprehensive evaluation of our proposed method against conventional training on the GPT-2 architecture across multiple datasets: Wiki-2, Wiki-103, Lambada, and 1-billion. Our method consistently outperforms the baseline in terms of perplexity, with the most significant gains observed in the 1-billion words dataset, where we reduce the perplexity from 88.35 to 72.35. Additionally, our method exhibits a significant reduction in the number of trainable parameters, with a mean over training as low as 151M for Wiki-103, and not much higher on the other datasets, compared to the baseline\u2019s 1.6B. Although the conventional method outperformed our method on Wiki-103, the gap is marginal.\nTable 2 showcases the results for the BERT architecture, presenting similar trends. Our method outperforms the conventional training across all datasets. Notably, in the 1-billion dataset, the perplexity is reduced drastically, from over 1000 in conventional training to 215.50 in our method. The number of trainable parameters also sees a substantial decrease, with a mean during training of 52M-57M, compared to the conventional 376M. In both architectures, we can observe that the mean number of independent layers (or, equivalently, the number of groups of identical layers) is rather low during training and is somewhat higher in the final model. Especially in BERT, we can observe that even for large datasets the number of independent layers is small. In our ablation study below we check whether one can simply train much less layers.\nWith respect to training time, the results are mixed. While in Tab. 2 it is demonstrated that our method somewhat slows down the training time, Tab. 1 presents a reduction of almost 50% in runtime. We believe, but have not yet verified, that this is due to the difference in hardware between the two experiments (GPT-2 runs on A100, the BERT runs on A6000/A5000).\nThe status at the end of the training is shown in Fig. 1. A line is drawn between every layer index i and the layer it replicates si. A layer i for which the state vector satisfies si = i is connected to itself. As shown, there are seven such layers for Wiki-2 and six for Wiki-103, matching the statistics report in Tab. 2. The dominance of layer zero is clear, see Sec. 5 for a discussion of this property and its implications.\nTraining dynamics The training process takes place under the guidance of a policy that is trained from scratch. This policy can change the layer topology drastically and it is, therefore, interesting to explore the training dynamics. First, it is not clear whether any changes are made at all to the topology throughout the training process. It could be the case that after a certain period of exploration, the policy is to keep the state fixed from one step to the next. As Fig. 2 demonstrates, this is not the case. We distinguish two types of state-change events, as detailed in Sec. 3. In the first, which we called \u201ctied events\u201d, a layer i replicates a layer it did not replicate previously. In the second type, termed \u201cuntied events\u201d, a layer i obtains a new state of s = i and is trained independently of\nprevious layers, which had replicated another layer j < i. Evidently, both types of events continue to occur throughout the training process and their frequency does not diminish.\nThe memory consumption during training is a result of the training dynamics. Tab. 3 depicts the peak and average memory consumption during the training of GPT-2. Our memory consumption is lower by 65% in peak consumption and 68% on average. This difference is obtained without any attempt to optimize memory usage during training or to release unused memory, and does not reflect in full the drop in the size of the model.\nOne may wonder if all layers have the same chance of being untied. We note that since the exploration factor \u03f5 is at least 0.1 throughout training, with the exception of layer 0, all layers are expected to be tied to other layers at one point or another. As can be seen in Fig. 3, this is indeed the case.\nIt can also be observed that the lower layers are more likely to have an untied status of si == i (other layers with index j > i may still have sj = i and train together in a tied way). This makes sense due to the increasing number of replication options that higher layers have. However, we note from Fig. 1(b) that the layers with si == i can be relatively evenly distributed at the end of training.\nAblation study Ablation experiments were conducted on the Wiki-2 dataset with the GPT-2 architecture. Since much of the ablations focus on validating that the success of the method does not arise from avoiding overfitting by reducing the network capacity, we also run ablations on the small Shakespeare dataset. This dataset has parts of Shakespeare\u2019s plays, sonnets, and other writings. It is small, with 250K tokens and the ablation uses a 12-layer GPT-2 like model.\nSince the model obtained with our method has about a sixth of the number of parameters in the original model, we need to explore whether the full model capacity is required at all. To validate this, we designed a few ablations: (i) a transformer in which the number of layers L is the number of independent layers obtained by our method, and (ii) training from scratch a static transformer architecture that has the same weight-tying structure as our method\u2019s final architecture.\nAs can be seen in Tab. 4, both these transformers are far behind our full method\u2019s results and also behind the conventional training results. The second ablation implies that our method is not suitable for finding \u201clottery tickets\u201d, i.e., pruned architectures for training from scratch (Frankle & Carbin, 2018; Chen et al., 2020a;b; Prasanna et al., 2020; Movva & Zhao, 2020).\nAnother related ablation (iii) checks whether the dynamic status changes can be made arbitrarily, by recording the state vector s during the course of training, and applying a permuted version of it \u03c0(s) when changing the status of a layer to copy another layer or to be tied, where \u03c0 is a fixed permutation operator that is applied element-wise.\nThe results of this ablation demonstrate that the layer identity is important and that a significant degradation occurs in the model\u2019s performance when the same dynamics are applied to a different set of layers. As a sanity check, we also (iv) run the recorded set of states on another training session (without performing Q-learning). As can be seen, this obtains results that are similar but slightly worse than those of the full unablated method.\nThe necessity of weight tying is demonstrated by ablation (v), in which weight replication occurs as in the full method, but weight tying does not take place. This leads to very unstable training and a very high perplexity score.\nWe also explore (vi) the effect of freezing all layers except for layer 0 at initialization by freeing all layers to train (removing line 2 of Alg. 1. This somewhat outperforms the full method on the shakespeare dataset but is less successful on Wiki-2. We conclude that freezing at initialization may not be crucial (more experiments are needed). However, it has a sizable advantage in the peak GPU memory consumption.\nWe also provide results for (vii) using half the layers and tying every layer i = 1, 2, . . . , L/2 to layer L/2+ i. This cuts the number of trained layers by a much smaller fraction than our own method and is given as a reference since it was outlined as motivation in Sec. 1. As mentioned, this improves perplexity over the conventional training, but not nearly as much as our full method.\nAs mentioned in Sec. 2, ablation (vii) is the Cyclic pattern of (Takase & Kiyono, 2021). The two other patterns there are provided for completeness as ablations (viii) and (ix). As can be seen, these patterns, which reuse only 50% of the layers, are not as effective as our method."
        },
        {
            "heading": "5 DISCUSSION AND LIMITATION",
            "text": "Replacing the weights of an entire layer with those of another is a drastic change to the network. Yet, as shown in Fig. 2 (blue graph), such changes occur throughout training. This ability to perform this change without causing a temporal setback to the training process is not trivial, since even functionally equivalent layers can be expressed in multiple ways, by permuting the attention heads or the outputs of the feed-forward network. However, permutation to the feed-forward network would\n(a) (b)\nFigure 4: (a) Pearson correlations between the weights of the feed-forward networks of the untied layers (Wiki2; GPT-2 architecture). The colorbar range is [0.93,1] (b) As a reference, the correlations between the same layers in the conventionally trained GPT-2 model. The value range is [0,1].\ndrastically modify the token embedding the next layer observes, and would cause the network\u2019s performance to degrade unless the other layers co-adapt.\nWe attribute the fact that no such setbacks occur to the way the training process initializes. Layer 0 trains in a way that cannot be too specific, due to the randomly initialized filters downstream, which require time to co-adapt. Then, layer 0 is replicated and multiple copies of it are trained simultaneously. Other layers are also copied and their copies begin to train. However, given that layer zero is a valid replication source for all layers, and given that the exploration constant \u03f5 is initialized at a high value, layer zero is dominant. This domination, as can be seen in Fig. 1, is maintained until the end of training.\nWe posit that all layers are exposed directly or through a replication chain to the information of layer 0, and that it spreads a specific order of attention heads and embeddings that are maintained across layers. Having this global alignment is crucial for smooth training despite large blocks of weights being copied during the process. Support for this hypothesis can be seen in Fig. 4(a), which depicts the Pearson correlations between the weights of the feed-forward networks of the independent transformer layers trained with our method. The minimal correlation is 0.93. For reference, the correlation between the same layers in the conventional training (some of the 48 untied layers) is shown in panel (b). The inter-layer correlations are close to zero, as expected by the arbitrary permutation argument.\nOur research is focused on training transformer models from the ground up, contrasting with the extensive body of work that primarily concentrates on the fine-tuning of pre-trained transformers. (Devlin et al., 2018; Liu et al., 2019; Dodge et al., 2020; Raffel et al., 2020; Brown et al., 2020; He et al., 2021). It is unclear whether a method that starts with one trainable layer and then gradually explores options to untie some layers can be applied in such a case, especially since, as shown in Sec. 4, the number of independent layers remains small throughout training. An alternative that makes sense, but which is left for future work, is to apply the dynamic weight tying to the lowrank updates (LoRA) of Hu et al. (2021). One can also try to apply RL methods that employ backtracking (Dary et al., 2022), or use alternative search strategies, such as CAB (Zhang, 1998) or MCTS (Chaslot et al., 2008), changing one state index at a time.\nThe evaluation of our work is limited to transformers in the language domain. However, transformers are ubiquitous. A preliminary computer vision experiment reinforcing our conclusions can be found in Appendix B. Finally, transformers are often finetuned on downstream tasks. Preliminatry results on the GLUE set of benchmarks Wang et al. (2018) are presented in Appendix C, demonstrating that the tied models can be effectively trained for downstream tasks."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We present a method that is, as far as we can ascertain, the most dynamic form of Neural Architecture Search presented. During the training process itself, a deep Q-learning network drives a layer replication process, which ends up with over 90% of the parameters being in layers that completely replicate an earlier layer. This order of magnitude reduction in the number of parameters is achieved without sacrificing the perplexity score and, in some cases, also leads to an improvement in this metric. These surprising findings are further explored by visualizing the dynamics of the training process and the crucial components of the method are demonstrated in an ablation study."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD)."
        },
        {
            "heading": "A A LINE-BY-LINE EXPLANATION OF THE METHOD.",
            "text": "The method is depicted in Alg. 1. In line 2, the replication pattern of the first k steps is determined to be such that layer 0 trains and the other layers are kept fixed at their initialization values. The closest-matching action vector a and stage vector s are set to be the all-zero vector, see line 3.\nThe method then iterates over the data set and performs a regular training step on T , see lines 5-6. Every k steps (where k << K) we obtain a new action a, using the \u03f5\u2212greedy policy in Eq. 2, see lines 7-8. In line 9, we compute the state s\u2032 based on the obtained action a according to Eq. 1. This state is acted upon by replicating and tying the weights in lines 12,13. The condition in line 11 ensures that this happens only when this exact replication did not occur in the previous state s.\nWe note that a layer that changes state can, based on the condition in line 12, either (i) shift from being untied or tied to one layer to being tied to a new layer, or (ii) shift to being trained independently. In the first type of shift, a new set of weights would be copied, which may change the transformer much more quickly than through gradient steps. In the second type of shift, the weights are not changed immediately. However, they begin to drift between layers that were previously tied.\nIn lines 20-21 a random mini-batch is used to estimate the perplexity (PPL) score of T . The reward r is set in a similar fashion to the reward of DQN (Mnih et al. (2013)) as the sum of the evaluation score and the discounted prediction of the next state Q(s\u2032) (lines 22, 23). We do so by using Bellman (Eq. 4) as an iterative update: Qi+1(s, a) = r + \u03b3maxa\u2032 Qi+1(s\u2032, a\u2032). The MSE loss is then used to update network Q in lines 24, 25. As mentioned, after every training step of the Q-network, the value of \u03f5 is modified to balance exploration vs. exploitation, see line 27."
        },
        {
            "heading": "B PRELIMINARY COMPUTER VISION EXPERIMENTS.",
            "text": "Since transformers are ubiquitous, evaluating our method only for transformers in the language domain constitutes a limitation. As a preliminary computer vision experiment, we have applied our method to the Vision Transformer (ViT) (Dosovitskiy et al., 2021) on the CIFAR-10 dataset (Krizhevsky et al., 2009).\nThe results are reported in Table 5. As can be seen, similarly to the NLP experiments, with an insignificant drop in accuracy, our model has only 22% of the original model\u2019s parameters and only 7 out of 32 layers are independent at the end of training."
        },
        {
            "heading": "C PRELIMINARY DOWNSTREAM TASKS EXPERIMENTS.",
            "text": "In the domain of NLP, transformers are often trained for a causal language modeling task on a large corpus and are then fine-tuned on a smaller dataset for a specific task such as sentiment analysis, questions answering, or named entity recognition.\nAs a preliminary downstream task experiment, we have taken our GPT-2 based model which was trained using our method the 1-billion word dataset and trained it on multiple GLUE tasks Wang et al. (2018). During training on the new tasks, the tied layers were kept as in the final state of the model and the language modeling head was replaced with a new trainable head suited for each task.\nAs the vanilla baseline, we also trained a conventional GPT-2 model on the 1-billion word dataset and then finetuned all layers.\nThe results are reported in Table 6. As can be seen, our method leads to a minimal drop in the given metrics compared to the conventional method, while having only 12% of the trainable parameters and only 5 out of 48 layers are untied."
        }
    ],
    "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
    "year": 2024
}