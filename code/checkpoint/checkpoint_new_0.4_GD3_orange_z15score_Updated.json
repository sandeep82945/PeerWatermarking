[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "$$$\\boxed{\\text{Paper Summary:}}$\n\nThe paper proposes the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for causal discovery. The CMA is designed to synergize the metadata-based reasoning capabilities of LLMs with the data-driven modelling of DSCMs. The authors evaluate the CMA's performance on synthetic experiments and causal discovery benchmarks, as well as on a real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). The results show that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery.\n\n$\\boxed{\\text{Strengths:}}$\n\n1. The CMA framework is novel and innovative, combining LLMs and DSCMs to tackle the causal discovery problem.\n2. The authors provide a clear and detailed explanation of the CMA's components and how they work together.\n3. The experimental results demonstrate the CMA's effectiveness in causal discovery, particularly on real-world datasets.\n4. The paper highlights the potential of LLMs in metadatabased reasoning and their ability to capture domain knowledge.\n\n$\\boxed{\\text{Weaknesses:}}$\n\n1. The paper may be too lengthy and dense, making it challenging for non-experts to follow.\n2. Some of the explanations and definitions may be too technical or assume prior knowledge of the underlying concepts.\n3. The authors may have oversimplified some of the complexities of the causal discovery problem.\n4. The evaluation of the CMA's performance may not be comprehensive enough, as it focuses primarily on synthetic experiments and benchmarks.\n\n$\\boxed{\\text{Suggestions for Improvement:}}$\n\n1. Provide more detailed explanations and definitions of technical terms and concepts.\n2. Consider including more comprehensive evaluation of the CMA's performance on real-world datasets.\n3. Explore the potential limitations and challenges of the CMA framework, such as the need for continuous relaxation of discrete variables.\n4. Investigate techniques to enable fully automated chain graph modelling and extend DSCMs to represent more flexible, non-Markovian causal graphs.\n\n$\\boxed{\\text{Recommendation:}}$\n\nThe CMA framework is a promising approach to causal discovery, and its novelty and innovation make it worth exploring further. However, the paper's technical density and complexity may make it challenging for non-experts to follow. To improve the paper, the authors should consider providing more comprehensive explanations and definitions, as well as a more thorough evaluation of the CMA's performance on real-world datasets. Additionally, exploring the potential limitations and challenges of the CMA framework will help to strengthen its overall impact and applicability.",
        "peer_review_with_watermark": "$$$\\begin{align*}\n\\text{\\em Paper Summary:} \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\q",
        "gr_score_with": 0.5789473684210527,
        "gr_score_without": 0.31675392670157065,
        "or_score_with": 0.10526315789473684,
        "or_score_without": 0.5445026178010471,
        "output_without": {
            "num_tokens_scored": 382,
            "num_green_tokens": 121,
            "green_fraction": 0.31675392670157065,
            "num_orange_tokens": 208,
            "orange_fraction": 0.5445026178010471,
            "green_z_score": -3.3211601454976383,
            "orange_z_score": 116.3839471701248,
            "green_p_value": 0.9995517795799231,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 19,
            "num_green_tokens": 11,
            "green_fraction": 0.5789473684210527,
            "num_orange_tokens": 2,
            "orange_fraction": 0.10526315789473684,
            "green_z_score": 1.5921958796888795,
            "orange_z_score": 4.698506298146943,
            "green_p_value": 0.05567034937777047,
            "orange_p_value": 1.3103555465392977e-06
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:}$$\nThe paper proposes a framework for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The proposed framework consists of a set of concept encoders that extract disentangled concept embeddings along various concept axes specified by language. The concept encoders are trained using a combination of reconstruction and anchoring losses to encourage disentanglement and compositionality. The paper presents quantitative and qualitative evaluations of the proposed framework, demonstrating its superiority over prior work in visual concept editing.\n\n$$\\textbf{Strengths:}$$\n1. The paper presents a novel approach to learning language-informed visual concepts from images by distilling from pre-trained vision-language models.\n2. The proposed framework encourages disentanglement and compositionality of concept embeddings, enabling flexible composition of concepts.\n3. The paper presents thorough evaluations of the proposed framework, demonstrating its superiority over prior work in visual concept editing.\n\n$$\\textbf{Weaknesses:}$$\n1. The paper relies on pre-trained vision-language models, which may not generalize well to unseen concepts.\n2. The proposed framework may require large amounts of training data to achieve good performance.\n3. The paper does not provide a clear comparison of the proposed framework with other state-of-the-art approaches in visual concept editing.\n\n$$\\textbf{Suggestions for Improvement:}$$\n1. The authors should provide more details on the hyperparameter tuning process and the effect of different hyperparameters on the performance of the proposed framework.\n2. The authors should conduct more extensive evaluations of the proposed framework on larger datasets and with more diverse visual concepts.\n3. The authors should provide a more detailed comparison of the proposed framework with other state-of-the-art approaches in visual concept editing.\n\n$$\\textbf{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise the paper to address the weaknesses and improve the clarity and comprehensiveness of the evaluation. The revised paper should provide more details on the experimental setup, the effect of hyperparameters, and the comparison with other state-of-the-art approaches. The authors should also consider conducting more extensive evaluations and providing more detailed results to strengthen the overall quality of the paper.",
        "peer_review_with_watermark": "$$\n\\begin{aligned}\n\\textbf{Paper Summary:} &\\quad The paper proposes a novel approach for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The method learns disentangled concept embeddings along various concept axes specified by language and can be used for image generation with novel concept compositions.\n\n\\textbf{Strengths:} &\\quad The paper introduces a novel approach for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The method is able to extract disentangled concept embeddings along various concept axes specified by language, which can be used for image generation with novel concept compositions. The approach is also able to generalize to unseen concepts via test-time finetuning.\n\n\\textbf{Weaknesses:} &\\quad The paper relies on pre-trained vision-language models which can suffer from limited training data and biased learning. The method also assumes that the language-informed concept axes are well-defined and can be easily specified by language, which may not always be the case in real-world scenarios.\n\n\\textbf{Suggestions for Improvement:} &\\quad To improve the robustness of the method, it would be beneficial to use more diverse and large-scale training datasets for the pre-trained vision-language models. Additionally, the method could be improved by incorporating more advanced techniques for learning disentangled concept embeddings, such as using techniques like variational autoencoders (VAEs) or generative adversarial networks (GANs).\n\n\\textbf{Recommendation:} &\\quad The proposed method is a significant contribution to the field of computer vision and natural language processing. However, further research is needed to improve the robustness and applicability of the method in real-world scenarios. The method has the potential to be used in various applications, such as image generation, image editing, and image captioning.\n\n$$\n\\begin{aligned} \n\\end{aligned} \n$$",
        "gr_score_with": 0.45112781954887216,
        "gr_score_without": 0.3392857142857143,
        "or_score_with": 0.41353383458646614,
        "or_score_without": 0.5285714285714286,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 95,
            "green_fraction": 0.3392857142857143,
            "num_orange_tokens": 148,
            "orange_fraction": 0.5285714285714286,
            "green_z_score": -2.0737876550156327,
            "orange_z_score": 108.16031456130287,
            "green_p_value": 0.9809504819100696,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 266,
            "num_green_tokens": 120,
            "green_fraction": 0.45112781954887216,
            "num_orange_tokens": 110,
            "orange_fraction": 0.41353383458646614,
            "green_z_score": 1.7021289926939795,
            "orange_z_score": 82.18925551564737,
            "green_p_value": 0.04436559501864636,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes Rapper, a two-stage Reinforced Rationale-Prompted Paradigm for enabling Natural Language Explanation (NLE) with sufficient plausible and faithful properties in Visual Question Answering (VQA). Rapper utilizes knowledge distillation from Large Language Models (LLMs) and Reinforcement Learning from NLE Feedback (RLNF) to generate rationales that are based on language-based facts and visual facts, respectively. The paper presents experimental results showing that Rapper outperforms state-of-the-art methods in VQA-NLE tasks, including VQA-X and e-SNLI-VE datasets.\n\n$$\\text{Strengths:}$$\n1. The paper presents a novel approach to NLE in VQA, which addresses the challenges of implausibility and hallucination.\n2. The use of knowledge distillation from LLMs and RLNF is effective in generating rationales that are based on language-based facts and visual facts, respectively.\n3. The paper presents experimental results showing that Rapper outperforms state-of-the-art methods in VQA-NLE tasks.\n\n$$\\text{Weaknesses:}$$\n1. The paper does not provide a clear explanation of the knowledge distillation process and how it is used to generate rationales.\n2. The paper does not discuss the limitations of the proposed approach and potential future work.\n3. The paper does not provide a detailed analysis of the experimental results and how they were obtained.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a clear explanation of the knowledge distillation process and how it is used to generate rationales.\n2. Discuss the limitations of the proposed approach and potential future work.\n3. Provide a detailed analysis of the experimental results and how they were obtained.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a conference or journal that focuses on Natural Language Processing and Machine Learning. The paper presents a novel approach to NLE in VQA and presents experimental results showing that Rapper outperforms state-of-the-art methods. However, the paper needs to be improved by providing a clear explanation of the knowledge distillation process and discussing the limitations of the proposed approach.\n\n$$\\text{Rating:}$$\nThe paper is of high quality and presents a novel approach to NLE in VQA. However, the paper needs to be improved by providing a clear explanation of the knowledge distillation process and discussing the limitations of the proposed approach. Based on the strengths and weaknesses of the paper, I give it a rating of 8 out of 10.",
        "peer_review_with_watermark": "$$\\begin{align*}\n$$The paper proposes Rapper, a two-stage Reinforced Rationale-Prompted Paradigm for generating plausible and faithful Natural Language Explanation (NLE) in visual question answering (VQA). The proposed method utilizes knowledge distillation from a large language model (LLM) and Reinforcement Learning from NLE Feedback (RLNF) to produce rationales that are both plausibility and faithfulness.\n\nThe paper presents a comprehensive overview of the related work in NLE and VL-NLE, and highlights the challenges in generating plausible and faithful NLEs. The authors also discuss the potential of rationale-based prompting techniques for improving the capability of LLMs on complex reasoning tasks.\n\nThe proposed Rapper method is evaluated on two VL-NLE benchmarks, VQA-X and e-SNLI-VE, and demonstrates state-of-the-art performance on NLE-related metrics. The authors also provide ablation studies and qualitative evaluations to demonstrate the effectiveness of their proposed method.\n\nThe paper highlights the potential of Rapper for mitigating implausible or hallucinated explanations in VQA tasks, and demonstrates its superiority over state-of-the-art VQA-NLE methods.\n\nThe proposed method has several strengths, including:\n\n*   The use of knowledge distillation from LLM to produce plausibility-enhancing rationales.\n*   The introduction of RLNF to enforce faithfulness in NLE generation.\n*   The ability to produce rationales that are both plausibility and faithfulness.\n\nThe proposed method also has several weaknesses, including:\n\n*   The potential for knowledge distillation to introduce hallucinations.\n*   The complexity of the proposed method, which may make it difficult to interpret and debug.\n\nSuggestions for improvement:\n\n*   The authors could provide more detailed explanations of the potential risks and challenges associated with the proposed method.\n*   The authors could also provide more comprehensive ablation studies to demonstrate the robustness and generalizability of their proposed method.\n\nRecommendation:\n\n*   The proposed method, Rapper, demonstrates state-of-the-art performance on NLE-related metrics and has the potential to mitigate implausible or hallucinated explanations in VQA tasks.\n*   However, the method also has several weaknesses, including potential risks and challenges associated with knowledge distillation and complexity.\n*   Further research is needed to fully address these challenges and demonstrate the robustness and generalizability of the proposed method.\n\n$$",
        "gr_score_with": 0.5076923076923077,
        "gr_score_without": 0.4414715719063545,
        "or_score_with": 0.4553846153846154,
        "or_score_without": 0.4782608695652174,
        "output_without": {
            "num_tokens_scored": 299,
            "num_green_tokens": 132,
            "green_fraction": 0.4414715719063545,
            "num_orange_tokens": 143,
            "orange_fraction": 0.4782608695652174,
            "green_z_score": 1.4637957104146446,
            "orange_z_score": 101.120240497152,
            "green_p_value": 0.07162488742454538,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 325,
            "num_green_tokens": 165,
            "green_fraction": 0.5076923076923077,
            "num_orange_tokens": 148,
            "orange_fraction": 0.4553846153846154,
            "green_z_score": 3.962969619506085,
            "orange_z_score": 100.31241083750457,
            "green_p_value": 3.7011602300732e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery from monocular input. The approach combines the Eulerian and Lagrangian views to model scene motion, enabling the discovery of rigid parts. The method consists of three modules: a canonical module, an Eulerian module, and a Lagrangian module. The Lagrangian module is used to discover parts by factorizing the scene motion as a composition of part-level rigid motions. The approach is trained using only rendering supervision and achieves high-quality dynamic scene reconstruction and realistic rendering results. The method also supports part discovery, allowing for direct downstream applications such as part tracking, object control, and scene editing.\n\nStrengths:\n1. The approach combines the Eulerian and Lagrangian views to model scene motion, enabling the discovery of rigid parts.\n2. The method uses a hybrid feature volume and neural network representation, ensuring fast convergence during training.\n3. The learned part representation can be directly applied to downstream tasks, such as object tracking, structured scene modeling, and editing.\n\nWeaknesses:\n1. The approach explicitly stores motion features in low-dimensional vectors, which may not be sufficient for modeling motion on very long sequences.\n2. The method requires a significant amount of training data, which may be a limitation for real-world applications.\n3. The approach may not be suitable for complex scenes with multiple moving objects.\n\nSuggestions for Improvement:\n1. Investigate the use of long sequence encoding-decoding schemes to improve motion modeling on very long sequences.\n2. Consider using additional data augmentation techniques to increase the size and diversity of the training dataset.\n3. Evaluate the approach on more complex scenes with multiple moving objects to assess its robustness.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery from monocular input. The approach combines the Eulerian and Lagrangian views to model scene motion, enabling the discovery of rigid parts. The method is trained using only rendering supervision and achieves high-quality dynamic scene reconstruction and realistic rendering results. The learned part representation can be directly applied to downstream tasks, such as object tracking, structured scene modeling, and editing. However, the approach may require further improvements to handle complex scenes and long sequences.",
        "peer_review_with_watermark": "$$$\\boxed{Summary:}\nThe paper proposes MovingParts, a novel NeRF-based method for dynamic scene reconstruction and part discovery from monocular image sequences. The method uses a hybrid representation of feature volume and neural network to achieve both high-quality reconstruction and reasonable part discovery within 30 minutes. The approach leverages the Lagrangian view to discover parts by factorizing the scene motion as a composition of part-level rigid motions.\n\n$$\\boxed{Strengths:}\n1. The authors provide a detailed explanation of the proposed method, making it easier for the reader to understand the approach.\n2. The use of a hybrid representation of feature volume and neural network enables efficient training and reconstruction.\n3. The authors demonstrate the ability of the method to achieve high-quality dynamic scene reconstruction and realistic rendering results.\n4. The proposed motion grouping module is effective in discovering meaningful parts in complex scenes.\n\n$$\\boxed{Weaknesses:}\n1. The method relies on the assumption that the scene is piece-wise rigid, which may not hold in all cases.\n2. The authors acknowledge that the current method has limitations in modeling motion on very long sequences, which may require future work.\n3. The lack of explicit shape priors in the Lagrangian view may limit the method's ability to handle complex geometry.\n\n$$\\boxed{Suggestions for Improvement:}\n1. To address the limitation in modeling motion on long sequences, the authors should explore the development of a unified long sequence encoding-decoding scheme.\n2. To improve the method's robustness, the authors could consider incorporating additional cues, such as depth or semantic segmentation, to supplement the motion information.\n3. To enhance the method's generalizability, the authors could evaluate the proposed approach on a diverse range of scenes and scenarios.\n\n$$\\boxed{Recommendation:}\nI recommend that the authors continue to work on the proposed MovingParts method, focusing on addressing the limitations and exploring new avenues for improvement. The method's ability to achieve high-quality dynamic scene reconstruction and part discovery makes it a promising contribution to the field of computer vision and graphics. With further development and testing, MovingParts has the potential to become a widely adopted and influential approach in the field.",
        "gr_score_with": 0.49702380952380953,
        "gr_score_without": 0.3867595818815331,
        "or_score_with": 0.44047619047619047,
        "or_score_without": 0.5365853658536586,
        "output_without": {
            "num_tokens_scored": 287,
            "num_green_tokens": 111,
            "green_fraction": 0.3867595818815331,
            "num_orange_tokens": 154,
            "orange_fraction": 0.5365853658536586,
            "green_z_score": -0.45786455807628723,
            "orange_z_score": 106.57366441428807,
            "green_p_value": 0.6764751244223843,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 336,
            "num_green_tokens": 167,
            "green_fraction": 0.49702380952380953,
            "num_orange_tokens": 148,
            "orange_fraction": 0.44047619047619047,
            "green_z_score": 3.630298535977097,
            "orange_z_score": 94.38075769792067,
            "green_p_value": 0.00014154679853759997,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
        "abstract": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes the Diffusion Model-Based Predictor (DMBP) framework, which leverages conditional diffusion models to recover the actual state for decision-making in state-based offline reinforcement learning (RL) tasks. DMBP is designed to enhance the robustness of existing offline RL algorithms against state observation perturbations, such as sensor errors or adversarial attacks. The proposed framework consists of two main components: a conditional diffusion model-based predictor and a non-Markovian loss function to mitigate error accumulation. Experimental results on D4RL benchmarks demonstrate the effectiveness of DMBP in improving the robustness of offline RL algorithms against different scales of random noises and even adversarial attacks.\n\n$$\\text{Strengths:}$$\n1. The proposed DMBP framework provides a novel approach to state-denoising in offline RL, leveraging the potential of diffusion models.\n2. The non-Markovian loss function proposed in the paper effectively mitigates error accumulation during test, enhancing the robustness of offline RL algorithms.\n3. The experimental results demonstrate the effectiveness of DMBP in improving the robustness of offline RL algorithms against different scales of random noises and even adversarial attacks.\n\n$$\\text{Weaknesses:}$$\n1. The paper may benefit from a more detailed analysis of the theoretical underpinnings of the proposed non-Markovian loss function.\n2. The experimental results may be improved by including more comprehensive comparisons with state-of-the-art offline RL algorithms.\n3. The paper may provide more insights into the potential limitations and challenges of the proposed DMBP framework.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a more detailed analysis of the theoretical underpinnings of the proposed non-Markovian loss function.\n2. Include more comprehensive comparisons with state-of-the-art offline RL algorithms in the experimental results.\n3. Provide more insights into the potential limitations and challenges of the proposed DMBP framework.\n\n$$\\text{Recommendation:}$$\nThe proposed DMBP framework is an innovative approach to state-denoising in offline RL, and its experimental results demonstrate its effectiveness in improving the robustness of offline RL algorithms against different scales of random noises and even adversarial attacks. However, the paper may benefit from further analysis, comparisons, and insights to strengthen its contributions.",
        "peer_review_with_watermark": "$$\\text{Paper Summary:}\n\\text{This paper proposes the Diffusion Model-Based Predictor (DMBP) for state-denoising in offline reinforcement learning (RL) tasks. DMBP leverages conditional diffusion models to recover the actual state for decision-making. To mitigate the error accumulation in state estimation, a non-Markovian training objective is proposed. The proposed framework can effectively deal with the cases of incomplete state observations for state-based RL tasks.}\n\n$$\\text{Strengths:}\n\\text{1. The proposed framework leverages conditional diffusion models to recover the actual state for decision-making, which can effectively mitigate the error accumulation in state estimation.}\n\\text{2. The non-Markovian training objective can reduce the error accumulation in the test phase.}\n\\text{3. The proposed framework can effectively deal with the cases of incomplete state observations for state-based RL tasks.}\n\\text{4. The experiments demonstrate the robustness of the proposed framework against different scales of random noises and even adversarial attacks.}\n\n$$\\text{Weaknesses:}\n\\text{1. The proposed framework may require significant computational resources due to the parallel denoising process in the diffusion model.}\n\\text{2. The non-Markovian training objective may require a large number of samples for training, which can be challenging in practice.}\n\\text{3. The experiments may not cover all the cases of state observation perturbations, which can limit the generalizability of the proposed framework.}\n\n$$\\text{Suggestions for Improvement:}\n\\text{1. To reduce the computational resources required for the parallel denoising process, the framework can be implemented using a more efficient parallelization scheme.}\n\\text{2. To reduce the number of samples required for training the non-Markovian training objective, the framework can be implemented using a more efficient sampling scheme.}\n\\text{3. To cover more cases of state observation perturbations in the experiments, the framework can be implemented using a more comprehensive testing suite.}\n\n$$\\text{Recommendation:}\n\\text{Based on the experimental results, the proposed framework is recommended for its robustness against different scales of random noises and even adversarial attacks in state-based RL tasks. However, the computational complexity of the framework needs to be further investigated to reduce the required computational resources. The non-Markovian training objective needs to be further investigated to reduce the number of samples required for training.}\n\n$$\\text{Additional Comments:}\n\\text{Overall, the proposed framework is a significant contribution to the field of offline RL tasks. The experimental results demonstrate the robustness of the proposed framework against different scales of random noises and even adversarial attacks in state-based RL tasks. However, the computational complexity of the framework needs to be further investigated to reduce the required computational resources. The non-Markovian training objective needs to be further investigated to reduce the number of samples required for training.}\n\n$$\\text{Conflicts of Interest:}\n\\text{There are no conflicts of interest in this paper. The authors declare that the paper is original and has not been previously published.}\n\n$$\\text{Funding:}\n\\text{This paper was supported by [insert funding information].}\n\n$$\\text{Acknowledgments:}\n\\text{The authors would like to thank [insert acknowledgments].}\n\n$$\\text{References:}\n\\text{[insert references].}$$",
        "gr_score_with": 0.4362017804154303,
        "gr_score_without": 0.3527272727272727,
        "or_score_with": 0.47477744807121663,
        "or_score_without": 0.5163636363636364,
        "output_without": {
            "num_tokens_scored": 275,
            "num_green_tokens": 97,
            "green_fraction": 0.3527272727272727,
            "num_orange_tokens": 142,
            "orange_fraction": 0.5163636363636364,
            "green_z_score": -1.6001893827313254,
            "orange_z_score": 103.13382228901659,
            "green_p_value": 0.9452217116086571,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 147,
            "green_fraction": 0.4362017804154303,
            "num_orange_tokens": 160,
            "orange_fraction": 0.47477744807121663,
            "green_z_score": 1.3565607878446515,
            "orange_z_score": 104.85261093259398,
            "green_p_value": 0.08746040410708167,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes a hierarchical data-efficient representation learning framework for designing RNA tertiary structures. The framework, called RDESIGN, leverages a large, well-curated benchmark dataset and a comprehensive structural modeling approach to represent the complex RNA tertiary structure. RDESIGN incorporates extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The framework consists of a hierarchical representation learning scheme and a secondary structure imposing strategy. The proposed method is evaluated on various RNA design benchmarks and generalization ability assessments, demonstrating its efficacy.\n\n$$\\text{Strengths:}$$\n1. The paper proposes a novel hierarchical data-efficient representation learning framework for RNA tertiary structure design.\n2. The framework incorporates extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process.\n3. The evaluation of RDESIGN on various RNA design benchmarks and generalization ability assessments demonstrates its efficacy.\n\n$$\\text{Weaknesses:}$$\n1. The paper does not provide a detailed analysis of the limitations of the proposed framework, such as its applicability to different types of RNA structures or its scalability to large datasets.\n2. The evaluation of RDESIGN is limited to a specific benchmark dataset, and it is unclear how well the framework generalizes to other datasets or scenarios.\n3. The paper does not provide a comprehensive comparison with existing RNA design methods, which may have different strengths and weaknesses.\n\n$$\\text{Suggestions for Improvement:}$$\n1. The authors should provide a more detailed analysis of the limitations of the proposed framework and its potential applications.\n2. The evaluation of RDESIGN should be extended to other benchmark datasets and scenarios to assess its generalization capability.\n3. A comprehensive comparison with existing RNA design methods should be provided to highlight the strengths and weaknesses of the proposed framework.\n\n$$\\text{Recommendation:}$$\nBased on the evaluation of RDESIGN, it is recommended for consideration as a novel approach for RNA tertiary structure design. However, further research is needed to address the limitations and potential applications of the framework.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper proposes a hierarchical data-efficient representation learning framework, called RDESIGN, for designing RNA tertiary structures. The framework utilizes a comprehensive structural modeling approach, which includes a local coordinate system, intra-nucleotide and inter-nucleotide level attributes, and a graph representation learning scheme. The authors also introduce a secondary structure imposing strategy to leverage extra structural information. The proposed method is evaluated on a benchmark dataset, and the results demonstrate its effectiveness in designing RNA sequences with structures similar to the target structure. The paper also includes ablation studies and experiments on generalization capability, which further supports the effectiveness of the proposed method.\n\nStrengths:\n1. The authors provide a comprehensive overview of the challenges in RNA design and the need for a hierarchical data-efficient representation learning framework.\n2. The proposed method is well-structured, and the authors provide a clear explanation of the different components and their contributions.\n3. The authors provide extensive experiments to evaluate the performance of the proposed method, including ablation studies and experiments on generalization capability.\n4. The paper includes a thorough discussion of the limitations of the current method and the need for wet-lab validation.\n\nWeaknesses:\n1. The paper relies heavily on the authors' assumption that the proposed method is effective, and the results may not be generalizable to other datasets or scenarios.\n2. The paper does not provide a thorough analysis of the hyperparameters and their impact on the performance of the proposed method.\n3. The paper includes a limited number of experiments on generalization capability, which may not be representative of the proposed method's performance in other scenarios.\n4. The paper does not provide a detailed explanation of the hyperspherical space and its impact on data-efficient learning.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the hyperparameters and their impact on the performance of the proposed method.\n2. The authors should include more experiments on generalization capability, including evaluations on different datasets and scenarios.\n3. The authors should provide a more detailed explanation of the hyperspherical space and its impact on data-efficient learning.\n4. The authors should consider including a wet-lab validation to further validate the proposed method's effectiveness.\n\nRecommendation:\nThe proposed method, RDESIGN, demonstrates its effectiveness in designing RNA sequences with structures similar to the target structure. However, the paper relies heavily on the authors' assumption that the proposed method is effective, and the results may not be generalizable to other datasets or scenarios. To improve the robustness and generalizability of the proposed method, the authors should provide a more thorough analysis of the hyperparameters, include more experiments on generalization capability, and provide a more detailed explanation of the hyperspherical space. With these improvements, the proposed method has the potential to become a leading approach in RNA design.",
        "gr_score_with": 0.5454545454545454,
        "gr_score_without": 0.5,
        "or_score_with": 0.37337662337662336,
        "or_score_without": 0.3814814814814815,
        "output_without": {
            "num_tokens_scored": 270,
            "num_green_tokens": 135,
            "green_fraction": 0.5,
            "num_orange_tokens": 103,
            "orange_fraction": 0.3814814814814815,
            "green_z_score": 3.3541019662496847,
            "orange_z_score": 73.1693936388295,
            "green_p_value": 0.0003981150787954046,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 168,
            "green_fraction": 0.5454545454545454,
            "num_orange_tokens": 115,
            "orange_fraction": 0.37337662337662336,
            "green_z_score": 5.210711578231436,
            "orange_z_score": 76.4570079948157,
            "green_p_value": 9.405885470123046e-08,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces SWAP, a novel sparse entropic Wasserstein regression pruning method for neural network pruning. SWAP formulates the pruning problem as a mixed-integer quadratic programming (MIQP) problem and leverages the entropic Wasserstein regression (EWR) framework to balance covariance information preservation and noise reduction. The method is compared with several state-of-the-art (SoTA) pruning algorithms on various pre-trained neural networks, demonstrating its effectiveness in achieving robust pruning and improved model performance.\n\nStrengths:\n\n1.  Novel formulation: SWAP provides a novel approach to pruning neural networks by leveraging the EWR framework, which strikes a balance between covariance information preservation and noise reduction.\n2.  Robustness to noise: SWAP is shown to be robust to noisy gradients, which is a common challenge in neural network pruning.\n3.  Competitive performance: SWAP is compared with several SoTA pruning algorithms and demonstrates competitive performance on various pre-trained neural networks.\n\nWeaknesses:\n\n1.  Complexity: SWAP is a mixed-integer quadratic programming (MIQP) problem, which can be computationally expensive to solve.\n2.  Hyperparameter tuning: The performance of SWAP may be sensitive to the choice of hyperparameters, such as the regularization parameter \u03bb and the step size \u03c4.\n3.  Limited experimental results: While the paper provides some experimental results, more extensive experiments are needed to fully evaluate the performance of SWAP.\n\nSuggestions for Improvement:\n\n1.  Simplify the formulation: Consider simplifying the MIQP formulation to make it more tractable and computationally efficient.\n2.  Regularization: Investigate the use of regularization techniques to improve the stability and robustness of SWAP.\n3.  More extensive experiments: Conduct more extensive experiments to fully evaluate the performance of SWAP on various pre-trained neural networks and datasets.\n\nRecommendation:\n\n1.  Publish the paper: The paper presents a novel approach to neural network pruning and provides competitive results on various pre-trained neural networks.\n2.  Future work: The authors should consider simplifying the formulation, exploring regularization techniques, and conducting more extensive experiments to fully evaluate the performance of SWAP.\n3.  Practical implementation: The authors should investigate the practical implementation of SWAP in large-scale model compression and deployment.",
        "peer_review_with_watermark": "$$$ Paper Summary: \nThe paper proposes a novel method, SWAP, for network pruning, which uses sparse entropic Wasserstein regression (EWR) to balance gradient noise reduction and covariance information preservation. The proposed method is designed to be more robust than existing methods, especially in the presence of noisy gradients. The authors evaluate their method on various neural networks and datasets, showing that it outperforms existing methods in terms of accuracy and robustness. \n\nStrengths: \n1. The proposed method, SWAP, is designed to be more robust than existing methods, especially in the presence of noisy gradients.\n2. The authors evaluate their method on various neural networks and datasets, showing that it outperforms existing methods in terms of accuracy and robustness.\n3. The method uses sparse EWR, which is a novel approach that balances gradient noise reduction and covariance information preservation.\n4. The authors provide a comprehensive theoretical analysis of their method, including a detailed explanation of the Neighborhood Interpolation mechanism and the advantage of using entropic regularization in terms of sample complexity.\n5. The authors provide a practical algorithm for implementing their method, which is efficient and scalable.\n\nWeaknesses: \n1. The authors rely on empirical evidence to evaluate their method, rather than providing a more theoretical analysis.\n2. The authors do not provide a detailed explanation of how their method is related to existing methods, such as sparse linear regression and magnitude pruning.\n3. The authors do not provide a comprehensive analysis of the computational complexity of their method.\n4. The authors do not provide a detailed explanation of how their method is affected by changes in the hyperparameters, such as the regularization parameter \u03bb.\n5. The authors do not provide a comprehensive analysis of the method's ability to generalize to new, unseen data.\n\nSuggestions for Improvement: \n1. The authors should provide a more theoretical analysis of their method, including a more detailed explanation of how it relates to existing methods.\n2. The authors should provide a comprehensive analysis of the computational complexity of their method.\n3. The authors should provide a detailed explanation of how their method is affected by changes in the hyperparameters, such as the regularization parameter \u03bb.\n4. The authors should provide a comprehensive analysis of the method's ability to generalize to new, unseen data.\n5. The authors should provide more experimental evidence to evaluate their method, including more comprehensive analysis of the method's performance on different datasets and under different conditions.\n\nRecommendation: \nThe proposed method, SWAP, is a promising approach for network pruning, especially in the presence of noisy gradients. While the authors provide some theoretical analysis and experimental evidence, there are several weaknesses that need to be addressed. With some improvements, the method has the potential to be more robust and scalable, and it may be a valuable contribution to the field of network pruning.",
        "gr_score_with": 0.5190311418685121,
        "gr_score_without": 0.403030303030303,
        "or_score_with": 0.4290657439446367,
        "or_score_without": 0.4484848484848485,
        "output_without": {
            "num_tokens_scored": 330,
            "num_green_tokens": 133,
            "green_fraction": 0.403030303030303,
            "num_orange_tokens": 148,
            "orange_fraction": 0.4484848484848485,
            "green_z_score": 0.11236664374387367,
            "orange_z_score": 94.52418149309759,
            "green_p_value": 0.45526635111464514,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 289,
            "num_green_tokens": 150,
            "green_fraction": 0.5190311418685121,
            "num_orange_tokens": 124,
            "orange_fraction": 0.4290657439446367,
            "green_z_score": 4.130512115281437,
            "orange_z_score": 84.56451437385991,
            "green_p_value": 1.8097800913599052e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}  \nThe paper proposes a novel approach to visual reinforcement learning (RL) called DrM, which aims to address the sample efficiency challenge in RL by minimizing the dormant ratio, a metric that measures the inactivity of an agent's policy network. The authors introduce three mechanisms to guide the exploration-exploitation trade-off: dormant-ratio-guided perturbation, awaken exploration, and dormant-ratio-guided exploitation. Experimental results show that DrM outperforms existing baselines in terms of sample efficiency and asymptotic performance across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit.}\n\n$$\\boxed{\\text{Strengths:}  \n1. The paper identifies a crucial issue in existing visual RL methods, namely the agents' tendency to exhibit sustained inactivity during early training, which limits their ability to explore effectively.\n2. The authors propose a novel approach to addressing this issue by minimizing the dormant ratio, which is a metric that measures the inactivity of an agent's policy network.\n3. The three mechanisms introduced in the paper, dormant-ratio-guided perturbation, awaken exploration, and dormant-ratio-guided exploitation, are effective in guiding the exploration-exploitation trade-off.\n4. The experimental results show that DrM outperforms existing baselines in terms of sample efficiency and asymptotic performance across three continuous control benchmark environments.}\n\n$$\\boxed{\\text{Weaknesses:}  \n1. The paper relies heavily on empirical results, which may not generalize well to other environments or tasks.\n2. The authors do not provide a detailed analysis of the dormant ratio's theoretical underpinnings, which may limit the paper's interpretability and applicability.\n3. The paper does not discuss potential limitations or challenges of the proposed approach, such as the need for careful tuning of hyperparameters or the potential for overfitting.\n4. The paper does not provide a clear comparison of the proposed approach with other state-of-the-art visual RL algorithms.}\n\n$$\\boxed{\\text{Suggestions for Improvement:}  \n1. Provide a more detailed analysis of the dormant ratio's theoretical underpinnings to improve the paper's interpretability and applicability.\n2. Discuss potential limitations and challenges of the proposed approach, such as the need for careful tuning of hyperparameters or the potential for overfitting.\n3. Provide a clear comparison of the proposed approach with other state-of-the-art visual RL algorithms to demonstrate its superiority.\n4. Consider conducting additional experiments to validate the proposed approach in more diverse environments and tasks.}\n\n$$\\boxed{\\text{Recommendation:}  \nBased on the paper's contributions and experimental results, I recommend that the authors revise and resubmit the paper for further review. The proposed approach shows promise in addressing the sample efficiency challenge in visual RL, and the experimental results demonstrate its effectiveness across three continuous control benchmark environments. However, the paper's reliance on empirical results and lack of theoretical analysis and discussion of limitations may limit its impact and applicability. With revisions to address these concerns, the paper has the potential to make a significant contribution to the field of visual reinforcement learning.}$$",
        "peer_review_with_watermark": "$$\n$$\n\nPaper Summary:\nThe paper proposes a novel visual reinforcement learning (RL) algorithm, DrM, which aims to improve the sample efficiency and asymptotic performance of RL agents. DrM is designed to address the issue of sustained inactivity in early training phases, where agents often exhibit motorically inactive exploration. To address this, DrM introduces three core mechanisms: a periodical neural network weight perturbation mechanism, a dormant-ratio-based exploration scheduler, and a dormant-ratio-based exploitation mechanism. The paper presents empirical results showing that DrM outperforms existing baselines in terms of sample efficiency and asymptotic performance across three continuous control benchmark environments.\n\nStrengths:\n1. The paper provides a clear and concise overview of the problem of sustained inactivity in early training phases and its impact on RL agents.\n2. The introduction of the dormant ratio metric is a significant contribution, and its correlation with the diversity and significance of an agent's action is an interesting finding.\n3. The proposed DrM algorithm is easy to implement and computationally efficient.\n4. The paper presents extensive empirical results, including experiments on DeepMind Control Suite, MetaWorld, and Adroit environments.\n\nWeaknesses:\n1. The paper lacks a clear and detailed explanation of the theoretical framework underlying DrM.\n2. The explanation of the dormant-ratio-guided perturbation mechanism is not entirely clear and requires further clarification.\n3. The paper does not discuss potential biases and assumptions in the use of the dormant ratio metric.\n4. The experiments are extensive, but the paper could benefit from more detailed analysis and comparison of the results with existing baselines.\n\nSuggestions for Improvement:\n1. Provide a clear and concise explanation of the theoretical framework underlying DrM.\n2. Clarify the explanation of the dormant-ratio-guided perturbation mechanism.\n3. Conduct more detailed analysis and comparison of the results with existing baselines.\n4. Address potential biases and assumptions in the use of the dormant ratio metric.\n5. Consider exploring potential applications of DrM in discrete action tasks using DQN/Efficient Rainbow algorithms.\n\nRecommendation:\nBased on the paper's contributions and empirical results, I recommend that DrM be published in a reputable conference or journal. However, I also suggest that the authors address the areas of clarification and comparison outlined above to strengthen the paper and improve its overall quality.",
        "gr_score_with": 0.52046783625731,
        "gr_score_without": 0.3836317135549872,
        "or_score_with": 0.4298245614035088,
        "or_score_without": 0.5063938618925832,
        "output_without": {
            "num_tokens_scored": 391,
            "num_green_tokens": 150,
            "green_fraction": 0.3836317135549872,
            "num_orange_tokens": 198,
            "orange_fraction": 0.5063938618925832,
            "green_z_score": -0.6606721112122677,
            "orange_z_score": 110.57425148779392,
            "green_p_value": 0.7455886936478053,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 178,
            "green_fraction": 0.52046783625731,
            "num_orange_tokens": 147,
            "orange_fraction": 0.4298245614035088,
            "green_z_score": 4.547561092589983,
            "orange_z_score": 87.52592052903114,
            "green_p_value": 2.7135584527092137e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
        "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}  \nThe paper proposes a novel method for dynamically selecting layers in deep transformer networks during training, which aims to reduce the number of trainable parameters. The method employs reinforcement learning to learn a pattern of layer replication and tying, which facilitates weight sharing, reduces the number of trainable parameters, and serves as an effective regularization technique. The experimental evaluations validate that the proposed model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. \n\nStrengths: \n1. The proposed method is a novel approach to reducing the number of trainable parameters in deep transformer networks.\n2. The use of reinforcement learning to learn the pattern of layer replication and tying is a creative and effective solution.\n3. The experimental evaluations demonstrate the effectiveness of the proposed method in reducing the number of trainable parameters and maintaining the perplexity score.\n\nWeaknesses: \n1. The paper does not provide a clear explanation of the theoretical justification for the proposed method.\n2. The experimental evaluations are limited to a few datasets and may not be representative of the overall performance of the proposed method.\n3. The paper does not discuss the potential limitations and challenges of the proposed method, such as the need for large amounts of training data and computational resources.\n\nSuggestions for Improvement: \n1. The paper should provide a clear theoretical justification for the proposed method, including the underlying mathematical frameworks and mathematical proofs.\n2. The experimental evaluations should be expanded to include more datasets and evaluation metrics to provide a more comprehensive understanding of the performance of the proposed method.\n3. The paper should discuss the potential limitations and challenges of the proposed method, including the need for large amounts of training data and computational resources.\n\nRecommendation: \nBased on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a top-tier conference in machine learning. However, I also suggest that the authors address the weaknesses and provide a clear theoretical justification for the proposed method, as well as expand the experimental evaluations to include more datasets and evaluation metrics.",
        "peer_review_with_watermark": "$$\n\n$$$Paper Summary:\nThe paper presents a novel method for dynamically tying layers in transformer networks, which reduces the number of trainable parameters while maintaining the same level of accuracy. The method employs reinforcement learning to select layers to tie or untie during training, and the results show a modest improvement in perplexity and a drastic reduction in the number of trainable parameters. The paper also presents a detailed ablation study to validate the success of the method and explore its limitations.\n\n\nStrengths:\n\n1. The paper presents a novel and effective method for reducing the number of trainable parameters in transformer networks.\n2. The use of reinforcement learning to select layers to tie or untie during training is a clever approach.\n3. The paper presents a detailed ablation study to validate the success of the method and explore its limitations.\n4. The results show a modest improvement in perplexity and a drastic reduction in the number of trainable parameters.\n\n\nWeaknesses:\n\n1. The paper does not provide a thorough theoretical analysis of the method, which may be a limitation.\n2. The results show that the method does not always achieve the best perplexity score, which may be a limitation.\n3. The paper does not discuss the potential applications of the method in other areas, which may be a limitation.\n\n\nSuggestions for Improvement:\n\n1. The paper could benefit from a more thorough theoretical analysis of the method to understand its strengths and limitations.\n2. The results could be improved by providing more detailed analyses of the perplexity scores for different datasets and models.\n3. The paper could discuss the potential applications of the method in other areas, such as computer vision or natural language processing.\n\n\nRecommendation:\n\nI recommend accepting the paper for publication in its current form. The paper presents a novel and effective method for reducing the number of trainable parameters in transformer networks, and the results show a modest improvement in perplexity and a drastic reduction in the number of trainable parameters. The paper also presents a detailed ablation study to validate the success of the method and explore its limitations. With some minor improvements, the paper has the potential to make a significant impact in the field of natural language processing.\n\n\n$$\n\n$$\n\n$$$Note: The review is written in a standard conference review style, which is concise and to the point. The review highlights the strengths and weaknesses of the paper, provides suggestions for improvement, and makes a recommendation for publication. The review is written in a clear and objective manner, without any personal opinions or biases.",
        "gr_score_with": 0.6702508960573477,
        "gr_score_without": 0.5350553505535055,
        "or_score_with": 0.25448028673835127,
        "or_score_without": 0.33579335793357934,
        "output_without": {
            "num_tokens_scored": 271,
            "num_green_tokens": 145,
            "green_fraction": 0.5350553505535055,
            "num_orange_tokens": 91,
            "orange_fraction": 0.33579335793357934,
            "green_z_score": 4.5382751083875,
            "orange_z_score": 65.59691712439441,
            "green_p_value": 2.835811901096324e-06,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 279,
            "num_green_tokens": 187,
            "green_fraction": 0.6702508960573477,
            "num_orange_tokens": 71,
            "orange_fraction": 0.25448028673835127,
            "green_z_score": 9.214327060508364,
            "orange_z_score": 50.10471949551679,
            "green_p_value": 1.5661843825283326e-20,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
        "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
        "peer_review_without_watermark": "$$$${Paper Summary:}\nThe paper presents a novel approach to neural subset selection, which aims to learn a conditional distribution P(Y|S,V) that adheres to permutation invariance. The authors introduce a theoretical framework and propose a model called INSET, which incorporates an information aggregation module to merge the representations of subsets and supersets from a permutation invariance perspective. The paper presents empirical evaluations across diverse tasks and datasets, demonstrating the effectiveness of INSET over conventional methods.\n\nStrengths:\n- The paper presents a clear and concise theoretical framework for permutation invariant neural subset selection.\n- The authors introduce a novel approach to incorporating background information from the superset into the subset representations.\n- The empirical evaluations demonstrate the effectiveness of INSET over conventional methods.\n\nWeaknesses:\n- The paper may benefit from a more detailed explanation of the technical details of the INSET model.\n- The authors assume that the reader is familiar with the concept of permutation invariance and its applications in set-based tasks.\n- The paper does not provide a thorough comparison of the proposed method with other state-of-the-art approaches.\n\nSuggestions for Improvement:\n- Provide more technical details about the INSET model, such as the architecture of the information aggregation module and the choice of hyperparameters.\n- Include a more detailed comparison with other state-of-the-art approaches to demonstrate the novelty and effectiveness of INSET.\n- Consider adding more experiments to evaluate the robustness of INSET to different datasets and tasks.\n\nRecommendation:\n- The paper presents a novel and theoretically driven approach to neural subset selection, which demonstrates its effectiveness over conventional methods.\n- The authors should continue to improve the paper by providing more technical details and a more comprehensive comparison with other state-of-the-art approaches.\n\n$$${Paper Summary:}\nThe paper presents a novel approach to neural subset selection, which aims to learn a conditional distribution P(Y|S,V) that adheres to permutation invariance. The authors introduce a theoretical framework and propose a model called INSET, which incorporates an information aggregation module to merge the representations of subsets and supersets from a permutation invariance perspective. The paper presents empirical evaluations across diverse tasks and datasets, demonstrating the effectiveness of INSET over conventional methods.\n\nStrengths:\n- The paper presents a clear and concise theoretical framework for permutation invariant neural subset selection.\n- The authors introduce a novel approach to incorporating background information from the superset into the subset representations.\n- The empirical evaluations demonstrate the effectiveness of INSET over conventional methods.\n\nWeaknesses:\n- The paper may benefit from a more detailed explanation of the technical details of the INSET model.\n- The authors assume that the reader is familiar with the concept of permutation invariance and its applications in set-based tasks.\n- The paper does not provide a thorough comparison of the proposed method with other state-of-the-art approaches.\n\nSuggestions for Improvement:\n- Provide more technical details about the INSET model, such as the architecture of the information aggregation module and the choice of hyperparameters.\n- Include a more detailed comparison with other state-of-the-art approaches to demonstrate the novelty and effectiveness of INSET.\n- Consider adding more experiments to evaluate the robustness of INSET to different datasets and tasks.\n\nRecommendation:\n- The paper presents a novel and theoretically driven approach to neural subset selection, which demonstrates its effectiveness over conventional methods.\n- The authors should continue to improve the paper by providing more technical details and a more comprehensive comparison with other state-of-the-art approaches.",
        "peer_review_with_watermark": "$$$\\label{review}$\nThe paper proposes a new approach for neural subset selection, INSET, which incorporates the concept of permutation invariance to model a conditional distribution $P(Y | S, V)$ that adheres to a specific property. The INSET model is designed to capture the information from both the subset $S$ and the superset $V$ in a probabilistic manner.\n\n**Paper Summary:**\n\nThe paper starts by introducing the problem of neural subset selection, where the goal is to select the optimal subset from a given set of options. The authors propose a new approach to model the conditional distribution $P(Y | S, V)$ that captures the information from both the subset $S$ and the superset $V$. The model is designed to be permutation invariant, which ensures that the output remains the same under any permutation of the elements in $S$ and $V$.\n\n**Strengths:**\n\n1. The paper proposes a new approach to neural subset selection that incorporates permutation invariance.\n2. The INSET model is designed to capture the information from both the subset $S$ and the superset $V$ in a probabilistic manner.\n3. The model is permutation invariant, which ensures that the output remains the same under any permutation of the elements in $S$ and $V$.\n\n**Weaknesses:**\n\n1. The paper does not provide a clear motivation for why permutation invariance is necessary in neural subset selection.\n2. The INSET model is not compared to other state-of-the-art approaches in the field.\n3. The paper does not provide a clear analysis of the time and space complexity of the INSET model.\n\n**Suggestions for Improvement:**\n\n1. The paper should provide a clear motivation for why permutation invariance is necessary in neural subset selection.\n2. The INSET model should be compared to other state-of-the-art approaches in the field to demonstrate its superiority.\n3. The paper should provide a clear analysis of the time and space complexity of the INSET model.\n\n**Recommendation:**\n\nOverall, the paper proposes a new approach to neural subset selection that incorporates permutation invariance. The INSET model is designed to capture the information from both the subset $S$ and the superset $V$ in a probabilistic manner. However, the paper needs to address the limitations and provide a clear motivation for why permutation invariance is necessary in neural subset selection. Additionally, the INSET model should be compared to other state-of-the-art approaches in the field to demonstrate its superiority.\n\n $$$\n\\label{recommendation}\n\nThe INSET model is a good start, but it needs further development to address the limitations and provide a clear motivation for why permutation invariance is necessary in neural subset selection. The model should be compared to other state-of-the-art approaches in the field to demonstrate its superiority. Additionally, the time and space complexity of the INSET model needs to be analyzed. With further development, the INSET model has the potential to become a state-of-the-art approach in neural subset selection.",
        "gr_score_with": 0.5111940298507462,
        "gr_score_without": 0.328,
        "or_score_with": 0.44776119402985076,
        "or_score_without": 0.556,
        "output_without": {
            "num_tokens_scored": 250,
            "num_green_tokens": 82,
            "green_fraction": 0.328,
            "num_orange_tokens": 139,
            "orange_fraction": 0.556,
            "green_z_score": -2.32379000772445,
            "orange_z_score": 99.41439444904583,
            "green_p_value": 0.9899316242248268,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 268,
            "num_green_tokens": 137,
            "green_fraction": 0.5111940298507462,
            "num_orange_tokens": 120,
            "orange_fraction": 0.44776119402985076,
            "green_z_score": 3.7157222770009324,
            "orange_z_score": 82.61300777537407,
            "green_p_value": 0.00010131202388657579,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
        "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$The paper proposes a novel batch normalization method called Temporal Accumulated Batch Normalization (TAB) for Spiking Neural Networks (SNNs). The authors aim to address the challenge of directly training SNNs due to the Temporal Covariate Shift (TCS) phenomenon and the temporal dependency of neuron dynamics. \n\nThe authors provide a comprehensive overview of the existing batch normalization methods for SNNs, highlighting their limitations. They then introduce the TAB method, which normalizes data using temporal accumulated statistics, effectively capturing historical temporal dependencies similar to that of the accumulation process of the membrane potential in the LIF neuron model. The alignment with the neuron dynamics means that the TAB method is tailored to mimic or capture the behavior of neurons as closely as possible.\n\nThe authors conduct extensive experiments on large-scale static and neuromorphic datasets, including CIFAR-10, CIFAR-100, and DVS-CIFAR10. The results demonstrate the effectiveness of the TAB method, achieving state-of-the-art performance on various benchmark datasets. The authors compare the performance of the TAB method with other state-of-the-art methods, including TEBN and surrogate gradient methods, and show that TAB outperforms them in terms of accuracy and robustness.\n\nThe paper makes a significant contribution to the field of SNNs by introducing a novel batch normalization method that addresses the challenges of directly training SNNs. The authors' work provides a promising approach for improving the performance and robustness of SNNs in various applications.\n\nStrengths:\n\n* The authors provide a comprehensive overview of the existing batch normalization methods for SNNs and highlight their limitations.\n* The TAB method is introduced and its effectiveness is demonstrated through extensive experiments.\n* The paper makes a significant contribution to the field of SNNs by introducing a novel batch normalization method that addresses the challenges of directly training SNNs.\n\nWeaknesses:\n\n* The paper may benefit from more detailed explanations of the mathematical derivations and the theoretical connection between the TAB method and the neuron dynamics.\n* The authors may consider providing more information about the computational complexity of the TAB method and its scalability for large-scale datasets.\n* The paper may benefit from more comparisons with other state-of-the-art methods, including those that use different optimization algorithms or architectures.\n\nSuggestions for improvement:\n\n* Provide more detailed explanations of the mathematical derivations and the theoretical connection between the TAB method and the neuron dynamics.\n* Consider providing more information about the computational complexity of the TAB method and its scalability for large-scale datasets.\n* Conduct more comprehensive comparisons with other state-of-the-art methods, including those that use different optimization algorithms or architectures.\n* Consider providing more information about the application of the TAB method in real-world scenarios and its potential for improving the performance and robustness of SNNs in various applications.\n\nRecommendation:\n\n* Based on the thorough review, I recommend that the paper be accepted for publication in a top-tier conference or journal in the field of SNNs and artificial intelligence.\n* The authors should consider addressing the suggestions for improvement and providing more detailed explanations of the mathematical derivations and the theoretical connection between the TAB method and the neuron dynamics.\n* The authors should also consider conducting more comprehensive comparisons with other state-of-the-art methods and providing more information about the application of the TAB method in real-world scenarios.",
        "peer_review_with_watermark": "$$$\\boxed{\\text{Summary: The paper proposes a novel batch normalization method for Spiking Neural Networks (SNNs) called Temporal Accumulated Batch Normalization (TAB). The method addresses the temporal covariate shift issue by aligning with the neuron dynamics, specifically the accumulated membrane potential, and utilizing temporal accumulated statistics for data normalization. Experimental results show that the TAB method outperforms other state-of-the-art methods on various datasets.}}\n\n$\\boxed{\\text{Strengths: The paper highlights the challenges of directly training SNNs due to the temporal covariate shift phenomenon and the intrinsic temporal dependency of neuron dynamics. The proposed TAB method is a significant improvement over existing batch normalization methods for SNNs, as it closely aligns with the neuron dynamics and effectively captures historical temporal dependencies. The experimental results demonstrate the effectiveness of the TAB method on various datasets.}}\n\n$\\boxed{\\text{Weaknesses: The paper does not provide a detailed analysis of the computational complexity of the TAB method. Additionally, the paper assumes that the temporal accumulated statistics are computed using a moving average approach, which may not be the only possible method. The paper also does not provide a detailed comparison of the TAB method with other batch normalization methods for SNNs in terms of their computational overhead.}}\n\n$\\boxed{\\text{Suggestions for Improvement: The paper could benefit from a more detailed analysis of the computational complexity of the TAB method. Additionally, the paper could explore alternative methods for computing temporal accumulated statistics, such as using a more aggressive moving average approach or utilizing more advanced temporal dependencies. The paper could also provide a more detailed comparison of the TAB method with other batch normalization methods for SNNs in terms of their computational overhead.}}\n\n$\\boxed{\\text{Recommendation: The proposed TAB method is a significant improvement over existing batch normalization methods for SNNs, and it has the potential to improve the performance of SNNs on various datasets. However, further analysis and comparison of the TAB method with other batch normalization methods are necessary to fully evaluate its potential.}}$$",
        "gr_score_with": 0.4708171206225681,
        "gr_score_without": 0.4318766066838046,
        "or_score_with": 0.48249027237354086,
        "or_score_without": 0.4755784061696658,
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 168,
            "green_fraction": 0.4318766066838046,
            "num_orange_tokens": 185,
            "orange_fraction": 0.4755784061696658,
            "green_z_score": 1.283338619381715,
            "orange_z_score": 115.31593639906446,
            "green_p_value": 0.09968673405735945,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 257,
            "num_green_tokens": 121,
            "green_fraction": 0.4708171206225681,
            "num_orange_tokens": 124,
            "orange_fraction": 0.48249027237354086,
            "green_z_score": 2.3173904103275462,
            "orange_z_score": 95.11159136935002,
            "green_p_value": 0.01024123712918965,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs). USF is based on the exponential integral formulation and allows for free choices of solver strategy at each step. The framework is designed to study the optimal solving strategies for DPMs and provides a systematic way to search for the best solver schedules. The authors also propose a predictor-based multi-stage search algorithm (S3) to search for the optimal solver schedules under a certain number of function evaluations (NFE) budget.\n\nStrengths:\n1. The proposed framework is well-structured and easy to follow, with a clear explanation of the mathematical formulation and the search algorithm.\n2. The authors provide a thorough analysis of the existing solvers and highlight the limitations of the current methods.\n3. The proposed framework allows for the incorporation of existing diffusion solvers and provides a flexible way to search for the optimal solver schedules.\n4. The authors provide a thorough evaluation of the proposed method on several datasets and compare it with the state-of-the-art methods.\n\nWeaknesses:\n1. The paper assumes a good understanding of the mathematical formulation of DPMs and the exponential integral. Readers who are not familiar with these concepts may find the paper challenging to follow.\n2. The search algorithm (S3) may not be efficient for large search spaces, and the authors may need to provide more detailed analysis of its performance.\n3. The paper does not provide a comprehensive analysis of the computational complexity of the proposed method.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the mathematical formulation of the proposed framework and the search algorithm.\n2. The authors should provide a more comprehensive analysis of the computational complexity of the proposed method.\n3. The authors should provide more detailed results on the performance of the proposed method on different datasets and compare it with the state-of-the-art methods.\n\nRecommendation:\nBased on the overall quality of the paper, I recommend it for publication in a top-tier conference or journal in the field of machine learning. The authors have made a significant contribution to the field of DPMs and have proposed a novel framework that allows for systematic study of solving strategies. The paper provides a thorough analysis of the existing methods and highlights the limitations of the current approaches. The proposed framework is well-structured and easy to follow, and the authors have provided a thorough evaluation of the method on several datasets. However, the authors should address the weaknesses mentioned above and provide more detailed analysis of the mathematical formulation and the search algorithm.",
        "peer_review_with_watermark": "$$\\begin{gathered}\n\\text{Paper Summary:} \\\\\n\\text{The paper proposes a unified sampling framework (USF) to study the optimal strategies for solver search in diffusion probabilistic models (DPMs). The framework allows for the choice of different solving strategies at each timestep, enabling the improvement of sample quality. The proposed framework is demonstrated to be effective in various datasets, including CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom.}\\\\\n\\text{Strengths:} \\\\\n\\text{The framework is well-organized, and the author presents a clear explanation of the mathematical concepts. The experimental results show promising performance of the proposed framework.}\\\\\n\\text{Weaknesses:} \\\\\n\\text{The search space of the framework is very large, which may lead to high computational costs.}\\\\\n\\text{Suggestions for Improvement:} \\\\\n\\text{The author should consider applying more efficient search methods to reduce computational costs.}\\\\\n\\text{Recommendation:} \\\\\n\\text{The proposed framework shows promising results, and its effectiveness in various datasets suggests that it can be a useful tool in the field of DPMs.}\\\\\n\\end{gathered}$$",
        "gr_score_with": 0.6073298429319371,
        "gr_score_without": 0.36990595611285265,
        "or_score_with": 0.33507853403141363,
        "or_score_without": 0.5203761755485894,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 118,
            "green_fraction": 0.36990595611285265,
            "num_orange_tokens": 166,
            "orange_fraction": 0.5203761755485894,
            "green_z_score": -1.0971607701179875,
            "orange_z_score": 114.75091290524777,
            "green_p_value": 0.8637144405837357,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 191,
            "num_green_tokens": 116,
            "green_fraction": 0.6073298429319371,
            "num_orange_tokens": 64,
            "orange_fraction": 0.33507853403141363,
            "green_z_score": 5.848882293547171,
            "orange_z_score": 56.778860334781804,
            "green_p_value": 2.4744356104863575e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES",
        "abstract": "We study principal-agent problems in which a principal commits to an outcomedependent payment scheme\u2014called contract\u2014in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent\u2019s action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. (2023). Moreover, it can also be employed to provide a \u00d5(T ) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n&\\text{Paper Summary: The paper studies principal-agent problems where a principal commits to a payment scheme to incentivize an agent to take a costly action. The paper focuses on a generalization of the classical single-round version of the problem to multiple rounds. The goal is to design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space.}\\\\\n&\\text{The authors provide an algorithm called Discover-and-Cover that learns an approximately-optimal bounded contract. The algorithm builds a set of meta-actions, where each meta-action identifies a set of contracts in which a given action is a best response. The algorithm progressively refines upper and lower bounds for such regions, until they coincide.}\\\\\n&\\text{The authors show that the algorithm has a sample complexity of learning optimal contracts in hidden-action principal-agent settings. The algorithm is also able to provide a regret bound in the related online learning setting.}\\\\\n&\\text{The paper also discusses the relation to repeated Stackelberg games and the assumptions relaxed compared to Stackelberg games.}\\\\\n&\\text{The authors provide a detailed proof of the algorithm and its guarantees. The proof is divided into several sections, each focusing on a specific aspect of the algorithm.}\\\\\n&\\text{The paper also provides a related work section, where the authors discuss previous works on principal-agent problems and learning in principal-agent problems.}\\\\\n&\\text{The authors conclude the paper by stating that their algorithm can be exploited to achieve a regret bound whose dependence on the number of rounds T is of the order of O\u0303(T 4/5), when the number of agent\u2019s actions n is constant.}\\\\\n&\\text{Strengths: The paper provides a detailed analysis of the Discover-and-Cover algorithm and its guarantees. The authors also discuss the relation to repeated Stackelberg games and the assumptions relaxed compared to Stackelberg games. The paper also provides a related work section, which is useful for readers interested in the topic.}\\\\\n&\\text{Weaknesses: The paper assumes that the agent has limited liability, which may not be the case in reality. The authors also assume that the principal can only observe the outcome realized by the agent\u2019s action, which may not be the case in some situations.}\\\\\n&\\text{Suggestions for Improvement: The authors could improve the paper by discussing the implications of the assumptions made in the paper. They could also provide more details on the algorithm and its guarantees. Additionally, they could discuss the limitations of the paper and potential future directions.}\\\\\n&\\text{Recommendation: Based on the review, I recommend that the authors revise and resubmit the paper. The authors have provided a detailed analysis of the Discover-and-Cover algorithm and its guarantees. The paper also discusses the relation to repeated Stackelberg games and the assumptions relaxed compared to Stackelberg games. The authors have also provided a related work section, which is useful for readers interested in the topic.}\\\\\n&\\text{Rating: 8/10}\\\\\n\\end{aligned}$$",
        "peer_review_with_watermark": "$$\\begin{aligned} $$$\\begin{array}{ll} \\text { Paper }&:\\text { LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION } \\\\  & \\text { Spaces } \\hfill \\\\  & {\\text { Title}}:\\text { LEARNING OPTIMAL CONTRACTS: HOW TO } \\\\  & {\\text { EXPLOIT SMALL ACTION } \\text { Spaces }} \\hfill \\\\  & {\\text { Authors}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Affiliations}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Date}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text { DOI}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {MS } }:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Keywords}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Abbreviated } }:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Conference} }:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Year}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Vol} }:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {Pages}}:\\text { {\\text{}} } \\hfill \\\\  & {\\text {ISSN}}:\\text { {\\text{}}",
        "gr_score_with": 0.6853932584269663,
        "gr_score_without": 0.32142857142857145,
        "or_score_with": 0.1797752808988764,
        "or_score_without": 0.5841836734693877,
        "output_without": {
            "num_tokens_scored": 392,
            "num_green_tokens": 126,
            "green_fraction": 0.32142857142857145,
            "num_orange_tokens": 229,
            "orange_fraction": 0.5841836734693877,
            "green_z_score": -3.1754264805429426,
            "orange_z_score": 122.12389638367031,
            "green_p_value": 0.9992519178551272,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 89,
            "num_green_tokens": 61,
            "green_fraction": 0.6853932584269663,
            "num_orange_tokens": 16,
            "orange_fraction": 0.1797752808988764,
            "green_z_score": 5.495827494580526,
            "orange_z_score": 17.2928007300445,
            "green_p_value": 1.944412586426614e-08,
            "orange_p_value": 2.664928859694584e-67
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper proposes a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based denoising diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The approach addresses challenges arising from data gaps and the complexity of generalizing knowledge across cities, outperforming state-of-the-art baselines on multiple real-world datasets.\n\nStrengths:\n1. The paper introduces a novel approach to spatio-temporal few-shot learning, addressing the challenges posed by data scarcity and heterogeneity in smart city applications.\n2. The proposed framework, GPD, leverages a Transformer-based denoising diffusion model to generate tailored neural networks, demonstrating adaptability to diverse data distributions and city-specific characteristics.\n3. The approach is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n\nWeaknesses:\n1. The paper lacks a detailed analysis of the theoretical foundations of the proposed framework, particularly the mathematical formulation of the denoising diffusion model.\n2. The experimental results, while promising, are limited to a small number of datasets and may not generalize to other scenarios.\n3. The paper does not provide a comprehensive comparison with other state-of-the-art approaches, such as meta-learning and few-shot learning methods.\n\nSuggestions for Improvement:\n1. The authors should provide a detailed theoretical analysis of the proposed framework, including the mathematical formulation of the denoising diffusion model and its implications for spatio-temporal few-shot learning.\n2. The experimental results should be replicated on a larger number of datasets and compared with other state-of-the-art approaches to establish the generalizability and robustness of the proposed framework.\n3. The authors should investigate the effect of different prompt selection methods on the performance of the proposed framework.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, we recommend that the authors revise and resubmit the manuscript for peer review. Specifically, we suggest that the authors provide a detailed theoretical analysis of the proposed framework, replicate the experimental results on a larger number of datasets, and compare the performance of the proposed framework with other state-of-the-art approaches. With these revisions, the paper has the potential to make a significant contribution to the field of spatio-temporal few-shot learning and urban transfer learning. \n\n$$",
        "peer_review_with_watermark": "$$\n\\begin{align*}\n\\textbf{Paper Summary:}\n\\text{The proposed framework, GPD, is a generative pre-training method for spatio-temporal few-shot learning. It addresses the challenges of data scarcity and heterogeneity in smart city applications. The framework leverages pre-trained models and a transformer-based diffusion model to generate tailored neural networks for spatio-temporal prediction tasks.}\n\n\\textbf{Strengths:}\n1. \\textbf{Novel Approach: } The proposed framework is a pioneering generative pre-training method for spatio-temporal few-shot learning, which addresses the challenges of data scarcity and heterogeneity in smart city applications.\n2. \\textbf{Improved Performance: } The framework consistently achieves superior performance compared with baseline approaches on various spatio-temporal prediction tasks.\n3. \\textbf{Model-Agnostic: } The framework is model-agnostic, ensuring compatibility with existing urban computing models.\n4. \\textbf{Flexibility: } The framework is designed to be flexible, allowing for the integration of different spatio-temporal models and the exploration of various conditioning strategies.\n\n\\textbf{Weaknesses:}\n1. \\textbf{Limited Experimentation: } The experimental results are primarily based on four datasets, which may not be representative of the broader scope of spatio-temporal prediction tasks.\n2. \\textbf{Limited Analysis: } The analysis of the conditioning strategies and prompt selection is limited, which may not provide a comprehensive understanding of the framework's performance.\n3. \\textbf{Potential Overfitting: } The framework may be prone to overfitting, especially when the number of source cities is limited.\n4. \\textbf{Limited Generalization: } The framework may not generalize well to new, unseen cities or scenarios.\n\n\\textbf{Suggestions for Improvement:}\n1. \\textbf{Increase Experimentation: } Experiment with more datasets and scenarios to evaluate the framework's performance in broader contexts.\n2. \\textbf{Deeper Analysis: } Perform a more in-depth analysis of the conditioning strategies and prompt selection to provide a comprehensive understanding of the framework's performance.\n3. \\textbf{Regularization Techniques: } Incorporate regularization techniques to prevent overfitting.\n4. \\textbf{Transfer Learning: } Evaluate the framework's ability to transfer knowledge to new cities or scenarios using transfer learning techniques.\n\n\\textbf{Recommendation:}\nBased on the strengths and weaknesses of the proposed framework, we recommend that the authors continue to refine and expand the framework to address the challenges of data scarcity and heterogeneity in smart city applications. The framework has the potential to revolutionize spatio-temporal prediction tasks, but further experimentation and analysis are necessary to fully realize its potential.\n$$\n$$",
        "gr_score_with": 0.5373563218390804,
        "gr_score_without": 0.363914373088685,
        "or_score_with": 0.3620689655172414,
        "or_score_without": 0.5229357798165137,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 119,
            "green_fraction": 0.363914373088685,
            "num_orange_tokens": 171,
            "orange_fraction": 0.5229357798165137,
            "green_z_score": -1.3319947409079074,
            "orange_z_score": 113.57971489987878,
            "green_p_value": 0.9085690416607723,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 348,
            "num_green_tokens": 187,
            "green_fraction": 0.5373563218390804,
            "num_orange_tokens": 126,
            "orange_fraction": 0.3620689655172414,
            "green_z_score": 5.23037290891228,
            "orange_z_score": 80.65109032962806,
            "green_p_value": 8.45842219220031e-08,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "HAZARD CHALLENGE: EMBODIED DECISION MAK-",
        "abstract": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire , flood , and wind , and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents\u2019 decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://sites.google. com/view/hazard-challenge.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces the HAZARD challenge, a novel benchmark for evaluating embodied agents' decision-making capabilities in dynamically changing environments. The challenge consists of three unexpected disaster scenarios (fire, flood, and wind) and requires agents to rescue valuable items from these continuously evolving circumstances. The authors develop a simulation system on top of the ThreeDWorld platform, which enables simulations of fire, flood, and wind scenarios. They also introduce an LLM-based agent and evaluate its performance using the generated dataset.\n\nStrengths:\n\n* The HAZARD challenge provides a unique and challenging environment for embodied agents to demonstrate their decision-making capabilities.\n* The use of a simulation system on top of the ThreeDWorld platform enables realistic and dynamic simulations of fire, flood, and wind scenarios.\n* The introduction of an LLM-based agent and its evaluation provide valuable insights into the potential of large language models in embodied AI tasks.\n\nWeaknesses:\n\n* The paper does not provide a thorough analysis of the limitations and challenges of the HAZARD challenge.\n* The evaluation of the LLM-based agent is limited to a single experiment, and the results may not be generalizable to other scenarios.\n* The paper does not provide a detailed explanation of the procedural generation pipeline and its impact on the generated dataset.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the limitations and challenges of the HAZARD challenge, including potential sources of bias and limitations of the simulation system.\n* The evaluation of the LLM-based agent should be extended to multiple experiments and scenarios to demonstrate its robustness and generalizability.\n* The authors should provide a more detailed explanation of the procedural generation pipeline and its impact on the generated dataset, including any potential biases or limitations.\n\nRecommendation:\n\n* The HAZARD challenge is a valuable contribution to the field of embodied AI and provides a unique and challenging environment for embodied agents to demonstrate their decision-making capabilities.\n* The use of a simulation system on top of the ThreeDWorld platform and the introduction of an LLM-based agent provide valuable insights into the potential of large language models in embodied AI tasks.\n* However, the paper should be improved by providing a more detailed analysis of the limitations and challenges of the HAZARD challenge, extending the evaluation of the LLM-based agent to multiple experiments and scenarios, and providing a more detailed explanation of the procedural generation pipeline.",
        "peer_review_with_watermark": "$$\n\\begin{aligned}\n\\text{Paper Summary:}\n \nThe paper proposes a new simulated embodied benchmark, called HAZARD, to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios (fire, flood, and wind) and supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. The benchmark enables evaluation of autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. The authors develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.\n\n\\text{Strengths:}\n \n1. The paper introduces a new and innovative benchmark (HAZARD) to assess embodied decision-making in dynamic environments.\n2. The benchmark supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making.\n3. The paper presents a comprehensive evaluation of various baseline methods (random, rule-based, MCTS, and greedy) and an LLM-based agent.\n4. The authors provide a detailed analysis of the performance of the LLM-based agent and its decision-making capabilities.\n\n\\text{Weaknesses:}\n \n1. The paper lacks a clear definition of the decision-making problem and the evaluation metrics.\n2. The benchmark may be limited to specific environments and scenarios, which may not be representative of real-world situations.\n3. The paper does not provide a detailed analysis of the challenges faced by the LLM-based agent in handling complex decision-making tasks.\n4. The results may be influenced by the specific choice of LLM and evaluation metrics.\n\n\\text{Suggestions for Improvement:}\n \n1. The paper should provide a clear definition of the decision-making problem and the evaluation metrics to ensure that the benchmark is well-defined and comparable across different methods.\n2. The benchmark should be extended to include more diverse environments and scenarios to improve its representativeness of real-world situations.\n3. The paper should provide a more detailed analysis of the challenges faced by the LLM-based agent in handling complex decision-making tasks.\n4. The results should be replicated using different LLMs and evaluation metrics to improve the robustness and generalizability of the findings.\n\n\\text{Recommendation:}\n \nBased on the paper's contribution to the field of embodied decision-making and its potential impact on the development of more advanced and robust decision-making algorithms, I recommend that the paper be published in a top-tier conference or journal in the field of artificial intelligence or robotics. The paper's innovative benchmark and comprehensive evaluation of various baseline methods make it a valuable contribution to the field, and its potential to advance the state-of-the-art in embodied decision-making is significant.",
        "gr_score_with": 0.5896739130434783,
        "gr_score_without": 0.5261194029850746,
        "or_score_with": 0.32608695652173914,
        "or_score_without": 0.35447761194029853,
        "output_without": {
            "num_tokens_scored": 268,
            "num_green_tokens": 141,
            "green_fraction": 0.5261194029850746,
            "num_orange_tokens": 95,
            "orange_fraction": 0.35447761194029853,
            "green_z_score": 4.214476945054749,
            "orange_z_score": 69.71922285279096,
            "green_p_value": 1.251788619455647e-05,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 217,
            "green_fraction": 0.5896739130434783,
            "num_orange_tokens": 120,
            "orange_fraction": 0.32608695652173914,
            "green_z_score": 7.42721323087008,
            "orange_z_score": 75.02796143334801,
            "green_p_value": 5.545474617958739e-14,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
        "abstract": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
        "peer_review_without_watermark": "$$$\\boxed{Summary:}\nThe paper proposes a novel formulation of retrosynthesis in terms of stochastic processes to account for uncertainty in the space of possible reactions. The authors present a novel greedy algorithm called retro-fallback, which maximizes the probability that at least one synthesis plan can be executed in the lab. The authors demonstrate that retro-fallback outperforms existing algorithms on an in-silico benchmark.\n\nStrengths:\n\n* The paper presents a novel and innovative approach to addressing the uncertainty in retrosynthesis.\n* The authors provide a clear and well-organized presentation of their work.\n* The paper includes a thorough experimental evaluation of retro-fallback and its comparison to existing algorithms.\n\nWeaknesses:\n\n* The paper assumes that the reader is familiar with the concepts of stochastic processes and retrosynthesis, which may not be the case for all readers.\n* The paper does not provide a comprehensive literature review of existing work on retrosynthesis and stochastic processes.\n* The experimental evaluation is limited to a single benchmark and may not be representative of the broader applicability of retro-fallback.\n\nSuggestions for Improvement:\n\n* The paper could benefit from a more detailed literature review of existing work on retrosynthesis and stochastic processes.\n* The experimental evaluation could be expanded to include more benchmarks and a more comprehensive analysis of the results.\n* The paper could provide more insight into the theoretical guarantees and limitations of retro-fallback.\n\nRecommendation:\n\n* The paper is well-written and presents a clear and innovative approach to addressing the uncertainty in retrosynthesis.\n* The experimental evaluation is thorough and demonstrates the effectiveness of retro-fallback.\n* The paper could benefit from a more detailed literature review and a more comprehensive experimental evaluation.\n\nRating: 8/10\n\n$$\\boxed{Paper Summary:}\nThe paper proposes a novel formulation of retrosynthesis in terms of stochastic processes to account for uncertainty in the space of possible reactions. The authors present a novel greedy algorithm called retro-fallback, which maximizes the probability that at least one synthesis plan can be executed in the lab. The authors demonstrate that retro-fallback outperforms existing algorithms on an in-silico benchmark.\n\n$$\\boxed{Strengths:}\nThe paper presents a novel and innovative approach to addressing the uncertainty in retrosynthesis.\n\n$$\\boxed{Weaknesses:}\nThe paper assumes that the reader is familiar with the concepts of stochastic processes and retrosynthesis.\n\n$$\\boxed{Suggestions for Improvement:}\nThe paper could benefit from a more detailed literature review and a more comprehensive experimental evaluation.\n\n$$\\boxed{Recommendation:}\nThe paper is well-written and presents a clear and innovative approach to addressing the uncertainty in retrosynthesis.",
        "peer_review_with_watermark": "$$$\\nabla\n\nPaper Summary:\n\nThe paper proposes a novel approach to retrosynthesis, a task in organic chemistry that involves proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. The authors introduce a new formulation of retrosynthesis using stochastic processes to account for uncertainty in the space of possible reactions. They also propose a novel greedy algorithm, retro-fallback, to maximize the successful synthesis probability (SSP), which measures the probability that at least one synthesis plan can be executed in the lab. The authors evaluate retro-fallback against existing algorithms, including breadth-first search, retro*, and Monte Carlo tree search, and show that it outperforms them in maximizing SSP.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of retrosynthesis and the challenges of using stochastic processes to account for uncertainty.\n2. The authors propose a novel and well-defined approach to maximizing SSP, which is a key metric for evaluating retrosynthesis algorithms.\n3. The retro-fallback algorithm is presented in detail, and the authors provide a clear explanation of its operation and its relationship to the SSP metric.\n4. The paper includes a thorough evaluation of retro-fallback against existing algorithms, which provides a solid foundation for comparing its performance.\n\nWeaknesses:\n\n1. The paper assumes that the reader has a strong background in stochastic processes and retrosynthesis, which may make it difficult for non-experts to follow.\n2. Some of the notation and terminology used in the paper, such as the \"binarized view\" and \"successful synthesis probability,\" may be unfamiliar to readers without a strong foundation in stochastic processes.\n3. The paper could benefit from more explicit explanations of the underlying mathematics and assumptions used in the stochastic processes and the retro-fallback algorithm.\n4. The evaluation of retro-fallback against existing algorithms is limited to a specific set of benchmark molecules, which may not be representative of the broader scope of retrosynthesis.\n\nSuggestions for Improvement:\n\n1. Consider adding more explicit explanations of the underlying mathematics and assumptions used in the stochastic processes and the retro-fallback algorithm.\n2. Provide more context and explanations for the notation and terminology used in the paper.\n3. Consider including more explicit discussions of the limitations and potential biases of the evaluation against existing algorithms.\n4. Consider adding more explicit explanations of the relationship between the retro-fallback algorithm and the SSP metric, including the underlying assumptions and mathematical formulations used to derive the algorithm.\n\nRecommendation:\n\nBased on the strengths and weaknesses of the paper, I would recommend acceptance. The paper provides a clear and concise introduction to the problem of retrosynthesis and the challenges of using stochastic processes to account for uncertainty. The authors propose a novel and well-defined approach to maximizing SSP, and the retro-fallback algorithm is presented in detail. While the paper assumes a strong background in stochastic processes and retrosynthesis, the authors provide a thorough evaluation of retro-fallback against existing algorithms, which provides a solid foundation for comparing its performance. With some additional explicit explanations and discussions, the paper could be even stronger.",
        "gr_score_with": 0.5695538057742782,
        "gr_score_without": 0.42857142857142855,
        "or_score_with": 0.37270341207349084,
        "or_score_without": 0.44047619047619047,
        "output_without": {
            "num_tokens_scored": 252,
            "num_green_tokens": 108,
            "green_fraction": 0.42857142857142855,
            "num_orange_tokens": 111,
            "orange_fraction": 0.44047619047619047,
            "green_z_score": 0.92582009977255,
            "orange_z_score": 54.75767326908124,
            "green_p_value": 0.17726973988675104,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 381,
            "num_green_tokens": 217,
            "green_fraction": 0.5695538057742782,
            "num_orange_tokens": 142,
            "orange_fraction": 0.37270341207349084,
            "green_z_score": 6.755607501987506,
            "orange_z_score": 56.59437514110109,
            "green_p_value": 7.111899848049189e-12,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
        "abstract": "Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input. TUVF provides a category-level texture representation disentangled from 3D shapes. Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning. Note that all samples are rendered under 1024\u00d71024 resolution; zoom-in is recommended. ABSTRACT",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes Texture UV Radiance Fields (TUVF) for generating high-quality, disentangled textures for a given 3D object shape. TUVF uses a canonical surface auto-encoder to establish dense correspondences between a UV sphere and the object's surface. A texture generator is then used to generate textures on the UV sphere, which can be applied to different shapes. The paper evaluates TUVF on various datasets and compares it to state-of-the-art approaches, achieving significant improvements in texture synthesis and editing applications.\n\n$\\boxed{Strengths:}\n1. The paper introduces a novel texture representation, TUVF, which provides a category-level texture representation disentangled from 3D shapes.\n2. The use of a canonical surface auto-encoder to establish dense correspondences between the UV sphere and the object's surface is a key innovation.\n3. The paper demonstrates the ability to generate high-quality, realistic textures and achieves significant improvements over state-of-the-art approaches.\n4. The paper provides a detailed evaluation of TUVF on various datasets and compares it to other approaches.\n\n$\\boxed{Weaknesses:}\n1. The assumption of one-to-one dense mapping between the UV sphere and the object's surface may not always hold in real-world scenarios.\n2. The paper does not address the issue of texture editing and transfer in detail.\n3. The evaluation of TUVF is limited to a few datasets, and it is unclear how well the approach generalizes to other domains.\n4. The paper does not provide a clear explanation of the role of the canonical surface auto-encoder in establishing dense correspondences.\n\n$\\boxed{Suggestions for Improvement:}\n1. Investigate the assumption of one-to-one dense mapping and explore alternative approaches to establish correspondences.\n2. Develop a more comprehensive evaluation of TUVF, including testing on a wider range of datasets and evaluating its performance in texture editing and transfer applications.\n3. Provide a detailed explanation of the role of the canonical surface auto-encoder in establishing dense correspondences.\n4. Explore the use of advanced data-driven priors, such as diffusion models, to improve the quality of generated textures.\n\n$\\boxed{Recommendation:}\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper, addressing the limitations and suggestions for improvement outlined above. The authors have made significant contributions to the field of texture synthesis, and with revisions, the paper has the potential to be a leading work in the field.",
        "peer_review_with_watermark": "$$\\text{Paper Title: TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS}$$\n$$\\text{Abstract:}$$\nThe proposed paper introduces Texture UV Radiance Fields (TUVF), a novel texture representation that disentangles the texture from the 3D shape of an object, enabling high-quality and realistic texture synthesis, texture swapping, and editing applications. TUVF leverages the UV sphere space with a continuous radiance field, allowing for efficient rendering and supervised learning from 2D single-view image collections using generative adversarial networks (GANs). The proposed approach achieves significant improvements over state-of-the-art methods, including mesh-based approaches, NeRF-based approaches, and instance-specific approaches, in terms of texture quality, diversity, and controllability. The paper also presents an ablation study, qualitative results, and a user study to evaluate the proposed approach.\n\n$$\\text{Strengths:}$$\n1. The proposed approach introduces a novel texture representation, TUVF, that disentangles the texture from the 3D shape of an object, enabling high-quality and realistic texture synthesis, texture swapping, and editing applications.\n2. TUVF leverages the UV sphere space with a continuous radiance field, allowing for efficient rendering and supervised learning from 2D single-view image collections using GANs.\n3. The proposed approach achieves significant improvements over state-of-the-art methods, including mesh-based approaches, NeRF-based approaches, and instance-specific approaches, in terms of texture quality, diversity, and controllability.\n4. The paper presents an ablation study, qualitative results, and a user study to evaluate the proposed approach, providing comprehensive evaluation and validation of the proposed method.\n\n$$\\text{Weaknesses:}$$\n1. The proposed approach assumes one-to-one dense mapping, which may not always hold in real-world scenarios, leading to limitations in terms of texture editing and swapping.\n2. The paper may benefit from further evaluation of the proposed approach on diverse datasets, including datasets with varying levels of complexity, texture diversity, and shape diversity.\n3. The paper may also benefit from further investigation of the role of the UV sphere space in the proposed approach, including its ability to capture complex textures and its limitations in terms of texture editing and swapping.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Investigate the role of the UV sphere space in the proposed approach, including its ability to capture complex textures and its limitations in terms of texture editing and swapping.\n2. Evaluate the proposed approach on diverse datasets, including datasets with varying levels of complexity, texture diversity, and shape diversity.\n3. Investigate the use of advanced data-driven priors, such as diffusion models, to mitigate distortions and improve the quality of the generated textures.\n4. Investigate the use of more sophisticated neural rendering architectures, such as ray transformers, to enhance the results of the proposed approach.\n\n$$\\text{Recommendation:}$$\nThe proposed paper introduces a novel texture representation, TUVF, that disentangles the texture from the 3D shape of an object, enabling high-quality and realistic texture synthesis, texture swapping, and editing applications. The proposed approach achieves significant improvements over state-of-the-art methods, including mesh-based approaches, NeRF-based approaches, and instance-specific approaches, in terms of texture quality, diversity, and controllability. However, the proposed approach assumes one-to-one dense mapping, which may not always hold in real-world scenarios, leading to limitations in terms of texture editing and swapping. Further evaluation, investigation, and validation of the proposed approach are recommended to further improve its performance and robustness.",
        "gr_score_with": 0.5317919075144508,
        "gr_score_without": 0.4209039548022599,
        "or_score_with": 0.4277456647398844,
        "or_score_without": 0.4632768361581921,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 149,
            "green_fraction": 0.4209039548022599,
            "num_orange_tokens": 164,
            "orange_fraction": 0.4632768361581921,
            "green_z_score": 0.8028316177150812,
            "orange_z_score": 102.60642822102601,
            "green_p_value": 0.21103603234942508,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 184,
            "green_fraction": 0.5317919075144508,
            "num_orange_tokens": 148,
            "orange_fraction": 0.4277456647398844,
            "green_z_score": 5.0040446068900755,
            "orange_z_score": 93.5402067606705,
            "green_p_value": 2.8069878676253756e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CODERS FOR INTRINSICALLY LOW-DIMENSIONAL DATA",
        "abstract": "Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied such as natural images are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes a framework to analyze error rates for learning unknown distributions using Wasserstein Autoencoders (WAEs). The authors aim to bridge the gap between the theory and practice of WAEs by providing a theoretical analysis of the error rates for WAEs. The paper assumes that the data distributions have an intrinsically low-dimensional structure within the high-dimensional feature space and that the network architectures are properly chosen. The authors show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.\n\nStrengths: \n1. The paper provides a comprehensive theoretical analysis of WAEs, which is essential for understanding the strengths and limitations of this model.\n2. The authors provide a clear and concise explanation of the assumptions and notation used in the paper.\n3. The paper presents a novel approach to analyzing the error rates for WAEs, which is independent of the high feature dimension.\n4. The authors provide a detailed discussion of the implications of their results, including the implications for data generation, decoding, and encoding guarantees.\n\nWeaknesses: \n1. The paper assumes that the data distributions have an intrinsically low-dimensional structure within the high-dimensional feature space, which may not always be the case.\n2. The authors rely on the assumption that the network architectures are properly chosen, which may not always be the case in practice.\n3. The paper does not provide a comprehensive analysis of the optimization error term, which is a significant challenge in WAEs.\n4. The authors do not provide a detailed comparison with other state-of-the-art models, which is essential for understanding the strengths and limitations of WAEs.\n\nSuggestions for Improvement: \n1. The authors should provide a more comprehensive analysis of the optimization error term, which is a significant challenge in WAEs.\n2. The authors should provide a detailed comparison with other state-of-the-art models, which is essential for understanding the strengths and limitations of WAEs.\n3. The authors should provide a more detailed discussion of the implications of their results, including the implications for data generation, decoding, and encoding guarantees.\n4. The authors should provide a more comprehensive analysis of the assumptions and notation used in the paper, which is essential for understanding the strengths and limitations of WAEs.\n\nRecommendation: \nBased on the paper's strengths and weaknesses, I recommend that the authors revise the paper to address the limitations mentioned above. The authors should provide a more comprehensive analysis of the optimization error term, a detailed comparison with other state-of-the-art models, a more detailed discussion of the implications of their results, and a more comprehensive analysis of the assumptions and notation used in the paper. This will enhance the paper's overall quality and provide a more comprehensive understanding of WAEs.",
        "peer_review_with_watermark": "$$\\boxed{\nPaper Summary:\nThe paper addresses the statistical guarantees of Wasserstein Autoencoders (WAEs) when the data distributions exhibit an intrinsically low-dimensional structure in the high-dimensional feature space. The authors develop a framework to analyze the error rates of WAEs and establish the convergence rates of the expected excess risk in terms of the intrinsic dimension of the data distribution.\n\nStrengths:\n- The paper provides a thorough analysis of the statistical guarantees of WAEs.\n- The authors develop a framework to analyze the error rates of WAEs and establish the convergence rates of the expected excess risk.\n- The paper discusses the implications of the results and provides valuable insights into the theoretical characteristics of WAEs.\n\nWeaknesses:\n- The paper assumes that the data distributions are realizable, which may not hold in practice.\n- The authors do not discuss the case where the optimization error is significant.\n- The paper does not provide a detailed discussion of the empirical results.\n\nSuggestions for Improvement:\n- The authors should discuss the case where the optimization error is significant.\n- The authors should provide a detailed discussion of the empirical results.\n- The authors should discuss the generalizability of the results to other domains.\n\nRecommendation:\nThe paper provides a thorough analysis of the statistical guarantees of WAEs and establishes the convergence rates of the expected excess risk in terms of the intrinsic dimension of the data distribution. The authors' framework can be useful in understanding the theoretical characteristics of WAEs. However, the authors should address the case where the optimization error is significant and provide a detailed discussion of the empirical results.\n\n$$}$$",
        "gr_score_with": 0.5191256830601093,
        "gr_score_without": 0.4397163120567376,
        "or_score_with": 0.4371584699453552,
        "or_score_without": 0.46808510638297873,
        "output_without": {
            "num_tokens_scored": 282,
            "num_green_tokens": 124,
            "green_fraction": 0.4397163120567376,
            "num_orange_tokens": 132,
            "orange_fraction": 0.46808510638297873,
            "green_z_score": 1.361406587313681,
            "orange_z_score": 87.47753030037867,
            "green_p_value": 0.08669261777301318,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 183,
            "num_green_tokens": 95,
            "green_fraction": 0.5191256830601093,
            "num_orange_tokens": 80,
            "orange_fraction": 0.4371584699453552,
            "green_z_score": 3.2894654395448395,
            "orange_z_score": 65.73339400087998,
            "green_p_value": 0.0005018894146449062,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TOOL-AUGMENTED REWARD MODELING",
        "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements1.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}  \nThe paper proposes a tool-augmented preference modeling approach, Themis, to address the limitations of conventional reward models (RMs) in aligning large language models with human preferences. Themis enables RMs to interact with external tools, such as calculators and search engines, to augment their functionality and improve interpretability. The authors validate their approach across a wide range of domains and demonstrate its effectiveness in enhancing truthfulness and factuality in preference modeling.\n\nStrengths:\n\n1.  The paper introduces a novel approach to tool-augmented preference modeling, addressing the limitations of conventional RMs.\n2.  The authors provide a comprehensive dataset, TARA, which encompasses detailed data on human preferences and intricate tool invocation processes.\n3.  The experimental results demonstrate the effectiveness of Themis in enhancing interpretive capacity and scoring reliability.\n4.  The approach is flexible and can be applied to various domains, including programming, dialogue assistance, and fact-related question answering.\n\nWeaknesses:\n\n1.  The paper relies heavily on the assumption that humans tend to resort to external tools when faced with complex tasks, which may not be universally true.\n2.  The authors do not provide a detailed analysis of the limitations of external tools and how they can be integrated into the RMs.\n3.  The paper focuses primarily on the theoretical aspects of Themis and does not provide a thorough evaluation of its practical applications.\n4.  The authors mention the potential for future research in multi-turn dialogue generation but do not provide a clear direction for this line of inquiry.\n\nSuggestions for Improvement:\n\n1.  Conduct a more detailed analysis of the limitations of external tools and how they can be integrated into the RMs.\n2.  Provide a more comprehensive evaluation of the practical applications of Themis, including its potential limitations and challenges.\n3.  Explore the potential for Themis to be applied to other domains beyond programming, dialogue assistance, and fact-related question answering.\n4.  Provide a clear direction for future research, including potential avenues for investigation in multi-turn dialogue generation.\n\nRecommendation:\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit their work. The authors should address the limitations of external tools and provide a more comprehensive evaluation of the practical applications of Themis. Additionally, they should provide a clear direction for future research and explore the potential for Themis to be applied to other domains. With these revisions, the paper has the potential to make a significant contribution to the field of natural language processing and reinforcement learning from human feedback.",
        "peer_review_with_watermark": "$$\n\\begin{tabular}{l}\nPaper Summary: \\\\\nThe paper proposes a tool-augmented reward modeling approach, named Themis, which integrates external tools into reinforcement learning from human feedback (RLHF). The approach empowers reward models (RMs) with access to external environments, including calculators and search engines, to enhance interpretive capacity and scoring reliability. The paper presents a comprehensive framework, validated across eight tasks in eight domains.\n\nStrengths: \\\\\nThe paper presents a well-designed and well-validated framework that integrates external tools into RLHF. The approach provides a transparent and sequential account of actions and verbal reasoning traces specific to a given task, enhancing human interpretability and trustworthiness.\n\nWeaknesses: \\\\\nThe paper lacks a comprehensive literature review and discusses related work in a superficial manner. The experimental results are well-presented, but some details, such as the specific tool invocation process and data filtering, are not fully explained.\n\nSuggestions for Improvement: \\\\\n1. Conduct a more comprehensive literature review and discuss related work in greater depth.\n2. Provide more detailed explanations for specific tool invocation processes and data filtering.\n3. Explore the application of Themis in multi-turn dialogue generation.\n\nRecommendation: \\\\\nThe paper presents a well-designed and well-validated framework that integrates external tools into RLHF. The approach provides a transparent and sequential account of actions and verbal reasoning traces specific to a given task, enhancing human interpretability and trustworthiness. The results demonstrate the substantial benefits of Themis in augmenting interpretive capacity and scoring reliability.\n\nScore: 8/10\n$$",
        "gr_score_with": 0.4345991561181435,
        "gr_score_without": 0.32409972299168976,
        "or_score_with": 0.5147679324894515,
        "or_score_without": 0.5761772853185596,
        "output_without": {
            "num_tokens_scored": 361,
            "num_green_tokens": 117,
            "green_fraction": 0.32409972299168976,
            "num_orange_tokens": 208,
            "orange_fraction": 0.5761772853185596,
            "green_z_score": -2.94368504176575,
            "orange_z_score": 120.88435117387951,
            "green_p_value": 0.9983783506259983,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 237,
            "num_green_tokens": 103,
            "green_fraction": 0.4345991561181435,
            "num_orange_tokens": 122,
            "orange_fraction": 0.5147679324894515,
            "green_z_score": 1.0872616217068296,
            "orange_z_score": 87.35994209777007,
            "green_p_value": 0.1384606022121841,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ADOPD: A LARGE-SCALE DOCUMENT PAGE DECOM-",
        "abstract": "Recent research in document image understanding is impeded by the scarcity of high-quality data. We introduce ADoPD, a large-scale dataset for document page decomposition. ADoPD distinguishes itself by utilizing a novel data-driven document taxonomy discovery method for data collection. Our approach takes advantage of both large-scale pretrained models and human-in-the-loop process, ensuring diversity and balance in our data collection. Leveraging our data-driven document taxonomy, we collected and densely annotated labels for document images, covering four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Specifically, for each image, the annotations include human-labeled entity masks, text bounding boxes, as well as automatically generated tags and captions. We provide detailed experimental analyses to validate our data-driven document taxonomy method and experimentally analyze the four tasks based on different models. We believe that ADoPD has the potential to become a cornerstone dataset to support future research on document image understanding.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces ADOPD, a large-scale dataset for document page decomposition. ADOPD is constructed using a novel data-driven document taxonomy discovery method, which utilizes large-scale pretrained models and a human-in-the-loop process to ensure diversity and balance in the data collection. The dataset comprises four document page decomposition tasks: entity segmentation, text detection, tagging, and captioning. The authors conduct extensive experiments and analysis to validate the effectiveness of ADOPD and its potential to support future research in document image understanding.\n\nStrengths:\n1. Novel data-driven approach for constructing document taxonomies during data collection.\n2. Comprehensive dataset for document page decomposition, encompassing four distinct tasks.\n3. Utilization of large-scale pretrained models and human-in-the-loop process to ensure diversity and balance in the data collection.\n4. Extensive experimental analysis to validate the effectiveness of ADOPD.\n\nWeaknesses:\n1. Limited evaluation of the data-driven taxonomy discovery method, with no comparison to traditional methods.\n2. No discussion of potential biases in the data collection process, particularly with regards to the use of pretrained models.\n3. Limited exploration of the potential applications and limitations of ADOPD in real-world scenarios.\n\nSuggestions for Improvement:\n1. Conduct a more comprehensive evaluation of the data-driven taxonomy discovery method, including comparison to traditional methods.\n2. Discuss potential biases in the data collection process and provide strategies for mitigating them.\n3. Explore the potential applications and limitations of ADOPD in real-world scenarios, including its potential use in various industries and domains.\n\nRecommendation:\nADOPD is a significant contribution to the field of document image understanding, and its potential to support future research in this area is substantial. However, further improvements in the data collection process and evaluation methodology are necessary to fully realize the potential of ADOPD.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces ADOPD, a large-scale dataset for document page decomposition. It presents a novel data-driven approach to constructing document taxonomies during data collection and proposes a collaborative approach that integrates human annotators with state-of-the-art models to enhance overall efficiency. The authors provide a detailed experimental analysis of ADOPD and perform comparative studies across four tasks: entity segmentation, text detection, tagging, and captioning.\n\nStrengths:\n1. The paper presents a novel and effective approach to constructing document taxonomies during data collection.\n2. The proposed collaborative approach integrates human annotators with state-of-the-art models, enhancing overall efficiency.\n3. The authors provide a detailed experimental analysis of ADOPD and perform comparative studies across four tasks.\n\nWeaknesses:\n1. The paper does not provide a clear and detailed description of the data collection process, which may make it difficult to reproduce and evaluate the results.\n2. The authors do not provide a comprehensive analysis of the data-driven document taxonomy discovery method, which may raise questions about its reliability and generalization capabilities.\n3. The paper does not provide a detailed evaluation of the proposed model-assisted data annotation approach, which may raise questions about its effectiveness and efficiency.\n\nSuggestions for Improvement:\n1. The authors should provide a clear and detailed description of the data collection process to make it possible to reproduce and evaluate the results.\n2. The authors should provide a comprehensive analysis of the data-driven document taxonomy discovery method to evaluate its reliability and generalization capabilities.\n3. The authors should provide a detailed evaluation of the proposed model-assisted data annotation approach to assess its effectiveness and efficiency.\n\nRecommendation:\nBased on the paper summary, strengths, weaknesses, and suggestions for improvement, I recommend that the authors revise and resubmit the paper. With revisions to address the weaknesses and provide a more comprehensive analysis of the proposed methods, the paper has the potential to make a significant contribution to the field of document image understanding.\n\n $$$\nBest regards,\n\n[Your Name]\nResearch Scientist",
        "gr_score_with": 0.44621513944223107,
        "gr_score_without": 0.3387096774193548,
        "or_score_with": 0.47808764940239046,
        "or_score_without": 0.5645161290322581,
        "output_without": {
            "num_tokens_scored": 248,
            "num_green_tokens": 84,
            "green_fraction": 0.3387096774193548,
            "num_orange_tokens": 140,
            "orange_fraction": 0.5645161290322581,
            "green_z_score": -1.9702082199878084,
            "orange_z_score": 93.56192511441249,
            "green_p_value": 0.9755927440670845,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 251,
            "num_green_tokens": 112,
            "green_fraction": 0.44621513944223107,
            "num_orange_tokens": 120,
            "orange_fraction": 0.47808764940239046,
            "green_z_score": 1.4945674088872114,
            "orange_z_score": 79.48634656241062,
            "green_p_value": 0.06751368857301474,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper presents FORMATSPREAD, an algorithm for estimating the performance spread across prompt formatting choices for few-shot learning tasks with large language models. The authors demonstrate that widely used open-source LLMs are sensitive to subtle changes in prompt formatting, with performance differences of up to 76 accuracy points. They propose FORMATSPREAD, a grammar-based algorithm that efficiently explores the space of plausible prompt formats under a user-specified computational budget. The authors evaluate FORMATSPREAD on several models and tasks, finding that the spread of performance is large regardless of model choice, even when increasing model size, number of few-shots, or when using instruction tuning.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The paper highlights the importance of considering prompt formatting variance in few-shot learning tasks with large language models.\n2. The authors propose a novel algorithm, FORMATSPREAD, that efficiently explores the space of plausible prompt formats under a user-specified computational budget.\n3. The paper demonstrates the effectiveness of FORMATSPREAD on several models and tasks, finding that the spread of performance is large regardless of model choice.\n4. The authors provide a detailed analysis of the performance spread, including its correlation with the separability of format embeddings.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The paper assumes that the prompt formatting choices are semantically equivalent, which may not always be the case in practice.\n2. The authors do not provide a thorough analysis of the limitations of FORMATSPREAD, including its computational complexity and potential biases.\n3. The paper does not provide a comparison with other algorithms for estimating performance spread, which may limit its applicability.\n4. The authors' assumption that the performance spread is large regardless of model choice may not always be true, and further research is needed to validate this claim.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. Provide a more detailed analysis of the limitations of FORMATSPREAD, including its computational complexity and potential biases.\n2. Compare FORMATSPREAD with other algorithms for estimating performance spread, to evaluate its applicability and effectiveness.\n3. Validate the claim that the performance spread is large regardless of model choice, by conducting a more comprehensive analysis.\n4. Consider incorporating additional features or modifications to FORMATSPREAD, to improve its accuracy and robustness.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the paper's findings and limitations, we recommend that future research:\n\n1. Develop and evaluate additional algorithms for estimating performance spread, to provide a more comprehensive comparison with FORMATSPREAD.\n2. Conduct a more thorough analysis of the limitations of FORMATSPREAD, including its computational complexity and potential biases.\n3. Validate the claim that the performance spread is large regardless of model choice, by conducting a more comprehensive analysis.\n4. Consider incorporating additional features or modifications to FORMATSPREAD, to improve its accuracy and robustness.",
        "peer_review_with_watermark": "$$\n\n# Paper Summary\n\nThe paper presents a method for analyzing the sensitivity of large language models (LLMs) to prompt formatting choices, which are a critical component of effective model deployment. The authors propose a new algorithm, FORMATSPREAD, that efficiently evaluates the performance spread across a wide range of plausible prompt formats for a given task. They demonstrate the efficacy of FORMATSPREAD on several widely used open-source LLMs, including LLaMA-2-7B, Falcon-7B, and GPT-3.5, on a diverse set of classification and open-ended text generation tasks. The results show that prompt format choices can significantly impact model performance, even when increasing model size, number of few-shot examples, or instruction tuning. The authors also propose a suite of analyses to characterize model sensitivity to formatting choices, including the influence of atomic perturbations and the internal representation of formats.\n\n# Strengths\n\n* The paper presents a critical analysis of the importance of prompt formatting choices in LLMs.\n* The proposed algorithm, FORMATSPREAD, efficiently evaluates the performance spread across a wide range of plausible prompt formats.\n* The results demonstrate the efficacy of FORMATSPREAD on several widely used open-source LLMs.\n* The proposed suite of analyses provides a comprehensive understanding of model sensitivity to formatting choices.\n\n# Weaknesses\n\n* The paper assumes a binary choice between a prompt format and its semantically equivalent alternatives, which may not reflect real-world complexity.\n* The evaluation of FORMATSPREAD on a limited set of tasks may not be generalizable to other domains or applications.\n* The proposed method does not address the issue of degeneration, where models may not generate valid outputs even when given a valid prompt format.\n* The paper does not provide a detailed discussion of the limitations of the proposed algorithm, such as the computational cost of generating all possible prompt formats.\n\n# Suggestions for Improvement\n\n* Expand the evaluation of FORMATSPREAD to a larger set of tasks and domains to ensure generalizability.\n* Develop a more nuanced approach to characterizing model sensitivity to formatting choices, including the influence of atomic perturbations and the internal representation of formats.\n* Address the issue of degeneration by incorporating additional constraints or heuristics into the proposed method.\n* Provide a more detailed discussion of the limitations of the proposed algorithm, including the computational cost of generating all possible prompt formats.\n* Consider incorporating additional features, such as the influence of input length or the use of multiple models, to further improve the robustness and generalizability of FORMATSPREAD.\n\n# Recommendation\n\nBased on the provided review, I recommend that the authors address the weaknesses and limitations identified in the paper and expand the evaluation of FORMATSPREAD to a larger set of tasks and domains. The proposed method has the potential to significantly improve the deployment and evaluation of LLMs, and addressing the identified weaknesses will ensure that FORMATSPREAD is a robust and generalizable approach to analyzing prompt formatting choices.",
        "gr_score_with": 0.5829145728643216,
        "gr_score_without": 0.4938650306748466,
        "or_score_with": 0.3592964824120603,
        "or_score_without": 0.42024539877300615,
        "output_without": {
            "num_tokens_scored": 326,
            "num_green_tokens": 161,
            "green_fraction": 0.4938650306748466,
            "num_orange_tokens": 137,
            "orange_fraction": 0.42024539877300615,
            "green_z_score": 3.459449582092929,
            "orange_z_score": 70.96958191213535,
            "green_p_value": 0.00027064030085864523,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 398,
            "num_green_tokens": 232,
            "green_fraction": 0.5829145728643216,
            "num_orange_tokens": 143,
            "orange_fraction": 0.3592964824120603,
            "green_z_score": 7.4487641325375265,
            "orange_z_score": 66.73862247185932,
            "green_p_value": 4.7109321586784664e-14,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS",
        "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$\\boxed{Paper Summary:}$ \nThe paper proposes a novel approach to learning robust world models in the reward-free setting, enabling agents to adapt to new tasks without additional training in the environment. The authors define a problem of learning a robust world model in the reward-free setting, in terms of minimax regret optimality, and show that this problem is equivalent to minimising the maximum expected error of the world model across all environments under a suitable exploration policy. They introduce an algorithm, WAKER, which selects environments for data collection based on the estimated error of the world model for each environment, and demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.\n\n$\\boxed{Strengths:}$ \n1. The paper provides a clear and concise introduction to the problem of learning robust world models in the reward-free setting, and establishes a connection between minimax regret and world model training.\n2. The authors present a well-defined problem statement, and provide a thorough analysis of the theoretical foundations of the approach.\n3. The proposed algorithm, WAKER, is well-designed and easy to understand, and the authors provide a clear description of how it works.\n4. The paper presents a comprehensive set of experiments, which demonstrate the effectiveness of the approach in various domains and scenarios.\n\n$\\boxed{Weaknesses:}$ \n1. The paper assumes a level of domain knowledge that may not be available to all readers, and could benefit from a more detailed explanation of the technical concepts.\n2. The authors rely heavily on previous works in robust RL, and may benefit from a more detailed discussion of the limitations and potential challenges of these approaches.\n3. The paper could benefit from a more detailed analysis of the trade-offs between different exploration policies, and the impact of these trade-offs on the overall performance of the algorithm.\n4. The authors present a comprehensive set of experiments, but may benefit from a more detailed discussion of the limitations and potential challenges of these experiments.\n\n$\\boxed{Suggestions for Improvement:}$ \n1. Provide a more detailed explanation of the technical concepts, and assume a lower level of domain knowledge.\n2. Discuss the limitations and potential challenges of the previous works in robust RL, and provide a more comprehensive analysis of the trade-offs between different exploration policies.\n3. Include a more detailed discussion of the limitations and potential challenges of the experiments, and provide a more nuanced analysis of the results.\n4. Consider including additional experiments or scenarios to further demonstrate the effectiveness of the approach.\n\n$\\boxed{Recommendation:}$ \nBased on the strengths and weaknesses of the paper, I would recommend that the authors revise and resubmit the paper for consideration at a top-tier conference in reinforcement learning. The authors have made significant contributions to the field, and the paper presents a well-defined and well-analyzed approach to learning robust world models in the reward-free setting. With some revisions to address the weaknesses and improve the clarity and comprehensiveness of the paper, I believe that the authors have a strong chance of making a significant impact in the field.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$\\boxed{\\text{Paper Summary:}$$\n\\begin{tabular}{l}\n    The paper proposes the first method for automatic curriculum learning for environment selection in the reward-free setting, where the goal is to develop generally-capable agents that can adapt to new tasks without additional training in the environment.\n    The approach involves learning a world model that represents the possible environments by a set of reward-free MDPs in latent space, and then using this world model to train policies that can adapt to new tasks.\n    The paper also introduces the concept of minimax regret, which is a commonly used objective for robust policy optimisation in RL, and shows that it can be connected to the maximum error of the world model across environment instances.\n    The authors also present an algorithm, WAKER, that selects environments to sample data from to train the world model, and demonstrate that it outperforms several baselines in several pixel-based continuous control domains.\n\\end{tabular}\n$$\n\n$$\\boxed{\\text{Strengths:}$$\n\\begin{tabular}{l}\n    The paper provides a clear and well-structured overview of the problem of learning robust world models in the reward-free setting.\n    The approach of using minimax regret as an objective for world model training is innovative and well-motivated.\n    The paper also provides a detailed evaluation of the proposed algorithm, WAKER, and compares it to several baselines.\n    The use of pixel-based continuous control domains as evaluation benchmarks is also a strength, as it provides a realistic and challenging testbed for the proposed approach.\n\\end{tabular}\n$$\n\n$$\\boxed{\\text{Weaknesses:}$$\n\\begin{tabular}{l}\n    The paper assumes that the world model can learn a suitable representation model, q, which encodes any sequence of observations and actions into a Markovian latent state, and may not provide a clear justification for this assumption.\n    The paper also assumes that the optimal world model policy can be recovered from the world model, which may not be the case in practice.\n    The evaluation of the proposed algorithm, WAKER, is limited to several pixel-based continuous control domains, and it may not be clear whether the approach will work well in more complex environments.\n    The paper also does not provide a clear discussion of the limitations and challenges of the proposed approach, and may not provide a clear direction for future research.\n\\end{tabular}\n$$\n\n$$\\boxed{\\text{Suggestions for Improvement:}$$\n\\begin{tabular}{l}\n    The paper could provide a more detailed justification for the assumption that the world model can learn a suitable representation model, q.\n    The paper could also explore the possibility of using alternative objectives for world model training, such as those based on ensemble methods or meta-learning.\n    The paper could also investigate the performance of the proposed algorithm, WAKER, in more complex environments, and provide a more detailed evaluation of the approach's limitations and challenges.\n    The paper could also explore the possibility of using WAKER for reward-free pretraining, followed by task-specific finetuning, as a potential direction for future research.\n\\end{tabular}\n$$\n\n$$\\boxed{\\text{Recommendation:}$$\n\\begin{tabular}{l}\n    The paper provides an innovative approach to the problem of learning robust world models in the reward-free setting, and the proposed algorithm, WAKER, shows promising results in several pixel-based continuous control domains.\n    However, the paper could benefit from a more detailed justification for the assumptions made, and a more extensive evaluation of the approach's limitations and challenges.\n    Nevertheless, the paper provides a clear and well-structured overview of the problem and the proposed approach, and provides a good starting point for further research in this area.",
        "gr_score_with": 0.49539170506912444,
        "gr_score_without": 0.39598997493734334,
        "or_score_with": 0.46774193548387094,
        "or_score_without": 0.47368421052631576,
        "output_without": {
            "num_tokens_scored": 399,
            "num_green_tokens": 158,
            "green_fraction": 0.39598997493734334,
            "num_orange_tokens": 189,
            "orange_fraction": 0.47368421052631576,
            "green_z_score": -0.16350382386265985,
            "orange_z_score": 112.70007109015452,
            "green_p_value": 0.5649391186702908,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 434,
            "num_green_tokens": 215,
            "green_fraction": 0.49539170506912444,
            "num_orange_tokens": 203,
            "orange_fraction": 0.46774193548387094,
            "green_z_score": 4.056484823640283,
            "orange_z_score": 116.04288890704365,
            "green_p_value": 2.490839892430689e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces the IDEAL method, an influence-driven selective annotation approach for in-context learning with large language models. The method aims to minimize annotation costs while improving the quality of in-context examples. It works in an unsupervised and end-to-end manner, using a directed graph to represent unlabeled data and a diffusion process to quantify the influence of candidate subsets. The method is compared to the existing Vote-k method and achieves better performance in most cases.\n\nStrengths:\n1. The paper introduces a novel approach to selective annotations, which is essential for efficient in-context learning with large language models.\n2. The use of a directed graph and diffusion process to quantify influence is innovative and effective.\n3. The theoretical analysis provides a lower bound for the subset influence selected by the method, demonstrating its reliability.\n4. The empirical evaluations demonstrate the superiority of the IDEAL method across various benchmarks.\n\nWeaknesses:\n1. The paper does not provide a clear explanation of the significance of the influence-driven selective annotation approach and its potential applications.\n2. The theoretical analysis assumes a specific condition (submodular function) that may not hold in all cases.\n3. The experiments are conducted on a limited number of datasets and models, which may not be representative of the broader scope of in-context learning.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the significance of the influence-driven selective annotation approach and its potential applications.\n2. Conduct experiments on a larger number of datasets and models to demonstrate the robustness of the IDEAL method.\n3. Investigate the effect of different annotation budgets and prompt retrieval methods on the performance of the IDEAL method.\n\nRecommendation:\nThe IDEAL method is a promising approach to selective annotations for in-context learning with large language models. While it has some limitations, the paper provides a solid foundation for further research and development. With some improvements in explanations, experiments, and theoretical analysis, the IDEAL method can be a valuable tool for researchers and practitioners in the field of natural language processing.",
        "peer_review_with_watermark": "$$$\\label{summary}\nThe paper proposes a novel method, IDEAL, for selective annotations in in-context learning. The method utilizes an influence-driven approach to select a pivotal subset from a large-scale unlabeled data pool. The proposed method is end-to-end, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. The experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at \\text{https://skzhang1.github.io/IDEAL/}. \\label{strengths}\nThe paper presents a well-structured and well-organized approach to selective annotations in in-context learning. The method is novel and innovative, addressing the challenges of obtaining high-quality annotated examples. The theoretical analysis provides a lower bound for the subset influence selected by the proposed method, demonstrating its effectiveness. The experiments demonstrate the superiority of the proposed method on various benchmarks. The paper also provides a detailed analysis of the results, including the time cost of subset selection and the performance of different prompt retrieval methods. \\label{weaknesses}\nThe paper relies heavily on previous work, and some of the notation and terminology used may be unclear to readers without a background in graph theory or machine learning. The paper could benefit from a more detailed explanation of the influence-driven approach and its theoretical foundations. The experiments could be improved by including more detailed analysis of the results, such as the distribution of the activation probabilities and the influence of each subset. \\label{suggestions_for_improvement}\nTo improve the clarity and readability of the paper, the authors could provide more detailed explanations of the notation and terminology used. The authors could also provide more detailed analysis of the results, such as the distribution of the activation probabilities and the influence of each subset. The authors could also explore other methods for selective annotations and provide a more detailed comparison of the proposed method with other approaches. \\label{recommendation}\nBased on the analysis of the paper, it is recommended that the authors revise the paper to address the issues mentioned above. The authors could improve the clarity and readability of the paper by providing more detailed explanations of the notation and terminology used. The authors could also provide more detailed analysis of the results and explore other methods for selective annotations. The revised paper should demonstrate the superiority of the proposed method on various benchmarks and provide a detailed analysis of the results. The revised paper should also provide a more detailed comparison of the proposed method with other approaches.",
        "gr_score_with": 0.46439628482972134,
        "gr_score_without": 0.3696369636963696,
        "or_score_with": 0.49226006191950467,
        "or_score_without": 0.5577557755775577,
        "output_without": {
            "num_tokens_scored": 303,
            "num_green_tokens": 112,
            "green_fraction": 0.3696369636963696,
            "num_orange_tokens": 169,
            "orange_fraction": 0.5577557755775577,
            "green_z_score": -1.0788495685864479,
            "orange_z_score": 117.60351932530737,
            "green_p_value": 0.8596726031952402,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 323,
            "num_green_tokens": 150,
            "green_fraction": 0.46439628482972134,
            "num_orange_tokens": 159,
            "orange_fraction": 0.49226006191950467,
            "green_z_score": 2.362416422206122,
            "orange_z_score": 106.99116736880462,
            "green_p_value": 0.009078117432717667,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel training strategy for high-resolution vision transformers, called Win-Win, which allows for efficient training and inference of high-resolution models. The strategy involves masking most of the high-resolution inputs during training, keeping only N random windows, and using relative positional embeddings to preserve global interactions. The authors demonstrate the effectiveness of Win-Win on two dense prediction tasks, semantic segmentation and optical flow estimation, achieving state-of-the-art performance on the Spring benchmark.\n\nStrengths:\n1. The paper presents a novel and effective training strategy for high-resolution vision transformers, addressing the challenge of quadratic complexity of global attention mechanisms.\n2. The authors provide a clear and concise explanation of the Win-Win strategy and its benefits.\n3. The experimental results demonstrate the effectiveness of Win-Win on both semantic segmentation and optical flow estimation tasks.\n\nWeaknesses:\n1. The paper does not provide a thorough analysis of the limitations of the Win-Win strategy, such as the choice of window size and the impact of relative positional embeddings on the model's performance.\n2. The authors rely heavily on experiments on specific datasets, which may not be representative of the broader vision transformer landscape.\n3. The paper does not discuss potential future work or directions for improving the Win-Win strategy.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of the Win-Win strategy and explore ways to address them.\n2. The paper should include experiments on a broader range of datasets and vision transformer architectures to demonstrate the generalizability of the Win-Win strategy.\n3. The authors should discuss potential future work, such as exploring the use of Win-Win with other attention mechanisms or incorporating additional regularization techniques.\n\nRecommendation:\nThe paper presents a novel and effective training strategy for high-resolution vision transformers, which has the potential to significantly reduce training costs and improve inference times. The authors provide clear and concise explanations of the Win-Win strategy and its benefits, and the experimental results demonstrate its effectiveness on both semantic segmentation and optical flow estimation tasks. However, the paper could benefit from a more detailed analysis of the limitations of the Win-Win strategy and a broader range of experiments to demonstrate its generalizability.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents a novel approach for training high-resolution vision transformers, referred to as Win-Win, which enables efficient training and inference of high-resolution vision transformers. The approach involves randomly selecting N windows from the input image during training, allowing the model to learn local and global interactions between tokens. This strategy is compared to other approaches, such as hierarchical architectures, fast and approximate attention, and training on low-resolution crops. The results show that Win-Win is effective and outperforms other approaches in terms of both training and inference time. The paper also presents extensions of the approach to binocular tasks, such as optical flow estimation, and demonstrates state-of-the-art performance on several benchmarks.\n\nStrengths:\n1. The paper presents a novel approach that addresses the limitations of existing methods for training high-resolution vision transformers.\n2. The approach is effective in reducing training and inference time, making it more practical for large-scale vision tasks.\n3. The paper presents extensions of the approach to binocular tasks, demonstrating its applicability to multiple image tasks.\n4. The results show that Win-Win outperforms other approaches in terms of both training and inference time.\n\nWeaknesses:\n1. The approach may not be suitable for all vision tasks, as it relies on randomly selecting windows from the input image, which may not capture the underlying structure of the data.\n2. The paper does not provide a comprehensive evaluation of the approach on multiple benchmarks, which may limit its generalizability.\n3. The approach may not be effective for tasks that require high-resolution outputs, as the model may not be able to capture the underlying structure of the data.\n4. The paper does not provide a clear comparison of the approach to other state-of-the-art methods, which may limit its competitiveness.\n\nSuggestions for Improvement:\n1. Evaluate the approach on multiple benchmarks to demonstrate its generalizability.\n2. Provide a comprehensive comparison of the approach to other state-of-the-art methods.\n3. Investigate the limitations of the approach and provide recommendations for improving its applicability to multiple vision tasks.\n4. Provide more details on the extensions of the approach to binocular tasks, such as optical flow estimation, and demonstrate its applicability to multiple image tasks.\n\nRecommendation:\nBased on the results, I recommend that the paper is accepted for publication. However, I suggest that the authors address the limitations and suggestions for improvement mentioned above to strengthen the paper's competitiveness and generalizability. Additionally, the authors should provide more details on the extensions of the approach to binocular tasks and demonstrate its applicability to multiple image tasks. Overall, the paper presents a novel approach that addresses the limitations of existing methods for training high-resolution vision transformers and has the potential to revolutionize the field of computer vision.",
        "gr_score_with": 0.6850152905198776,
        "gr_score_without": 0.5357142857142857,
        "or_score_with": 0.2599388379204893,
        "or_score_without": 0.35,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 150,
            "green_fraction": 0.5357142857142857,
            "num_orange_tokens": 98,
            "orange_fraction": 0.35,
            "green_z_score": 4.635525346505532,
            "orange_z_score": 69.3537867425523,
            "green_p_value": 1.7801604649763726e-06,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 224,
            "green_fraction": 0.6850152905198776,
            "num_orange_tokens": 85,
            "orange_fraction": 0.2599388379204893,
            "green_z_score": 10.520500834967528,
            "orange_z_score": 55.27533555503693,
            "green_p_value": 3.4751571677801977e-26,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "THRESHOLD-CONSISTENT MARGIN LOSS FOR OPEN- WORLD DEEP METRIC LEARNING",
        "abstract": "Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate (FAR) and false reject rate (FRR) across test classes and data distributions. We define this issue in DML as threshold inconsistency. In real-world applications, such inconsistency often complicates the threshold selection process when deploying commercial image retrieval systems. To measure this inconsistency, we propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. Using the OPIS metric, we find that achieving high accuracy levels in a DML model does not automatically guarantee threshold consistency. In fact, our investigation reveals a Pareto frontier in the high-accuracy regime, where existing methods to improve accuracy often lead to degradation in threshold consistency. To address this trade-off, we introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. Extensive experiments demonstrate TCM\u2019s effectiveness in enhancing threshold consistency while preserving accuracy, simplifying the threshold selection process in practical DML settings.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$The paper proposes a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) to quantify the threshold inconsistency of a Deep Metric Learning (DML) model. The authors demonstrate that achieving high accuracy in DML does not automatically guarantee threshold consistency, and that introducing the proposed OPIS metric as an additional evaluation criterion is crucial for threshold-based commercial DML applications.\n\nStrengths:\n\n*   The paper provides a comprehensive analysis of the issue of threshold inconsistency in DML and proposes a novel solution to address this problem.\n*   The authors introduce a new variance-based metric, OPIS, to quantify threshold inconsistency, which is a significant contribution to the field.\n*   The paper presents a thorough evaluation of the proposed method, including experiments on four standard image retrieval benchmarks.\n\nWeaknesses:\n\n*   The paper relies heavily on theoretical analysis and does not provide a clear connection to the real-world applications of DML.\n*   The authors do not discuss the limitations of the OPIS metric, such as the requirement for a sufficient number of samples per class.\n*   The paper does not provide a clear comparison with other methods for evaluating threshold consistency.\n\nSuggestions for Improvement:\n\n*   The authors should provide a more detailed analysis of the limitations of the OPIS metric and discuss potential ways to address these limitations.\n*   The paper should include a more thorough comparison with other methods for evaluating threshold consistency, including posthoc calibration methods and conformal prediction.\n*   The authors should provide a more detailed explanation of the theoretical connection between the OPIS metric and the real-world applications of DML.\n\nRecommendation:\n\n*   The paper presents a significant contribution to the field of DML and proposes a novel solution to address the issue of threshold inconsistency.\n*   The authors should address the limitations of the OPIS metric and provide a more thorough comparison with other methods for evaluating threshold consistency.\n*   The paper should be revised to provide a more detailed analysis of the theoretical connection between the OPIS metric and the real-world applications of DML.",
        "peer_review_with_watermark": "$$$$\n\nPaper Title: Threshold-Consistent Margin Loss for Open-World Deep Metric Learning\n\nSummary:\nThe paper presents a novel approach to address threshold inconsistency in open-world deep metric learning (DML). Threshold inconsistency refers to the phenomenon where different test classes and data distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy. The authors introduce a novel variance-based metric, Operating-Point-Inconsistency-Score (OPIS), to quantify threshold inconsistency. They also propose a simple yet effective regularization technique, Threshold-Consistent Margin (TCM) loss, to promote uniformity in representation structures across classes. The results show that TCM improves threshold consistency while preserving accuracy, and it outperforms SOTA DML losses in various image retrieval benchmarks.\n\nStrengths:\n\n1. The authors provide a clear and concise introduction to the problem of threshold inconsistency in open-world DML.\n2. The introduction of OPIS as a novel variance-based metric to quantify threshold inconsistency is a significant contribution.\n3. The TCM loss is a simple and effective regularization technique that promotes uniformity in representation structures across classes.\n4. The results show that TCM improves threshold consistency while preserving accuracy, and it outperforms SOTA DML losses in various image retrieval benchmarks.\n\nWeaknesses:\n\n1. The authors may want to provide more details on how OPIS is calculated, especially for the \u03f5-OPIS metric.\n2. The ablation study may want to include more variations of the TCM loss, such as different values of \u03bb+ and \u03bb-, to show its robustness.\n3. The results may want to include more visualizations, such as plots of OPIS and \u03f5-OPIS against recall@k, to show the effectiveness of TCM in improving threshold consistency.\n4. The authors may want to provide more details on how TCM is compatible with different backbone architectures, such as ResNet variants and Vision Transformers.\n\nSuggestions for Improvement:\n\n1. The authors may want to provide more details on how OPIS is calculated, especially for the \u03f5-OPIS metric.\n2. The ablation study may want to include more variations of the TCM loss, such as different values of \u03bb+ and \u03bb-, to show its robustness.\n3. The results may want to include more visualizations, such as plots of OPIS and \u03f5-OPIS against recall@k, to show the effectiveness of TCM in improving threshold consistency.\n4. The authors may want to provide more details on how TCM is compatible with different backbone architectures, such as ResNet variants and Vision Transformers.\n\nRecommendation:\nOverall, the paper presents a significant contribution to the field of open-world deep metric learning by introducing a novel approach to address threshold inconsistency. The results show that TCM improves threshold consistency while preserving accuracy, and it outperforms SOTA DML losses in various image retrieval benchmarks. The authors provide a clear and concise introduction to the problem, and their results are well-organized and easy to follow. With some additional details and visualizations, the paper can be even more effective in demonstrating the effectiveness of TCM.\n\nRating: 8/10\n\n$$$$",
        "gr_score_with": 0.4727272727272727,
        "gr_score_without": 0.4089068825910931,
        "or_score_with": 0.4727272727272727,
        "or_score_without": 0.4534412955465587,
        "output_without": {
            "num_tokens_scored": 247,
            "num_green_tokens": 101,
            "green_fraction": 0.4089068825910931,
            "num_orange_tokens": 112,
            "orange_fraction": 0.4534412955465587,
            "green_z_score": 0.28573838340637064,
            "orange_z_score": 83.2490774774772,
            "green_p_value": 0.38753925203275086,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 330,
            "num_green_tokens": 156,
            "green_fraction": 0.4727272727272727,
            "num_orange_tokens": 156,
            "orange_fraction": 0.4727272727272727,
            "green_z_score": 2.696799449852968,
            "orange_z_score": 100.38325383423569,
            "green_p_value": 0.0035004709947243163,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
        "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a new data compression method called RECOMBINER, which extends the existing COMBINER method. RECOMBINER addresses the limitations of COMBINER, including the use of factorized priors and posterior approximations, the inability to adapt to local deviations from global patterns, and the susceptibility to modeling choices and initializations. The proposed method includes a linear reparameterization of the neural network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals.\n\nStrengths:\n1. RECOMBINER addresses the limitations of COMBINER and provides a more robust and effective data compression method.\n2. The proposed method includes a linear reparameterization of the neural network weights, which provides a more expressive prior and posterior approximation.\n3. The use of learnable positional encodings enables the method to adapt to local deviations from global patterns.\n4. The hierarchical Bayesian model provides a more robust and expressive representation of high-resolution signals.\n\nWeaknesses:\n1. The encoding time complexity of RECOMBINER is high, which may limit its practical use.\n2. The method may not be suitable for all types of data, as it requires high-resolution signals and may not be effective for low-resolution signals.\n3. The performance of RECOMBINER may be sensitive to modeling choices and initializations, which may require careful tuning.\n\nSuggestions for Improvement:\n1. Reduce the encoding time complexity of RECOMBINER by reducing the number of parameters to optimize over and using inference over modulations instead of weights.\n2. Improve the robustness of RECOMBINER to modeling choices and initializations by using more robust optimization algorithms or techniques.\n3. Extend RECOMBINER to work with low-resolution signals by using techniques such as downsampling or image compression algorithms.\n4. Experiment with different types of data to determine the effectiveness of RECOMBINER for different applications.\n\nRecommendation:\nRECOMBINER is a promising data compression method that addresses the limitations of COMBINER. However, its high encoding time complexity and sensitivity to modeling choices and initializations may limit its practical use. To improve the effectiveness of RECOMBINER, further research is needed to reduce its encoding time complexity and improve its robustness to modeling choices and initializations. Additionally, experiments with different types of data are necessary to determine the effectiveness of RECOMBINER for different applications.",
        "peer_review_with_watermark": "$$$\\text{Paper Summary:}$\nThe paper proposes a new data compression method called RECOMBINER, which is an extension of the COMBINER method. RECOMBINER addresses the limitations of COMBINER by introducing a linear reparameterization of the network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The method is evaluated on several data modalities, including images, audio, video, and 3D protein structures.\n\n$\\text{Strengths:}$\n\n* RECOMBINER achieves competitive results with state-of-the-art INR-based methods and outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n* The method is robust to model size and can handle high-resolution signals effectively.\n* The use of learnable positional encodings and a hierarchical Bayesian model enhances the performance of RECOMBINER.\n\n$\\text{Weaknesses:}$\n\n* The encoding time complexity of RECOMBINER is high, which can be a limitation in practical applications.\n* The method is suboptimal for compressing data with patches, as it leads to block artifacts.\n\n$\\text{Suggestions for Improvement:}$\n\n* To address the high encoding time complexity, it is recommended to reduce the number of parameters to optimize over and switch from inference over weights to modulations using, e.g., FiLM layers.\n* An exact REC algorithm could be adapted to solve the issue of approximate samples given by A\u02da coding, which significantly impacts the method's performance.\n* The hierarchical model can be further refined to improve its performance and robustness.\n\n$\\text{Recommendation:}$\n\nI recommend that the authors address the limitations of RECOMBINER and further improve its performance and efficiency. The method shows promising results and has the potential to become a state-of-the-art data compression method. However, it requires significant improvement in terms of encoding time complexity and robustness to high-resolution signals.\n\n$$$ \\text{Recommendation Level:} \\  \\text{Recommendation Level: Strong}$",
        "gr_score_with": 0.5189873417721519,
        "gr_score_without": 0.391304347826087,
        "or_score_with": 0.4430379746835443,
        "or_score_without": 0.5183946488294314,
        "output_without": {
            "num_tokens_scored": 299,
            "num_green_tokens": 117,
            "green_fraction": 0.391304347826087,
            "num_orange_tokens": 155,
            "orange_fraction": 0.5183946488294314,
            "green_z_score": -0.3069249070224267,
            "orange_z_score": 100.58728204490622,
            "green_p_value": 0.620549734793804,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 316,
            "num_green_tokens": 164,
            "green_fraction": 0.5189873417721519,
            "num_orange_tokens": 140,
            "orange_fraction": 0.4430379746835443,
            "green_z_score": 4.317562994441477,
            "orange_z_score": 88.14680804012004,
            "green_p_value": 7.888069549247439e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n&\\text{Paper Summary:} \\; The paper proposes a novel framework LEMUR that integrates large language models (LLMs) and automated reasoners for automated program verification. LEMUR combines the strengths of LLMs and automated reasoners to provide a sound and terminating algorithm for checking whether a property is an invariant on a program. The paper also presents strategies to instantiate LEMUR as an automated decision procedure and demonstrates its efficiency on standard benchmark sets.\\\\\n&\\text{Strengths:} \\; The paper provides a novel and innovative approach to combining LLMs and automated reasoners for program verification. The use of LLMs to generate new properties and repair existing ones is a significant contribution. The paper also demonstrates the efficiency of LEMUR on standard benchmark sets.\\\\\n&\\text{Weaknesses:} \\; The paper assumes that the LLM oracles are correct, which may not always be the case. The paper also does not provide a detailed analysis of the limitations of LLMs and automated reasoners.\\\\\n&\\text{Suggestions for Improvement:} \\; The paper should provide a more detailed analysis of the limitations of LLMs and automated reasoners. The paper should also explore the use of multiple LLM oracles to improve the robustness of LEMUR.\\\\\n&\\text{Recommendation:} \\; The paper makes a significant contribution to the field of program verification by proposing a novel framework that integrates LLMs and automated reasoners. The paper demonstrates the efficiency of LEMUR on standard benchmark sets. The paper provides a good starting point for further research on the use of LLMs in program verification.\n\n$$\\begin{aligned}\n&\\text{Paper Summary:} \\; The paper proposes a novel framework LEMUR that integrates large language models (LLMs) and automated reasoners for automated program verification. LEMUR combines the strengths of LLMs and automated reasoners to provide a sound and terminating algorithm for checking whether a property is an invariant on a program. The paper also presents strategies to instantiate LEMUR as an automated decision procedure and demonstrates its efficiency on standard benchmark sets.\\\\\n&\\text{Strengths:} \\; The paper provides a novel and innovative approach to combining LLMs and automated reasoners for program verification. The use of LLMs to generate new properties and repair existing ones is a significant contribution. The paper also demonstrates the efficiency of LEMUR on standard benchmark sets.\\\\\n&\\text{Weaknesses:} \\; The paper assumes that the LLM oracles are correct, which may not always be the case. The paper also does not provide a detailed analysis of the limitations of LLMs and automated reasoners.\\\\\n&\\text{Suggestions for Improvement:} \\; The paper should provide a more detailed analysis of the limitations of LLMs and automated reasoners. The paper should also explore the use of multiple LLM oracles to improve the robustness of LEMUR.\\\\\n&\\text{Recommendation:} \\; The paper makes a significant contribution to the field of program verification by proposing a novel framework that integrates LLMs and automated reasoners. The paper demonstrates the efficiency of LEMUR on standard benchmark sets. The paper provides a good starting point for further research on the use of LLMs in program verification.",
        "peer_review_with_watermark": "$$ $$$\n\nPaper Title: LEMUR: INTEGRATING LARGE LANGUAGE MODELS\n\nSummary:\nThe paper proposes a novel framework, LEMUR, that combines Large Language Models (LLMs) and automated reasoners for automated program verification. The framework uses a calculus to integrate LLMs' ability to perform abstract high-level reasoning and automated reasoners' ability to perform precise low-level reasoning. The paper demonstrates the efficiency of LEMUR on several benchmark sets, outperforming state-of-the-art conventional verification tools and LLM-based approaches.\n\nStrengths:\n1. The paper provides a novel approach to combining LLMs and automated reasoners for program verification, which has not been attempted before.\n2. The framework uses a calculus to integrate LLMs' and automated reasoners' capabilities, providing a formal and systematic approach to program verification.\n3. The paper demonstrates the efficiency of LEMUR on several benchmark sets, showing its potential to solve challenging program verification problems.\n\nWeaknesses:\n1. The paper assumes that the LLMs are capable of generating correct and relevant properties, which may not always be the case. This assumption needs to be validated through empirical evaluation.\n2. The framework may be vulnerable to over-reliance on LLMs, potentially leading to inaccurate or incomplete results. This needs to be addressed through design improvements or evaluation strategies.\n3. The paper does not provide a comprehensive evaluation of the framework's scalability, performance, or usability, which are essential for its practical application.\n\nSuggestions for Improvement:\n1. Conduct empirical evaluation of the framework's performance, scalability, and usability to validate its assumptions and identify potential weaknesses.\n2. Design improvements to address the potential risks associated with over-reliance on LLMs, such as incorporating multiple LLMs or automated reasoners, or using techniques like ensemble methods or uncertainty quantification.\n3. Develop strategies for guiding the LLMs' output, such as using explicit prompts or post-processing techniques, to ensure that the generated properties are relevant, correct, and complete.\n\nRecommendation:\nBased on the paper's contributions, potential, and limitations, we recommend that the authors continue to develop and refine the LEMUR framework. This can be achieved through empirical evaluation, design improvements, and further research into the capabilities and limitations of LLMs and automated reasoners. The framework has the potential to significantly advance the state-of-the-art in program verification, and with continued development, it can be made more reliable, efficient, and practical for solving challenging program verification problems.",
        "gr_score_with": 0.44959128065395093,
        "gr_score_without": 0.28640776699029125,
        "or_score_with": 0.49318801089918257,
        "or_score_without": 0.6310679611650486,
        "output_without": {
            "num_tokens_scored": 206,
            "num_green_tokens": 59,
            "green_fraction": 0.28640776699029125,
            "num_orange_tokens": 130,
            "orange_fraction": 0.6310679611650486,
            "green_z_score": -3.3279487253331603,
            "orange_z_score": 94.88819093687074,
            "green_p_value": 0.9995625601636833,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 367,
            "num_green_tokens": 165,
            "green_fraction": 0.44959128065395093,
            "num_orange_tokens": 181,
            "orange_fraction": 0.49318801089918257,
            "green_z_score": 1.9392452439694015,
            "orange_z_score": 98.58268820871757,
            "green_p_value": 0.026235741407031398,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. The authors consider a training dataset with well-separated input vectors and analyze the early alignment phase of the gradient flow dynamics. They provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5), where n is the number of data points and \u00b5 measures how well the data are separated. The authors also show that after the early alignment phase, the loss converges to zero at a O(1t) rate, and the weight matrix on the first layer is approximately low-rank.\n\n$$\\begin{aligned}\n$$Strengths:\n1. The paper provides a complete analysis of the early alignment phase of the gradient flow dynamics, which is crucial for understanding the training dynamics of two-layer ReLU networks.\n2. The authors provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is a significant contribution to the field.\n3. The paper includes numerical experiments to validate the theoretical findings, which provides empirical evidence for the proposed results.\n\n$$\\begin{aligned}\n$$Weaknesses:\n1. The paper assumes that the initialization is small, which may not be realistic in practice. The authors mention that they consider a deterministic choice of \u03f5, W0, and v(0), but they do not discuss the case where W0 is chosen randomly via some distribution.\n2. The paper does not provide a detailed analysis of the alignment phase for the case where the initialization scale \u03f5 is finite but small. The authors mention that the approximation in equation (4) is only valid before T = \u0398(1n log 1\u221ah\u03f5), but they do not provide a detailed analysis of the alignment phase for the case where T > t1.\n3. The paper does not discuss the case where the data are not well-separated. The authors mention that the data assumption is Assumption 1, which states that any pair of data with the same (different) label is positively (negatively) correlated, but they do not discuss the case where the data are not well-separated.\n\n$$\\begin{aligned}\n$$Suggestions for Improvement:\n1. The authors should discuss the case where W0 is chosen randomly via some distribution.\n2. The authors should provide a detailed analysis of the alignment phase for the case where T > t1.\n3. The authors should discuss the case where the data are not well-separated.\n\n$$\\begin{aligned}\n$$Recommendation:\nThe paper provides a significant contribution to the field of neural networks and training dynamics. However, the authors should address the weaknesses mentioned above to improve the paper. Specifically, the authors should discuss the case where W0 is chosen randomly via some distribution, provide a detailed analysis of the alignment phase for the case where T > t1, and discuss the case where the data are not well-separated.",
        "peer_review_with_watermark": "$$\\begin{aligned} &\\text{Paper Summary:} \\\\ &\\text{The paper studies the early neuron alignment in two-layer ReLU networks with small initialization. It considers a training dataset with well-separated input vectors and a two-layer ReLU network with gradient flow dynamics. The analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data.}\\\\ &\\text{Strengths:} \\\\ &\\text{The paper provides a thorough analysis of the early alignment phase in two-layer ReLU networks with small initialization. It considers a training dataset with well-separated input vectors and a two-layer ReLU network with gradient flow dynamics.}\\\\ &\\text{Weaknesses:} \\\\ &\\text{The paper assumes that the ReLU subgradient is fixed as \u03c3\u2032(x) = 1x>01, which may not be realistic in practice. Additionally, the analysis relies heavily on the assumption that all columns of W0 are nonzero, which may not be true in all cases.}\\\\ &\\text{Suggestions for Improvement:} \\\\ &\\text{The paper could benefit from a more comprehensive analysis that accounts for different types of initialization and data separability. Additionally, the analysis could be made more general by dropping the assumption that all columns of W0 are nonzero.}\\\\ &\\text{Recommendation:} \\\\ &\\text{The paper contributes to the existing literature on neural networks and their alignment with the input data. It provides a thorough analysis of the early alignment phase in two-layer ReLU networks with small initialization and considers a training dataset with well-separated input vectors.}\\\\ &\\text{However, the paper relies heavily on some strong assumptions and may not be generalizable to all cases. A more comprehensive analysis that accounts for different types of initialization and data separability would be beneficial.}\\\\ &\\end{aligned}$$",
        "gr_score_with": 0.5096525096525096,
        "gr_score_without": 0.42528735632183906,
        "or_score_with": 0.4749034749034749,
        "or_score_without": 0.5201149425287356,
        "output_without": {
            "num_tokens_scored": 348,
            "num_green_tokens": 148,
            "green_fraction": 0.42528735632183906,
            "num_orange_tokens": 181,
            "orange_fraction": 0.5201149425287356,
            "green_z_score": 0.9629138409712967,
            "orange_z_score": 115.85412222839122,
            "green_p_value": 0.16779538151308615,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 259,
            "num_green_tokens": 132,
            "green_fraction": 0.5096525096525096,
            "num_orange_tokens": 123,
            "orange_fraction": 0.4749034749034749,
            "green_z_score": 3.6021586549660705,
            "orange_z_score": 91.14302256807508,
            "green_p_value": 0.00015779282517159048,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "VONET: UNSUPERVISED VIDEO OBJECT LEARNING WITH PARALLEL U-NET ATTENTION",
        "abstract": "Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.",
        "peer_review_without_watermark": "$$$\\boxed{VONET: UNSUPERVISED VIDEO OBJECT LEARNING WITH PARALLEL U-NET ATTENTION}$$\n\nPaper Summary:\nThe paper introduces VONet, an innovative approach for unsupervised video object learning. VONet leverages a U-Net architecture and a parallel attention mechanism to generate attention masks for all slots simultaneously. The attention masks are then used to generate object representations from complex video scenes. The paper presents VONet's architecture, training loss, and evaluation on five MOVI datasets. VONet consistently demonstrates superior effectiveness compared to several strong baselines.\n\nStrengths:\n\n1. Parallel attention mechanism: VONet's parallel attention mechanism allows for efficient generation of attention masks for all slots simultaneously.\n2. Strong inductive bias of spatial locality: The U-Net architecture's spatial locality bias leads to smoother and more compact object segmentation masks.\n3. Object-wise sequential VAE framework: The object-wise sequential VAE framework promotes temporal consistency of individual slots across consecutive video frames.\n4. Effective handling of complex video scenes: VONet's transformer-based decoder empowers itself to derive object representations from complex video scenes.\n\nWeaknesses:\n\n1. Limited handling of over-segmentation: VONet may struggle with over-segmentation when the video scene's object count is much lower than the pre-allocated slot number.\n2. Incomplete object understanding: The absence of objectness priors may lead to incomplete object understanding, especially when an object exhibits multiple texture regions.\n3. Potential failure modes: The model may exhibit failure modes such as over-segmentation, incomplete object understanding, and failure to enforce temporal consistency.\n\nSuggestions for Improvement:\n\n1. Additional constraints or priors: Incorporating additional constraints or priors, such as objectness priors, may help address over-segmentation and incomplete object understanding.\n2. Long-term memory model: Using a long-term memory model, such as a recurrent neural network, may help enforce temporal consistency and address failure modes.\n3. Pretrained knowledge: Integrating pretrained knowledge about the appearances of everyday objects may help improve object understanding and reduce failure modes.\n\nRecommendation:\n\nBased on the paper's presentation and evaluation, I highly recommend VONet as a state-of-the-art approach for unsupervised video object learning. However, I suggest that the authors address the limitations and potential failure modes mentioned in the paper to further improve the model's performance and robustness.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$VONet: UNSUPERVISED VIDEO OBJECT LEARNING WITH PARALLEL U-NET ATTENTION$$\n$$Abstract:$$\nUnsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.\n$$Paper Summary:$$\nThe paper introduces VONet, a novel approach for unsupervised video object learning. VONet is inspired by MONet, but with a parallel attention mechanism that generates attention masks for all slots simultaneously. To ensure temporal consistency, VONet employs an object-wise sequential VAE framework. The paper provides a detailed overview of the VONet architecture, including the parallel attention mechanism, context vector calculation, and KLD loss. It also presents experiments on five MOVI datasets, showcasing the effectiveness of VONet compared to several baselines.\n$$Strengths:$$\n1. The paper introduces a novel parallel attention mechanism for unsupervised video object learning, which is efficient and effective.\n2. The use of an object-wise sequential VAE framework enhances temporal consistency and promotes robustness.\n3. The VONet architecture is well-structured and easy to follow.\n4. The experiments demonstrate the effectiveness of VONet compared to several baselines.\n$$Weaknesses:$$\n1. The paper assumes a high level of background knowledge on unsupervised video object learning and U-Net architecture.\n2. Some sections, such as the discussion of failure modes, could be more detailed and comprehensive.\n3. The comparison with baselines could be more rigorous, including more comprehensive experiments and analysis.\n4. The paper does not provide explicit guidance on how to implement VONet, which might limit its practicality.\n$$Suggestions for Improvement:$$\n1. Provide more detailed guidance on how to implement VONet, including code and implementation details.\n2. Increase the comprehensiveness of the comparison with baselines, including more experiments and analysis.\n3. Provide more explicit guidance on how to address failure modes, such as over-segmentation and incomplete object understanding.\n4. Consider adding more experimental results, such as visualizations and qualitative analysis, to further demonstrate the effectiveness of VONet.\n$$Recommendation:$$\nThe paper introduces a novel and effective approach for unsupervised video object learning, VONet. While there are some areas for improvement, the paper provides a well-structured and comprehensive overview of the VONet architecture and its experimental results. With more comprehensive guidance on implementation and failure modes, VONet has the potential to be a leading method for unsupervised video object learning.",
        "gr_score_with": 0.5345622119815668,
        "gr_score_without": 0.39577836411609496,
        "or_score_with": 0.4308755760368664,
        "or_score_without": 0.5145118733509235,
        "output_without": {
            "num_tokens_scored": 379,
            "num_green_tokens": 150,
            "green_fraction": 0.39577836411609496,
            "num_orange_tokens": 195,
            "orange_fraction": 0.5145118733509235,
            "green_z_score": -0.16776244879600807,
            "orange_z_score": 108.75542782859908,
            "green_p_value": 0.566614917368728,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 434,
            "num_green_tokens": 232,
            "green_fraction": 0.5345622119815668,
            "num_orange_tokens": 187,
            "orange_fraction": 0.4308755760368664,
            "green_z_score": 5.722191152188227,
            "orange_z_score": 97.15196927049215,
            "green_p_value": 5.257944711908531e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FLOW MATCHING ON GENERAL GEOMETRIES",
        "abstract": "We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world nonEuclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes Riemannian Flow Matching (RFM), a framework for training continuous normalizing flows on general Riemannian manifolds. RFM bypasses the limitations of existing methods, such as requiring expensive simulation or approximations for limiting quantities, and achieves state-of-the-art performance on various real-world non-Euclidean datasets. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which can be computed efficiently using spectral decompositions.\n\nStrengths:\n1. RFM is simulation-free on simple geometries, making it more scalable and efficient.\n2. The method introduces zero approximation errors on simple geometries with closed-form geodesics.\n3. RFM achieves state-of-the-art performance on various real-world non-Euclidean datasets.\n4. The framework is highly scalable, allowing for tractable training on general geometries, including closed manifolds and manifolds with boundaries.\n\nWeaknesses:\n1. The method relies on the construction of a premetric, which may not be suitable for all types of manifolds.\n2. The use of spectral distances as an alternative to geodesic distances may introduce bias in the training procedure.\n3. The method requires careful tuning of hyperparameters, particularly for the diffusion distance hyperparameter \u03c4.\n\nSuggestions for Improvement:\n1. Investigate alternative premetric constructions that can be more robust and suitable for a wider range of manifolds.\n2. Develop methods to reduce the impact of bias introduced by the use of spectral distances.\n3. Provide more detailed analysis of the effects of hyperparameter tuning on the performance of RFM.\n\nRecommendation:\nThe paper presents a highly scalable and efficient framework for training continuous normalizing flows on general Riemannian manifolds. While there are some limitations and potential biases in the method, the results demonstrate its effectiveness and potential for real-world applications. With further development and refinement, RFM has the potential to become a leading approach in the field of continuous generative modeling on manifolds.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces Riemannian Flow Matching (RFM), a framework for training continuous normalizing flows on general Riemannian manifolds. RFM is a simulation-free approach that bypasses the limitations of existing methods for generative modeling on manifolds. The key ingredient behind RFM is the construction of a premetric for defining target vector fields, which is done using spectral decompositions. RFM achieves state-of-the-art performance on many real-world nonEuclidean datasets and is highly scalable, allowing for tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.\n\nStrengths:\n1. RFM is a simulation-free approach that avoids the limitations of existing methods for generative modeling on manifolds.\n2. RFM is highly scalable and allows for tractable training on general geometries.\n3. The use of spectral decompositions for computing premetrics is efficient and effective.\n4. RFM achieves state-of-the-art performance on many real-world nonEuclidean datasets.\n\nWeaknesses:\n1. RFM requires a premetric, which can be challenging to design for general manifolds.\n2. The method requires a scheduler that determines the rate at which the premetric decreases, which can be a hyperparameter that needs to be tuned.\n3. The method does not work well for manifolds with highly non-trivial curvature and boundaries, as shown in the experiments with triangular meshes.\n\nSuggestions for Improvement:\n1. Design a more efficient algorithm for computing premetrics on general manifolds.\n2. Develop a method for choosing the scheduler that determines the rate at which the premetric decreases.\n3. Experiment with different premetrics and schedulers to see if they improve the performance of RFM on manifolds with highly non-trivial curvature and boundaries.\n\nRecommendation:\nRFM is a promising approach for training continuous normalizing flows on general Riemannian manifolds. With some improvements to the design of the premetric and scheduler, RFM has the potential to achieve even better performance on a wide range of manifolds.",
        "gr_score_with": 0.4721189591078067,
        "gr_score_without": 0.391025641025641,
        "or_score_with": 0.49070631970260226,
        "or_score_without": 0.5160256410256411,
        "output_without": {
            "num_tokens_scored": 312,
            "num_green_tokens": 122,
            "green_fraction": 0.391025641025641,
            "num_orange_tokens": 161,
            "orange_fraction": 0.5160256410256411,
            "green_z_score": -0.32357511446471826,
            "orange_z_score": 111.2279347459829,
            "green_p_value": 0.626870134522129,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 269,
            "num_green_tokens": 127,
            "green_fraction": 0.4721189591078067,
            "num_orange_tokens": 132,
            "orange_fraction": 0.49070631970260226,
            "green_z_score": 2.4144597452047876,
            "orange_z_score": 98.14613139013383,
            "green_p_value": 0.007879282091323249,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MODERNTCN: A MODERN PURE CONVOLUTION STRUCTURE FOR GENERAL TIME SERIES ANALYSIS",
        "abstract": "Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN.",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper proposes a modern pure convolution structure, namely ModernTCN, to efficiently utilize cross-time and cross-variable dependency for general time series analysis. The authors modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models.\n\nStrengths:\n- The authors successfully bring convolution back to time series analysis by modernizing the traditional TCN.\n- ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks.\n- The paper provides a better balance of efficiency and performance compared to state-of-the-art Transformer-based and MLP-based models.\n- The authors propose a novel solution to the question of how to better use convolution in time series analysis.\n\nWeaknesses:\n- The paper does not provide a clear comparison with other state-of-the-art models in terms of computational complexity and memory usage.\n- The authors do not provide a detailed analysis of the effect of kernel size on the performance of ModernTCN.\n- The paper does not discuss the potential limitations of ModernTCN in terms of handling non-stationary time series.\n\nSuggestions for Improvement:\n- The authors should provide a detailed comparison with other state-of-the-art models in terms of computational complexity and memory usage.\n- The authors should analyze the effect of kernel size on the performance of ModernTCN.\n- The authors should discuss the potential limitations of ModernTCN in terms of handling non-stationary time series.\n\nRecommendation:\n- The authors have made a significant contribution to the field of time series analysis by proposing a modern pure convolution structure, namely ModernTCN.\n- ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models.\n- The authors should continue to improve ModernTCN by addressing the potential limitations and providing a detailed analysis of its performance.\n\n$$",
        "peer_review_with_watermark": "$$$\\$\n\nPaper Summary:\nThe paper proposes a modern pure convolution structure, called ModernTCN, for general time series analysis. The authors aim to bring convolution back to the arena of time series analysis by modernizing the traditional TCN (Temporal Convolutional Networks) block with time series related modifications. ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection, while maintaining the efficiency advantage of convolution-based models.\n\nStrengths:\n1. The authors successfully bring convolution back to the arena of time series analysis, providing a new perspective on the importance of convolution in time series analysis.\n2. ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, demonstrating its excellent task-generality.\n3. The authors provide a better balance of efficiency and performance by maintaining the efficiency advantage of convolution-based models while competing favorably with or even better than state-of-the-art Transformer-based models.\n4. The design of ModernTCN block, particularly the decoupling design of DWConv and ConvFFN, allows for better handling of the variable dimension in time series analysis.\n\nWeaknesses:\n1. The paper does not provide a detailed theoretical analysis of the effectiveness of the proposed design, which may be a significant limitation.\n2. The authors rely on experimental results to demonstrate the effectiveness of ModernTCN, which may not be generalizable to all time series analysis tasks.\n3. The paper does not discuss the potential issues with the proposed design, such as the choice of kernel size and the number of layers, which may be crucial for optimal performance.\n\nSuggestions for Improvement:\n1. The authors should provide a detailed theoretical analysis of the proposed design to demonstrate its effectiveness and provide a solid foundation for the results.\n2. The authors should explore the potential issues with the proposed design, such as the choice of kernel size and the number of layers, to ensure optimal performance.\n3. The authors should conduct more extensive experiments to demonstrate the generalizability of ModernTCN to different time series analysis tasks and datasets.\n\nRecommendation:\nThe paper presents an innovative approach to bring convolution back to the arena of time series analysis, achieving consistent state-of-the-art performance on five mainstream time series analysis tasks. However, the lack of theoretical analysis and the potential issues with the proposed design should be addressed to ensure the robustness and generalizability of the results.",
        "gr_score_with": 0.5161290322580645,
        "gr_score_without": 0.4343891402714932,
        "or_score_with": 0.43548387096774194,
        "or_score_without": 0.4841628959276018,
        "output_without": {
            "num_tokens_scored": 221,
            "num_green_tokens": 96,
            "green_fraction": 0.4343891402714932,
            "num_orange_tokens": 107,
            "orange_fraction": 0.4841628959276018,
            "green_z_score": 1.0435465691240697,
            "orange_z_score": 90.01278951375306,
            "green_p_value": 0.14834761217328535,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 310,
            "num_green_tokens": 160,
            "green_fraction": 0.5161290322580645,
            "num_orange_tokens": 135,
            "orange_fraction": 0.43548387096774194,
            "green_z_score": 4.173650061841514,
            "orange_z_score": 95.74855701373079,
            "green_p_value": 1.4987903171198751e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "NAVIGATING TEXT-TO-IMAGE CUSTOMIZATION: FROM",
        "abstract": "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied finetuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces LyCORIS, an open-source library that provides a wide range of fine-tuning methodologies for Stable Diffusion, a leading open-source text-to-image generative model. The library offers a comprehensive framework for systematic evaluation of fine-tuning techniques, including hyperparameter adjustments and evaluation with different prompt types across various concept categories. The authors present extensive experiments to compare different LyCORIS algorithms and assess the impact of hyperparameters on model performance. They also propose a more comprehensive evaluation framework that better captures the nuances of different fine-tuning methods.\n\nStrengths:\n1. The authors provide a comprehensive overview of the challenges involved in fine-tuning text-to-image models and propose a solution in the form of LyCORIS.\n2. The library offers a wide range of fine-tuning methodologies, including LoRA, LoHa, and LoKr, which can be used to adapt Stable Diffusion to specific domains or concepts.\n3. The authors propose a systematic evaluation framework that captures key aspects of fine-tuning, including concept fidelity, text-image alignment, image diversity, and preservation of the base model's style.\n4. The extensive experiments provide valuable insights into the impact of hyperparameters on model performance and highlight the relative strengths and limitations of different fine-tuning methods.\n\nWeaknesses:\n1. The paper's scope is limited, as it does not explore the task of generating images with multiple learned concepts, which is a highly sensitive and challenging task.\n2. The evaluation framework proposed by the authors may not be comprehensive enough to capture all the nuances of fine-tuning techniques.\n3. The paper does not provide a clear comparison of the performance of LyCORIS algorithms with other state-of-the-art methods, which may limit its applicability.\n\nSuggestions for Improvement:\n1. The authors should explore the task of generating images with multiple learned concepts to further enrich the comparison of fine-tuning methods.\n2. The evaluation framework should be extended to capture more nuances of fine-tuning techniques, such as the impact of different prompt types and the role of concept descriptors.\n3. The authors should provide a clear comparison of the performance of LyCORIS algorithms with other state-of-the-art methods to demonstrate its applicability and superiority.\n\nRecommendation:\nI recommend that the authors revise and expand their paper to address the limitations mentioned above. Specifically, they should explore the task of generating images with multiple learned concepts, extend the evaluation framework to capture more nuances of fine-tuning techniques, and provide a clear comparison of the performance of LyCORIS algorithms with other state-of-the-art methods. With these revisions, the paper will provide a more comprehensive and applicable solution for fine-tuning text-to-image models.",
        "peer_review_with_watermark": "$$\n$$ $$$$\n\nPaper Title: NAVIGATING TEXT-TO-IMAGE CUSTOMIZATION: FROM\nText-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.\n\nThe introduction of LyCORIS, an open-source library, presents a significant improvement in the field, providing a comprehensive framework for the systematic assessment of varied fine-tuning techniques. The proposed library encapsulates a spectrum of methodologies, including LoRA, LoHa, LoKr, and others, and offers a flexible and user-friendly interface for users to experiment with different fine-tuning strategies.\n\nThe experimental results demonstrate the effectiveness of LyCORIS in fine-tuning Stable Diffusion, showcasing its ability to improve concept fidelity, text-image alignment, and diversity while preserving the base model's style. The study highlights the importance of considering multiple evaluation criteria, including concept fidelity, text-image alignment, image diversity, and base model preservation.\n\nHowever, the study has some limitations, such as the small dataset used and the lack of evaluation on multiple learned concepts. Furthermore, the study could benefit from more comprehensive analysis of the influence of dataset, hyperparameters, and other factors on the performance of fine-tuning algorithms.\n\nRecommendations:\n\n1. The study should consider the use of more comprehensive evaluation frameworks that incorporate multiple learned concepts and evaluation metrics.\n2. The study could benefit from more in-depth analysis of the influence of dataset, hyperparameters, and other factors on the performance of fine-tuning algorithms.\n3. The study should consider the integration of emerging evaluation frameworks that aim to address the challenges of generating images with multiple learned concepts.\n\nPaper Strengths:\n\n1. The introduction of LyCORIS, an open-source library, presents a significant improvement in the field, providing a comprehensive framework for the systematic assessment of varied fine-tuning techniques.\n2. The experimental results demonstrate the effectiveness of LyCORIS in fine-tuning Stable Diffusion, showcasing its ability to improve concept fidelity, text-image alignment, and diversity while preserving the base model's style.\n3. The study highlights the importance of considering multiple evaluation criteria, including concept fidelity, text-image alignment, image diversity, and base model preservation.\n\nPaper Weaknesses:\n\n1. The study has some limitations, such as the small dataset used and the lack of evaluation on multiple learned concepts.\n2. The study could benefit from more comprehensive analysis of the influence of dataset, hyperparameters, and other factors on the performance of fine-tuning algorithms.\n3. The study does not provide a clear comparison of the performance of different fine-tuning algorithms and their configurations.\n\nSuggestions for Improvement:\n\n1. Conduct more comprehensive analysis of the influence of dataset, hyperparameters, and other factors on the performance of fine-tuning algorithms.\n2. Evaluate the performance of different fine-tuning algorithms and their configurations on multiple learned concepts.\n3. Provide more detailed analysis of the influence of dataset, hyperparameters, and other factors on the performance of fine-tuning algorithms.\n\nRecommendation:\n\nThe study presents a significant improvement in the field of text-to-image customization, and the introduction of LyCORIS, an open-source library, provides a comprehensive framework for the systematic assessment of varied fine-tuning techniques. However, the study has some limitations, such as the small dataset used and the lack of evaluation on multiple learned concepts. To further improve the study, more comprehensive analysis of the influence of dataset, hyperparameters, and other factors on the performance of fine-tuning algorithms, as well as evaluation of the performance of different fine-tuning algorithms and their configurations on multiple learned concepts, are needed.\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$$\n\n$$ $$$",
        "gr_score_with": 0.628099173553719,
        "gr_score_without": 0.5145348837209303,
        "or_score_with": 0.3333333333333333,
        "or_score_without": 0.4273255813953488,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 177,
            "green_fraction": 0.5145348837209303,
            "num_orange_tokens": 147,
            "orange_fraction": 0.4273255813953488,
            "green_z_score": 4.336220713653698,
            "orange_z_score": 85.80907512778613,
            "green_p_value": 7.247669884027336e-06,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 363,
            "num_green_tokens": 228,
            "green_fraction": 0.628099173553719,
            "num_orange_tokens": 121,
            "orange_fraction": 0.3333333333333333,
            "green_z_score": 8.870975982158502,
            "orange_z_score": 68.37587751453462,
            "green_p_value": 3.625379641146575e-19,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ENHANCING NEURAL TRAINING VIA A CORRELATED DYNAMICS MODEL",
        "abstract": "As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this, we introduce correlation mode decomposition (CMD). This algorithm clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables CMD to efficiently represent the training dynamics of complex networks, like ResNets and Transformers, using only a few modes. Moreover, test set generalization is enhanced. We introduce an efficient CMD variant, designed to run concurrently with training. Our experiments indicate that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification. Our modeling can improve training efficiency and lower communication overhead, as shown by our preliminary experiments in the context of federated learning.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces Correlation Mode Decomposition (CMD), a novel approach to model neural network parameter dynamics. CMD captures the correlated behavior of the training process and efficiently represents the training dynamics of complex networks using only a few modes. The authors demonstrate the effectiveness of CMD in various architectures and learning tasks, including image classification, segmentation, and generative image style transfer. Additionally, they propose an efficient online variant of CMD, which can be performed concurrently with training, reducing the computational overhead. The paper also explores the application of CMD in federated learning, where it significantly reduces the communication overhead.\n\nStrengths:\n\n* CMD provides a novel and effective approach to model neural network parameter dynamics, capturing the correlated behavior of the training process.\n* The authors demonstrate the effectiveness of CMD in various architectures and learning tasks, showcasing its ability to improve accuracy and reduce computational overhead.\n* The proposed online variant of CMD is efficient and can be performed concurrently with training, reducing the computational overhead.\n* The paper explores the application of CMD in federated learning, where it significantly reduces the communication overhead.\n\nWeaknesses:\n\n* The paper does not provide a comprehensive discussion of the limitations and potential biases of CMD, which is a common criticism of correlation-based methods.\n* The authors rely heavily on numerical experiments to demonstrate the effectiveness of CMD, which may not generalize well to other architectures or learning tasks.\n* The paper does not provide a detailed analysis of the theoretical properties of CMD, which is essential for understanding its behavior and limitations.\n* The authors propose a novel approach, but it may be challenging for other researchers to replicate and verify the results due to the complexity of the methodology.\n\nSuggestions for Improvement:\n\n* Provide a more comprehensive discussion of the limitations and potential biases of CMD, including a discussion of its assumptions and potential sources of error.\n* Include more theoretical analysis of CMD, such as an analysis of its convergence properties and its relationship to other optimization algorithms.\n* Provide more detailed experimental results, including a comparison with other state-of-the-art methods, to demonstrate the generalizability of CMD.\n* Consider including more qualitative analysis, such as visualizations of the learned modes and their relationship to the original parameter space.\n\nRecommendation:\n\n* The paper presents a novel and effective approach to model neural network parameter dynamics, capturing the correlated behavior of the training process. While there are some limitations and potential biases, the authors demonstrate the effectiveness of CMD in various architectures and learning tasks, including image classification, segmentation, and generative image style transfer. The proposed online variant of CMD is efficient and can be performed concurrently with training, reducing the computational overhead. Overall, the paper is well-written and provides a clear and concise presentation of the CMD methodology. However, the authors should consider addressing the limitations and potential biases of CMD and providing more theoretical analysis to enhance the rigor and validity of the results.",
        "peer_review_with_watermark": "$$$$\n$$$$Peer Review$$$$\n$$$$\n\n**Paper Summary:**\nThe paper proposes a novel approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process. The authors introduce Correlation Mode Decomposition (CMD), a data-driven approach that efficiently models training dynamics using only a few modes. They also present efficient CMD variants, including an online version that can be performed concurrently with training, and a gradual embedding method that incorporates modeled dynamics into the training process. The authors demonstrate the effectiveness of their approach on various architectures and learning tasks, including image classification, segmentation, and generative image style transfer. Additionally, they show that their method can improve training efficiency and reduce communication overhead in federated learning.\n\n**Strengths:**\n\n1. The authors provide a thorough literature review, highlighting the limitations of existing approaches to modeling neural network parameter dynamics.\n2. The proposed CMD approach is well-structured, and the authors provide a clear and concise explanation of the algorithm's components and operations.\n3. The authors present a range of experimental results, including comparisons with state-of-the-art methods, to demonstrate the effectiveness of their approach.\n4. The gradual embedding method is an innovative contribution, as it allows for the incorporation of modeled dynamics into the training process, reducing the need for frequent model updates.\n\n**Weaknesses:**\n\n1. The paper's introduction could be more concise, as it includes unnecessary details about the authors' research interest and the context of the work.\n2. The authors rely heavily on existing literature, which may make the paper less original and less relevant to researchers who are already familiar with these topics.\n3. The evaluation of the proposed approach is sometimes biased, as the authors only present results that support their claims, without providing a more nuanced discussion of the limitations and potential drawbacks.\n4. The paper's discussion of potential applications and future work could be more comprehensive, as it only highlights a few potential avenues for further research, without providing a more detailed outline of the opportunities and challenges that lie ahead.\n\n**Suggestions for Improvement:**\n\n1. Provide a more concise introduction, focusing on the main contributions and implications of the paper.\n2. Consider revising the literature review to highlight more recent and relevant works, to make the paper more relevant to current research trends.\n3. Improve the evaluation section by providing more nuanced discussions of the limitations and potential drawbacks of the proposed approach, as well as more comprehensive comparisons with existing methods.\n4. Expand the discussion of potential applications and future work, highlighting more potential avenues for further research, as well as the opportunities and challenges that lie ahead.\n\n**Recommendation:**\nThe paper proposes a novel approach to modeling neural network parameter dynamics, leveraging the underlying correlated behavior of the training process. While the authors provide a thorough literature review and present effective experimental results, the paper could benefit from more nuanced discussions of the limitations and potential drawbacks of the proposed approach, as well as more comprehensive comparisons with existing methods. Overall, the paper is well-structured, and the authors demonstrate a good understanding of the underlying concepts and techniques. With some revisions, the paper has the potential to contribute significantly to the field of neural network training dynamics.",
        "gr_score_with": 0.45057471264367815,
        "gr_score_without": 0.3256484149855908,
        "or_score_with": 0.4804597701149425,
        "or_score_without": 0.5331412103746398,
        "output_without": {
            "num_tokens_scored": 347,
            "num_green_tokens": 113,
            "green_fraction": 0.3256484149855908,
            "num_orange_tokens": 185,
            "orange_fraction": 0.5331412103746398,
            "green_z_score": -2.8271532305570206,
            "orange_z_score": 116.4231407970069,
            "green_p_value": 0.9976518075271138,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 435,
            "num_green_tokens": 196,
            "green_fraction": 0.45057471264367815,
            "num_orange_tokens": 209,
            "orange_fraction": 0.4804597701149425,
            "green_z_score": 2.1531408048872462,
            "orange_z_score": 117.29719848047658,
            "green_p_value": 0.015653807096507658,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RingAttention with Blockwise Transformers for Near-Infinite Context",
        "abstract": "Transformers have emerged as the architecture of choice for many state-ofthe-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Blockwise RingAttention, which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach called RingAttention with Blockwise Transformers to reduce the memory requirements of Transformers. The authors leverage blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. The approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers without incurring additional communication and computation overheads.\n\nStrengths:\n1. The paper presents a novel approach to reduce the memory requirements of Transformers, which is essential for tackling complex AI problems.\n2. The authors provide a clear and concise explanation of the blockwise parallel transformer framework and its application to the RingAttention approach.\n3. The experimental results demonstrate the effectiveness of the RingAttention approach in reducing memory requirements and improving performance.\n\nWeaknesses:\n1. The paper relies heavily on prior work, such as the blockwise parallel transformer framework, and does not provide a comprehensive analysis of the underlying mathematical foundations.\n2. The authors do not provide a thorough discussion of the communication overheads and how they are mitigated in the RingAttention approach.\n3. The experimental results are mostly presented in tables and figures, which can make it difficult to fully understand the implications of the findings.\n\nSuggestions for Improvement:\n1. Provide a more comprehensive analysis of the underlying mathematical foundations of the blockwise parallel transformer framework and its application to the RingAttention approach.\n2. Include a detailed discussion of the communication overheads and how they are mitigated in the RingAttention approach.\n3. Consider providing additional experimental results, such as analysis of the communication overheads and the impact of varying device counts on performance.\n\nRecommendation:\nThe paper presents a novel approach to reduce the memory requirements of Transformers and demonstrates its effectiveness in experimental results. However, the lack of a comprehensive analysis of the underlying mathematical foundations and the communication overheads may limit the generalizability of the findings. To improve the paper, the authors should provide a more thorough discussion of the mathematical foundations and the communication overheads, as well as consider providing additional experimental results to further support the findings.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach called RingAttention with Blockwise Transformers, which aims to reduce the memory requirements of Transformers, enabling them to handle long sequences and modalities. The approach leverages blockwise computation of self-attention and feedforward, distributing the sequence dimensions across multiple devices, and overlapping communication with computation. The paper presents extensive experiments on language modeling and reinforcement learning tasks, demonstrating the effectiveness of the approach in allowing millions of tokens context size and improving performance.\n\nStrengths:\n1. The paper presents a novel approach to reducing memory requirements of Transformers, which is crucial for handling long sequences and modalities.\n2. The approach leverages blockwise computation, which can significantly reduce memory costs and enable zero-overhead scaling of context size.\n3. The paper presents extensive experiments, which demonstrate the effectiveness of the approach in improving performance on language modeling and reinforcement learning tasks.\n4. The approach can handle arbitrarily large context sizes, making it suitable for applications that require processing long sequences and modalities.\n\nWeaknesses:\n1. The paper assumes that the block size is larger than the ratio of FLOPs over bandwidth, which may not always be the case, and the analysis assumes that the computation time exceeds the communication time, which may not be true in all scenarios.\n2. The paper does not provide a detailed analysis of the communication overhead, which can be significant in certain scenarios, and the analysis assumes that the communication overhead can be fully overlapped with computation, which may not always be possible.\n3. The paper does not provide a comprehensive comparison with other approaches, and the analysis assumes that the proposed approach is superior to existing approaches, which may not be the case.\n4. The paper does not provide a detailed explanation of the implementation details, which can make it difficult to replicate the results and understand the approach.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the communication overhead and the scenarios where it can be fully overlapped with computation.\n2. Compare the proposed approach with other approaches, such as tensor parallelism, sequence parallelism, and sharding, and provide a comprehensive analysis of their strengths and weaknesses.\n3. Provide a more detailed explanation of the implementation details, including the choice of algorithms, data structures, and hardware architectures.\n4. Include more experiments and analysis to demonstrate the effectiveness of the approach in different scenarios and applications.\n\nRecommendation:\nBased on the paper summary, strengths, weaknesses, and suggestions for improvement, I recommend that the paper be accepted for publication in a conference or journal that focuses on AI and machine learning. The paper presents a novel approach that can significantly reduce memory requirements of Transformers, and the extensive experiments demonstrate its effectiveness in improving performance on language modeling and reinforcement learning tasks. However, the paper should address the limitations and challenges of the approach and provide a more comprehensive analysis of its strengths and weaknesses.",
        "gr_score_with": 0.49184782608695654,
        "gr_score_without": 0.43727598566308246,
        "or_score_with": 0.4483695652173913,
        "or_score_without": 0.45161290322580644,
        "output_without": {
            "num_tokens_scored": 279,
            "num_green_tokens": 122,
            "green_fraction": 0.43727598566308246,
            "num_orange_tokens": 126,
            "orange_fraction": 0.45161290322580644,
            "green_z_score": 1.2709416635183939,
            "orange_z_score": 96.47925878966518,
            "green_p_value": 0.10187470145745825,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 181,
            "green_fraction": 0.49184782608695654,
            "num_orange_tokens": 165,
            "orange_fraction": 0.4483695652173913,
            "green_z_score": 3.596558842455711,
            "orange_z_score": 109.99763148373914,
            "green_p_value": 0.00016122733350923816,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS",
        "abstract": "Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https://github.com/THUDM/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256\u00d7256 and CelebA-HQ 256\u00d7256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256\u00d7256 generation without any guidance. RDM can achieve a FID of 1.99 (and a class-balanced FID of 1.87) if with classifier-free guidance.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes the Relay Diffusion Model (RDM), a new cascaded framework for high-resolution image synthesis. The main contributions of the paper are:\n- Analysis of the difficulty of noise scheduling in high-resolution diffusion models in frequency domain.\n- Proposal of block noise to bridge the gap between low-resolution and high-resolution images.\n- Introduction of a new cascaded pipeline, Relay Diffusion Model, which transfers a low-resolution image or noise into an equivalent high-resolution one via blurring diffusion and block noise.\n- Evaluation of the effectiveness of RDM on unconditional CelebA-HQ 256\u00d7256 and conditional ImageNet 256\u00d7256 datasets, achieving state-of-the-art FID on CelebA-HQ and sFID on ImageNet.\n\nStrengths:\n- The paper provides a thorough analysis of the challenges in high-resolution image synthesis using diffusion models.\n- The introduction of block noise is a novel idea that bridges the gap between low-resolution and high-resolution images.\n- The Relay Diffusion Model achieves state-of-the-art results on CelebA-HQ and ImageNet datasets.\n\nWeaknesses:\n- The paper relies heavily on the analysis of frequency domain, which might not be the most practical approach for noise scheduling.\n- The training efficiency and convergence of the model are not thoroughly discussed.\n- The paper does not provide a comprehensive comparison with other state-of-the-art methods.\n\nSuggestions for Improvement:\n- Provide a more comprehensive analysis of the challenges in high-resolution image synthesis using diffusion models.\n- Discuss the training efficiency and convergence of the model in more detail.\n- Provide a more detailed comparison with other state-of-the-art methods.\n\nRecommendation:\nThe paper proposes a novel cascaded framework, Relay Diffusion Model, which achieves state-of-the-art results on CelebA-HQ and ImageNet datasets. However, the paper relies heavily on the analysis of frequency domain, which might not be the most practical approach for noise scheduling. The training efficiency and convergence of the model are not thoroughly discussed. To improve the paper, a more comprehensive analysis of the challenges in high-resolution image synthesis using diffusion models, a more detailed comparison with other state-of-the-art methods, and a discussion of the training efficiency and convergence of the model are necessary.",
        "peer_review_with_watermark": "$$$\\boxed{Reviewer Report: RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS}$$\n\n**Paper Summary:**\nThis paper proposes Relay Diffusion Model (RDM), a new cascaded framework for image synthesis that improves upon the shortcomings of previous cascaded methods. The main contributions of this paper are: 1) analysis of the difficulty of noise scheduling in high-resolution diffusion models in frequency domain, 2) proposal of Relay Diffusion Model (RDM) to disentangle the diffusion process and the underlying neural networks in the cascaded pipeline, and 3) evaluation of RDM on unconditional and class-conditional datasets.\n\n**Strengths:**\n\n1) The paper provides a comprehensive analysis of the frequency domain and noise scheduling in diffusion models, which is crucial for understanding the limitations of previous methods.\n2) The proposal of RDM is well-structured and clearly explains the benefits of block noise and blurring diffusion in improving training efficiency and performance.\n3) The experiments demonstrate the effectiveness of RDM on both CelebA-HQ and ImageNet datasets, with competitive results on state-of-the-art methods.\n\n**Weaknesses:**\n\n1) The theoretical derivation of the optimal noise schedule is lacking, and the empirical results on noise scheduling are limited to frequency analysis.\n2) The comparison with other methods is limited to FID and sFID metrics, and more comprehensive evaluations on other metrics such as IS and Precision are required.\n3) The ablation study on block noise and stochasticity has limited experiments and results.\n\n**Suggestions for Improvement:**\n\n1) Provide more comprehensive theoretical analysis on noise scheduling and its relation to the size of the model and inductive bias.\n2) Perform more comprehensive evaluations on other metrics such as IS and Precision to better understand the performance of RDM.\n3) Increase the number of experiments in ablation study to better understand the effects of block noise and stochasticity.\n\n**Recommendation:**\nBased on the review, I recommend that the paper be accepted with revisions to address the limitations mentioned above. Specifically, the authors should provide more comprehensive theoretical analysis on noise scheduling and perform more comprehensive evaluations on other metrics. With these revisions, the paper has the potential to make significant contributions to the field of diffusion models and image synthesis.",
        "gr_score_with": 0.5136612021857924,
        "gr_score_without": 0.36693548387096775,
        "or_score_with": 0.4207650273224044,
        "or_score_without": 0.5645161290322581,
        "output_without": {
            "num_tokens_scored": 248,
            "num_green_tokens": 91,
            "green_fraction": 0.36693548387096775,
            "num_orange_tokens": 140,
            "orange_fraction": 0.5645161290322581,
            "green_z_score": -1.0628754870986863,
            "orange_z_score": 108.21489850927827,
            "green_p_value": 0.8560807896098697,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 188,
            "green_fraction": 0.5136612021857924,
            "num_orange_tokens": 154,
            "orange_fraction": 0.4207650273224044,
            "green_z_score": 4.438611837673727,
            "orange_z_score": 97.58807773509763,
            "green_p_value": 4.527046729947097e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "A TOPOLOGICAL PERSPECTIVE ON DEMYSTIFYING GNN-BASED LINK PREDICTION PERFORMANCE",
        "abstract": "Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies aim to improve the overall LP performance of GNNs, none have explored its varying performance across different nodes and its underlying reasons. To this end, we aim to demystify which nodes will perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit poorer LP performance, our empirical findings provide nuances to this viewpoint and prompt us to propose a better metric, Topological Concentration (TC), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that TC has a higher correlation with LP performance than other node-level topological metrics like degree and subgraph density, offering a better way to identify low-performing nodes than using degree. With TC, we discover a novel topological distribution shift issue in which newly joined neighbors of a node tend to become less interactive with that node\u2019s existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To make the computation of TC scalable, We further propose Approximated Topological Concentration (ATC) and theoretically/empirically justify its efficacy in approximating TC and reducing the computation complexity. Given the positive correlation between node TC and its LP performance, we explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing and discuss its effectiveness with limitations. Our code is publicly available at https://github.com/submission2024/Topo Concentration.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes Topological Concentration (TC) to measure the local topology of nodes in a graph and its correlation with link prediction (LP) performance. The authors investigate the varying performance of Graph Neural Networks (GNNs) on different nodes within a graph, focusing on the role of local topology in LP. They demonstrate the superiority of TC over other node-level topological metrics, such as degree and subgraph density, in predicting LP performance. Additionally, the authors identify a novel topological distribution shift (TDS) issue, where newly joined neighbors of a node become less interactive with its existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To address this issue, they propose Approximated Topological Concentration (ATC), which reduces the computational complexity of TC while maintaining high correlations with LP performance. The authors also explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing process.\n\nStrengths:\n1. The paper provides a novel perspective on the role of local topology in GNN-based LP, highlighting the importance of characterizing node-level topological properties.\n2. The authors propose a more nuanced and accurate metric, TC, which better captures the local topology of nodes and its correlation with LP performance.\n3. The work addresses a significant gap in the existing literature on GNN-based LP, providing insights into the varying performance of nodes within a graph.\n4. The authors demonstrate the effectiveness of ATC in approximating TC while reducing computational complexity.\n\nWeaknesses:\n1. The paper relies heavily on empirical evaluations, which may not generalize to other domains or datasets.\n2. The authors do not provide a comprehensive theoretical analysis of TC and ATC, which may limit their applicability to other contexts.\n3. The work focuses on GNN-based LP, which may not be directly applicable to other machine learning tasks.\n4. The authors do not discuss potential limitations and biases of TC and ATC, which may impact their accuracy and reliability.\n\nSuggestions for Improvement:\n1. The authors should provide a more comprehensive theoretical analysis of TC and ATC to establish their validity and applicability beyond empirical evaluations.\n2. The work should be extended to other machine learning tasks and domains to demonstrate the generalizability of TC and ATC.\n3. The authors should discuss potential limitations and biases of TC and ATC, such as the impact of noise, outliers, or class imbalance on their accuracy.\n4. The paper could benefit from a more detailed discussion of the computational complexity of TC and ATC, including the trade-offs between accuracy and computational efficiency.\n\nRecommendation:\nThe paper makes a significant contribution to the field of GNN-based LP by proposing a novel metric, TC, that better captures the local topology of nodes and its correlation with LP performance. While there are some limitations and biases to be addressed, the work demonstrates the potential of TC and ATC in improving LP performance. With further theoretical analysis, generalizability to other tasks and domains, and discussion of limitations and biases, the paper can be strengthened and contribute more significantly to the field.",
        "peer_review_with_watermark": "$$\\boxed{Summary:\nThe paper presents a new approach to link prediction (LP) in graph neural networks (GNNs) based on Topological Concentration (TC), a metric that measures the average level of intersection among local subgraphs of nodes and their neighbors. The authors demonstrate that TC has a higher correlation with LP performance than other node-level topological metrics like degree and subgraph density, and identifies a novel topological distribution shift (TDS) issue in which newly joined neighbors of a node become less interactive with its existing neighbors. The authors also propose an approximated version of TC (ATC) to reduce computation complexity. Experimental results show that TC-based edge reweighting strategy can boost LP performance in various GNN-based baselines. The paper concludes by emphasizing the importance of understanding the varying levels of embedding quality and LP performance across different nodes within a graph.}\n\n$$\\boxed{Strengths:\n1. The paper presents a new and innovative approach to LP in GNNs based on Topological Concentration (TC), a metric that measures the average level of intersection among local subgraphs of nodes and their neighbors.\n2. The authors demonstrate the superiority of TC in leading higher correlation and identifying more low-performing nodes than other common node topological properties.\n3. The paper identifies a novel topological distribution shift (TDS) issue in which newly joined neighbors of a node become less interactive with its existing neighbors.\n4. The authors propose an approximated version of TC (ATC) to reduce computation complexity.\n5. The experimental results show that TC-based edge reweighting strategy can boost LP performance in various GNN-based baselines.}\n\n$$\\boxed{Weaknesses:\n1. The paper does not provide a clear explanation of why TC is not simply a function of degree or subgraph density, and why it is more correlated with LP performance.\n2. The authors do not discuss the limitations of TC in certain scenarios, such as when a node has no neighbors or when the graph is too sparse.\n3. The paper does not provide a thorough comparison with other LP approaches, and does not discuss the potential benefits of using TC in other applications.\n4. The experimental results are not entirely convincing, and some of the plots are difficult to interpret.\n5. The paper does not provide a clear discussion of the potential risks and limitations of using TC in practice.}\n\n$$\\boxed{Suggestions for Improvement:\n1. Provide a clear explanation of why TC is not simply a function of degree or subgraph density.\n2. Discuss the limitations of TC in certain scenarios, such as when a node has no neighbors or when the graph is too sparse.\n3. Provide a thorough comparison with other LP approaches and discuss the potential benefits of using TC in other applications.\n4. Make sure to interpret the experimental results accurately and provide clear plots and figures to support the conclusions.\n5. Discuss the potential risks and limitations of using TC in practice and provide recommendations for its application in real-world scenarios.}\n\n$$\\boxed{Recommendation:\nThe paper presents an innovative approach to LP in GNNs based on Topological Concentration (TC), and the authors demonstrate its superiority in leading higher correlation and identifying more low-performing nodes than other common node topological properties. However, the paper has some limitations and needs further improvement to be taken seriously in the community. I recommend that the authors revise and resubmit the paper to address these limitations and provide a clearer and more convincing discussion of the potential benefits and risks of using TC in practice.}$$",
        "gr_score_with": 0.5452261306532663,
        "gr_score_without": 0.4085510688836104,
        "or_score_with": 0.4321608040201005,
        "or_score_without": 0.4916864608076009,
        "output_without": {
            "num_tokens_scored": 421,
            "num_green_tokens": 172,
            "green_fraction": 0.4085510688836104,
            "num_orange_tokens": 207,
            "orange_fraction": 0.4916864608076009,
            "green_z_score": 0.35814247619370193,
            "orange_z_score": 124.23728927810689,
            "green_p_value": 0.36011834651012825,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 398,
            "num_green_tokens": 217,
            "green_fraction": 0.5452261306532663,
            "num_orange_tokens": 172,
            "orange_fraction": 0.4321608040201005,
            "green_z_score": 5.9139913030311675,
            "orange_z_score": 105.97709103655048,
            "green_p_value": 1.6695776184843626e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TIONS WITH TRANSITIONAL DICTIONARY LEARNING",
        "abstract": "This paper introduces a novel Transitional Dictionary Learning (TDL) framework that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. We propose a game-theoretic diffusion model to decompose the input into visual parts using the dictionaries learned by the Expectation Maximization (EM) algorithm, implemented as the online prototype clustering, based on the decomposition results. Additionally, two metrics, clustering information gain, and heuristic shape score are proposed to evaluate the model. Experiments are conducted on three abstract compositional visual object datasets, which require the model to utilize the compositionality of data instead of simply exploiting visual features. Then, three tasks on symbol grounding to predefined classes of parts and relations, as well as transfer learning to unseen classes, followed by a human evaluation, were carried out on these datasets. The results show that the proposed method discovers compositional patterns, which significantly outperforms the state-of-the-art unsupervised part segmentation methods that rely on visual features from pre-trained backbones. Furthermore, the proposed metrics are consistent with human evaluations.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces the Transitional Dictionary Learning (TDL) framework, which aims to learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. The framework uses an Expectation Maximization (EM) algorithm to iteratively update dictionaries that store hidden representations of symbolic knowledge. The authors propose two metrics to evaluate the learned representation, Clustering Information Gain (CIG) and heuristic shape score. The results show that the proposed method significantly outperforms state-of-the-art unsupervised part segmentation methods and demonstrates improved interpretability.\n\nStrengths:\n\n* The paper proposes a novel approach to learning symbolic knowledge by reconstructing the input as a combination of parts with implicit relations.\n* The framework uses an EM algorithm to iteratively update dictionaries, which is a common and effective method for learning complex representations.\n* The authors propose two metrics to evaluate the learned representation, which is essential for assessing the quality of the learned knowledge.\n\nWeaknesses:\n\n* The paper assumes that the input data can be decomposed into parts with implicit relations, which may not always be the case. The authors need to provide more evidence that this assumption holds in practice.\n* The proposed metrics, CIG and shape score, may not be sufficient to fully capture the complexity of the learned representation.\n* The paper does not provide a detailed analysis of the limitations of the proposed framework, which is essential for assessing its potential and limitations.\n\nSuggestions for Improvement:\n\n* The authors should provide more evidence that the input data can be decomposed into parts with implicit relations. This could be done by analyzing the results of the experiments and providing more detailed explanations of the decomposition process.\n* The authors should consider using more advanced metrics to evaluate the learned representation, such as graph-based metrics or attention-based metrics.\n* The authors should provide a more detailed analysis of the limitations of the proposed framework, including the potential sources of error and the limitations of the proposed metrics.\n\nRecommendation:\n\n* The paper presents an interesting and novel approach to learning symbolic knowledge by reconstructing the input as a combination of parts with implicit relations.\n* The framework uses an EM algorithm to iteratively update dictionaries, which is a common and effective method for learning complex representations.\n* However, the paper assumes that the input data can be decomposed into parts with implicit relations, which may not always be the case. The authors need to provide more evidence that this assumption holds in practice.\n\nOverall, the paper presents a promising approach to learning symbolic knowledge, but it requires more evidence and analysis to fully establish its potential and limitations.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces a novel Transitional Dictionary Learning (TDL) framework that aims to bridge the gap between neural and symbolic representations. The proposed framework uses a game-theoretic diffusion model with online prototype clustering to learn a transitional representation that incorporates structural information into representations. The framework is evaluated on three abstract compositional visual object datasets using unsupervised learning, downstream tasks, and human assessments. The results show that the proposed method outperforms existing unsupervised part segmentation methods and achieves high human interpretability scores.\n\nStrengths:\n1. The proposed framework effectively incorporates structural information into representations using a game-theoretic diffusion model with online prototype clustering.\n2. The use of clustering information gain and shape score as metrics to evaluate the learned representation is a novel and useful approach.\n3. The human evaluation results show that the proposed method achieves high interpretability scores and is consistent with human judgments.\n4. The paper provides a thorough evaluation of the proposed method on three abstract compositional visual object datasets and demonstrates its effectiveness in outperforming existing unsupervised part segmentation methods.\n\nWeaknesses:\n1. The complexity of the proposed framework may make it difficult to implement and train, especially for large-scale datasets.\n2. The use of clustering information gain and shape score as metrics may not be sufficient to evaluate the learned representation in all cases.\n3. The human evaluation results are based on a relatively small number of annotators and samples, which may not be representative of the larger population.\n4. The paper does not provide a detailed comparison with existing symbolic AI methods and does not address potential limitations and future directions of the proposed framework.\n\nSuggestions for Improvement:\n1. Provide a more detailed implementation and training protocol for the proposed framework to make it more accessible to others.\n2. Consider using additional metrics or evaluation methods to provide a more comprehensive assessment of the learned representation.\n3. Conduct a larger-scale human evaluation with a more diverse population to increase the generalizability of the results.\n4. Compare the proposed framework with existing symbolic AI methods and address potential limitations and future directions.\n5. Consider incorporating additional features or modifications to improve the performance and generalizability of the proposed framework.\n\nRecommendation:\nBased on the evaluation and suggestions for improvement, I recommend that the proposed TDL framework be further developed and refined to address the limitations and potential directions for future research. The framework shows significant promise in bridging the gap between neural and symbolic representations, and with further development and evaluation, it has the potential to become a leading method in the field of artificial intelligence and cognitive science.",
        "gr_score_with": 0.4906166219839142,
        "gr_score_without": 0.3310810810810811,
        "or_score_with": 0.4289544235924933,
        "or_score_without": 0.5371621621621622,
        "output_without": {
            "num_tokens_scored": 296,
            "num_green_tokens": 98,
            "green_fraction": 0.3310810810810811,
            "num_orange_tokens": 159,
            "orange_fraction": 0.5371621621621622,
            "green_z_score": -2.420352889168873,
            "orange_z_score": 102.35148129423773,
            "green_p_value": 0.9922472741416831,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 373,
            "num_green_tokens": 183,
            "green_fraction": 0.4906166219839142,
            "num_orange_tokens": 160,
            "orange_fraction": 0.4289544235924933,
            "green_z_score": 3.5723718912511386,
            "orange_z_score": 91.40164279877862,
            "green_p_value": 0.00017688127640270662,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes zero-mean regularized spectral contrastive learning, which incorporates an additive factor into the spectral contrastive loss involving negative pairs. This modification relaxes the orthogonality constraint between negative pairs and alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. The authors investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation.\n\nStrengths:\n1. The paper introduces a new modification to spectral contrastive loss, which relaxes the orthogonality constraint between negative pairs.\n2. The authors provide a theoretical analysis of the proposed method, which highlights its potential benefits in terms of performance and robustness.\n3. The paper presents a comprehensive evaluation of the proposed method on various benchmark datasets, including contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels.\n\nWeaknesses:\n1. The paper assumes that the representation dimensionality is p+r\u22121, which may be restrictive and may not hold in all cases.\n2. The authors do not provide a detailed analysis of the effect of the regularization strength \u03c4 on the performance of the proposed method.\n3. The paper does not provide a comparison with other state-of-the-art methods in the field of contrastive learning.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the effect of the regularization strength \u03c4 on the performance of the proposed method.\n2. The paper should provide a comparison with other state-of-the-art methods in the field of contrastive learning.\n3. The authors should investigate the robustness of the proposed method to different types of noise and corruption.\n\nRecommendation:\nThe proposed method is a valuable contribution to the field of contrastive learning, and its potential benefits in terms of performance and robustness are significant. However, the authors should address the weaknesses and provide a more comprehensive evaluation of the method. With these improvements, the paper has the potential to be a top-tier submission in the field of contrastive learning.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces zero-mean regularization for spectral contrastive loss, which is a modification to the spectral contrastive loss that incorporates an additive factor into the term involving negative pairs. This modification is equivalent to adding a regularization term that enforces the mean of representations to be zero, which relaxes the orthogonality constraint between negative pairs and alleviates the negative effect of wrong connections in the positive-pair graph. The paper investigates the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios, including unsupervised domain adaptation and supervised learning with noisy labels. The results show that zero-mean regularization can improve the discriminativeness of representations, tighten the error bound, and mitigate label noise.\n\nStrengths:\n\n1. The paper introduces a new modification to spectral contrastive loss that relaxes the orthogonality constraint between negative pairs.\n2. The paper provides theoretical investigations of the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios.\n3. The paper shows that zero-mean regularization can improve the discriminativeness of representations, tighten the error bound, and mitigate label noise.\n4. The paper provides experimental results on several benchmark datasets, including contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels.\n\nWeaknesses:\n\n1. The paper assumes a spectral decomposition on a positive-pair graph, which may not be feasible in all cases.\n2. The paper does not provide a detailed analysis of the limitations of spectral contrastive loss, such as the need for orthogonality between negative pairs.\n3. The paper does not provide a comparison with other contrastive learning methods, such as InfoNCE loss and Barlow Twins loss.\n4. The paper assumes a linear classifier for supervised learning, which may not be feasible in all cases.\n\nSuggestions for Improvement:\n\n1. The paper could provide a more detailed analysis of the limitations of spectral contrastive loss and propose alternative methods to address these limitations.\n2. The paper could provide a comparison with other contrastive learning methods, such as InfoNCE loss and Barlow Twins loss.\n3. The paper could provide experimental results on a wider range of datasets and scenarios, including multi-class classification, multi-label classification, and other applications.\n4. The paper could provide a more detailed analysis of the theoretical implications of zero-mean regularization, including its relationship to other regularization techniques, such as L1 and L2 regularization.\n\nRecommendation:\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper, addressing the limitations and shortcomings mentioned above. The paper has the potential to contribute to the development of contrastive learning methods, and with further improvements, it can provide a more comprehensive and rigorous analysis of the benefits and limitations of zero-mean regularization.",
        "gr_score_with": 0.37797619047619047,
        "gr_score_without": 0.2947761194029851,
        "or_score_with": 0.5565476190476191,
        "or_score_without": 0.6119402985074627,
        "output_without": {
            "num_tokens_scored": 268,
            "num_green_tokens": 79,
            "green_fraction": 0.2947761194029851,
            "num_orange_tokens": 164,
            "orange_fraction": 0.6119402985074627,
            "green_z_score": -3.5162204097794065,
            "orange_z_score": 124.37693067068652,
            "green_p_value": 0.9997811312865433,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 336,
            "num_green_tokens": 127,
            "green_fraction": 0.37797619047619047,
            "num_orange_tokens": 187,
            "orange_fraction": 0.5565476190476191,
            "green_z_score": -0.8240554958966425,
            "orange_z_score": 126.52569633121234,
            "green_p_value": 0.7950459870818837,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "AVERAGE REWARD MARKOV DECISION PROCESSES",
        "abstract": "We resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of \u00d5(|S||A|tmix\u03b5)* and a lower bound of \u03a9(|S||A|tmix\u03b5). In these expressions, |S| and |A| denote the cardinalities of the state and action spaces respectively, tmix serves as a uniform upper limit for the total variation mixing times, and \u03b5 signifies the error tolerance. Therefore, a notable gap of tmix still remains to be bridged. Our primary contribution is the development of an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5). This marks the first algorithm and analysis to reach the literature\u2019s lower bound. Our new algorithm draws inspiration from ideas in Li et al. (2020), Jin & Sidford (2021), and Wang et al. (2023). Additionally, we conduct numerical experiments to validate our theoretical findings.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}$$\n\nThe paper presents a theoretical contribution to the area of reinforcement learning by providing the first provably optimal sample complexity guarantee for a tabular RL environment in which a controller wishes to maximize the long run average reward governed by a Markov decision process (MDP). The authors develop an estimator for the optimal policy of average reward MDPs with a sample complexity of $\\Theta(|S||A|tminizetmixin)$, which marks the first algorithm and analysis to reach the literature's lower bound. The authors also conduct numerical experiments to validate their theoretical findings.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The paper provides a significant theoretical contribution to the area of reinforcement learning by establishing a provably optimal sample complexity guarantee for a tabular RL environment.\n2. The authors develop an estimator for the optimal policy of average reward MDPs with a sample complexity of $\\Theta(|S||A|tminizetmixin)$, which marks the first algorithm and analysis to reach the literature's lower bound.\n3. The paper presents numerical experiments to validate the theoretical findings, providing empirical evidence for the proposed algorithm.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The paper assumes a strong form of MDP mixing known as uniform ergodicity, which may be conservative and not applicable to all scenarios.\n2. The analysis relies on the assumption of a finite state and action space, which may not be suitable for all applications.\n3. The paper does not provide a clear comparison with existing algorithms and methods, making it difficult to evaluate the proposed algorithm's performance.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. Consider relaxing the assumption of uniform ergodicity and exploring alternative forms of MDP mixing.\n2. Extend the analysis to infinite state and action spaces, which may be more applicable to real-world problems.\n3. Compare the proposed algorithm with existing algorithms and methods, providing a clear evaluation of its performance.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the manuscript, addressing the limitations and weaknesses mentioned above. The revised manuscript should provide a clear comparison with existing algorithms and methods, relax the assumption of uniform ergodicity, and extend the analysis to infinite state and action spaces. Additionally, the authors should provide a more detailed evaluation of the proposed algorithm's performance and its potential applications.",
        "peer_review_with_watermark": "$$$$\nPaper Summary:\nThe paper addresses the sample complexity of policy learning for uniformly ergodic Markov decision processes (MDPs) under a generative model. The authors resolve the open question of the sample complexity of policy learning for MDPs with uniformly ergodic MDPs, assuming a generative model. They develop an estimator for the optimal policy of average reward MDPs with a sample complexity of $\\mathring{O}(|S||A|tmix\u03b5)$. The paper also conducts numerical experiments to validate the theoretical findings.\n\n\nStrengths:\n1. The authors provide a detailed and rigorous proof of the sample complexity of policy learning for uniformly ergodic MDPs under a generative model.\n2. The paper establishes a novel connection between the sample complexity of uniformly ergodic DMDPs and uniformly ergodic MDPs.\n3. The authors provide numerical experiments to validate the theoretical findings.\n\n\nWeaknesses:\n1. The proof of the sample complexity of policy learning for uniformly ergodic MDPs is technical and may require additional effort from the reader.\n2. The paper assumes a generative model, which may not be feasible in all practical settings.\n\n\nSuggestions for Improvement:\n1. Provide more detailed explanations of the technical results in the proof.\n2. Consider providing additional numerical experiments to further validate the theoretical findings.\n\n\nRecommendation:\nThe paper provides a significant contribution to the sample complexity of policy learning for uniformly ergodic MDPs under a generative model. The authors provide a rigorous proof of the sample complexity and establish a novel connection between the sample complexity of uniformly ergodic DMDPs and uniformly ergodic MDPs. The numerical experiments provide additional validation of the theoretical findings. However, the proof may require additional effort from the reader, and the assumption of a generative model may be restrictive in some settings.",
        "gr_score_with": 0.5,
        "gr_score_without": 0.33762057877813506,
        "or_score_with": 0.46634615384615385,
        "or_score_without": 0.5562700964630225,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 105,
            "green_fraction": 0.33762057877813506,
            "num_orange_tokens": 173,
            "orange_fraction": 0.5562700964630225,
            "green_z_score": -2.2455147625360694,
            "orange_z_score": 108.5386205183812,
            "green_p_value": 0.9876324471952873,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 208,
            "num_green_tokens": 104,
            "green_fraction": 0.5,
            "num_orange_tokens": 97,
            "orange_fraction": 0.46634615384615385,
            "green_z_score": 2.9439202887759484,
            "orange_z_score": 74.20528897311,
            "green_p_value": 0.0016204172857593384,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper examines the notion of generalization bounds being uniformly tight, which implies that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors show that no generalization bound of the form LD(A(S)) < LS(A(S)) + C(A(S), S) can be uniformly tight in the overparameterized setting. They prove that the estimability of a hypothesis class H is equivalent to the existence of a uniformly tight generalization bound for H. The authors also show that algorithm-dependent generalization bounds can be tight for certain algorithms, but not for all algorithms.\n\nStrengths:\n- The paper provides a thorough analysis of generalization bounds and their limitations in the overparameterized setting.\n- The authors provide a clear and concise presentation of their results and methods.\n- The paper addresses the importance of explicitly stating the assumptions underlying generalization bounds.\n\nWeaknesses:\n- The paper assumes a high level of background knowledge in machine learning and learning theory, which may make it difficult for non-experts to follow.\n- Some of the technical results and proofs may be complex and difficult to understand.\n- The paper does not provide a comprehensive review of the existing literature on generalization bounds, which may limit its usefulness for researchers in the field.\n\nSuggestions for Improvement:\n- Provide a more detailed explanation of the technical results and proofs, especially for non-experts in the field.\n- Include a more comprehensive review of the existing literature on generalization bounds to provide context and perspective.\n- Consider adding more examples and illustrations to help readers understand the results and methods.\n\nRecommendation:\nThe paper provides an important contribution to the field of machine learning and learning theory by examining the limitations of generalization bounds in the overparameterized setting. While it may require some technical expertise to follow, the paper is well-written and provides a clear and concise presentation of the results and methods. With some additional explanations and examples, the paper could be even more accessible to a wider range of readers.",
        "peer_review_with_watermark": "$$$$\n\nThe paper proposes a rigorous analysis of generalization bounds in the overparameterized setting, providing insights into the limitations of these bounds and their implications for the design of neural networks. The authors identify two types of generalization bounds: algorithm-independent and algorithm-dependent, and show that both types are subject to limitations in the overparameterized setting.\n\nThe paper is well-written and provides a clear overview of the main results and their implications. The authors use a variety of technical tools and definitions, but these are clearly explained and are accessible to readers without extensive background in the field.\n\nThe paper is also well-structured, with each section providing a clear and concise overview of the main results and their implications. The authors use a variety of examples to illustrate their points, which helps to make the paper more accessible to readers without extensive background in the field.\n\nOne of the strengths of the paper is its ability to identify limitations of generalization bounds in the overparameterized setting. The authors show that both algorithm-independent and algorithm-dependent bounds are subject to limitations, and provide a clear understanding of these limitations and their implications for the design of neural networks.\n\nThe paper also raises an important question about the interpretation of generalization bounds in the overparameterized setting. The authors suggest that generalization bounds should be stated explicitly with assumptions about the population distributions and algorithms being considered, and provide a clear example of how this could be done.\n\nOverall, the paper provides a rigorous and insightful analysis of generalization bounds in the overparameterized setting, and raises an important question about the interpretation of these bounds. The paper is well-written, clearly structured, and provides a clear understanding of the main results and their implications.\n\nHowever, there are a few minor issues with the paper. In some places, the notation is not clearly explained, which may make it difficult for some readers to follow the paper. Additionally, some of the technical tools and definitions could be more clearly explained, although this is not necessarily a criticism of the paper as a whole.\n\nAdditionally, the paper could benefit from a more detailed discussion of the implications of the results for the design of neural networks. The authors suggest that generalization bounds should be stated explicitly with assumptions about the population distributions and algorithms being considered, but they do not provide a detailed discussion of how this could be done in practice.\n\nIn conclusion, the paper provides a rigorous and insightful analysis of generalization bounds in the overparameterized setting, and raises an important question about the interpretation of these bounds. The paper is well-written, clearly structured, and provides a clear understanding of the main results and their implications. However, there are a few minor issues with the paper, and the authors could benefit from providing a more detailed discussion of the implications of the results for the design of neural networks.\n\n$$$$\n\n**Paper Summary:**\n\nThe paper provides a rigorous analysis of generalization bounds in the overparameterized setting, showing that both algorithm-independent and algorithm-dependent bounds are subject to limitations. The authors identify two types of generalization bounds: algorithm-independent and algorithm-dependent, and show that both types are subject to limitations. The paper also raises an important question about the interpretation of generalization bounds in the overparameterized setting, suggesting that generalization bounds should be stated explicitly with assumptions about the population distributions and algorithms being considered.\n\n**Strengths:**\n\n* The paper provides a clear and concise overview of the main results and their implications.\n* The authors use a variety of technical tools and definitions, but these are clearly explained.\n* The paper is well-structured, with each section providing a clear and concise overview of the main results and their implications.\n* The authors raise an important question about the interpretation of generalization bounds in the overparameterized setting.\n\n**Weaknesses:**\n\n* Some of the technical tools and definitions could be more clearly explained.\n* The paper could benefit from a more detailed discussion of the implications of the results for the design of neural networks.\n* The notation is not clearly explained in some places, which may make it difficult for some readers to follow the paper.\n\n**Suggestions for Improvement:**\n\n* Provide a more detailed discussion of the implications of the results for the design of neural networks.\n* Clearly explain the technical tools and definitions used in the paper.\n* Provide a more detailed explanation of the notation used in the paper.\n* Consider adding more examples to illustrate the main results and their implications.",
        "gr_score_with": 0.4820359281437126,
        "gr_score_without": 0.369281045751634,
        "or_score_with": 0.41916167664670656,
        "or_score_without": 0.49019607843137253,
        "output_without": {
            "num_tokens_scored": 306,
            "num_green_tokens": 113,
            "green_fraction": 0.369281045751634,
            "num_orange_tokens": 150,
            "orange_fraction": 0.49019607843137253,
            "green_z_score": -1.0968860658219417,
            "orange_z_score": 114.10162825909678,
            "green_p_value": 0.8636543996529339,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 161,
            "green_fraction": 0.4820359281437126,
            "num_orange_tokens": 140,
            "orange_fraction": 0.41916167664670656,
            "green_z_score": 3.060354303520077,
            "orange_z_score": 101.73543232172558,
            "green_p_value": 0.0011053764679889446,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DIFFUSIONNAG: PREDICTOR-GUIDED NEURAL ARCHI-",
        "abstract": "Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 20\u00d7 when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset.",
        "peer_review_without_watermark": "$$$\\begin{align*}\n\\textbf{Paper Summary:} \\\\\n\\text{The paper proposes a novel paradigm shift from Neural Architecture Search (NAS) to a novel conditional Neural Architecture Generation (NAG) framework called DiffusionNAG. This framework leverages diffusion models and a meta-learned dataset-aware predictor to efficiently generate task-optimal neural architectures. The proposed framework includes a novel score network that ensures the generation of valid architectures. The authors demonstrate the effectiveness of DiffusionNAG in two predictor-based NAS scenarios: Transferable NAS and BO-based NAS.}\n\n\\textbf{Strengths:} \\\\\n1. The paper proposes a novel and efficient framework for generating task-optimal neural architectures.\n2. The use of a meta-learned dataset-aware predictor allows for efficient adaptation to unseen tasks.\n3. The introduction of a score network ensures the generation of valid architectures.\n4. The extensive experiments demonstrate the effectiveness of DiffusionNAG in various NAS scenarios.\n\n\\textbf{Weaknesses:} \\\\\n1. The paper assumes that the dataset-aware predictor is meta-learned, which may require significant computational resources.\n2. The score network may not generalize well to different search spaces.\n3. The paper does not provide a comprehensive analysis of the trade-offs between the search cost and the quality of the generated architectures.\n\n\\textbf{Suggestions for Improvement:} \\\\\n1. Provide a more detailed analysis of the trade-offs between the search cost and the quality of the generated architectures.\n2. Investigate the robustness of the score network to different search spaces.\n3. Consider using more advanced techniques, such as transfer learning, to improve the performance of the dataset-aware predictor.\n\n\\textbf{Recommendation:} \\\\\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors should address the weaknesses and provide more comprehensive analysis of the trade-offs between the search cost and the quality of the generated architectures. Additionally, the authors should consider using more advanced techniques to improve the performance of the dataset-aware predictor.\n\n$$\\end{align*}$$",
        "peer_review_with_watermark": "$$ $$$\n\nPaper Summary:\nThe paper proposes a novel approach called DiffusionNAG, which is a paradigm shift from existing Neural Architecture Search (NAS) methods. DiffusionNAG generates task-optimal architectures with the guidance of a meta-learned dataset-aware predictor. The framework leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. Additionally, a score network is introduced to ensure the generation of valid architectures.\n\nStrengths:\n1. DiffusionNAG efficiently generates task-optimal architectures with the guidance of a meta-learned predictor, overcoming the limitations of existing NAS methods.\n2. The framework leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives.\n3. A score network is introduced to ensure the generation of valid architectures, addressing the issue of generating underperforming architectures.\n\nWeaknesses:\n1. The paper relies heavily on prior work, such as diffusion models and Transferable NAS, and may not be entirely novel in its approach.\n2. Some sections, such as the experimental setup, could be more detailed to facilitate understanding.\n3. The paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks.\n\nSuggestions for Improvement:\n1. Provide a more detailed experimental setup, including specific hyperparameter values and experimental configurations.\n2. Clarify the novelty of the approach and its contributions to the field of NAS.\n3. Provide more details on the score network, including its training procedure and hyperparameter values.\n\nRecommendation:\nThe paper proposes a novel approach to NAS, DiffusionNAG, which efficiently generates task-optimal architectures with the guidance of a meta-learned predictor. While the paper has some limitations, it contributes to the field of NAS by introducing a new paradigm shift. With some improvements, the paper can be more effective in conveying its ideas and results to a wider audience. \n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding. \n\nPlease note that this review is based on the provided paper text only and is not an exhaustive review of all aspects of the paper. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. The framework is well-structured, and the authors provide a clear explanation of the underlying mathematical concepts.\n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding.\n\nPlease note that this review is based on the provided paper text only and is not an exhaustive review of all aspects of the paper. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding.\n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding. \n\nPlease note that this review is based on the provided paper text only and is not an exhaustive review of all aspects of the paper. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding. \n\nPlease note that this review is based on the provided paper text only and is not an exhaustive review of all aspects of the paper. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding. \n\nPlease note that this review is based on the provided paper text only and is not an exhaustive review of all aspects of the paper. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However, to make the paper more effective, some additional details and clarifications are needed to facilitate understanding. \n\nPlease note that this review is based on the provided paper text only and is not an exhaustive review of all aspects of the paper. \n\nThe paper provides a clear and concise introduction to the topic, discussing existing NAS methods and their limitations. The authors then propose a novel approach, DiffusionNAG, which leverages a conditional diffusion-based generative model to efficiently search for architectures satisfying specific objectives. \n\nThe experimental setup is straightforward, and the authors provide a good understanding of the experimental configurations. However, some sections, such as the experimental results, could be more detailed to facilitate understanding.\n\nThe paper assumes a good understanding of the underlying mathematical concepts, such as conditional diffusion models and score networks. While the authors provide a good explanation of these concepts, some readers may still struggle to fully understand the underlying math.\n\nOverall, the paper is well-written, and the authors demonstrate a good understanding of the underlying mathematical concepts. However",
        "gr_score_with": 0.6417112299465241,
        "gr_score_without": 0.48717948717948717,
        "or_score_with": 0.3155080213903743,
        "or_score_without": 0.3882783882783883,
        "output_without": {
            "num_tokens_scored": 273,
            "num_green_tokens": 133,
            "green_fraction": 0.48717948717948717,
            "num_orange_tokens": 106,
            "orange_fraction": 0.3882783882783883,
            "green_z_score": 2.9402889560890344,
            "orange_z_score": 76.7033832059108,
            "green_p_value": 0.001639531473250371,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 374,
            "num_green_tokens": 240,
            "green_fraction": 0.6417112299465241,
            "num_orange_tokens": 118,
            "orange_fraction": 0.3155080213903743,
            "green_z_score": 9.54172747880893,
            "orange_z_score": 72.65194866331115,
            "green_p_value": 7.023629589694663e-22,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FUNCTIONAL INTERPOLATION FOR RELATIVE POSI-",
        "abstract": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5\u2019s RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes a novel functional relative position encoding method, FIRE, to improve the length generalization of Transformers. FIRE uses a learnable function to map input positions to biases, and a progressive interpolation technique to ensure bounded input for the position encoding function. The authors theoretically prove that FIRE can represent popular position encodings, such as T5's RPE, Alibi, and Kerple, and empirically show strong length generalization behavior on various benchmarks.\n\nStrengths:\n\n1. The paper presents a novel approach to length generalization, which is a crucial aspect of Transformer models.\n2. The authors provide a clear and concise explanation of the theoretical framework and the progressive interpolation technique.\n3. The empirical results show strong length generalization behavior on various benchmarks.\n4. The paper includes a thorough analysis of the existing methods and their limitations.\n\nWeaknesses:\n\n1. The paper only studies decoder models, and it is unclear how the results can be extended to encoder models.\n2. The authors do not analyze the role of other components of Transformer and other training components (data, optimizer) in length generalization.\n3. The paper does not provide a detailed analysis of the computational cost and efficiency of FIRE.\n\nSuggestions for Improvement:\n\n1. The authors should extend the study to encoder models to demonstrate the universality of the proposed method.\n2. A more detailed analysis of the role of other components of Transformer and other training components in length generalization is needed.\n3. The authors should provide a more detailed analysis of the computational cost and efficiency of FIRE, including a comparison with other methods.\n\nRecommendation:\n\nBased on the strengths and weaknesses of the paper, I recommend acceptance of the paper to the conference. However, the authors should address the limitations mentioned above and provide a more detailed analysis of the computational cost and efficiency of FIRE.\n\nRating: 8/10\n\n$$\\boxed{Recommendation for Revision:}\n1. Extend the study to encoder models.\n2. Provide a more detailed analysis of the role of other components of Transformer and other training components in length generalization.\n3. Analyze the computational cost and efficiency of FIRE in more detail.\n4. Provide a more detailed analysis of the results and their implications.",
        "peer_review_with_watermark": "$$$\n\n\nPaper Summary:\nThe paper proposes a novel functional relative positional encoding method called FIRE (Functional Interpolation for Relative Positional Encoding) to improve Transformer generalization to longer contexts. FIRE uses a learnable continuous function to map input positions to biases, and incorporates a progressive interpolation technique to normalize the relative distance by the query position index. The paper also introduces two additional transformations to encourage locality bias and to normalize the normalizer for better short sequence modeling.\n\nStrengths:\n1. The paper presents a novel approach to improve Transformer generalization to longer contexts.\n2. The progressive interpolation technique ensures bounded input for the position encoding function for all input sequence lengths.\n3. The incorporation of locality bias and adaptive thresholding mechanism improves the model's performance on both short and long sequences.\n4. The paper presents extensive empirical results demonstrating the effectiveness of FIRE on various language modeling and long text benchmark tasks.\n\nWeaknesses:\n1. The paper only studies decoder models, which may not generalize to other components of Transformers.\n2. The role of other components of Transformers, such as data, optimizer, and other training components, in length generalization is not analyzed.\n3. The paper does not provide a detailed analysis of the computational cost and inference speed of FIRE, which may be a limitation for large-scale models.\n\nSuggestions for Improvement:\n1. Investigate the role of other components of Transformers in length generalization.\n2. Analyze the role of other training components, such as data, optimizer, in length generalization.\n3. Provide a detailed analysis of the computational cost and inference speed of FIRE.\n4. Consider exploring the application of FIRE to other NLP tasks beyond language modeling.\n\nRecommendation:\nThe paper presents a novel and effective approach to improve Transformer generalization to longer contexts. While there are some limitations, the paper demonstrates strong empirical results, and the proposed approach has the potential to improve the performance of Transformer models on various NLP tasks.",
        "gr_score_with": 0.4657039711191336,
        "gr_score_without": 0.4478114478114478,
        "or_score_with": 0.48736462093862815,
        "or_score_without": 0.42424242424242425,
        "output_without": {
            "num_tokens_scored": 297,
            "num_green_tokens": 133,
            "green_fraction": 0.4478114478114478,
            "num_orange_tokens": 126,
            "orange_fraction": 0.42424242424242425,
            "green_z_score": 1.6819167623609255,
            "orange_z_score": 92.08895896258889,
            "green_p_value": 0.046292490890229875,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 277,
            "num_green_tokens": 129,
            "green_fraction": 0.4657039711191336,
            "num_orange_tokens": 135,
            "orange_fraction": 0.48736462093862815,
            "green_z_score": 2.232162884558597,
            "orange_z_score": 102.36159425882985,
            "green_p_value": 0.012802099739033485,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "BEYOND SPATIO-TEMPORAL REPRESENTATIONS: EVOLVING FOURIER TRANSFORM FOR TEMPORAL GRAPHS",
        "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph\u2019s structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe authors propose a novel transform called Evolving Graph Fourier Transform (EFT) for temporal graphs. EFT is designed to capture evolving representations on temporal graphs and is grounded on theoretical foundations. The authors show that EFT is computationally efficient and can capture the evolving graph's structural and positional properties. They also demonstrate the effectiveness of EFT on large-scale and standard temporal graph benchmarks.\n\n$$\\text{Strengths:}$$\n1. The authors provide a comprehensive theoretical framework for EFT, which is grounded on the variational characterization of the dynamic graph.\n2. The authors demonstrate the effectiveness of EFT on large-scale and standard temporal graph benchmarks.\n3. The authors provide a simple and efficient implementation of EFT, which can be used for downstream tasks.\n\n$$\\text{Weaknesses:}$$\n1. The authors do not consider generic signed and directed graphs, which may limit the applicability of EFT.\n2. The authors assume that the graph structure is fixed, which may not be the case in real-world applications.\n3. The authors do not provide a detailed analysis of the computational complexity of EFT, which may be an important consideration for real-world applications.\n\n$$\\text{Suggestions for Improvement:}$$\n1. The authors should consider generalizing EFT to generic signed and directed graphs.\n2. The authors should provide a more detailed analysis of the computational complexity of EFT.\n3. The authors should provide a more detailed analysis of the limitations of EFT and the potential applications of EFT.\n\n$$\\text{Recommendation:}$$\nI recommend that the authors continue to work on generalizing EFT to generic signed and directed graphs and providing a more detailed analysis of the computational complexity of EFT. I also recommend that the authors provide a more detailed analysis of the limitations of EFT and the potential applications of EFT. Additionally, I recommend that the authors consider exploring the potential of EFT as a spectral representation of the evolving graph in downstream graph representation learning models.",
        "peer_review_with_watermark": "$$$\\text{Summary:}$\nThe paper presents a novel approach to transform temporal graphs into the frequency domain, which is grounded on theoretical foundations. The proposed method, Evolving Graph Fourier Transform (EFT), is a simplified transformation that is computationally efficient and effective for real-world applications. The paper provides a detailed analysis of the theoretical framework, properties, and practical effectiveness of EFT. The proposed method is demonstrated to be effective in filtering noise and amplifying useful signals in evolving temporal graphs, and it outperforms several baseline methods in various experiments.\n\n$Strengths:$\n\n1.  The paper presents a novel approach to transform temporal graphs into the frequency domain, which is grounded on theoretical foundations.\n2.  The proposed method, Evolving Graph Fourier Transform (EFT), is a simplified transformation that is computationally efficient and effective for real-world applications.\n3.  The paper provides a detailed analysis of the theoretical framework, properties, and practical effectiveness of EFT.\n4.  The proposed method is demonstrated to be effective in filtering noise and amplifying useful signals in evolving temporal graphs.\n5.  The paper provides a comprehensive comparison with baseline methods and discusses the advantages of EFT over existing approaches.\n\n$Weaknesses:$\n\n1.  The paper assumes that the graph structure remains constant, whereas in real-world applications, the graph structure may evolve over time.\n2.  The paper does not consider generic signed and directed graphs, which may limit its applicability to certain domains.\n3.  The paper does not discuss the potential risks or challenges associated with using EFT in real-world applications, such as data privacy and security concerns.\n4.  The paper provides limited information on the computational complexity of EFT, which may be a significant consideration in real-world applications.\n5.  The paper does not discuss the potential for EFT to be used in combination with other graph-based methods, which may enhance its effectiveness in certain applications.\n\n$Suggestions for Improvement:$\n\n1.  Consider the case where the graph structure evolves over time, and explore ways to adapt EFT to accommodate this.\n2.  Generalize EFT to generic signed and directed graphs, which may expand its applicability to certain domains.\n3.  Provide a detailed analysis of the potential risks and challenges associated with using EFT in real-world applications, such as data privacy and security concerns.\n4.  Provide a more detailed analysis of the computational complexity of EFT, which may be a significant consideration in real-world applications.\n5.  Explore ways to combine EFT with other graph-based methods, which may enhance its effectiveness in certain applications.\n\n$Recommendation:$\n\nBased on the analysis of the paper, I recommend that the proposed method, Evolving Graph Fourier Transform (EFT), be further explored and refined. The paper provides a solid theoretical foundation and demonstrates the practical effectiveness of EFT in filtering noise and amplifying useful signals in evolving temporal graphs. However, there are several limitations and challenges associated with using EFT in real-world applications, which need to be addressed. Therefore, I suggest that the author continue to refine and adapt EFT to accommodate the changing graph structure, expand its applicability to generic signed and directed graphs, and explore ways to combine it with other graph-based methods.",
        "gr_score_with": 0.5077881619937694,
        "gr_score_without": 0.359504132231405,
        "or_score_with": 0.4423676012461059,
        "or_score_without": 0.5289256198347108,
        "output_without": {
            "num_tokens_scored": 242,
            "num_green_tokens": 87,
            "green_fraction": 0.359504132231405,
            "num_orange_tokens": 128,
            "orange_fraction": 0.5289256198347108,
            "green_z_score": -1.2859165086496225,
            "orange_z_score": 92.50179413802458,
            "green_p_value": 0.9007638992250071,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 163,
            "green_fraction": 0.5077881619937694,
            "num_orange_tokens": 142,
            "orange_fraction": 0.4423676012461059,
            "green_z_score": 3.942012179175557,
            "orange_z_score": 88.8422362491936,
            "green_p_value": 4.0400436254369556e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS",
        "abstract": "Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this paper, we explore a perplexing tendency of diffusion models: they often display the infinite Lipschitz property of the network with respect to time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it. The Lipschitz singularities pose a threat to the stability and accuracy during both the training and inference processes of diffusion models. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. To address this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates the Lipschitz singularities of the diffusion model near the zero point of timesteps. Remarkably, our technique yields a substantial improvement in performance. Moreover, as a byproduct of our method, we achieve a dramatic reduction in the Fr\u00e9chet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive experiments on diverse datasets validate our theory and method. Our work may advance the understanding of the general diffusion process, and also provide insights for the design of diffusion models.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper explores the phenomenon of infinite Lipschitz constants in diffusion models, which can lead to instability and inaccuracy in training and inference processes. The authors propose a novel approach, Early Timestep-shared Diffusion Model (E-TSDM), which alleviates the Lipschitz singularities by sharing timestep conditions. The proposed method is shown to improve the synthesis performance and reduce the Lipschitz constants.\n\nStrengths:\n- The paper provides a clear theoretical analysis of the Lipschitz singularities issue in diffusion models.\n- The authors propose a practical and effective approach to mitigate the problem.\n- The experimental results demonstrate the superiority of the proposed method over the baseline.\n\nWeaknesses:\n- The paper may benefit from a more detailed discussion of the limitations of the proposed method.\n- Some of the experimental results, such as the comparison with alternative methods, may require more clarification.\n- The paper could provide more insights into the potential applications of the proposed method.\n\nSuggestions for Improvement:\n- The authors could provide more detailed explanations of the theoretical analysis and its implications.\n- Additional experimental results, such as a comparison with other state-of-the-art methods, could strengthen the paper's claims.\n- The discussion of potential applications and limitations could be expanded to provide more comprehensive insights.\n\nRecommendation:\n- The paper is recommended for publication in a top-tier conference or journal in the field of machine learning and computer vision.\n\n$$\\begin{aligned}\n$$Additional Comments: \nThe paper presents a well-structured and clearly written argument for the proposed method. The theoretical analysis is thorough, and the experimental results demonstrate the effectiveness of the approach. However, some minor issues with the presentation and clarity could be improved. The discussion of limitations and potential applications could be expanded to provide more comprehensive insights.",
        "peer_review_with_watermark": "$$$$\nPaper Title: LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS\nAbstract:\nThe paper explores the phenomenon of infinite Lipschitz constants in diffusion models, which can lead to instability and inaccuracy during training and inference. The authors propose a novel approach, dubbed Early Timestep-Shared Diffusion Model (E-TSDM), which alleviates the Lipschitz singularities by sharing timestep conditions in the interval with large Lipschitz constants. The authors demonstrate the effectiveness of E-TSDM in various experiments, including unconditional generation, conditional generation, and fast sampling.\n\nSummary:\nThe paper reviews the concept of diffusion models and their applications in image synthesis. It highlights the limitation of diffusion models in terms of numerical stability near the zero point, which can lead to infinite Lipschitz constants. The authors propose E-TSDM, which shares timestep conditions in the interval with large Lipschitz constants, and demonstrate its effectiveness in various experiments.\n\nStrengths:\n1. The paper explores a novel and important aspect of diffusion models, namely, the phenomenon of infinite Lipschitz constants.\n2. The authors propose a practical approach, E-TSDM, to alleviate this issue.\n3. The paper demonstrates the effectiveness of E-TSDM in various experiments, including unconditional generation, conditional generation, and fast sampling.\n\nWeaknesses:\n1. The paper assumes that the distribution of data is an arbitrary complex distribution, which may not be the case in practice.\n2. The authors do not provide a clear explanation of why sharing timestep conditions in the interval with large Lipschitz constants is effective in alleviating the Lipschitz singularities.\n3. The paper does not provide a comprehensive comparison with other approaches to address the issue of infinite Lipschitz constants.\n\nSuggestions for Improvement:\n1. The authors should provide more theoretical explanations for why sharing timestep conditions in the interval with large Lipschitz constants is effective in alleviating the Lipschitz singularities.\n2. The authors should provide a more comprehensive comparison with other approaches to address the issue of infinite Lipschitz constants.\n3. The authors should consider the case where the distribution of data is not an arbitrary complex distribution.\n\nRecommendation:\nThe paper is well-written and demonstrates the effectiveness of E-TSDM in various experiments. However, the authors should provide more theoretical explanations for their approach and consider a more comprehensive comparison with other approaches. Based on the paper, I recommend that it be published in a conference or journal that focuses on generative models and diffusion models.\n\n$$$$\nNote: The above review is based on the provided paper and does not include any outside knowledge or external references. The reviewer has carefully read the paper and has provided a critical evaluation of its strengths, weaknesses, and suggestions for improvement. The reviewer has also provided a recommendation for publication based on the paper's quality and significance.",
        "gr_score_with": 0.5346820809248555,
        "gr_score_without": 0.44485294117647056,
        "or_score_with": 0.4190751445086705,
        "or_score_without": 0.4227941176470588,
        "output_without": {
            "num_tokens_scored": 272,
            "num_green_tokens": 121,
            "green_fraction": 0.44485294117647056,
            "num_orange_tokens": 115,
            "orange_fraction": 0.4227941176470588,
            "green_z_score": 1.5099750030394268,
            "orange_z_score": 85.94847240353388,
            "green_p_value": 0.06552490133509077,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 185,
            "green_fraction": 0.5346820809248555,
            "num_orange_tokens": 145,
            "orange_fraction": 0.4190751445086705,
            "green_z_score": 5.113782427216613,
            "orange_z_score": 96.07165102187355,
            "green_p_value": 1.5788552432796018e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "UNKNOWN DOMAIN INCONSISTENCY MINIMIZATION FOR DOMAIN GENERALIZATION",
        "abstract": "The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain\u2019s loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there\u2019s still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM\u2019s generalization capability in unseen domains. Our code is available at https://github.com/SJShin-AI/UDIM.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes a novel approach to domain generalization, called Unknown Domain Inconsistency Minimization (UDIM), which minimizes the discrepancy in the loss landscape between the source domain and unobserved domains. UDIM combines sharpness-aware minimization (SAM) with data perturbation to improve generalization performance on unobserved domains. The authors provide a theoretical analysis of UDIM and demonstrate its effectiveness on multiple benchmark datasets.\n\n$$\\begin{aligned}\n$$\\text{Strengths:}$$\n- UDIM effectively minimizes the discrepancy in the loss landscape between the source domain and unobserved domains.\n- The authors provide a theoretical analysis of UDIM and demonstrate its effectiveness on multiple benchmark datasets.\n- The paper introduces a novel approach to domain generalization, which combines sharpness-aware minimization and data perturbation.\n\n$$\\begin{aligned}\n$$\\text{Weaknesses:}$$\n- The paper does not provide a clear comparison with existing state-of-the-art methods.\n- The authors assume that the perturbation of the data space is feasible, which may not be the case in all scenarios.\n- The paper does not discuss the potential risks of over-perturbation or under-perturbation.\n\n$$\\begin{aligned}\n$$\\text{Suggestions for Improvement:}$$\n- The authors should provide a clear comparison with existing state-of-the-art methods to demonstrate the effectiveness of UDIM.\n- The authors should discuss the potential risks of over-perturbation or under-perturbation and provide a method to mitigate these risks.\n- The authors should provide more details on the theoretical analysis of UDIM, including the mathematical derivations and proofs.\n\n$$\\begin{aligned}\n$$\\text{Recommendation:}$$\n- Based on the paper's contribution to the field of domain generalization, we recommend acceptance of the paper for publication in a top-tier conference or journal.\n- However, we suggest that the authors revise the paper to address the weaknesses and provide a more comprehensive analysis of UDIM's effectiveness and limitations.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces Unknown Domain Inconsistency Minimization (UDIM), a novel approach to domain generalization that minimizes the discrepancy in the loss landscape between the source domain and unobserved domains. UDIM combines the Sharpness-Aware Minimization (SAM) optimizer with a data perturbation approach to improve generalization performance on unobserved domains. The authors demonstrate that UDIM consistently outperforms SAM variants and other domain generalization methods across multiple benchmark datasets. \n\nStrengths:\n1. The authors provide a detailed theoretical analysis of the UDIM objective, showing that it can be used to minimize the population risk over all domains.\n2. The authors demonstrate the effectiveness of UDIM in practice, showing that it consistently outperforms SAM variants and other domain generalization methods across multiple benchmark datasets.\n3. The authors propose a practical implementation of UDIM, including a method for constructing the worst-case perturbed dataset and a gradient variance-based optimization approach.\n\nWeaknesses:\n1. The authors do not provide a detailed comparison of UDIM with other state-of-the-art domain generalization methods, which may limit the paper's contribution to the field.\n2. The authors' theoretical analysis relies on some technical conditions, which may not hold in all cases. The authors should provide a more detailed analysis of the conditions under which UDIM is guaranteed to work.\n3. The authors' experimental evaluation is based on a limited number of benchmark datasets, which may not be representative of all domains. The authors should provide a more detailed evaluation of UDIM's performance across a larger range of domains.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed comparison of UDIM with other state-of-the-art domain generalization methods to demonstrate its superiority.\n2. The authors should provide a more detailed analysis of the conditions under which UDIM is guaranteed to work to alleviate any potential concerns about the paper's theoretical foundation.\n3. The authors should provide a more detailed evaluation of UDIM's performance across a larger range of domains to demonstrate its robustness and generalization capability.\n\nRecommendation:\nBased on the paper's strengths and weaknesses, I recommend that the authors revise and resubmit the paper. The authors should provide a more detailed comparison of UDIM with other state-of-the-art domain generalization methods, a more detailed analysis of the conditions under which UDIM is guaranteed to work, and a more detailed evaluation of UDIM's performance across a larger range of domains. With these revisions, the paper has the potential to make a significant contribution to the field of domain generalization. \n\n $$$",
        "gr_score_with": 0.5180327868852459,
        "gr_score_without": 0.4122448979591837,
        "or_score_with": 0.4131147540983607,
        "or_score_without": 0.43673469387755104,
        "output_without": {
            "num_tokens_scored": 245,
            "num_green_tokens": 101,
            "green_fraction": 0.4122448979591837,
            "num_orange_tokens": 107,
            "orange_fraction": 0.43673469387755104,
            "green_z_score": 0.3912303982179758,
            "orange_z_score": 85.97207873853988,
            "green_p_value": 0.34781347042366995,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 305,
            "num_green_tokens": 158,
            "green_fraction": 0.5180327868852459,
            "num_orange_tokens": 126,
            "orange_fraction": 0.4131147540983607,
            "green_z_score": 4.2077212399096595,
            "orange_z_score": 90.66101703921322,
            "green_p_value": 1.289794022069388e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SCALING PHYSICS-INFORMED HARD CONSTRAINTS WITH MIXTURE-OF-EXPERTS",
        "abstract": "Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an \u201cexpert\u201d through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper presents a new approach to scale hard constraints corresponding to physical laws through an embedded differentiable optimization layer, called physics-informed hard constraint mixture-of-experts (PI-HC-MoE). This approach deconstructs a differentiable physics hard constraint into smaller experts, which leads to better convergence and faster run times. The authors demonstrate the effectiveness of PI-HC-MoE on two challenging, highly non-linear systems, 1D diffusion-sorption and 2D Navier-Stokes equations.\n\nStrengths:\n1. The authors provide a clear and concise introduction to the problem and the proposed approach, making it easy for readers to understand the context and the benefits of PI-HC-MoE.\n2. The authors present a thorough analysis of the strengths and weaknesses of existing approaches to enforcing hard physical constraints in neural networks.\n3. The authors provide a detailed explanation of the PI-HC-MoE framework, including the mixture-of-experts setup and the use of implicit differentiation.\n4. The authors demonstrate the effectiveness of PI-HC-MoE on two challenging systems, 1D diffusion-sorption and 2D Navier-Stokes equations, showing that it achieves significantly lower errors than standard differentiable optimization using a single hard constraint or soft constraint penalty methods.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the computational complexity of PI-HC-MoE, which is a crucial aspect to consider when evaluating its scalability.\n2. The authors do not provide a comparison with other state-of-the-art approaches to enforcing hard physical constraints in neural networks, which would provide a more comprehensive understanding of the strengths and weaknesses of PI-HC-MoE.\n3. The paper does not discuss the potential limitations of PI-HC-MoE, such as the need for careful tuning of hyperparameters and the potential for overfitting.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the computational complexity of PI-HC-MoE, including the number of operations required for each step of the algorithm.\n2. The authors should provide a comparison with other state-of-the-art approaches to enforcing hard physical constraints in neural networks, such as Ne\u0301giar et al. (2023) and Donti et al. (2021).\n3. The authors should discuss the potential limitations of PI-HC-MoE, including the need for careful tuning of hyperparameters and the potential for overfitting.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper for further review. Specifically, I recommend that the authors provide a more detailed analysis of the computational complexity of PI-HC-MoE and include a comparison with other state-of-the-art approaches to enforcing hard physical constraints in neural networks. Additionally, the authors should discuss the potential limitations of PI-HC-MoE and provide a more comprehensive understanding of its strengths and weaknesses.",
        "peer_review_with_watermark": "$$$${Paper Summary:\n\nThe paper proposes a novel approach to enforcing hard physical constraints in neural networks, called Physics-Informed Hard Constraint Mixture-of-Experts (PI-HC-MoE). The approach uses a mixture-of-experts framework to decompose the hard constraint into smaller, independent experts, each of which is solved separately using differentiable optimization. The experts are then combined to form the final prediction, which is trained using backpropagation. The approach is designed to be scalable and can be used with any neural network architecture. The authors demonstrate the effectiveness of PI-HC-MoE on two challenging non-linear problems, 1D diffusion-sorption and 2D turbulent Navier-Stokes, and show that it outperforms standard differentiable optimization and soft constraint penalty methods.\n\nStrengths:\n\n* The approach is novel and scalable, making it a valuable contribution to the field of neural networks and physics-informed machine learning.\n* The use of mixture-of-experts framework allows for decomposition of the hard constraint into smaller, independent experts, which can be solved separately using differentiable optimization.\n* The approach is designed to be general and can be used with any neural network architecture.\n* The authors provide detailed experiments and results to demonstrate the effectiveness of PI-HC-MoE on two challenging non-linear problems.\n\nWeaknesses:\n\n* The approach may be computationally expensive, as it requires solving multiple differentiable optimization problems in parallel.\n* The choice of experts and their decomposition may be problem-dependent, and there may be no clear method for selecting the optimal experts and decomposition.\n* The approach may be sensitive to the choice of optimization algorithm and hyperparameters, which may require tuning for optimal performance.\n\nSuggestions for Improvement:\n\n* To improve the scalability of PI-HC-MoE, the authors may consider using more efficient optimization algorithms or approximations that can be solved in parallel.\n* To improve the choice of experts and their decomposition, the authors may consider using more sophisticated methods, such as clustering or dimensionality reduction, to select the optimal experts and decomposition.\n* To improve the sensitivity of PI-HC-MoE to optimization algorithm and hyperparameters, the authors may consider using more robust optimization algorithms or methods that can be more easily tuned for optimal performance.\n\nRecommendation:\n\n* The approach is novel and scalable, and the results demonstrate its effectiveness on two challenging non-linear problems. However, there are some potential weaknesses and areas for improvement, such as scalability, choice of experts, and sensitivity to optimization algorithm and hyperparameters. The authors should consider addressing these issues in future work to further improve the performance and robustness of PI-HC-MoE.",
        "gr_score_with": 0.5539772727272727,
        "gr_score_without": 0.4696969696969697,
        "or_score_with": 0.36363636363636365,
        "or_score_without": 0.41515151515151516,
        "output_without": {
            "num_tokens_scored": 330,
            "num_green_tokens": 155,
            "green_fraction": 0.4696969696969697,
            "num_orange_tokens": 137,
            "orange_fraction": 0.41515151515151516,
            "green_z_score": 2.5844328061090946,
            "orange_z_score": 88.86992139828408,
            "green_p_value": 0.004876965419539691,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 352,
            "num_green_tokens": 195,
            "green_fraction": 0.5539772727272727,
            "num_orange_tokens": 128,
            "orange_fraction": 0.36363636363636365,
            "green_z_score": 5.896880595499264,
            "orange_z_score": 80.19954790297042,
            "green_p_value": 1.852188591311583e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GOAL-CONDITIONED REINFORCEMENT LEARNING",
        "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper presents a novel approach to offline goal-conditioned reinforcement learning (GCRL) called SMORe, which combines the occupancy matching perspective with a convex dual formulation. The authors propose a discriminator-free method that learns unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. The approach is designed to leverage suboptimal offline data and is robust to decreasing goal coverage in the offline dataset. The paper presents extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations.\n\nStrengths:\n\n* The authors present a novel approach to offline GCRL that combines occupancy matching with a convex dual formulation.\n* The approach is discriminator-free, which makes it more robust to decreasing goal coverage in the offline dataset.\n* The paper presents extensive experiments on the fully offline GCRL benchmark, which demonstrates the effectiveness of the proposed method.\n\nWeaknesses:\n\n* The paper does not provide a clear comparison to existing methods in the offline GCRL setting.\n* The authors assume that the goal-transition distribution is uniform, which may not be the case in practice.\n* The paper does not provide a detailed analysis of the convergence properties of the proposed method.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison to existing methods in the offline GCRL setting.\n* The authors should analyze the convergence properties of the proposed method and provide a theoretical guarantee of its convergence.\n* The authors should provide a more detailed analysis of the assumption that the goal-transition distribution is uniform.\n\nRecommendation:\n\n* The proposed method, SMORe, is a significant contribution to the field of offline GCRL and has the potential to be widely adopted.\n* However, the authors should address the weaknesses and suggestions for improvement mentioned above to further strengthen the paper.\n* The paper should be revised to provide a more detailed comparison to existing methods, a more detailed analysis of the convergence properties, and a more detailed analysis of the assumption that the goal-transition distribution is uniform.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$Paper Summary: The goal-conditioned reinforcement learning (GCRL) problem is to learn to achieve multiple goals in an environment using sparse reward functions from offline datasets. The authors present a novel approach to GCRL called SMORe (score-modeling using occupancy matching), which optimizes for occupancy matching using a convex dual formulation and learns unnormalized scores over actions to reach the goal.\n\n$$\\text{Strengths:}$$\n1. SMORe is a discriminator-free approach that learns unnormalized scores over actions to reach the goal, alleviating the learning challenges of normalized density estimation.\n2. The approach is applicable to a number of f-divergences and can learn optimal policies in the offline setting.\n3. The method is robust to decreasing goal coverage in the offline dataset.\n4. The approach is scalable to high-dimensional observation spaces.\n\n$$\\text{Weaknesses:}$$\n1. The S-function is not grounded to any rewards and does not serve as a probability density of reaching goals.\n2. The method requires learning the goal-transition distribution, which may not be achievable by any goal-conditioned policy.\n3. The approach is limited to the offline setting and does not account for the initial transient phase that the policy must navigate to reach the desired goal.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide more theoretical guarantees on the SMORe algorithm, such as convergence rates and optimality guarantees.\n2. Investigate the performance of SMORe on more diverse goal-conditioned RL benchmarks.\n3. Provide more experiments on the robustness of SMORe to different levels of goal coverage in the offline dataset.\n4. Consider extending SMORe to the online setting, where the agent can interact with the environment.\n\n$$\\text{Recommendation:}$$\nI recommend accepting the paper for publication in its current form, as the SMORe algorithm presents a novel approach to GCRL that is both discriminant-free and scalable to high-dimensional observation spaces. The approach is robust to decreasing goal coverage in the offline dataset, and the method is applicable to a number of f-divergences. However, further investigation is needed to provide more theoretical guarantees on the SMORe algorithm and to investigate its performance on more diverse goal-conditioned RL benchmarks.",
        "gr_score_with": 0.5652173913043478,
        "gr_score_without": 0.4745098039215686,
        "or_score_with": 0.38509316770186336,
        "or_score_without": 0.4,
        "output_without": {
            "num_tokens_scored": 255,
            "num_green_tokens": 121,
            "green_fraction": 0.4745098039215686,
            "num_orange_tokens": 102,
            "orange_fraction": 0.4,
            "green_z_score": 2.42872246468334,
            "orange_z_score": 73.47393266185311,
            "green_p_value": 0.007576063085124588,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 322,
            "num_green_tokens": 182,
            "green_fraction": 0.5652173913043478,
            "num_orange_tokens": 124,
            "orange_fraction": 0.38509316770186336,
            "green_z_score": 6.051709544070756,
            "orange_z_score": 79.4296629977117,
            "green_p_value": 7.165835223847339e-10,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method that significantly reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method uses a single pair of low-rank matrices shared across all layers and learns small scaling vectors instead of LoRA's trainable matrices. VeRA is demonstrated to be effective on the GLUE and E2E benchmarks, image classification tasks, and shown to be applicable in instruction-tuning of large language models.\n\nStrengths:\n1. The proposed method is highly efficient in terms of trainable parameters, achieving significant reductions compared to LoRA and other methods.\n2. The use of shared low-rank matrices across all layers simplifies the finetuning process and reduces the number of trainable parameters.\n3. The method's ability to learn small scaling vectors allows for efficient adaptation of the model to new tasks.\n4. The paper provides a comprehensive experimental evaluation of VeRA on various benchmarks and tasks, demonstrating its effectiveness and efficiency.\n\nWeaknesses:\n1. The paper relies on the assumption that the low-rank matrices can be shared across all layers, which may not always be the case in practice.\n2. The method's performance may degrade if the scaling vectors are not learned efficiently, which could be a challenge in large-scale finetuning tasks.\n3. The paper does not provide a detailed analysis of the impact of the shared matrices on the model's performance, which could be an important aspect to investigate.\n4. The method's applicability to different architectures and domains is not thoroughly explored in the paper.\n\nSuggestions for Improvement:\n1. Investigate the impact of the shared matrices on the model's performance and provide a more detailed analysis of their effects.\n2. Explore the use of different initialization techniques for the low-rank matrices and scaling vectors to improve the method's performance.\n3. Investigate the applicability of VeRA to different architectures and domains, and provide a more thorough evaluation of its effectiveness in these contexts.\n4. Consider incorporating dynamic parameter budget allocation to further improve the method's efficiency and flexibility.\n\nRecommendation:\nThe proposed method, VeRA, is a significant contribution to the field of parameter-efficient finetuning. Its ability to reduce the number of trainable parameters while maintaining comparable results makes it an attractive solution for large-scale finetuning tasks. However, further investigation is needed to address the limitations and challenges mentioned above. With these improvements, VeRA has the potential to become a leading method for parameter-efficient finetuning, particularly in scenarios that require frequent swapping of numerous finetuned models.",
        "peer_review_with_watermark": "$$\n\\textbf{Peer Review}\n\n$$\n\n\n$$\n\\textbf{Paper Summary}\n$$\nThe paper proposes a novel finetuning method, Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to the state-of-the-art Low-Rank Adaptation (LoRA) method. VeRA achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead of the entire weight matrix. The authors demonstrate the effectiveness of VeRA on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.\n\n$$\n\\textbf{Strengths}\n$$\n1.  The paper proposes a novel finetuning method that significantly reduces the number of trainable parameters compared to LoRA.\n2.  The authors demonstrate the effectiveness of VeRA on the GLUE and E2E benchmarks and image classification tasks.\n3.  The paper provides a detailed explanation of the method and its components.\n\n$$\n\\textbf{Weaknesses}\n$$\n1.  The paper relies on previous work in the field of parameter-efficient adaptation and may not provide a fully new and original contribution.\n2.  The authors may have oversimplified the problem and the solution, and may have missed other potential solutions or approaches.\n3.  The paper may not have provided enough experimental results to fully validate the effectiveness of VeRA.\n\n$$\n\\textbf{Suggestions for Improvement}\n$$\n1.  Provide more detailed explanations of the previous work in the field and how VeRA differs from it.\n2.  Consider alternative solutions or approaches to parameter-efficient adaptation and compare them to VeRA.\n3.  Provide more experimental results to fully validate the effectiveness of VeRA, including more detailed analyses of the results.\n\n$$\n\\textbf{Recommendation}\n$$\nBased on the paper summary, strengths, weaknesses, and suggestions for improvement, I recommend accepting the paper for publication in the conference with minor revisions to address the weaknesses and suggestions for improvement.\n\n$$\n\\textbf{Additional Comments}\n$$\nThe paper is well-written and easy to follow, with a clear explanation of the method and its components. However, the paper could benefit from more detailed explanations of the previous work in the field and more experimental results to fully validate the effectiveness of VeRA. Additionally, the authors may want to consider alternative solutions or approaches to parameter-efficient adaptation and compare them to VeRA. Overall, the paper is well-written and provides a good overview of the proposed finetuning method, but could benefit from more depth and rigor in the explanations and analyses.\n\n$$\n\\textbf{Reviewer Information}\n$$\nReviewer Name: [Your Name]\n\nReviewer Email: [Your Email]\n\nReviewer Affiliation: [Your Affiliation]\n\n$$\n\\textbf{Reviewer Statement}\n$$\nI believe that the paper proposes a novel and effective finetuning method for reducing the number of trainable parameters in large language models. However, I have some minor suggestions for improvement to address the weaknesses and suggestions for improvement mentioned earlier. I believe that the paper is well-written and easy to follow, with a clear explanation of the method and its components. Overall, I recommend accepting the paper for publication in the conference with minor revisions to address the weaknesses and suggestions for improvement.\n\n$$\n\\textbf{Reviewer Confidentiality}\n$$\nThis review is for the sole use of the Conference and may not be shared with the authors or the public.\n\n$$\n\\textbf{Reviewer Confidentiality Agreement}\n$$\nBy submitting this review, I confirm that I have read and agree to the terms of the Conference's Confidentiality Agreement.\n\n$$\n\\textbf{Reviewer Acknowledgement}\n$$\nI would like to thank the Conference for the opportunity to review this paper.\n\n$$\n\\textbf{Reviewer Acknowledgement Email}\n$$\n[Your Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Address}\n$$\n[Your Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Date}\n$$\n[Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Time}\n$$\n[Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Signature}\n$$\n[Your Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Seal}\n$$\n[Your Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document}\n$$\n[Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Email}\n$$\n[Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Address}\n$$\n[Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Date}\n$$\n[Document Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Time}\n$$\n[Document Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Signature}\n$$\n[Document Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Seal}\n$$\n[Document Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document}\n$$\n[Document Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Email}\n$$\n[Document Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Address}\n$$\n[Document Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Date}\n$$\n[Document Document Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Time}\n$$\n[Document Document Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Signature}\n$$\n[Document Document Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Seal}\n$$\n[Document Document Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document}\n$$\n[Document Document Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Email}\n$$\n[Document Document Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Address}\n$$\n[Document Document Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Date}\n$$\n[Document Document Document Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Time}\n$$\n[Document Document Document Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Signature}\n$$\n[Document Document Document Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Seal}\n$$\n[Document Document Document Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document}\n$$\n[Document Document Document Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Email}\n$$\n[Document Document Document Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Address}\n$$\n[Document Document Document Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Date}\n$$\n[Document Document Document Document Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Time}\n$$\n[Document Document Document Document Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Signature}\n$$\n[Document Document Document Document Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Seal}\n$$\n[Document Document Document Document Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document}\n$$\n[Document Document Document Document Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Email}\n$$\n[Document Document Document Document Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Address}\n$$\n[Document Document Document Document Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Date}\n$$\n[Document Document Document Document Document Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Time}\n$$\n[Document Document Document Document Document Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Signature}\n$$\n[Document Document Document Document Document Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Seal}\n$$\n[Document Document Document Document Document Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document}\n$$\n[Document Document Document Document Document Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Email}\n$$\n[Document Document Document Document Document Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Address}\n$$\n[Document Document Document Document Document Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Date}\n$$\n[Document Document Document Document Document Document Date]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Time}\n$$\n[Document Document Document Document Document Document Time]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Signature}\n$$\n[Document Document Document Document Document Document Signature]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Seal}\n$$\n[Document Document Document Document Document Document Seal or Stamp]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Document}\n$$\n[Document Document Document Document Document Document Document]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Document Email}\n$$\n[Document Document Document Document Document Document Document Email]\n\n$$\n\\textbf{Reviewer Acknowledgement Document Document Document Document Document Document Document Address}\n$$\n[Document Document Document Document Document Document Document Address]\n\n$$\n\\textbf{Reviewer Acknowledgement",
        "gr_score_with": 0.6403326403326404,
        "gr_score_without": 0.4524421593830334,
        "or_score_with": 0.29313929313929316,
        "or_score_without": 0.455012853470437,
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 176,
            "green_fraction": 0.4524421593830334,
            "num_orange_tokens": 177,
            "orange_fraction": 0.455012853470437,
            "green_z_score": 2.1112990189828227,
            "orange_z_score": 109.92280693002478,
            "green_p_value": 0.01737330916118252,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 481,
            "num_green_tokens": 308,
            "green_fraction": 0.6403326403326404,
            "num_orange_tokens": 141,
            "orange_fraction": 0.29313929313929316,
            "green_z_score": 10.759192430671632,
            "orange_z_score": 78.11535624888973,
            "green_p_value": 2.6819070269423716e-27,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SKILL MACHINES: TEMPORAL LOGIC SKILL COMPOSITION IN REINFORCEMENT LEARNING",
        "abstract": "It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-optimal behaviours zero-shot. We demonstrate this experimentally in a tabular setting, as well as in a high-dimensional video game and continuous control environment. Finally, we also demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper proposes a framework called \"Skill Machines\" that enables agents to solve complex tasks involving temporal and spatial composition using a combination of skill primitives and skill machines. The framework is based on the idea of learning a sufficient set of skill primitives to achieve all high-level goals in an environment, and then composing them logically and temporally to solve new tasks. The authors demonstrate the effectiveness of the framework in several environments, including a high-dimensional video game and a continuous control environment. They also show that the framework can be improved with regular off-policy reinforcement learning algorithms.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The paper proposes a novel and effective framework for solving complex tasks involving temporal and spatial composition.\n2. The authors demonstrate the effectiveness of the framework in several environments, including a high-dimensional video game and a continuous control environment.\n3. The framework provides soundness guarantees, ensuring that the resulting policy adheres to the logical task specification.\n4. The authors show that the framework can be improved with regular off-policy reinforcement learning algorithms.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The paper assumes that the environment dynamics are deterministic, which may not be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the curse of dimensionality in temporal logic tasks.\n3. The paper does not provide a comprehensive comparison with other approaches to temporal composition.\n4. The authors assume that the skill primitives can be learned from a sufficient set of high-level goals, which may not be the case in all environments.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. The authors should provide a more detailed analysis of the curse of dimensionality in temporal logic tasks and propose ways to address it.\n2. The authors should provide a comprehensive comparison with other approaches to temporal composition.\n3. The authors should provide a more detailed analysis of the assumption that the environment dynamics are deterministic.\n4. The authors should provide a more detailed analysis of the assumption that the skill primitives can be learned from a sufficient set of high-level goals.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors have proposed a novel and effective framework for solving complex tasks involving temporal and spatial composition, and the framework has been demonstrated to be effective in several environments. However, the authors need to address the weaknesses of the paper, including providing a more detailed analysis of the curse of dimensionality in temporal logic tasks, providing a comprehensive comparison with other approaches to temporal composition, and providing a more detailed analysis of the assumption that the environment dynamics are deterministic.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper proposes a framework for skill machines that can learn to solve complex tasks involving temporal and spatial composition. The framework uses skill primitives, which are finite state machines that can be learned from reward machines, to encode the solution to tasks specified using regular languages. The skill machines can be composed to solve tasks with little or no further learning, and the resulting policies are provably satisficing. The authors demonstrate the framework using several experiments, including a high-dimensional video game and a continuous control environment.\n\nStrengths:\n1. The framework provides a novel approach to solving complex tasks involving temporal and spatial composition.\n2. The use of skill primitives and skill machines provides a clear and explicit way to represent and compose tasks.\n3. The framework guarantees that the resulting policies are provably satisficing, which provides assurances of safety and verifiability to the agent\u2019s decision making.\n4. The authors demonstrate the framework using several experiments, including a high-dimensional video game and a continuous control environment.\n\nWeaknesses:\n1. The framework relies on the assumption that the task primitives can be learned from reward machines, which may not always be possible.\n2. The use of skill primitives and skill machines may require a large number of tasks to learn, which can be a significant challenge in real-world applications.\n3. The framework does not provide a clear way to address the issue of overfitting, which can occur when the skill machines are learned to solve a specific task.\n4. The experiments demonstrated in the paper may not be representative of real-world applications, and further experimentation is needed to confirm the results.\n\nSuggestions for Improvement:\n1. Provide a clear and detailed explanation of how the task primitives can be learned from reward machines, and how to address the issue of overfitting.\n2. Provide a more detailed analysis of the experiments demonstrated in the paper, and provide more results to confirm the findings.\n3. Discuss the limitations of the framework, and provide a clear way to address the issues of overfitting and the assumption that task primitives can be learned from reward machines.\n4. Provide a clear and explicit way to modify and extend the framework to address real-world applications.\n\nRecommendation:\nI recommend that the authors revise and resubmit the paper to address the weaknesses and limitations of the framework. The revised paper should provide a clear and detailed explanation of how the task primitives can be learned from reward machines, and how to address the issue of overfitting. The revised paper should also provide more detailed results and analysis of the experiments, and should address the limitations of the framework. Overall, the framework has the potential to be a significant advancement in the field of reinforcement learning, and with further development and refinement, it has the potential to be a major breakthrough.",
        "gr_score_with": 0.5396341463414634,
        "gr_score_without": 0.4788732394366197,
        "or_score_with": 0.39939024390243905,
        "or_score_without": 0.43309859154929575,
        "output_without": {
            "num_tokens_scored": 284,
            "num_green_tokens": 136,
            "green_fraction": 0.4788732394366197,
            "num_orange_tokens": 123,
            "orange_fraction": 0.43309859154929575,
            "green_z_score": 2.7132088654244506,
            "orange_z_score": 84.39835071128408,
            "green_p_value": 0.0033317546009397586,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 328,
            "num_green_tokens": 177,
            "green_fraction": 0.5396341463414634,
            "num_orange_tokens": 131,
            "orange_fraction": 0.39939024390243905,
            "green_z_score": 5.1620586584710475,
            "orange_z_score": 83.52088324331167,
            "green_p_value": 1.221243613649411e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GNERP: GAUSSIAN-GUIDED NEURAL RECONSTRUC-",
        "abstract": "Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)\u2013based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$GNeRP: A Novel Gaussian-Guided Neural Reconstruction Framework for Reflective Scenes$$\n\nPaper Summary:\nThe paper proposes a novel Gaussian-based representation of normals in Signed Distance Function (SDF) fields to address the challenges of reconstructing 3D shapes of Lambertian scenes. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. The authors also introduce a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. The proposed framework, GNeRP, is evaluated on a new challenging multi-view dataset and compared with state-of-the-art methods.\n\nStrengths:\n1.  The paper presents a novel approach to reconstructing detailed geometry of reflective scenes by incorporating polarization priors and Gaussian-based representation of normals.\n2.  The proposed framework, GNeRP, is evaluated on a new challenging multi-view dataset, providing a comprehensive evaluation of the method.\n3.  The authors introduce a reweighting strategy to resolve noise issues in polarization priors, improving the efficiency of polarization priors.\n\nWeaknesses:\n1.  The paper lacks a detailed explanation of the hyper-parameter settings used in the experiments.\n2.  The authors do not provide a thorough discussion of the limitations of the proposed framework, such as the potential issues with polarization priors in diffuse-dominant regions.\n3.  The paper does not provide a clear comparison with other state-of-the-art methods in terms of computational complexity and training time.\n\nSuggestions for Improvement:\n1.  Provide a detailed explanation of the hyper-parameter settings used in the experiments, including the value of the covariance matrix and the learning rate.\n2.  Discuss the limitations of the proposed framework, such as the potential issues with polarization priors in diffuse-dominant regions.\n3.  Compare the proposed framework with other state-of-the-art methods in terms of computational complexity and training time.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise the paper to address the limitations and provide a more comprehensive evaluation of the proposed framework. Additionally, the authors should provide a clear comparison with other state-of-the-art methods in terms of computational complexity and training time.\n\n$$\\begin{aligned}\n$$\\end{aligned}$$",
        "peer_review_with_watermark": "$$$\\label{review}\n\\textbf{Paper Summary:}\nThe paper proposes GNeRP, a novel method for reconstructing detailed geometry of reflective scenes using Gaussian-based representation of normals and polarization priors. The method leverages a 3D Gaussian estimation of distributions of normals, which is supervised by polarization priors. To alleviate noise issues of polarization priors, a reweighting strategy based on the degree of polarization is introduced. The method is evaluated on a newly collected dataset, PolRef, which consists of objects with reflective and less-textured surfaces captured with various illumination.\n\n\\textbf{Strengths:}\n1.  The method addresses the challenges of reconstructing detailed geometry of reflective scenes, which are often overlooked by existing methods.\n2.  The proposed Gaussian-based representation of normals captures more details than existing methods, which are limited to scalar SDFs.\n3.  The method leverages polarization priors, which provide information about surface normals, and introduces a reweighting strategy to resolve noise issues.\n4.  The method is evaluated on a newly collected dataset, PolRef, which provides a comprehensive evaluation of existing methods.\n\n\\textbf{Weaknesses:}\n1.  The method requires a large amount of data, which may be challenging to collect, especially for reflective scenes.\n2.  The method relies on polarization priors, which may not always be available or accurate, which can lead to noise in the reconstruction.\n3.  The reweighting strategy may not always be effective in resolving noise issues, and more robust methods may be required.\n\n\\textbf{Suggestions for Improvement:}\n1.  To address the challenge of collecting large amounts of data, the method can be modified to learn from smaller datasets or to use transfer learning techniques.\n2.  To improve the robustness of the method, the reweighting strategy can be modified or additional noise reduction techniques can be used.\n3.  To further evaluate the method, more datasets, including more reflective scenes, can be collected.\n\n\\textbf{Recommendation:}\nThe method proposed in this paper has significant strengths, including its ability to capture detailed geometry of reflective scenes and its use of polarization priors. However, the method also has some weaknesses, including the requirement of large amounts of data and the potential for noise in the reconstruction. To address these challenges, the method can be modified or additional noise reduction techniques can be used. Overall, the method has the potential to be a significant advancement in 3D reconstruction, but further evaluation and refinement are required.\n\n\\label{review}\n\\textbf{Paper Summary:}\nThe paper proposes GNeRP, a novel method for reconstructing detailed geometry of reflective scenes using Gaussian-based representation of normals and polarization priors. The method leverages a 3D Gaussian estimation of distributions of normals, which is supervised by polarization priors. To alleviate noise issues of polarization priors, a reweighting strategy based on the degree of polarization is introduced. The method is evaluated on a newly collected dataset, PolRef, which consists of objects with reflective and less-textured surfaces captured with various illumination.\n\n\\textbf{Strengths:}\n1.  The method addresses the challenges of reconstructing detailed geometry of reflective scenes, which are often overlooked by existing methods.\n2.  The proposed Gaussian-based representation of normals captures more details than existing methods, which are limited to scalar SDFs.\n3.  The method leverages polarization priors, which provide information about surface normals, and introduces a reweighting strategy to resolve noise issues.\n4.  The method is evaluated on a newly collected dataset, PolRef, which provides a comprehensive evaluation of existing methods.\n\n\\textbf{Weaknesses:}\n1.  The method requires a large amount of data, which may be challenging to collect, especially for reflective scenes.\n2.  The method relies on polarization priors, which may not always be available or accurate, which can lead to noise in the reconstruction.\n3.  The reweighting strategy may not always be effective in resolving noise issues, and more robust methods may be required.\n\n\\textbf{Suggestions for Improvement:}\n1.  To address the challenge of collecting large amounts of data, the method can be modified to learn from smaller datasets or to use transfer learning techniques.\n2.  To improve the robustness of the method, the reweighting strategy can be modified or additional noise reduction techniques can be used.\n3.  To further evaluate the method, more datasets, including more reflective scenes, can be collected.\n\n\\textbf{Recommendation:}\nThe method proposed in this paper has significant strengths, including its ability to capture detailed geometry of reflective scenes and its use of polarization priors. However, the method also has some weaknesses, including the requirement of large amounts of data and the potential for noise in the reconstruction. To address these challenges, the method can be modified or additional noise reduction techniques can be used. Overall, the method has the potential to be a significant advancement in 3D reconstruction, but further evaluation and refinement are required.\n\n\\label{review}\n\\textbf{Paper Summary:}\nThe paper proposes GNeRP, a novel method for reconstructing detailed geometry of reflective scenes using Gaussian-based representation of normals and polarization priors. The method leverages a 3D Gaussian estimation of distributions of normals, which is supervised by polarization priors. To alleviate noise issues of polarization priors, a reweighting strategy based on the degree of polarization is introduced. The method is evaluated on a newly collected dataset, PolRef, which consists of objects with reflective and less-textured surfaces captured with various illumination.\n\n\\textbf{Strengths:}\n1.  The method addresses the challenges of reconstructing detailed geometry of reflective scenes, which are often overlooked by existing methods.\n2.  The proposed Gaussian-based representation of normals captures more details than existing methods, which are limited to scalar SDFs.\n3.  The method leverages polarization priors, which provide information about surface normals, and introduces a reweighting strategy to resolve noise issues.\n4.  The method is evaluated on a newly collected dataset, PolRef, which provides a comprehensive evaluation of existing methods.\n\n\\textbf{Weaknesses:}\n1.  The method requires a large amount of data, which may be challenging to collect, especially for reflective scenes.\n2.  The method relies on polarization priors, which may not always be available or accurate, which can lead to noise in the reconstruction.\n3.  The reweighting strategy may not always be effective in resolving noise issues, and more robust methods may be required.\n\n\\textbf{Suggestions for Improvement:}\n1.  To address the challenge of collecting large amounts of data, the method can be modified to learn from smaller datasets or to use transfer learning techniques.\n2.  To improve the robustness of the method, the reweighting strategy can be modified or additional noise reduction techniques can be used.\n3.  To further evaluate the method, more datasets, including more reflective scenes, can be collected.\n\n\\textbf{Recommendation:}\nThe method proposed in this paper has significant strengths, including its ability to capture detailed geometry of reflective scenes and its use of polarization priors. However, the method also has some weaknesses, including the requirement of large amounts of data and the potential for noise in the reconstruction. To address these challenges, the method can be modified or additional noise reduction techniques can be used. Overall, the method has the potential to be a significant advancement in 3D reconstruction, but further evaluation and refinement are required.\n\n\\label{review}\n\\textbf{Paper Summary:}\nThe paper proposes GNeRP, a novel method for reconstructing detailed geometry of reflective scenes using Gaussian-based representation of normals and polarization priors. The method leverages a 3D Gaussian estimation of distributions of normals, which is supervised by polarization priors. To alleviate noise issues of polarization priors, a reweighting strategy based on the degree of polarization is introduced. The method is evaluated on a newly collected dataset, PolRef, which consists of objects with reflective and less-textured surfaces captured with various illumination.\n\n\\textbf{Strengths:}\n1.  The method addresses the challenges of reconstructing detailed geometry of reflective scenes, which are often overlooked by existing methods.\n2.  The proposed Gaussian-based representation of normals captures more details than existing methods, which are limited to scalar SDFs.\n3.  The method leverages polarization priors, which provide information about surface normals, and introduces a reweighting strategy to resolve noise issues.\n4.  The method is evaluated on a newly collected dataset, PolRef, which provides a comprehensive evaluation of existing methods.\n\n\\textbf{Weaknesses:}\n1.  The method requires a large amount of data, which may be challenging to collect, especially for reflective scenes.\n2.  The method relies on polarization priors, which may not always be available or accurate, which can lead to noise in the reconstruction.\n3.  The reweighting strategy may not always be effective in resolving noise issues, and more robust methods may be required.\n\n\\textbf{Suggestions for Improvement:}\n1.  To address the challenge of collecting large amounts of data, the method can be modified to learn from smaller datasets or to use transfer learning techniques.\n2.  To improve the robustness of the method, the reweighting strategy can be modified or additional noise reduction techniques can be used.\n3.  To further evaluate the method, more datasets, including more reflective scenes, can be collected.\n\n\\textbf{Recommendation:}\nThe method proposed in this paper has significant strengths, including its ability to capture detailed geometry of reflective scenes and its use of polarization priors. However, the method also has",
        "gr_score_with": 0.47368421052631576,
        "gr_score_without": 0.3581081081081081,
        "or_score_with": 0.4444444444444444,
        "or_score_without": 0.5,
        "output_without": {
            "num_tokens_scored": 296,
            "num_green_tokens": 106,
            "green_fraction": 0.3581081081081081,
            "num_orange_tokens": 148,
            "orange_fraction": 0.5,
            "green_z_score": -1.471194893416374,
            "orange_z_score": 102.18854688733747,
            "green_p_value": 0.9293807910940375,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 162,
            "green_fraction": 0.47368421052631576,
            "num_orange_tokens": 152,
            "orange_fraction": 0.4444444444444444,
            "green_z_score": 2.7815179498365907,
            "orange_z_score": 97.46578908166012,
            "green_p_value": 0.0027052669280424467,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}\n$$ The paper introduces concept bottleneck generative models (CBGMs), which are generative models with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. The paper demonstrates the effectiveness of CBGMs in steering and debugging generative models.\n\n$$\\boxed{\\text{Strengths:}}\n$$ \n1. The concept bottleneck layer is a novel and effective way to introduce interpretability and controllability into generative models.\n2. The paper provides a comprehensive evaluation of CBGMs across different generative model families, including VAEs, GANs, and diffusion models.\n3. The authors provide a clear and detailed explanation of the concept bottleneck layer and its training procedure.\n4. The paper demonstrates the ability of CBGMs to steer and debug generative models, which is a significant improvement over current approaches.\n\n$$\\boxed{\\text{Weaknesses:}}\n$$ \n1. The paper assumes that the entire training set needs to be annotated with pre-defined concepts, which can be a laborious requirement in practice.\n2. The paper only tests CBGMs for image tasks, and it is unclear whether the approach can be applied to other domains, such as text.\n3. The paper does not provide a detailed analysis of the limitations of the concept bottleneck layer, such as feature leakage.\n4. The paper's evaluation is limited to a small number of datasets and generative model families, and it is unclear whether the results can be generalized to other settings.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}\n$$ \n1. The authors should investigate the feasibility of annotating only a subset of the training data with pre-defined concepts.\n2. The authors should explore the application of CBGMs to other domains, such as text, and provide a detailed analysis of the challenges and limitations.\n3. The authors should provide a more detailed analysis of the limitations of the concept bottleneck layer, such as feature leakage, and propose potential solutions.\n4. The authors should consider evaluating CBGMs on a larger and more diverse set of datasets and generative model families to provide more robust results.\n\n$$\\boxed{\\text{Recommendation:}}\n$$ \nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper for further evaluation. Specifically, I recommend that the authors address the limitations mentioned above and provide a more detailed analysis of the concept bottleneck layer and its applications.",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\nThe paper proposes concept bottleneck generative models (CBGMs) that incorporate an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the concept bottleneck (CB) layer, and the post-concept bottleneck portion. The pre-concept bottleneck portion maps from the input to activations, which are then mapped into human understandable features by the CB layer. The concept bottleneck layer is model agnostic and is demonstrated to be effective across three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models.\n\nStrengths:\n\n1. The concept bottleneck layer is a novel and effective way to incorporate human-understandable features into generative models.\n2. The model is demonstrated to be effective across different types of generative models and datasets.\n3. The use of concept bottleneck layer enables capabilities such as steering, debugging, and interpreting the output of the model.\n4. The paper proposes a simple and effective way to incorporate concept bottleneck layer into different types of generative models.\n\nWeaknesses:\n\n1. The concept bottleneck layer is susceptible to feature leakage, which hampers steerability.\n2. The paper does not provide a comprehensive discussion of the limitations of the concept bottleneck layer.\n3. The paper does not provide a clear discussion of how to address the limitations of the concept bottleneck layer.\n4. The paper does not provide a clear discussion of how to scale the concept bottleneck layer to more realistic settings.\n\nSuggestions for Improvement:\n\n1. Provide a more comprehensive discussion of the limitations of the concept bottleneck layer and propose potential solutions to address them.\n2. Provide a clear discussion of how to scale the concept bottleneck layer to more realistic settings.\n3. Provide a clear discussion of how to address the issue of feature leakage.\n4. Provide a clear discussion of how to incorporate concept bottleneck layer into different types of generative models.\n\nRecommendation:\nThe proposed concept bottleneck generative models (CBGMs) are a novel and effective way to incorporate human-understandable features into generative models. However, the paper needs to address the limitations of the concept bottleneck layer and provide a clear discussion of how to scale it to more realistic settings. Additionally, the paper needs to provide a clear discussion of how to address the issue of feature leakage and how to incorporate concept bottleneck layer into different types of generative models.\n\n$$$$",
        "gr_score_with": 0.5077519379844961,
        "gr_score_without": 0.3757396449704142,
        "or_score_with": 0.46511627906976744,
        "or_score_without": 0.514792899408284,
        "output_without": {
            "num_tokens_scored": 338,
            "num_green_tokens": 127,
            "green_fraction": 0.3757396449704142,
            "num_orange_tokens": 174,
            "orange_fraction": 0.514792899408284,
            "green_z_score": -0.9104369629528732,
            "orange_z_score": 115.55928137119756,
            "green_p_value": 0.8187039441048873,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 258,
            "num_green_tokens": 131,
            "green_fraction": 0.5077519379844961,
            "num_orange_tokens": 120,
            "orange_fraction": 0.46511627906976744,
            "green_z_score": 3.5328835460386743,
            "orange_z_score": 91.0928519246701,
            "green_p_value": 0.0002055267470525836,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$\n\\begin{align*}\n\\textbf{Paper Summary:} \\\\\n\\text{The paper proposes the DiLu framework, a knowledge-driven approach to autonomous driving that leverages large language models (LLMs) to enable decision-making based on common-sense knowledge and continuous experience accumulation. The framework consists of an interactive environment, a driver agent with recall, reasoning, and reflection abilities, and a memory component to persist experiences. The authors demonstrate the effectiveness of DiLu in various experiments, showcasing its ability to generalize to diverse scenarios and outperform reinforcement learning-based methods.}\n\n\\textbf{Strengths:} \\\\\n1. The paper presents a novel and promising approach to autonomous driving that combines knowledge-driven and experience-based decision-making.\n2. The authors provide a clear and concise explanation of the DiLu framework and its components.\n3. The experimental results demonstrate the effectiveness of DiLu in various scenarios, including generalization and transformation.\n\n\\textbf{Weaknesses:} \\\\\n1. The decision-making latency of the DiLu framework is a significant limitation, as it may not be suitable for real-time autonomous driving applications.\n2. The paper does not fully address the issue of hallucinations generated by LLMs, which may lead to incorrect decisions.\n3. The experimental results could be improved by including more diverse and challenging scenarios.\n\n\\textbf{Suggestions for Improvement:} \\\\\n1. Investigate techniques to reduce the decision-making latency of the DiLu framework.\n2. Develop methods to mitigate hallucinations generated by LLMs.\n3. Expand the experimental results to include more diverse and challenging scenarios.\n\n\\textbf{Recommendation:} \\\\\nThe DiLu framework is a promising approach to autonomous driving that combines knowledge-driven and experience-based decision-making. While it has some limitations, such as decision-making latency and hallucinations, the authors demonstrate its effectiveness in various experiments. Further improvements and investigations are needed to address these limitations and fully realize the potential of the DiLu framework.\n\n$$",
        "peer_review_with_watermark": "$$$ Paper Summary:\nThe paper proposes the DiLu framework, a knowledge-driven approach to autonomous driving that leverages large language models (LLMs) to enable decision-making based on common-sense knowledge. The framework consists of an environment, a driver agent, and a memory component, which work together to accumulate experiences and generalize to new environments. The authors demonstrate the effectiveness of DiLu in various experiments, including comparison with a state-of-the-art reinforcement learning (RL) method, and show that it can generalize to new environments and learn from real-world datasets. \n\nStrengths:\n1. The authors have made a significant contribution to the field of autonomous driving by proposing a novel knowledge-driven approach that leverages LLMs.\n2. The DiLu framework is well-structured, and the authors have provided a clear explanation of each component.\n3. The authors have demonstrated the effectiveness of DiLu in various experiments, including comparison with a state-of-the-art RL method.\n4. The paper has been thoroughly referenced, and the authors have provided a clear explanation of the related work.\n\nWeaknesses:\n1. The decision-making latency of the DiLu framework is a significant drawback, as it can be as high as 5-10 seconds, which may not be suitable for real-world autonomous driving systems.\n2. The paper does not address the issue of hallucinations generated by LLMs, which can lead to incorrect decisions.\n3. The authors have not provided a clear explanation of the limitations of the DiLu framework, which may be a concern for readers.\n4. The paper could benefit from more detailed explanations of the LLMs used in the DiLu framework, including their architecture and training methods.\n\nSuggestions for Improvement:\n1. Address the issue of decision-making latency by exploring optimization techniques or using more efficient LLMs.\n2. Investigate methods to reduce hallucinations generated by LLMs, such as using more robust prompting techniques or incorporating domain knowledge.\n3. Provide a clear explanation of the limitations of the DiLu framework, including its potential drawbacks and challenges.\n4. Include more detailed explanations of the LLMs used in the DiLu framework, including their architecture and training methods.\n\nRecommendation:\nThe DiLu framework is a novel and effective approach to autonomous driving that leverages LLMs to enable decision-making based on common-sense knowledge. While there are some limitations to the framework, it has the potential to be deployed on practical autonomous driving systems. With some improvements, such as addressing the decision-making latency and hallucinations, the DiLu framework could be a significant contributor to the field of autonomous driving. \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$ \n\n $$$",
        "gr_score_with": 0.46646341463414637,
        "gr_score_without": 0.3674911660777385,
        "or_score_with": 0.4878048780487805,
        "or_score_without": 0.49823321554770317,
        "output_without": {
            "num_tokens_scored": 283,
            "num_green_tokens": 104,
            "green_fraction": 0.3674911660777385,
            "num_orange_tokens": 141,
            "orange_fraction": 0.49823321554770317,
            "green_z_score": -1.1163207276674676,
            "orange_z_score": 108.41712197483406,
            "green_p_value": 0.8678575638709476,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 328,
            "num_green_tokens": 153,
            "green_fraction": 0.46646341463414637,
            "num_orange_tokens": 160,
            "orange_fraction": 0.4878048780487805,
            "green_z_score": 2.4570497544687506,
            "orange_z_score": 114.24687152565586,
            "green_p_value": 0.007004164202020568,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GENERALIZATION: UNRAVELING LEARNING DIFFER-",
        "abstract": "This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension (ddata) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to ddata, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic \u201clabel sharpness\u201d (KF ) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model\u2019s adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our ddata formalism to the related metric of learned representation intrinsic dimension (drepr), derive a generalization scaling law with respect to drepr, and show that ddata serves as an upper bound for drepr. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks.1",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}~ The~ authors~ investigate~ the~ discrepancies~ in~ how~ neural~ networks~ learn~ from~ different~ imaging~ domains,~ particularly~ focusing~ on~ medical~ images.~ They~ propose~ a~ generalization~ scaling~ law~ with~ respect~ to~ dataset~ intrinsic~ dimension~ (ddata)~ and~ label~ sharpness~ (KF),~ which~ explains~ the~ differences~ in~ generalization~ ability~ and~ adversarial~ robustness~ between~ natural~ and~ medical~ image~ domains.~ The~ authors~ also~ derive~ a~ generalization~ scaling~ law~ for~ learned~ representation~ intrinsic~ dimension~ (drepr)~ and~ show~ that~ ddata~ serves~ as~ an~ upper~ bound~ for~ drepr.~ The~ results~ are~ validated~ with~ thorough~ experiments~ using~ six~ CNN~ architectures~ and~ eleven~ datasets~ from~ natural~ and~ medical~ imaging~ domains.}\n\n$$\\boxed{\\text{Strengths:}~ The~ paper~ presents~ an~ interesting~ and~ important~ contribution~ to~ the~ field~ of~ deep~ learning,~ focusing~ on~ the~ relationship~ between~ neural~ networks~ and~ their~ training~ set.~ The~ authors~ provide~ a~ clear~ and~ concise~ presentation~ of~ the~ theoretical~ framework~ and~ empirical~ results.~ The~ use~ of~ multiple~ datasets~ and~ architectures~ is~ a~ strength~ of~ the~ paper,~ allowing~ for~ a~ more~ comprehensive~ understanding~ of~ the~ generalization~ ability~ and~ adversarial~ robustness~ in~ different~ imaging~ domains.}\n\n$$\\boxed{\\text{Weaknesses:}~ One~ of~ the~ main~ limitations~ of~ the~ paper~ is~ the~ difficulty~ of~ ruling~ out~ other~ potential~ factors~ that~ may~ contribute~ to~ the~ observed~ generalization~ scaling~ discrepancy~ between~ natural~ and~ medical~ image~ domains.~ The~ authors~ acknowledge~ this~ limitation,~ but~ it~ remains~ a~ significant~ challenge~ for~ future~ research.~ Additionally,~ the~ use~ of~ the~ FGSM~ attack~ may~ not~ be~ the~ most~ effective~ method~ for~ evaluating~ adversarial~ robustness,~ and~ alternative~ methods~ should~ be~ explored.~ Furthermore,~ the~ results~ are~ mostly~ based~ on~ binary~ classification~ tasks,~ and~ it~ would~ be~ interesting~ to~ investigate~ the~ generalization~ ability~ and~ adversarial~ robustness~ in~ multi-class~ classification~ tasks.}\n\n$$\\boxed{\\text{Suggestions~ for~ Improvement:}~ The~ authors~ should~ consider~ exploring~ alternative~ methods~ for~ evaluating~ adversarial~ robustness,~ such~ as~ the~ Fast~ Gradient~ Method~ (FGSM)~ or~ the~ Projected~ Gradient~ Descent~ (PGD)~ attack.~ Additionally,~ investigating~ the~ generalization~ ability~ and~ adversarial~ robustness~ in~ multi-class~ classification~ tasks~ could~ provide~ more~ comprehensive~ insights~ into~ the~ relationship~ between~ neural~ networks~ and~ their~ training~ set.~ The~ authors~ could~ also~ consider~ using~ more~ diverse~ and~ representative~ datasets,~ including~ datasets~ with~ varying~ levels~ of~ noise~ and~ outliers.}\n\n$$\\boxed{\\text{Recommendation:}~ Based~ on~ the~ thorough~ analysis~ and~ experimental~ results,~ I~ recommend~ that~ the~ authors~ continue~ to~ investigate~ the~ relationship~ between~ neural~ networks~ and~ their~ training~ set,~ particularly~ focusing~ on~ the~ generalization~ ability~ and~ adversarial~ robustness~ in~ different~ imaging~ domains.~ The~ authors~ could~ also~ explore~ alternative~ methods~ for~ evaluating~ adversarial~ robustness~ and~ investigating~ the~ generalization~ ability~ and~ adversarial~ robustness~ in~ multi-class~ classification~ tasks.~ The~ findings~ of~ this~ paper~ have~ the~ potential~ to~ significantly~ impact~ the~ field~ of~ deep~ learning,~ and~ I~ believe~ that~ further~ research~ in~ this~ area~ is~ warranted.}$$",
        "peer_review_with_watermark": "$$\\label{review}\n$$\n\nThe paper \"Generalization: Unraveling Learning Discrepancies between Natural and Medical Images\" by N. Konz et al. presents an intriguing study on the differences in generalization ability and adversarial robustness of deep neural networks trained on natural versus medical images. The authors investigate the role of dataset intrinsic dimension (ddata) and label sharpness (KF) in shaping these phenomena.\n\n**Paper Summary:**\nThe paper focuses on understanding how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. The authors introduce a generalization scaling law with respect to ddata and derive a neural network generalization scaling law with respect to ddata. They also investigate the relationship between generalization error and learned representation intrinsic dimension (drepr).\n\n**Strengths:**\n\n1. The paper provides a comprehensive analysis of the generalization ability and adversarial robustness of deep neural networks trained on natural versus medical images.\n2. The authors introduce a novel measure of intrinsic label sharpness (KF) and demonstrate its negative correlation with the trained model's adversarial robustness.\n3. The study provides insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks.\n\n**Weaknesses:**\n\n1. The paper relies on a binary classification task, which may not be representative of more complex tasks such as object detection or segmentation.\n2. The authors assume that the model is well-trained and learns to interpolate all training data, which may not always be the case.\n3. The study focuses on a limited number of models and datasets, which may limit the generalizability of the findings.\n\n**Suggestions for Improvement:**\n\n1. The authors should investigate the generalization ability and adversarial robustness of deep neural networks trained on other specialized domains such as satellite imaging or histopathology.\n2. The study should be expanded to include more complex tasks such as object detection or segmentation.\n3. The authors should investigate the relationship between generalization error and learned representation intrinsic dimension (drepr) in more depth.\n\n**Recommendation:**\nThe paper provides a significant contribution to the understanding of generalization ability and adversarial robustness in deep neural networks. The introduction of intrinsic label sharpness (KF) as a measure of dataset properties is a novel aspect of the study. However, the limitations of the study should be acknowledged, and the authors should consider expanding the scope of the study to include more complex tasks and datasets.\n\nOverall, the paper presents an important study on the differences in generalization ability and adversarial robustness of deep neural networks trained on natural versus medical images. The authors provide a comprehensive analysis of the generalization scaling law with respect to ddata and introduce a novel measure of intrinsic label sharpness (KF). The study provides insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks.",
        "gr_score_with": 0.4610951008645533,
        "gr_score_without": 0.2548076923076923,
        "or_score_with": 0.4697406340057637,
        "or_score_without": 0.25240384615384615,
        "output_without": {
            "num_tokens_scored": 416,
            "num_green_tokens": 106,
            "green_fraction": 0.2548076923076923,
            "num_orange_tokens": 105,
            "orange_fraction": 0.25240384615384615,
            "green_z_score": -6.04483780614204,
            "orange_z_score": 62.12699826113701,
            "green_p_value": 0.9999999992521981,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 347,
            "num_green_tokens": 160,
            "green_fraction": 0.4610951008645533,
            "num_orange_tokens": 163,
            "orange_fraction": 0.4697406340057637,
            "green_z_score": 2.323087150690263,
            "orange_z_score": 106.90233009461421,
            "green_p_value": 0.01008723555583772,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TRANSFORMER-MODULATED DIFFUSION MODELS FOR PROBABILISTIC MULTIVARIATE TIME SERIES FORECASTING",
        "abstract": "Transformers have gained widespread usage in multivariate time series (MTS) forecasting, delivering impressive performance. Nonetheless, these existing transformer-based methods often neglect an essential aspect: the incorporation of uncertainty into the predicted series, which holds significant value in decisionmaking. In this paper, we introduce a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. TMDM harnesses the power of transformers to extract essential insights from historical time series data. This information is then utilized as prior knowledge, capturing covariate-dependence in both the forward and reverse processes within the diffusion model. Furthermore, we seamlessly integrate well-designed transformerbased forecasting methods into TMDM to enhance its overall performance. Additionally, we introduce two novel metrics for evaluating uncertainty estimation performance. Through extensive experiments on six datasets using four evaluation metrics, we establish the effectiveness of TMDM in probabilistic MTS forecasting.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes a novel framework called the Transformer-Modulated Diffusion Model (TMDM) for probabilistic multivariate time series forecasting. TMDM combines the conditional diffusion generative process with well-designed transformer structures, enabling accurate distribution forecasting for time series. The authors introduce a hybrid optimization strategy that integrates the condition generative model and denoising model, allowing for efficient training and seamless compatibility with existing well-designed transformer-based forecasting models.\n\n$$\\text{Strengths:}$$\n1. The authors introduce a new and innovative approach that merges diffusion generative process with well-designed transformer models, providing a robust framework for probabilistic multivariate time series forecasting.\n2. The proposed method leverages the strengths of transformers, particularly their accuracy in estimating conditional means, and extends this capability as priors across both forward and reverse processes within the diffusion model.\n3. The authors introduce two novel evaluation metrics, Prediction Interval Coverage Probability (PICP) and Quantile Interval Coverage Error (QICE), which provide valuable insights into assessing the uncertainty estimation abilities of probabilistic multivariate time series forecasting models.\n4. The proposed framework is versatile and can be seamlessly integrated with advanced transformer models for point estimation, promising even better forecasting accuracy.\n\n$$\\text{Weaknesses:}$$\n1. The paper lacks a clear explanation of the role of the NSformer in the proposed framework, which may lead to confusion among readers.\n2. The authors do not provide a detailed analysis of the impact of different transformer structures on the performance of TMDM, which may be important for future research.\n3. The proposed method requires a large amount of computational resources and data, which may be a limitation for some users.\n4. The paper does not provide a clear comparison with state-of-the-art methods in the field, which may be necessary for a comprehensive evaluation of the proposed framework.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a clear explanation of the role of the NSformer in the proposed framework, including its advantages and limitations.\n2. Conduct a detailed analysis of the impact of different transformer structures on the performance of TMDM, including a comparison with state-of-the-art methods.\n3. Provide a clear comparison with state-of-the-art methods in the field, including a discussion of the strengths and weaknesses of each approach.\n4. Consider providing a more detailed explanation of the proposed hybrid optimization strategy and its advantages over other optimization methods.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the proposed framework, I recommend that the authors revise and resubmit the paper for peer review. The authors should address the weaknesses and provide additional details to support their claims, while maintaining the strengths of the proposed framework. With revisions, the paper has the potential to make a significant contribution to the field of probabilistic multivariate time series forecasting.",
        "peer_review_with_watermark": "$$$\\begin{align*}\n\\textbf{Paper Summary:} \\qquad &\\text{TMDM} \\text{unifies diffusion generative process with transformers to estimate distribution of future time series y0:M. The framework harnesses representations captured by well-designed transformer-based models as priors, capturing covariate-dependence across both forward and reverse processes.}\n\n\\textbf{Strengths:} \\qquad &\\text{TMDM} \\text{unifies diffusion generative process with transformers to estimate distribution of future time series y0:M. The framework harnesses representations captured by well-designed transformer-based models as priors, capturing covariate-dependence across both forward and reverse processes.}\n\n\\textbf{Weaknesses:} \\qquad &\\text{TMDM} \\text{only focuses on distribution estimation. There is a need to explore its capabilities in point-to-point forecasting tasks.}\n\n\\textbf{Suggestions for Improvement:} \\qquad &\\text{TMDM} \\text{would benefit from incorporating techniques to address issues such as mode collapse, vanishing or exploding gradients during training.}\n\n\\textbf{Recommendation:} \\qquad &\\text{TMDM} \\text{shows promise in addressing the limitations of existing probabilistic multivariate time series forecasting models. However, its performance should be compared with other state-of-the-art models to establish its superiority.}\n\n\\textbf{Future Work:} \\qquad &\\text{TMDM} \\text{would benefit from exploring its capabilities in point-to-point forecasting tasks. Incorporating techniques to address issues such as mode collapse, vanishing or exploding gradients during training would also be beneficial.}\n\n\\textbf{Code and Data:} \\qquad &\\text{TMDM} \\text{would benefit from being open-sourced to facilitate comparison with other models.}\n\n\\textbf{Impact:} \\qquad &\\text{TMDM} \\text{would have a significant impact on the field of probabilistic multivariate time series forecasting by providing a unified framework to estimate distribution of future time series y0:M.}\n\n\\textbf{Originality:} \\qquad &\\text{TMDM} \\text{provides a novel approach to address the limitations of existing probabilistic multivariate time series forecasting models by harnessing representations captured by well-designed transformer-based models as priors.}\n\n\\textbf{Comparison to State-of-the-Art:} \\qquad &\\text{TMDM} \\text{would benefit from being compared to other state-of-the-art models to establish its superiority.}\n\n\\textbf{Limitations:} \\qquad &\\text{TMDM} \\text{only focuses on distribution estimation. There is a need to explore its capabilities in point-to-point forecasting tasks.}\n\n\\textbf{Uncertainty Quantification:} \\qquad &\\text{TMDM} \\text{would benefit from incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Real-world Applications:} \\qquad &\\text{TMDM} \\text{would benefit from being explored in real-world applications to establish its practical value.}\n\n\\textbf{Future Work:} \\qquad &\\text{TMDM} \\text{would benefit from exploring its capabilities in point-to-point forecasting tasks and incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Code and Data:} \\qquad &\\text{TMDM} \\text{would benefit from being open-sourced to facilitate comparison with other models.}\n\n\\textbf{Impact:} \\qquad &\\text{TMDM} \\text{would have a significant impact on the field of probabilistic multivariate time series forecasting by providing a unified framework to estimate distribution of future time series y0:M.}\n\n\\textbf{Originality:} \\qquad &\\text{TMDM} \\text{provides a novel approach to address the limitations of existing probabilistic multivariate time series forecasting models by harnessing representations captured by well-designed transformer-based models as priors.}\n\n\\textbf{Comparison to State-of-the-Art:} \\qquad &\\text{TMDM} \\text{would benefit from being compared to other state-of-the-art models to establish its superiority.}\n\n\\textbf{Limitations:} \\qquad &\\text{TMDM} \\text{only focuses on distribution estimation. There is a need to explore its capabilities in point-to-point forecasting tasks.}\n\n\\textbf{Uncertainty Quantification:} \\qquad &\\text{TMDM} \\text{would benefit from incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Real-world Applications:} \\qquad &\\text{TMDM} \\text{would benefit from being explored in real-world applications to establish its practical value.}\n\n\\textbf{Future Work:} \\qquad &\\text{TMDM} \\text{would benefit from exploring its capabilities in point-to-point forecasting tasks and incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Code and Data:} \\qquad &\\text{TMDM} \\text{would benefit from being open-sourced to facilitate comparison with other models.}\n\n\\textbf{Impact:} \\qquad &\\text{TMDM} \\text{would have a significant impact on the field of probabilistic multivariate time series forecasting by providing a unified framework to estimate distribution of future time series y0:M.}\n\n\\textbf{Originality:} \\qquad &\\text{TMDM} \\text{provides a novel approach to address the limitations of existing probabilistic multivariate time series forecasting models by harnessing representations captured by well-designed transformer-based models as priors.}\n\n\\textbf{Comparison to State-of-the-Art:} \\qquad &\\text{TMDM} \\text{would benefit from being compared to other state-of-the-art models to establish its superiority.}\n\n\\textbf{Limitations:} \\qquad &\\text{TMDM} \\text{only focuses on distribution estimation. There is a need to explore its capabilities in point-to-point forecasting tasks.}\n\n\\textbf{Uncertainty Quantification:} \\qquad &\\text{TMDM} \\text{would benefit from incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Real-world Applications:} \\qquad &\\text{TMDM} \\text{would benefit from being explored in real-world applications to establish its practical value.}\n\n\\textbf{Future Work:} \\qquad &\\text{TMDM} \\text{would benefit from exploring its capabilities in point-to-point forecasting tasks and incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Code and Data:} \\qquad &\\text{TMDM} \\text{would benefit from being open-sourced to facilitate comparison with other models.}\n\n\\textbf{Impact:} \\qquad &\\text{TMDM} \\text{would have a significant impact on the field of probabilistic multivariate time series forecasting by providing a unified framework to estimate distribution of future time series y0:M.}\n\n\\textbf{Originality:} \\qquad &\\text{TMDM} \\text{provides a novel approach to address the limitations of existing probabilistic multivariate time series forecasting models by harnessing representations captured by well-designed transformer-based models as priors.}\n\n\\textbf{Comparison to State-of-the-Art:} \\qquad &\\text{TMDM} \\text{would benefit from being compared to other state-of-the-art models to establish its superiority.}\n\n\\textbf{Limitations:} \\qquad &\\text{TMDM} \\text{only focuses on distribution estimation. There is a need to explore its capabilities in point-to-point forecasting tasks.}\n\n\\textbf{Uncertainty Quantification:} \\qquad &\\text{TMDM} \\text{would benefit from incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Real-world Applications:} \\qquad &\\text{TMDM} \\text{would benefit from being explored in real-world applications to establish its practical value.}\n\n\\textbf{Future Work:} \\qquad &\\text{TMDM} \\text{would benefit from exploring its capabilities in point-to-point forecasting tasks and incorporating techniques to address issues such as uncertainty quantification during training.}\n\n\\textbf{Code and Data:} \\qquad &\\text{TMDM} \\text{would benefit from being open-sourced to facilitate comparison with other models.}\n\n\\textbf{Impact:} \\qquad &\\text{TMDM} \\text{would have a significant impact on the field of probabilistic multivariate time series forecasting by providing a unified framework to estimate distribution of future time series y0:M.}\n\n\\textbf{Originality:} \\qquad &\\text{TMDM} \\text{provides a novel approach to address the limitations of existing probabilistic multivariate time series forecasting models by harnessing representations captured by well-designed transformer-based models as priors.}\n\n\\textbf{Comparison to State-of-the-Art:} \\qquad &\\text{TMDM} \\text{would benefit from being compared to other state-of-the-art models to establish its superiority.}\n\n\\textbf{Limitations:} \\qquad &\\text{TMDM} \\text{only focuses on distribution",
        "gr_score_with": 0.5230769230769231,
        "gr_score_without": 0.29081632653061223,
        "or_score_with": 0.38461538461538464,
        "or_score_without": 0.6071428571428571,
        "output_without": {
            "num_tokens_scored": 392,
            "num_green_tokens": 114,
            "green_fraction": 0.29081632653061223,
            "num_orange_tokens": 238,
            "orange_fraction": 0.6071428571428571,
            "green_z_score": -4.412605628806427,
            "orange_z_score": 145.3242324812397,
            "green_p_value": 0.9999948933031443,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 260,
            "num_green_tokens": 136,
            "green_fraction": 0.5230769230769231,
            "num_orange_tokens": 100,
            "orange_fraction": 0.38461538461538464,
            "green_z_score": 4.0509574683346665,
            "orange_z_score": 74.48846243035153,
            "green_p_value": 2.5504239750774438e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ING IN WHOLE SLIDE IMAGES",
        "abstract": "The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the ContextAware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5%, 95.9%, and 88.1%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interpretability by identifying regions of high diagnostic value.",
        "peer_review_without_watermark": "$$$\\boxed{Peer Review}$$\n\n$\\boxed{Paper Summary:}$\n\nThe paper proposes a novel framework called Context-Aware Multiple Instance Learning (CAMIL) for whole slide imaging (WSI) classification in cancer diagnostics. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the multiple instance learning model. The proposed framework is evaluated on three histopathology datasets (CAMELYON16, CAMELYON17, and TCGA-NSCLC) and demonstrates state-of-the-art performance in terms of tile-level accuracy, area under the receiver operating characteristic curve (AUC), and F1 score.\n\n$\\boxed{Strengths:}$\n\n1. The proposed framework, CAMIL, addresses the limitations of existing attention-based multiple instance learning (MIL) models by incorporating neighbor-constrained attention and contextual constraints.\n2. CAMIL achieves state-of-the-art performance on three histopathology datasets, demonstrating its effectiveness in WSI classification.\n3. The use of a transformer layer and Nystromformer module enables the model to capture both global and local contextual relationships within the WSI.\n4. The proposed framework is interpretable, as it provides attention maps that highlight sub-regions of high diagnostic value within the WSI.\n\n$\\boxed{Weaknesses:}$\n\n1. The paper lacks a comprehensive discussion on the potential risks and limitations of using CAMIL, such as the computational requirements and the need for large amounts of training data.\n2. The evaluation of CAMIL is limited to three histopathology datasets, and it is unclear how the model would perform on other datasets or in different clinical settings.\n3. The paper does not provide a detailed analysis of the attention weights and their implications for the model's performance and interpretability.\n4. The use of the Nystromformer module may introduce a trade-off between slide-level accuracy and localization performance, as discussed in the ablation study.\n\n$\\boxed{Suggestions for Improvement:}$\n\n1. Conduct a more comprehensive evaluation of CAMIL on a larger number of datasets and in different clinical settings to assess its generalizability and robustness.\n2. Provide a detailed analysis of the attention weights and their implications for the model's performance and interpretability.\n3. Investigate the potential risks and limitations of using CAMIL, such as the computational requirements and the need for large amounts of training data.\n4. Develop a more detailed discussion on the potential applications and implications of CAMIL in clinical practice and research.\n\n$\\boxed{Recommendation:}$\n\nBased on the evaluation and analysis presented in the paper, I recommend that CAMIL be considered as a promising approach for WSI classification in cancer diagnostics. However, further research is needed to address the limitations and potential risks associated with the framework, as well as to explore its applications and implications in clinical practice and research.",
        "peer_review_with_watermark": "$$$\\label{summary}\nThe paper proposes a novel framework dubbed Context-Aware Multiple Instance Learning (CAMIL) that harnesses the dependencies among the individual tiles within a Whole Slide Image (WSI) and imposes contextual constraints as prior knowledge into the Multiple Instance Learning (MIL) model. The proposed architecture, which combines the transformer and neighbor-constrained attention mechanisms, is designed to capture both local and global contextual information in WSIs.\n\nThe paper evaluates the performance of CAMIL on three histopathology datasets: CAMELYON16, CAMELYON17, and TCGA-NSCLC. The results show that CAMIL outperforms other state-of-the-art methods in terms of tile-level accuracy and AUC across all datasets.\n\nThe strengths of the paper lie in its novel approach to incorporating contextual information into the MIL model and the thorough evaluation of the proposed architecture on multiple datasets. The use of transformer and neighbor-constrained attention mechanisms enables the model to capture both local and global contextual information in WSIs.\n\nHowever, the paper has several limitations. The explanation of the neighbor-constrained attention mechanism is not very detailed, and the results of the ablation study are not fully discussed. Additionally, the paper could benefit from more thorough evaluation of the proposed architecture on additional datasets.\n\n\\label{strengths}\nThe paper's strengths lie in its novel approach to incorporating contextual information into the MIL model and the thorough evaluation of the proposed architecture on multiple datasets.\n\n\\label{weaknesses}\nThe explanation of the neighbor-constrained attention mechanism is not very detailed, and the results of the ablation study are not fully discussed.\n\n\\label{improvement}\nTo strengthen the paper, the explanation of the neighbor-constrained attention mechanism should be more detailed, and the results of the ablation study should be fully discussed.\n\n\\label{recommendation}\nBased on the evaluation of the proposed architecture, I recommend that the paper be accepted for publication.\n\n$$\\label{recom}\nBased on the evaluation of the proposed architecture, I recommend that the paper be accepted for publication.\n\n$$\\label{ref}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refup}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the field.\n\n$$\\label{refdown}\nThe reference list should be updated to include the latest publications in the",
        "gr_score_with": 0.49640287769784175,
        "gr_score_without": 0.3801452784503632,
        "or_score_with": 0.4568345323741007,
        "or_score_without": 0.5351089588377724,
        "output_without": {
            "num_tokens_scored": 413,
            "num_green_tokens": 157,
            "green_fraction": 0.3801452784503632,
            "num_orange_tokens": 221,
            "orange_fraction": 0.5351089588377724,
            "green_z_score": -0.8236319887825672,
            "orange_z_score": 127.41757334880127,
            "green_p_value": 0.7949256529241351,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 278,
            "num_green_tokens": 138,
            "green_fraction": 0.49640287769784175,
            "num_orange_tokens": 127,
            "orange_fraction": 0.4568345323741007,
            "green_z_score": 3.2810041160584245,
            "orange_z_score": 89.04009978167048,
            "green_p_value": 0.0005171911736180474,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "COLIDE: CONCOMITANT LINEAR DAG ESTIMATION",
        "abstract": "We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the unknown SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE (Concomitant Linear DAG Estimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of noise variances in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}~\\text{The}~\\text{paper}~\\text{proposes}~\\text{CoLiDE},~\\text{a}~\\text{novel}~\\text{convex}~\\text{score}~\\text{function}~\\text{for}~\\text{learning}~\\text{linear}~\\text{directed}~\\text{acyclic}~\\text{graphs}~\\text{(DAGs)}~\\text{with}~\\text{concomitant}~\\text{estimation}~\\text{of}~\\text{scale}~\\text{parameters}.\\text{The}~\\text{approach}~\\text{utilizes}~\\text{smoothed}~\\text{concomitant}~\\text{lasso}~\\text{estimators}~\\text{to}~\\text{jointly}~\\text{estimate}~\\text{the}~\\text{DAG}~\\text{and}~\\text{exogenous}~\\text{noise}~\\text{levels}.\\text{The}~\\text{paper}~\\text{evaluates}~\\text{CoLiDE}~\\text{on}~\\text{synthetic}~\\text{data}~\\text{and}~\\text{real-world}~\\text{data}~\\text{and}~\\text{finds}~\\text{CoLiDE}~\\text{to}~\\text{outperform}~\\text{existing}~\\text{state-of-the-art}~\\text{methods}~\\text{in}~\\text{recovering}~\\text{the}~\\text{DAG}~\\text{structure}~\\text{and}~\\text{edge}~\\text{weights}.\\text{The}~\\text{paper}~\\text{also}~\\text{provides}~\\text{a}~\\text{full}~\\text{derivation}~\\text{of}~\\text{the}~\\text{score}~\\text{function}~\\text{and}~\\text{provides}~\\text{a}~\\text{comprehensive}~\\text{evaluation}~\\text{of}~\\text{CoLiDE}~\\text{on}~\\text{various}~\\text{metrics}~\\text{such}~\\text{as}~\\text{SHD},~\\text{SHD-C},~\\text{SID},~\\text{TPR},~\\text{FDR},~\\text{and}~\\text{the}~\\text{noise}~\\text{estimation}~\\text{metric}.\\text{The}~\\text{paper}~\\text{also}~\\text{provides}~\\text{an}~\\text{in-depth}~\\text{analysis}~\\text{of}~\\text{the}~\\text{role}~\\text{of}~\\text{score}~\\text{functions}~\\text{in}~\\text{recovering}~\\text{the}~\\text{underlying}~\\text{DAG}~\\text{structure}~\\text{and}~\\text{edge}~\\text{weights}.\\text{The}~\\text{paper}~\\text{also}~\\text{discusses}~\\text{the}~\\text{practical}~\\text{implications}~\\text{of}~\\text{CoLiDE}~\\text{and}~\\text{provides}~\\text{a}~\\text{ roadmap}~\\text{for}~\\text{future}~\\text{research}~\\text{on}~\\text{DAG}~\\text{learning}~\\text{methods}.\\text{Strengths:}~\\text{The}~\\text{paper}~\\text{provides}~\\text{a}~\\text{novel}~\\text{convex}~\\text{score}~\\text{function}~\\text{for}~\\text{learning}~\\text{linear}~\\text{DAGs}~\\text{with}~\\text{concomitant}~\\text{estimation}~\\text{of}~\\text{scale}~\\text{parameters}.\\text{The}~\\text{paper}~\\text{utilizes}~\\text{smoothed}~\\text{concomitant}~\\text{lasso}~\\text{estimators}~\\text{to}~\\text{jointly}~\\text{estimate}~\\text{the}~\\text{DAG}~\\text{and}~\\text{exogenous}~\\text{noise}~\\text{levels}.\\text{Weaknesses:}~\\text{The}~\\text{paper}~\\text{may}~\\text{not}~\\text{be}~\\text{clear}~\\text{on}~\\text{how}~\\text{the}~\\text{score}~\\text{function}~\\text{is}~\\text{related}~\\text{to}~\\text{the}~\\text{concomitant}~\\text{estimation}~\\text{of}~\\text{scale}~\\text{parameters}.\\text{Suggestions}~\\text{for}~\\text{Improvement:}~\\text{The}~\\text{paper}~\\text{may}~\\text{consider}~\\text{providing}~\\text{a}~\\text{clear}~\\text{explanation}~\\text{of}~\\text{how}~\\text{the}~\\text{score}~\\text{function}~\\text{is}~\\text{related}~\\text{to}~\\text{the}~\\text{concomitant}~\\text{estimation}~\\text{of}~\\text{scale}~\\text{parameters}.\\text{Recommendation:}~\\text{The}~\\text{paper}~\\text{provides}~\\text{a}~\\text{novel}~\\text{convex}~\\text{score}~\\text{function}~\\text{for}~\\text{learning}~\\text{linear}~\\text{DAGs}~\\text{with}~\\text{concomitant}~\\text{estimation}~\\text{of}~\\text{scale}~\\text{parameters}.\\text{The}~\\text{approach}~\\text{utilizes}~\\text{smoothed}~\\text{concomitant}~\\text{lasso}~\\text{estimators}~\\text{to}~\\text{jointly}~\\text{estimate}~\\text{the}~\\text{DAG}~\\text{and}~\\text{exogenous}~\\text{noise}~\\text{levels}.\\text{The}~\\text{paper}~\\text{evaluates}~\\text{CoLiDE}~\\text{on}~\\text{synthetic}~\\text{data}~\\text{and}~\\text{real-world}~\\text{data}~\\text{and}~\\text{finds}~\\text{CoLiDE}~\\text{to}~\\text{outperform}~\\text{existing}~\\text{state-of-the-art}~\\text{methods}~\\text{in}~\\text{recovering}~\\text{the}~\\text{DAG}~\\text{structure}~\\text{and}~\\text{edge}~\\text{weights}.\\text{Overall},~\\text{CoLiDE}~\\text{is}~\\text{a}~\\text{novel}~\\text{approach}~\\text{that}~\\text{utilizes}~\\text{smoothed}~\\text{concomitant}~\\text{lasso}~\\text{estimators}~\\text{to}~\\text{jointly}~\\text{estimate}~\\text{the}~\\text{DAG}~\\text{and}~\\text{exogenous}~\\text{noise}~\\text{levels}.\\text{The}~\\text{paper}~\\text{provides}~\\text{a}~\\text{full}~\\text{derivation}~\\text{of}~\\text{the}",
        "peer_review_with_watermark": "$$$$Paper Summary:\nThe paper proposes CoLiDE, a novel framework for learning linear DAGs that simultaneously estimates both the DAG structure and the exogenous noise levels. The authors leverage advances in differentiable, nonconvex characterizations of acyclicity and propose a new convex score function for sparsity-aware learning of linear DAGs, incorporating concomitant estimation of scale. CoLiDE outperforms state-of-the-art methods in diverse synthetic and real-world settings, including the recovery of the DAG edges as well as their weights.\n\n$$$$Strengths:\n1. CoLiDE provides a flexible framework for learning linear DAGs that accounts for heteroscedastic noise levels.\n2. The proposed score function is jointly convex in W and \u03a3, which facilitates efficient computation of gradients and estimation of exogenous noise levels.\n3. CoLiDE is robust to deviations from Gaussianity and can handle large DAGs with varying noise levels.\n4. The authors provide extensive experimental results demonstrating CoLiDE's superior performance in various settings.\n\n$$$$Weaknesses:\n1. The paper does not address the issue of overfitting, which may occur when the number of parameters in CoLiDE is large.\n2. The authors do not provide a clear explanation of how CoLiDE can be adapted to non-linear SEMs.\n3. Some of the experimental results, such as Figure 16, seem to contradict the paper's claims.\n\n$$$$Suggestions for Improvement:\n1. Add more theoretical analysis to explain how CoLiDE can address overfitting.\n2. Investigate the applicability of CoLiDE to non-linear SEMs.\n3. Address the apparent contradiction in the experimental results.\n\n$$$$Recommendation:\nOverall, CoLiDE appears to be a promising framework for learning linear DAGs that accounts for heteroscedastic noise levels. However, addressing the mentioned weaknesses and providing further theoretical analysis and experimental results is necessary to fully establish CoLiDE as a state-of-the-art method.",
        "gr_score_with": 0.4899328859060403,
        "gr_score_without": 0.3333333333333333,
        "or_score_with": 0.4798657718120805,
        "or_score_without": 0.4523809523809524,
        "output_without": {
            "num_tokens_scored": 252,
            "num_green_tokens": 84,
            "green_fraction": 0.3333333333333333,
            "num_orange_tokens": 114,
            "orange_fraction": 0.4523809523809524,
            "green_z_score": -2.1602468994692883,
            "orange_z_score": 61.52441895217654,
            "green_p_value": 0.9846232193703628,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 298,
            "num_green_tokens": 146,
            "green_fraction": 0.4899328859060403,
            "num_orange_tokens": 143,
            "orange_fraction": 0.4798657718120805,
            "green_z_score": 3.168991257930694,
            "orange_z_score": 71.08981823533904,
            "green_p_value": 0.0007648449803702869,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CONQUER: DIFFUSION-BASED 3D DEPTH AWARE COMPOSABLE IMAGE SYNTHESIS",
        "abstract": "Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, COMPOSE AND CONQUER (CNC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n\\text{Paper Summary:} \n\\text{The paper presents a novel text-conditional diffusion model called Compose and Conquer (CNC), which addresses two main challenges in the field: three-dimensional placement of multiple objects and region-specific localization of global semantics from multiple sources. CNC employs two main components: the local and global fuser, which respectively leverages the new Depth Disentanglement Training (DDT) and soft guidance techniques. The proposed model demonstrates improved performance in various metrics, including FID, Inception Score, and CLIPScore, on both COCO-stuff and Pick-a-Pic datasets.}\n\n\\text{Strengths:}\n- The proposed model addresses two significant challenges in the field of text-conditional diffusion models.\n- The use of Depth Disentanglement Training (DDT) and soft guidance techniques enables the model to infer absolute depth placement of objects and incorporate semantics onto localized regions, respectively.\n- The extensive experimental results on both COCO-stuff and Pick-a-Pic datasets demonstrate the model's proficiency in addressing these challenges.\n\n\\text{Weaknesses:}\n- The current framework limits the number of available conditions and the disentangled spatial grounds to the foreground and background, which may be a limitation for future work.\n- The dataset choices are intentional but may not be representative of real-world scenarios, which may affect the model's generalizability.\n\n\\text{Suggestions for Improvement:}\n- Further decomposition of images into depth portraying primitives and the middle ground could be explored in future work.\n- Additional experiments with different datasets and conditions could be conducted to further validate the model's performance.\n\n\\text{Recommendation:}\n- The proposed model, Compose and Conquer (CNC), is a significant contribution to the field of text-conditional diffusion models, addressing two main challenges in a novel and effective manner. Its improved performance on various metrics, including FID, Inception Score, and CLIPScore, on both COCO-stuff and Pick-a-Pic datasets, makes it a valuable tool for applications such as image synthesis, object placement, and semantic localization. However, further work is recommended to address the limitations mentioned above and to explore the potential of this framework for future applications.}\n\n$$",
        "peer_review_with_watermark": "$$$\\begin{aligned} &\\text{Paper Summary: } \\\\ &\\text{The paper presents a novel text-conditional diffusion model, } \\\\ &\\text{Compose and Conquer (CNC), that integrates control over three-dimensional } \\\\ &\\text{object placement with disentangled representations of global stylistic } \\\\ &\\text{semantics from multiple exemplar images. It utilizes a new training paradigm, } \\\\ &\\text{depth disentanglement training, to leverage the relative depth of objects as } \\\\ &\\text{an estimator, and soft guidance to impose global semantics onto targeted } \\\\ &\\text{regions. } \\\\ &\\text{Strengths: } \\\\ &\\text{1. } \\\\ &\\text{2. } \\\\ &\\text{3. } \\\\ &\\text{Weaknesses: } \\\\ &\\text{1. } \\\\ &\\text{2. } \\\\ &\\text{3. } \\\\ &\\text{Suggestions for Improvement: } \\\\ &\\text{1. } \\\\ &\\text{2. } \\\\ &\\text{3. } \\\\ &\\text{Recommendation: } \\\\ &\\text{Based on the paper, I } \\\\ &\\text{Recommend: } \\\\ &\\text{1. } \\\\ &\\text{2. } \\\\ &\\text{3. } \\\\ &\\end{aligned}\n\n$$$ \n\nThe paper presents a novel text-conditional diffusion model, COMPOSE AND CONQUER (CNC), that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. It utilizes a new training paradigm, depth disentanglement training, to leverage the relative depth of objects as an estimator, and soft guidance to impose global semantics onto targeted regions.\n\nThe model's local fuser captures the relative z-axis placements of objects through depth maps, while the global fuser employs a method termed soft guidance, which aids the model in localizing global conditions without any explicit structural signals. The integrated framework, COMPOSE AND CONQUER (CNC), unifies these techniques to localize multiple conditions in a disentangled manner.\n\nThe model's depth disentanglement training (DDT) allows the model to infer the absolute depth placement of objects, while the soft guidance method enables the model to impose global semantics onto localized regions. The model's ability to condition multiple signals or objects and to localize global semantics onto localized areas makes it a versatile framework for composing localized objects with different global semantics.\n\nHowever, the paper could be improved in the following ways: \n\n1. The paper could provide more details on how the depth disentanglement training works and how it is used to improve the model's performance.\n2. The paper could provide more experiments to evaluate the model's performance on different datasets and scenarios.\n3. The paper could provide more discussions on the limitations of the model and how to improve them.\n\nBased on the paper, I recommend the following: \n\n1. The model's ability to condition multiple signals or objects makes it a promising approach for applications such as image synthesis, image manipulation, and image-based rendering.\n2. The model's ability to localize global semantics onto localized regions makes it a promising approach for applications such as image captioning, image description, and image search.\n3. The model's ability to handle three-dimensional object placement makes it a promising approach for applications such as 3D reconstruction, 3D scene understanding, and 3D image synthesis.\n\nHowever, the paper could be improved by providing more details on how the model's components work together and how they improve the model's performance. Additionally, providing more experiments to evaluate the model's performance on different datasets and scenarios could provide more insight into the model's limitations and potential improvements. \n\n Overall, the paper presents a novel text-conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. The model's ability to condition multiple signals or objects and to localize global semantics onto localized areas makes it a versatile framework for composing localized objects with different global semantics. However, providing more details on how the model's components work together and how they improve the model's performance, as well as providing more experiments to evaluate the model's performance on different datasets and scenarios, could provide more insight into the model's limitations and potential improvements.",
        "gr_score_with": 0.5372750642673522,
        "gr_score_without": 0.4186746987951807,
        "or_score_with": 0.4190231362467866,
        "or_score_without": 0.5,
        "output_without": {
            "num_tokens_scored": 332,
            "num_green_tokens": 139,
            "green_fraction": 0.4186746987951807,
            "num_orange_tokens": 166,
            "orange_fraction": 0.5,
            "green_z_score": 0.694571608169741,
            "orange_z_score": 111.80493685716944,
            "green_p_value": 0.2436619051216511,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 389,
            "num_green_tokens": 209,
            "green_fraction": 0.5372750642673522,
            "num_orange_tokens": 163,
            "orange_fraction": 0.4190231362467866,
            "green_z_score": 5.526635667337393,
            "orange_z_score": 101.16402351252484,
            "green_p_value": 1.6321492748814367e-08,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "BENO: BOUNDARY-EMBEDDED NEURAL OPERATORS FOR ELLIPTIC PDES",
        "abstract": "Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green\u2019s function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%. Our source code can be found https://github.com/AI4Science-WestlakeU/beno.git.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture to solve elliptic partial differential equations (PDEs) with complex boundary conditions. BENO incorporates physics intuition through a boundary-embedded architecture, consisting of Graph Neural Networks (GNNs) and a Transformer, to model the influence of boundary conditions on the solution. The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems.\n\n$$\\text{Strengths:}$$\n1. The paper presents a novel approach to solving elliptic PDEs with complex boundary conditions.\n2. The proposed architecture, BENO, incorporates physics intuition through a boundary-embedded design.\n3. The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods.\n4. The paper provides a comprehensive experimental evaluation, including various boundary shapes, values, and resolutions.\n\n$$\\text{Weaknesses:}$$\n1. The paper does not provide a clear motivation for the use of GNNs and a Transformer in BENO.\n2. The authors assume that the reader is familiar with the basics of GNNs and Transformers.\n3. The paper does not provide a detailed analysis of the performance of BENO on different datasets.\n4. The authors rely on a single dataset to evaluate the performance of BENO.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a clear motivation for the use of GNNs and a Transformer in BENO.\n2. Include a detailed analysis of the performance of BENO on different datasets.\n3. Evaluate the robustness of BENO to different boundary shapes, values, and resolutions.\n4. Provide a comparison with other state-of-the-art methods for solving elliptic PDEs.\n\n$$\\text{Recommendation:}$$\nBased on the paper's contributions and the potential of BENO, I recommend that it be accepted for publication in a top-tier conference or journal in the field of machine learning and partial differential equations. However, I suggest that the authors address the weaknesses and provide a more comprehensive evaluation of BENO's performance.",
        "peer_review_with_watermark": "$$$\\label{summary}$\nBENO: BOUNDARY-EMBEDDED NEURAL OPERATORS FOR ELLIPTIC PDES - Paper Summary:\nThe paper introduces Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture for solving elliptic partial differential equations (PDEs) with complex boundary conditions. BENO addresses the challenges of inhomogeneous boundary values and complex boundary geometry by incorporating a boundary-embedded architecture, consisting of graph neural networks (GNNs) and a transformer. The approach is demonstrated to outperform existing state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems.\n\nStrengths:\n- BENO addresses the challenges of inhomogeneous boundary values and complex boundary geometry.\n- The approach incorporates physics intuition through a boundary-embedded architecture.\n- The use of transformer for boundary embedding provides strong generalization capabilities.\n- BENO exhibits strong generalization capabilities across different scenarios.\n\nWeaknesses:\n- The paper does not provide explicit details on the dataset used for training and testing.\n- The implementation details for the baselines are not provided.\n- The paper does not discuss the limitations of BENO in terms of scalability and computational complexity.\n\nSuggestions for Improvement:\n- Provide explicit details on the dataset used for training and testing.\n- Include implementation details for the baselines.\n- Discuss the limitations of BENO in terms of scalability and computational complexity.\n- Consider providing more details on the transformer architecture used for boundary embedding.\n\nRecommendation:\n- BENO is a novel approach for solving elliptic PDEs with complex boundary conditions.\n- The approach addresses the challenges of inhomogeneous boundary values and complex boundary geometry.\n- BENO exhibits strong generalization capabilities across different scenarios.\n- However, further improvements are needed in terms of scalability and computational complexity.\n\n$$$",
        "gr_score_with": 0.45228215767634855,
        "gr_score_without": 0.3298611111111111,
        "or_score_with": 0.46887966804979253,
        "or_score_without": 0.5347222222222222,
        "output_without": {
            "num_tokens_scored": 288,
            "num_green_tokens": 95,
            "green_fraction": 0.3298611111111111,
            "num_orange_tokens": 154,
            "orange_fraction": 0.5347222222222222,
            "green_z_score": -2.429682382839675,
            "orange_z_score": 109.15336606004288,
            "green_p_value": 0.992443970246632,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 241,
            "num_green_tokens": 109,
            "green_fraction": 0.45228215767634855,
            "num_orange_tokens": 113,
            "orange_fraction": 0.46887966804979253,
            "green_z_score": 1.6567478015703827,
            "orange_z_score": 87.3973537338223,
            "green_p_value": 0.04878523712988417,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TRAIN NEURAL PROGRAM SYNTHESIZERS",
        "abstract": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one\u2019s intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample. We then use the informative dataset to train models to improve the synthesizer\u2019s ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.",
        "peer_review_without_watermark": "$$\n\nPaper Summary:\nThe paper proposes a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics. The approach bootstraps pragmatic synthesizers by iteratively generating data using speaker and listener models, filtering the generated data with an example selection method, and further training the speaker and listener models on the generated data. The paper evaluates the approach on the challenging task of synthesizing regular expressions and shows that it outperforms models trained without choosing pragmatic examples.\n\nStrengths:\n1. The approach is novel and innovative in modeling pragmatics to resolve ambiguity in program specification examples.\n2. The paper provides a clear and concise explanation of the approach and its components.\n3. The evaluation on the challenging task of synthesizing regular expressions is thorough and well-conducted.\n4. The paper highlights the potential of the approach to improve the performance of program synthesis models with minimal supervision.\n\nWeaknesses:\n1. The approach relies heavily on the quality of the initial speaker and listener models, which may not be robust enough to generate informative examples.\n2. The paper does not provide a detailed analysis of the example selection method used to filter the generated data.\n3. The evaluation is limited to a single task and domain, and it is unclear how well the approach will generalize to other tasks and domains.\n4. The paper does not discuss potential limitations and challenges of the approach, such as the potential for overfitting or the need for large amounts of data to train the models.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the initial speaker and listener models and how they are trained to ensure robustness.\n2. Investigate the effectiveness of different example selection methods and provide a comparison of their performance.\n3. Evaluate the approach on multiple tasks and domains to assess its generalizability.\n4. Discuss potential limitations and challenges of the approach and provide strategies for addressing them.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics. The approach shows promising results on the challenging task of synthesizing regular expressions, and it has the potential to improve the performance of program synthesis models with minimal supervision. However, further work is needed to address the limitations and challenges of the approach, such as the potential for overfitting or the need for large amounts of data to train the models. I recommend that the authors continue to investigate and refine the approach to make it more robust and generalizable.\n\n$$",
        "peer_review_with_watermark": "$$\n\\begin{array}{lr}\n\\textbf{Paper Summary:} \\\\\n\\hline\nThe authors propose a novel approach to training neural program synthesizers to resolve ambiguity in program specification examples by modeling pragmatics. The approach bootstraps pragmatic synthesizers iteratively generating data using speaker and listener models, filtering the generated data with an example selection method, and further training the speaker and listener models on the generated data.\n\nThe authors validate their method on the challenging task of synthesizing regular expressions from example strings. The results show that their method outperforms models trained without choosing pragmatic examples by 23% and matches the performance of supervised learning on a dataset of pragmatic examples provided by humans.\n\nThe authors also evaluate their method on a user study with 11 human participants, where they compare the performance of their method with a literal model and a human fine-tuned model. The results show that their method performs on par with the human fine-tuned model.\n\n\\hline\n\\textbf{Strengths:} \\\\\n\\hline\nThe authors propose a novel approach to training neural program synthesizers to resolve ambiguity in program specification examples by modeling pragmatics. The approach bootstraps pragmatic synthesizers iteratively generating data using speaker and listener models, filtering the generated data with an example selection method, and further training the speaker and listener models on the generated data.\n\nThe authors validate their method on a challenging task and a user study with human participants. The results show that their method outperforms models trained without choosing pragmatic examples and matches the performance of supervised learning on a dataset of pragmatic examples provided by humans.\n\nThe authors provide a detailed explanation of their approach and results. The paper is well-organized and easy to follow.\n\n\\hline\n\\textbf{Weaknesses:} \\\\\n\\hline\nOne of the limitations of the authors' approach is that it requires a large number of training examples. The authors mention that collecting a large number of training examples can be challenging, but they do not provide a detailed explanation of how they overcame this challenge.\n\nThe authors also mention that their approach is not suitable for domains with an even larger space of possible programs and input/output specifications. This limitation needs to be considered when evaluating the applicability of the authors' approach.\n\nThe authors do not provide a comparison with other state-of-the-art approaches. This limitation needs to be considered when evaluating the authors' approach.\n\n\\hline\n\\textbf{Suggestions for Improvement:} \\\\\n\\hline\nThe authors should provide a more detailed explanation of how they overcame the challenge of collecting a large number of training examples.\n\nThe authors should consider the limitations of their approach when evaluating its applicability to domains with an even larger space of possible programs and input/output specifications.\n\nThe authors should compare their approach with other state-of-the-art approaches to evaluate its performance.\n\nThe authors should provide a more detailed explanation of how they select the best model from a set of models.\n\n\\hline\n\\textbf{Recommendation:} \\\\\n\\hline\nThe authors have proposed a novel approach to training neural program synthesizers to resolve ambiguity in program specification examples by modeling pragmatics. The approach bootstraps pragmatic synthesizers iteratively generating data using speaker and listener models, filtering the generated data with an example selection method, and further training the speaker and listener models on the generated data.\n\nThe authors have validated their method on a challenging task and a user study with human participants. The results show that their method outperforms models trained without choosing pragmatic examples and matches the performance of supervised learning on a dataset of pragmatic examples provided by humans.\n\nThe authors have provided a detailed explanation of their approach and results. The paper is well-organized and easy to follow.\n\nBased on the authors' work, I would recommend their approach for training neural program synthesizers to resolve ambiguity in program specification examples. However, the authors should consider the limitations of their approach when evaluating its applicability to domains with an even larger space of possible programs and input/output specifications.\n\n$$\n$$",
        "gr_score_with": 0.5058823529411764,
        "gr_score_without": 0.345679012345679,
        "or_score_with": 0.4323529411764706,
        "or_score_without": 0.5246913580246914,
        "output_without": {
            "num_tokens_scored": 324,
            "num_green_tokens": 112,
            "green_fraction": 0.345679012345679,
            "num_orange_tokens": 170,
            "orange_fraction": 0.5246913580246914,
            "green_z_score": -1.995880531156663,
            "orange_z_score": 122.07418167094941,
            "green_p_value": 0.9770265358309954,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 172,
            "green_fraction": 0.5058823529411764,
            "num_orange_tokens": 147,
            "orange_fraction": 0.4323529411764706,
            "green_z_score": 3.985266984930429,
            "orange_z_score": 102.79490114853856,
            "green_p_value": 3.370214716042805e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CIRCUMVENTING CONCEPT ERASURE METHODS FOR TEXT-TO-IMAGE GENERATIVE MODELS",
        "abstract": "Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. Yet, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to \u201cerase\u201d sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we devise an algorithm to learn special input word embeddings that can retrieve \u201cerased\u201d concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper presents a critical evaluation of seven concept erasure methods for text-to-image generative models, specifically Stable Diffusion. The authors show that these methods are not effective in removing targeted concepts from the models, and that they can be circumvented using a novel \"attack\" technique called Concept Inversion (CI). The authors demonstrate that the learned word embeddings from CI can be used to generate images of the erased concept, indicating that the models are not fully erasing the concepts.\n\nStrengths:\n1. The paper provides a thorough evaluation of seven concept erasure methods, covering both fine-tuning-based and inference-guiding-based methods.\n2. The authors propose a novel \"attack\" technique, Concept Inversion (CI), which can be used to circumvent the concept erasure methods.\n3. The paper highlights the limitations of post-hoc erasure methods and calls for a more fundamental understanding of the \"input filtering\" phenomenon.\n\nWeaknesses:\n1. The paper's scope is limited to Stable Diffusion models, and it is unclear how the results would generalize to other models.\n2. The authors do not provide a comprehensive evaluation of the concept erasure methods' effectiveness in removing sensitive concepts.\n3. The paper's conclusion is somewhat speculative, and it is unclear what the implications of the findings are for the broader field of text-to-image generative models.\n\nSuggestions for Improvement:\n1. The authors should conduct a more comprehensive evaluation of the concept erasure methods, including a more detailed analysis of their effectiveness in removing sensitive concepts.\n2. The authors should provide a more thorough explanation of the \"input filtering\" phenomenon and its implications for the design of better concept erasure methods.\n3. The authors should consider evaluating the concept erasure methods on a wider range of models, including those from other architectures.\n\nRecommendation:\nBased on the paper's findings, I recommend that researchers and practitioners in the field of text-to-image generative models be cautious when relying on post-hoc erasure methods to ensure the safety and robustness of their models. Instead, they should consider developing more fundamental methods that address the underlying issues with concept erasure. The authors' proposed Concept Inversion (CI) technique is a promising approach that could be further developed and refined to address these issues.\n\n$$",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThis paper examines the effectiveness of seven concept erasure methods for text-to-image generative models, specifically Stable Diffusion. The authors show that these methods are not effective in fully erasing targeted concepts, as they can be circumvented using a novel concept inversion (CI) technique. The CI approach learns special word embeddings that can retrieve erased concepts from the sanitized models with no alterations to their weights. The results highlight the brittleness of post-hoc concept erasure methods and call into question their use in AI safety.\n\nStrengths:\n\n* The paper provides a comprehensive review of seven concept erasure methods for text-to-image generative models.\n* The authors propose a novel concept inversion (CI) technique that can circumvent the erasure methods.\n* The results show that the CI approach is effective in recovering erased concepts from the sanitized models.\n* The paper highlights the importance of evaluating the effectiveness of concept erasure methods and the need for more robust methods.\n\nWeaknesses:\n\n* The paper may be too long and may benefit from a more detailed review of the concept erasure methods.\n* The authors may have been too generous in their assessment of the effectiveness of the concept erasure methods.\n* The paper may not provide enough guidance on how to improve the effectiveness and robustness of concept erasure methods.\n* The authors may have not fully explored the implications of the findings on AI safety.\n\nSuggestions for Improvement:\n\n* The authors should provide more detailed explanations of the concept erasure methods and their limitations.\n* The authors should explore the implications of the findings on AI safety and provide more guidance on how to improve the effectiveness and robustness of concept erasure methods.\n* The authors should provide more results and case studies to support the claims made in the paper.\n* The authors should provide more guidance on how to apply the concept inversion (CI) technique in practice.\n\nRecommendation:\n\n* The paper provides a good contribution to the field of text-to-image generative models and concept erasure methods.\n* The authors propose a novel concept inversion (CI) technique that can circumvent the erasure methods.\n* The results highlight the brittleness of post-hoc concept erasure methods and call into question their use in AI safety.\n* However, the paper may benefit from more detailed explanations of the concept erasure methods and their limitations, as well as more guidance on how to improve the effectiveness and robustness of concept erasure methods.\n* The authors should provide more results and case studies to support the claims made in the paper.\n* The authors should provide more guidance on how to apply the concept inversion (CI) technique in practice.\n* Overall, the paper is well-written and provides a good contribution to the field of text-to-image generative models and concept erasure methods.",
        "gr_score_with": 0.5232558139534884,
        "gr_score_without": 0.37383177570093457,
        "or_score_with": 0.43410852713178294,
        "or_score_without": 0.4984423676012461,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 120,
            "green_fraction": 0.37383177570093457,
            "num_orange_tokens": 160,
            "orange_fraction": 0.4984423676012461,
            "green_z_score": -0.9570202978345291,
            "orange_z_score": 104.5314962470601,
            "green_p_value": 0.8307214949909503,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 258,
            "num_green_tokens": 135,
            "green_fraction": 0.5232558139534884,
            "num_orange_tokens": 112,
            "orange_fraction": 0.43410852713178294,
            "green_z_score": 4.041212113814023,
            "orange_z_score": 81.44252842180151,
            "green_p_value": 2.658781632582655e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "NONNEGATIVE LOW-RANK SEMIDEFINITE PROGRAM- MING",
        "abstract": "K-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the K-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed K-means formulation using a nonconvex Burer\u2013Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-theart NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments, we observe that our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art.",
        "peer_review_without_watermark": "$$\\boxed{\\text{The paper presents a novel algorithm for the K-means clustering problem. The algorithm uses a nonnegative low-rank SDP formulation, which achieves the statistical optimality of the K-means problem. The algorithm is simple, scalable, and enjoys strong statistical optimality guarantees. The authors provide a thorough theoretical analysis of the algorithm's convergence rate and empirical results to demonstrate its effectiveness.}}$$",
        "peer_review_with_watermark": "$$$Summary:\n\nThe paper proposes a novel algorithm, BM, for the K-means clustering problem. BM is based on a nonnegative low-rank semidefinite program (SDP) relaxation of the K-means problem. The algorithm uses a projected gradient descent (PGD) approach to solve the SDP relaxation. The paper shows that BM has a local linear convergence rate, which implies that it can achieve high accuracy for small separations. The algorithm has been tested on various datasets, and the results show that BM outperforms other methods in terms of accuracy and computational time. The paper also discusses the theoretical analysis of the algorithm, which shows that BM has a local linear convergence rate under certain assumptions. The results of the paper suggest that BM is a promising algorithm for the K-means clustering problem.\n\n$Strengths:\n\n1. The paper proposes a novel algorithm, BM, for the K-means clustering problem.\n2. The algorithm uses a projected gradient descent (PGD) approach to solve the SDP relaxation, which is a well-known method for solving SDP relaxations.\n3. The paper provides a theoretical analysis of the algorithm, which shows that BM has a local linear convergence rate under certain assumptions.\n4. The algorithm has been tested on various datasets, and the results show that BM outperforms other methods in terms of accuracy and computational time.\n\n$Weaknesses:\n\n1. The algorithm relies on a semidefinite relaxation, which can be computationally expensive to solve.\n2. The algorithm requires a good initialization, as the local linear convergence rate depends on the initialization of the algorithm.\n3. The paper does not discuss the impact of noise in the data on the algorithm's performance.\n4. The algorithm's performance for high-dimensional data is not well analyzed in the paper.\n\n$Suggestions for Improvement:\n\n1. The algorithm could be improved by using a more efficient SDP relaxation, such as the one based on the Burer-Monteiro factorization.\n2. The algorithm could be improved by using a more robust initialization method, such as the one based on the K-means++ algorithm.\n3. The algorithm's performance for high-dimensional data could be improved by using techniques such as dimensionality reduction or noise robustness analysis.\n4. The paper could be improved by discussing the impact of noise in the data on the algorithm's performance and providing results for high-dimensional data.\n\n$Recommendation:\n\nBased on the analysis, I recommend that the algorithm BM be further improved by addressing the weaknesses mentioned above. Additionally, I recommend that the paper provide more results for high-dimensional data to demonstrate the algorithm's robustness.",
        "gr_score_with": 0.5254237288135594,
        "gr_score_without": 0.4246575342465753,
        "or_score_with": 0.4101694915254237,
        "or_score_without": 0.4657534246575342,
        "output_without": {
            "num_tokens_scored": 73,
            "num_green_tokens": 31,
            "green_fraction": 0.4246575342465753,
            "num_orange_tokens": 34,
            "orange_fraction": 0.4657534246575342,
            "green_z_score": 0.43003663431074624,
            "orange_z_score": 38.362676752595924,
            "green_p_value": 0.33358449630334935,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 295,
            "num_green_tokens": 155,
            "green_fraction": 0.5254237288135594,
            "num_orange_tokens": 121,
            "orange_fraction": 0.4101694915254237,
            "green_z_score": 4.397289869009137,
            "orange_z_score": 67.70494858114519,
            "green_p_value": 5.480545830405179e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SLICEGPT: COMPRESS LARGE LANGUAGE MODELS",
        "abstract": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for OPT 66B and LLAMA-2 70B models with modest loss in accuracy (superior to 2:4 sparsity). Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA-2 70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces SliceGPT, a new post-training sparsification scheme for large language models. SliceGPT replaces each weight matrix with a smaller dense matrix, reducing the embedding dimension of the network. The authors demonstrate that SliceGPT can remove up to 25% of the model parameters for OPT 66B and LLAMA-2 70B models with modest loss in accuracy. The authors also show that SliceGPT can reduce the computational requirements of the models, requiring fewer GPUs and running faster without any additional code optimization.\n\nStrengths:\n1. The authors introduce a new idea of computational invariance in transformer networks, which enables SliceGPT.\n2. The authors demonstrate that SliceGPT can remove a significant amount of model parameters without sacrificing too much accuracy.\n3. The authors show that SliceGPT can reduce the computational requirements of the models, making them more energy-efficient.\n\nWeaknesses:\n1. The authors do not provide a thorough analysis of the impact of SliceGPT on the model's accuracy.\n2. The authors do not provide a detailed explanation of how SliceGPT is different from other sparsification techniques.\n3. The authors do not provide a thorough evaluation of SliceGPT on a wide range of tasks and datasets.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the impact of SliceGPT on the model's accuracy.\n2. The authors should provide a more detailed explanation of how SliceGPT is different from other sparsification techniques.\n3. The authors should provide a thorough evaluation of SliceGPT on a wide range of tasks and datasets.\n\nRecommendation:\nBased on the paper summary, strengths, weaknesses, and suggestions for improvement, I recommend that the authors revise and resubmit the paper. The authors should provide a more detailed analysis of the impact of SliceGPT on the model's accuracy, provide a more detailed explanation of how SliceGPT is different from other sparsification techniques, and provide a thorough evaluation of SliceGPT on a wide range of tasks and datasets. Additionally, the authors should consider addressing the limitations of the current approach and exploring potential avenues for future research.",
        "peer_review_with_watermark": "$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$\n$$",
        "gr_score_with": 1.0,
        "gr_score_without": 0.43621399176954734,
        "or_score_with": 0.0,
        "or_score_without": 0.46502057613168724,
        "output_without": {
            "num_tokens_scored": 243,
            "num_green_tokens": 106,
            "green_fraction": 0.43621399176954734,
            "num_orange_tokens": 113,
            "orange_fraction": 0.46502057613168724,
            "green_z_score": 1.1523221619336326,
            "orange_z_score": 86.0204402706276,
            "green_p_value": 0.12459435667052648,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 2,
            "num_green_tokens": 2,
            "green_fraction": 1.0,
            "num_orange_tokens": 0,
            "orange_fraction": 0.0,
            "green_z_score": 1.7320508075688772,
            "orange_z_score": -0.11821790160987877,
            "green_p_value": 0.0416322583317752,
            "orange_p_value": 0.5470524970070174
        }
    },
    {
        "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
        "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
        "peer_review_without_watermark": "$$$${Paper Summary:}\nThe paper proposes a novel federated learning algorithm called FedCOG, which tackles the issue of data heterogeneity in FL from the perspective of modifying the local dataset. FedCOG consists of two key components: complementary data generation and knowledge distillation-based model training. The algorithm generates data to complement the original dataset, which helps to mitigate the effects of data heterogeneity, and distills knowledge from the global model to the local model based on the generated data. The paper presents extensive experiments on classical and real-world FL datasets, which show that FedCOG consistently outperforms state-of-the-art methods.\n\n$${Strengths:}\n1. Novel approach: The paper proposes a novel approach to tackle data heterogeneity in FL, which is a critical issue in FL.\n2. Plug-and-play property: FedCOG has a plug-and-play property, which means it can be easily combined with most existing FL methods.\n3. Compatibility with standard FL protocol: FedCOG is compatible with standard FL protocols, such as Secure Aggregation.\n4. Improved performance: The paper presents extensive experiments that show FedCOG consistently outperforms state-of-the-art methods.\n\n$${Weaknesses:}\n1. Complexity: The paper proposes a complex algorithm, which may be difficult to implement and understand.\n2. Limited experiments: While the paper presents extensive experiments, it may not be sufficient to cover all possible scenarios and datasets.\n3. Lack of theoretical analysis: The paper does not provide a theoretical analysis of the algorithm's performance, which may be a limitation.\n\n$${Suggestions for Improvement:}\n1. Simplify the algorithm: The algorithm may be simplified to make it easier to implement and understand.\n2. Conduct more experiments: The paper should conduct more experiments to cover a wider range of scenarios and datasets.\n3. Provide theoretical analysis: The paper should provide a theoretical analysis of the algorithm's performance to better understand its strengths and weaknesses.\n\n$${Recommendation:}\nBased on the paper's strengths and weaknesses, I recommend that the authors revise the paper to address the limitations and provide more detailed explanations of the algorithm's performance. Additionally, the authors should consider simplifying the algorithm and conducting more experiments to demonstrate its effectiveness. Overall, the paper proposes a novel approach to tackle data heterogeneity in FL, and with revisions, it has the potential to make a significant contribution to the field.",
        "peer_review_with_watermark": "$$$\\begin{align*}\n\\textbf{Paper Summary:}\n\\text{The paper proposes Federated Learning with Consensus-Oriented Generation (FedCOG), a novel FL algorithm that mitigates data heterogeneity by generating data to complement the original dataset and distilling knowledge from the global model to local models. FedCOG is plug-and-play, compatible with standard FL protocols, and makes no compromise on communication cost and privacy. The proposed method outperforms state-of-the-art methods in classical and real-world FL datasets.\n\n\\textbf{Strengths:}\n1. Novel approach to addressing data heterogeneity by generating data to complement the original dataset.\n2. Effective combination of knowledge distillation-based model training and complementary data generation.\n3. Compatibility with standard FL protocols, including Secure Aggregation.\n\n\\textbf{Weaknesses:}\n1. Limited explanation of the disagreement loss and its impact on performance.\n2. Lack of discussion on the role of hyperparameters, such as \u03bbdis and \u03bbkd, in FedCOG.\n\n\\textbf{Suggestions for Improvement:}\n1. Provide more detailed explanations of the disagreement loss and its role in FedCOG.\n2. Discuss the impact of hyperparameters, such as \u03bbdis and \u03bbkd, on FedCOG performance.\n3. Consider adding regularization terms during generation to improve FedCOG performance.\n\n\\textbf{Recommendation:}\nBased on the proposed method and experimental results, FedCOG is a promising approach to addressing data heterogeneity in FL. However, further investigation is needed to understand the role of disagreement loss and hyperparameters in FedCOG performance. The authors should consider addressing these limitations to improve the robustness and generalizability of FedCOG.\n\n\\end{align*}$$$",
        "gr_score_with": 0.5465116279069767,
        "gr_score_without": 0.44970414201183434,
        "or_score_with": 0.3798449612403101,
        "or_score_without": 0.4349112426035503,
        "output_without": {
            "num_tokens_scored": 338,
            "num_green_tokens": 152,
            "green_fraction": 0.44970414201183434,
            "num_orange_tokens": 147,
            "orange_fraction": 0.4349112426035503,
            "green_z_score": 1.8652854850741736,
            "orange_z_score": 101.14020632896958,
            "green_p_value": 0.031070699299653472,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 258,
            "num_green_tokens": 141,
            "green_fraction": 0.5465116279069767,
            "num_orange_tokens": 98,
            "orange_fraction": 0.3798449612403101,
            "green_z_score": 4.8037049654770465,
            "orange_z_score": 77.01629369234178,
            "green_p_value": 7.78781440561526e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LMUFORMER: LOW COMPLEXITY YET POWERFUL SPIKING MODEL WITH LEGENDRE MEMORY UNITS",
        "abstract": "Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and state-of-the-art (SOTA) performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a fully-sequential recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LMUFormer, which augments the LMU with convolutional patch embedding and convolutional channel mixer. Moreover, we present a spiking version of this architecture, which introduces the benefit of states within the patch embedding and channel mixer modules while simultaneously reducing the computing complexity. We evaluated our architectures on multiple sequence datasets. Of particular note is our performance on the Speech Commands V2 dataset (35 classes). In comparison to SOTA transformer-based models within the ANN domain, our LMUFormer demonstrates comparable performance while necessitating a remarkable 53\u00d7 reduction in parameters and a substantial 65\u00d7 decrement in FLOPs. Furthermore, when benchmarked against extant low-complexity SNN variants, our model establishes a new SOTA with an accuracy of 96.12%. Additionally, owing to our model\u2019s proficiency in real-time data processing, we are able to achieve a 32.03% reduction in sequence length, all while incurring an inconsequential decline in performance. Our code is publicly available here.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes a new neural network architecture, LMUFormer, which combines the benefits of Legendre Memory Units (LMU) and convolutional patch embedding. The proposed architecture is designed to process data sequentially and has the potential to achieve state-of-the-art performance while reducing complexity and computational requirements. The authors also present a spiking version of the LMUFormer model, which achieves even better performance and efficiency.\n\n$$\\text{Strengths:}$$\n1. The proposed architecture is novel and innovative, combining the benefits of LMU and convolutional patch embedding.\n2. The authors provide extensive experimental results, demonstrating the effectiveness of the proposed architecture on various sequence learning tasks.\n3. The spiking version of the LMUFormer model achieves better performance and efficiency compared to the non-spiking version.\n4. The authors provide a clear and concise explanation of the proposed architecture and its components.\n\n$$\\text{Weaknesses:}$$\n1. The paper assumes a certain level of prior knowledge in the field of neural networks and sequence learning, which may make it difficult for non-experts to follow.\n2. The experimental results, while extensive, may not be sufficient to fully demonstrate the superiority of the proposed architecture.\n3. The paper does not provide a thorough analysis of the computational complexity of the proposed architecture, which may be a concern for resource-constrained environments.\n4. The authors may have overlooked potential limitations and challenges associated with the proposed architecture.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a more detailed analysis of the computational complexity of the proposed architecture and discuss potential optimizations.\n2. Include a more comprehensive comparison with existing architectures, including SOTA models in the same domain.\n3. Consider providing a more detailed explanation of the mathematical foundations of the proposed architecture, particularly in relation to the Legendre polynomials used in the LMU.\n4. Provide more insight into the potential applications and limitations of the proposed architecture in real-world scenarios.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit their manuscript for further review. The proposed architecture shows promise, and the authors have provided extensive experimental results to support its effectiveness. However, addressing the weaknesses and suggestions for improvement will help strengthen the manuscript and increase its impact in the field of neural networks and sequence learning.",
        "peer_review_with_watermark": "$$\n\n$$\n\n**Paper Summary:**\nThe paper introduces LMUFormer, a low-complexity yet powerful spiking model for sequence learning tasks. It proposes a novel architecture that combines Legendre Memory Units (LMU) with convolutional patch embedding and convolutional channel mixers. The model can process data sequentially and has been evaluated on multiple sequence datasets, achieving competitive performance with state-of-the-art (SOTA) transformer-based models. The paper also presents a spiking version of the architecture, called Spiking LMUFormer, which achieves SOTA performance on the Speech Commands V2 dataset.\n\n**Strengths:**\n\n1. The paper presents a novel architecture that addresses the limitations of traditional transformer models, such as high complexity and lack of sequential processing capability.\n2. The use of Legendre Memory Units (LMU) and convolutional patch embedding and channel mixers allows for efficient processing of sequential data.\n3. The paper presents extensive experimental results, including comparisons with SOTA transformer-based models and low-complexity SNN variants.\n4. The Spiking LMUFormer architecture achieves SOTA performance on the Speech Commands V2 dataset, demonstrating the potential of SNN models for sequence learning tasks.\n\n**Weaknesses:**\n\n1. The paper does not provide a clear explanation of the Legendre Memory Units (LMU) and their properties, which may limit its understanding by non-experts.\n2. The use of convolutional patch embedding and channel mixers may not be effective for all sequence learning tasks, and more experiments are needed to evaluate its robustness.\n3. The paper does not discuss the limitations of the Spiking LMUFormer architecture, such as its sensitivity to parameter tuning and its potential for overfitting.\n4. The experimental results, while extensive, are limited to specific datasets and may not generalize well to other sequence learning tasks.\n\n**Suggestions for Improvement:**\n\n1. Provide a more detailed explanation of the Legendre Memory Units (LMU) and their properties, including their mathematical formulation and behavior.\n2. Include more experiments to evaluate the robustness of the convolutional patch embedding and channel mixers, including comparisons with other architectures.\n3. Discuss the limitations of the Spiking LMUFormer architecture, including its sensitivity to parameter tuning and potential for overfitting.\n4. Include more experiments to evaluate the generalization of the LMUFormer and Spiking LMUFormer architectures to other sequence learning tasks and datasets.\n\n**Recommendation:**\n\nBased on the paper's contributions and experimental results, I recommend that it be accepted for publication. However, I also recommend that the authors address the limitations and suggestions outlined above to improve the paper's robustness and generality. Specifically, I recommend that they provide a more detailed explanation of the Legendre Memory Units (LMU) and include more experiments to evaluate the robustness of the convolutional patch embedding and channel mixers. Additionally, I recommend that they discuss the limitations of the Spiking LMUFormer architecture and include more experiments to evaluate its generalization to other sequence learning tasks and datasets.",
        "gr_score_with": 0.519774011299435,
        "gr_score_without": 0.4115942028985507,
        "or_score_with": 0.3813559322033898,
        "or_score_without": 0.4115942028985507,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 142,
            "green_fraction": 0.4115942028985507,
            "num_orange_tokens": 142,
            "orange_fraction": 0.4115942028985507,
            "green_z_score": 0.439586982263858,
            "orange_z_score": 91.93093289785774,
            "green_p_value": 0.3301181353768202,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 354,
            "num_green_tokens": 184,
            "green_fraction": 0.519774011299435,
            "num_orange_tokens": 135,
            "orange_fraction": 0.3813559322033898,
            "green_z_score": 4.60000818798911,
            "orange_z_score": 86.16713298847797,
            "green_p_value": 2.112371670880262e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MOTION GUIDANCE: DIFFUSION-BASED IMAGE EDIT-",
        "abstract": "Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:} \nThe paper proposes a zero-shot technique called motion guidance for image editing using diffusion models. The authors aim to enable users to specify dense, complex motion fields that indicate where each pixel in an image should move. They design a guidance loss that encourages the sample to have the desired motion while being visually similar to the source image. The proposed method works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network.\n\n$$\\textbf{Strengths:}\n1. The authors propose a novel approach to motion guidance that can handle dense and complex motion fields.\n2. The method is simple and does not require any training or text inputs.\n3. The authors demonstrate the effectiveness of the proposed method on a variety of complex motions and images.\n4. The paper provides a comprehensive discussion of the limitations of the proposed method and potential areas of future research.\n\n$$\\textbf{Weaknesses:}\n1. The authors mention that the method is susceptible to slow sampling speed, which may be a limitation for real-world applications.\n2. The paper inherits the limitations of diffusion models and guidance-based methods, such as the inability to edit objects with complex shapes or textures.\n3. The authors note that certain target flows may not be possible, which may limit the applicability of the proposed method.\n\n$$\\textbf{Suggestions for Improvement:}\n1. The authors could explore ways to improve the sampling speed of the proposed method.\n2. The authors could investigate the use of more advanced motion estimation models or techniques to handle complex shapes and textures.\n3. The authors could explore the use of more robust guidance losses or optimization techniques to handle occlusions and other challenges.\n\n$$\\textbf{Recommendation:}\nBased on the strengths and weaknesses of the proposed method, I recommend that the authors continue to develop and refine their approach. The authors have made significant contributions to the field of image editing and motion guidance, and their work has the potential to open up new areas of research in computer vision and machine learning. With further development and refinement, the proposed method has the potential to become a powerful tool for image editing and manipulation.",
        "peer_review_with_watermark": "$$$ \n\nPaper Title: MOTION GUIDANCE: DIFFUSION-BASED IMAGE EDIT-\n\nThe paper proposes a zero-shot technique for motion guidance, which allows users to specify dense, complex motion fields that indicate where each pixel in an image should move. This is achieved by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. The authors demonstrate that their technique works on complex motions and produces high-quality edits of real and generated images.\n\nThe paper has several strengths. Firstly, the authors provide a clear and concise introduction to the problem and propose a novel solution. They also provide a comprehensive literature review, highlighting the limitations of existing methods and the importance of motion guidance in image manipulation. The authors' use of a zero-shot approach is also a significant advantage, as it does not require any training or fine-tuning of the diffusion model.\n\nThe proposed motion guidance technique is also well-designed and easy to follow. The authors provide a clear explanation of how the guidance loss function is constructed and how it is used to guide the diffusion sampling process. They also provide several experiments to demonstrate the effectiveness of their approach.\n\nHowever, the paper also has some weaknesses. Firstly, the authors do not provide a comprehensive analysis of the limitations of their approach. They mention that their method inherits the deficiencies of diffusion models and guidance-based methods, such as slow sampling speed, but they do not provide a detailed analysis of these limitations. Additionally, the authors' use of a pre-composed decoder with the motion guidance function may not be the most effective approach, and they should provide more experiments to evaluate this assumption.\n\nAnother area for improvement is the evaluation of the proposed method. The authors provide several experiments to demonstrate the effectiveness of their approach, but they do not provide a comprehensive analysis of the results. They should provide more experiments to evaluate the proposed method under different scenarios and provide a more detailed analysis of the results.\n\nFinally, the paper could benefit from more discussion on the broader implications of the proposed approach. The authors mention that their work opens two broad areas of research, but they do not provide a more detailed discussion on the potential applications and future work.\n\nRecommendation:\n\nThe proposed motion guidance technique is a significant contribution to the field of image manipulation and motion guidance. However, the paper could benefit from more comprehensive analysis of the limitations and broader implications of the approach. The authors should provide more experiments to evaluate the proposed method under different scenarios and provide a more detailed analysis of the results. Additionally, they should provide more discussion on the potential applications and future work.\n\nRating: 8/10",
        "gr_score_with": 0.42329545454545453,
        "gr_score_without": 0.37941176470588234,
        "or_score_with": 0.5056818181818182,
        "or_score_without": 0.4852941176470588,
        "output_without": {
            "num_tokens_scored": 340,
            "num_green_tokens": 129,
            "green_fraction": 0.37941176470588234,
            "num_orange_tokens": 165,
            "orange_fraction": 0.4852941176470588,
            "green_z_score": -0.7749130248475834,
            "orange_z_score": 114.93703326211607,
            "green_p_value": 0.7808044724128691,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 352,
            "num_green_tokens": 149,
            "green_fraction": 0.42329545454545453,
            "num_orange_tokens": 178,
            "orange_fraction": 0.5056818181818182,
            "green_z_score": 0.8921479867729503,
            "orange_z_score": 121.92181538069498,
            "green_p_value": 0.186156808448601,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "STRUCTURAL ESTIMATION OF PARTIALLY OBSERVED LINEAR NON-GAUSSIAN ACYCLIC MODEL: A PRAC-",
        "abstract": "Conventional causal discovery approaches, which seek to uncover causal relationships among measured variables, are typically sensitive to the presence of latent variables. While various methods have been developed to address this confounding issue, they often rely on strong assumptions about the underlying causal structure. In this paper, we consider a general scenario where measured and latent variables collectively form a partially observed causally sufficient linear system and latent variables may be anywhere in the causal structure. We theoretically show that with the aid of high-order statistics, the causal graph is (almost) fully identifiable if, roughly speaking, each latent set has a sufficient number of pure children, which can be either latent or measured. Naturally, LiNGAM, a model without latent variables, is encompassed as a special case. Based on the identification theorem, we develop a principled algorithm to identify the causal graph by testing for statistical independence involving only measured variables in specific manners. Experimental results show that our method effectively recovers the causal structure, even when latent variables are influenced by measured variables.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper presents a novel approach to causal discovery in the presence of partially observed latent variables. The authors propose a partially observed linear non-Gaussian acyclic model (PO-LiNGAM) and develop an algorithm to identify the causal graph. The algorithm consists of three phases: Phase I, which identifies leaf nodes and their parents; Phase II, which discovers new atomic units; and Phase III, which refines the atomic units. The authors demonstrate the identifiability of the causal graph under the PO-LiNGAM model and show that the algorithm can recover the causal structure even when latent variables are influenced by measured variables.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The paper presents a novel approach to causal discovery in the presence of partially observed latent variables.\n2. The authors develop a feasible iterative algorithm to identify the causal graph.\n3. The paper demonstrates the identifiability of the causal graph under the PO-LiNGAM model.\n4. The algorithm can recover the causal structure even when latent variables are influenced by measured variables.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The paper assumes that each latent variable has a sufficient number of pure children, which may not always be the case in real-world scenarios.\n2. The algorithm requires a sufficient number of pure children for each atomic unit, which may limit its applicability to certain types of causal structures.\n3. The paper does not provide a comprehensive comparison with other existing methods for causal discovery.\n4. The experimental results are limited to synthetic data and real-world datasets, and the paper could benefit from more extensive evaluations.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. Develop a method to estimate the number of pure children required for each atomic unit, rather than relying on a fixed threshold.\n2. Compare the proposed algorithm with other existing methods for causal discovery, such as LiNGAM, BPC, and FastGIN.\n3. Evaluate the algorithm on more extensive datasets and consider using techniques such as bootstrapping or cross-validation to assess its robustness.\n4. Investigate the application of the algorithm to nonlinear causal models, which could provide a more general framework for causal discovery.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors should address the limitations of the algorithm and provide more extensive evaluations of its performance on real-world datasets. Additionally, they should consider developing a method to estimate the number of pure children required for each atomic unit and comparing the proposed algorithm with other existing methods for causal discovery.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper presents a theoretical framework for inferring causal relationships among measured variables involving latent variables. The authors consider a general scenario where measured and latent variables collectively form a partially observed causally sufficient linear system and latent variables may be anywhere in the causal structure. The authors propose a Partially Observed Linear Non-Gaussian Acyclic Model (PO-LiNGAM) and develop a three-phase algorithm to identify the causal graph. The algorithm leverages high-order statistics and the Generalized Independent Noise (GIN) condition to identify the causal structure. The authors demonstrate the identifiability of the causal graph under the PO-LiNGAM assumptions and show that the algorithm can effectively recover the causal structure even when latent variables are influenced by measured variables.\n\n$$\\text{Strengths:}$$\n1. The paper presents a comprehensive theoretical framework for inferring causal relationships among measured variables involving latent variables.\n2. The authors propose a Partially Observed Linear Non-Gaussian Acyclic Model (PO-LiNGAM) and develop a three-phase algorithm to identify the causal graph.\n3. The algorithm leverages high-order statistics and the Generalized Independent Noise (GIN) condition to identify the causal structure.\n4. The authors demonstrate the identifiability of the causal graph under the PO-LiNGAM assumptions.\n5. The paper provides a thorough analysis of the identifiability of the causal graph and presents a detailed algorithm for identification.\n\n$$\\text{Weaknesses:}$$\n1. The paper may be too technical for non-experts in the field of causal discovery.\n2. Some of the mathematical proofs may be too lengthy or complex for readers to understand.\n3. The paper does not provide a clear example of how to implement the algorithm in practice.\n4. The paper may not consider the limitations of the PO-LiNGAM assumptions.\n5. The paper does not discuss the potential risks or biases of using the proposed algorithm in real-world applications.\n\n$$\\text{Suggestions for Improvement:}$$\n1. The authors may want to consider providing more intuitive examples to help readers understand the mathematical proofs and the algorithm.\n2. The authors may want to discuss the potential risks or biases of using the proposed algorithm in real-world applications.\n3. The authors may want to consider providing more details on how to implement the algorithm in practice.\n4. The authors may want to discuss the limitations of the PO-LiNGAM assumptions and how they may affect the identifiability of the causal graph.\n5. The authors may want to consider providing more evidence on the effectiveness of the proposed algorithm in real-world datasets.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise the paper to address the weaknesses and improve the clarity and comprehensiveness of the paper. The authors may want to consider providing more intuitive examples, discussing the potential risks or biases of using the proposed algorithm, and providing more details on how to implement the algorithm in practice. The authors may also want to consider discussing the limitations of the PO-LiNGAM assumptions and providing more evidence on the effectiveness of the proposed algorithm in real-world datasets. With these revisions, the paper can be considered for publication in a conference or journal in the field of causal discovery.",
        "gr_score_with": 0.4575757575757576,
        "gr_score_without": 0.4051724137931034,
        "or_score_with": 0.4575757575757576,
        "or_score_without": 0.47413793103448276,
        "output_without": {
            "num_tokens_scored": 348,
            "num_green_tokens": 141,
            "green_fraction": 0.4051724137931034,
            "num_orange_tokens": 165,
            "orange_fraction": 0.47413793103448276,
            "green_z_score": 0.19695964928958196,
            "orange_z_score": 111.14916155969634,
            "green_p_value": 0.4219295572035208,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 330,
            "num_green_tokens": 151,
            "green_fraction": 0.4575757575757576,
            "num_orange_tokens": 151,
            "orange_fraction": 0.4575757575757576,
            "green_z_score": 2.1349662311335997,
            "orange_z_score": 104.4054820686121,
            "green_p_value": 0.01638188428928874,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DISTRIBUTIONAL MINIMAX PROBLEMS",
        "abstract": "In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose mean-field Langevin averaged gradient (MFL-AG), a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging, and establish average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Furthermore, we propose mean-field Langevin anchored best response (MFL-ABR), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. Finally, we study applications to zero-sum Markov games and conduct simulations demonstrating long-term optimality.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\n\nThe authors propose a new class of mean-field Langevin dynamics for distributional minimax optimization problems. They extend mean-field Langevin dynamics to minimax optimization over probability distributions and establish average-iterate convergence to the mixed Nash equilibrium. The authors also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result that accounts for the dependency of the particle interactions on all previous distributions.\n\n$$\\text{Strengths:}$$\n\n*   The authors provide a rigorous analysis of the mean-field Langevin dynamics for distributional minimax optimization problems.\n*   They establish average-iterate convergence to the mixed Nash equilibrium and study both time and particle discretization regimes.\n*   The authors also prove a new uniform-in-time propagation of chaos result that accounts for the dependency of the particle interactions on all previous distributions.\n\n$$\\text{Weaknesses:}$$\n\n*   The paper assumes that the functional derivatives of the objective function are uniformly bounded, which may not hold in all cases.\n*   The authors do not provide a detailed analysis of the convergence rate of the algorithm.\n*   The paper assumes that the weights $\\beta_t$ are chosen such that $B_t \\to \\infty$ and $\\beta_t/B_t \\to 0$ as $t \\to \\infty$, which may not hold in all cases.\n\n$$\\text{Suggestions for Improvement:}$$\n\n*   The authors should provide a more detailed analysis of the convergence rate of the algorithm.\n*   The authors should provide a more detailed analysis of the assumptions made in the paper, such as the uniform boundedness of the functional derivatives.\n*   The authors should provide a more detailed analysis of the dependence of the particle interactions on all previous distributions.\n\n$$\\text{Recommendation:}$$\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors should provide a more detailed analysis of the convergence rate of the algorithm, the assumptions made in the paper, and the dependence of the particle interactions on all previous distributions. The authors should also provide a more detailed analysis of the dependence of the weights $\\beta_t$ on $t$. With these revisions, the paper has the potential to be a strong contribution to the field of mean-field Langevin dynamics for distributional minimax optimization problems.",
        "peer_review_with_watermark": "$$\\begin{align*}\n\\textbf{Summary:}\n\\end{align*}$$\n\nThe paper presents a novel approach to mean-field Langevin dynamics for distributional minimax optimization problems. The authors propose two single-loop algorithms, MFL-AG and MFL-ABR, and establish average-iterate convergence to the mixed Nash equilibrium. The MFL-AG algorithm is shown to achieve linear convergence, while the MFL-ABR algorithm is proven to converge at a linear rate. The authors also demonstrate the superior optimality of both algorithms compared to MFL-DA. The work represents early steps towards an understanding of mean-field dynamics for multiple learning agents and opens up further avenues of investigation.\n\n$$\\begin{align*}\n\\textbf{Strengths:}\n\\end{align*}$$\n\n* The authors present a novel approach to mean-field Langevin dynamics for distributional minimax optimization problems.\n* The MFL-AG algorithm is shown to achieve average-iterate convergence to the mixed Nash equilibrium.\n* The MFL-ABR algorithm is proven to converge at a linear rate.\n* The authors demonstrate the superior optimality of both algorithms compared to MFL-DA.\n* The work represents early steps towards an understanding of mean-field dynamics for multiple learning agents and opens up further avenues of investigation.\n\n$$\\begin{align*}\n\\textbf{Weaknesses:}\n\\end{align*}$$\n\n* The authors assume a strong convexity of the functional U\u03bc and U\u03bd, which may not be universally applicable.\n* The authors do not provide a detailed analysis of the dependence of the particle interactions on previous distributions.\n* The authors assume that the functional derivatives \u03b4L\u03b4\u03bc and \u03b4L\u03b4\u03bd are uniformly bounded, which may not be universally applicable.\n* The authors do not provide a detailed analysis of the effect of the weighting scheme \u03b2t on the convergence rate of the MFL-AG algorithm.\n* The authors assume that the gradients \u2207xQ and \u2207yQ are bounded and Lipschitz, which may not be universally applicable.\n\n$$\\begin{align*}\n\\textbf{Suggestions for Improvement:}\n\\end{align*}$$\n\n* The authors should provide a more detailed analysis of the dependence of the particle interactions on previous distributions.\n* The authors should provide a detailed analysis of the effect of the weighting scheme \u03b2t on the convergence rate of the MFL-AG algorithm.\n* The authors should assume a weak convexity of the functional U\u03bc and U\u03bd, which would allow for a more universally applicable result.\n* The authors should assume that the functional derivatives \u03b4L\u03b4\u03bc and \u03b4L\u03b4\u03bd are not uniformly bounded, which would allow for a more universally applicable result.\n* The authors should provide a detailed analysis of the effect of the weighting scheme \u03b2t on the convergence rate of the MFL-ABR algorithm.\n\n$$\\begin{align*}\n\\textbf{Recommendation:}\n\\end{align*}$$\n\n* The authors' work presents a novel approach to mean-field Langevin dynamics for distributional minimax optimization problems.\n* The authors demonstrate the superior optimality of both algorithms compared to MFL-DA.\n* The work represents early steps towards an understanding of mean-field dynamics for multiple learning agents and opens up further avenues of investigation.\n* However, the authors should provide a more detailed analysis of the dependence of the particle interactions on previous distributions and the effect of the weighting scheme \u03b2t on the convergence rate of the MFL-AG algorithm.\n* Additionally, the authors should assume a weak convexity of the functional U\u03bc and U\u03bd and assume that the functional derivatives \u03b4L\u03b4\u03bc and \u03b4L\u03b4\u03bd are not uniformly bounded.",
        "gr_score_with": 0.46332046332046334,
        "gr_score_without": 0.3441295546558704,
        "or_score_with": 0.47876447876447875,
        "or_score_without": 0.4939271255060729,
        "output_without": {
            "num_tokens_scored": 247,
            "num_green_tokens": 85,
            "green_fraction": 0.3441295546558704,
            "num_orange_tokens": 122,
            "orange_fraction": 0.4939271255060729,
            "green_z_score": -1.7923589504581539,
            "orange_z_score": 89.75612852988647,
            "green_p_value": 0.9634622570620974,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 259,
            "num_green_tokens": 120,
            "green_fraction": 0.46332046332046334,
            "num_orange_tokens": 124,
            "orange_fraction": 0.47876447876447875,
            "green_z_score": 2.080119786670547,
            "orange_z_score": 89.04669265523763,
            "green_p_value": 0.018757273573635515,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TOWARDS ROBUST OUT-OF-DISTRIBUTION GENERALIZATION BOUNDS VIA SHARPNESS",
        "abstract": "Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by \"robustness\" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for \"flat minima leads to better OOD generalization\". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:}$$\nThe paper proposes a new framework for out-of-distribution (OOD) generalization bounds, which takes into account the optimization property of the learned model. The authors introduce the concept of sharpness, which is a measure of the curvature of the loss landscape, and show that it has a negative impact on generalization. They also provide a robust OOD bound that effectively captures the algorithmic robustness in the presence of shifting data distributions. The authors experimentally verify their results on several datasets and demonstrate that their bound is tighter than existing bounds.\n\n$$\\textbf{Strengths:}$$\n1. The paper provides a new framework for OOD generalization bounds that takes into account the optimization property of the learned model.\n2. The authors introduce the concept of sharpness, which is a measure of the curvature of the loss landscape, and show that it has a negative impact on generalization.\n3. The paper provides a robust OOD bound that effectively captures the algorithmic robustness in the presence of shifting data distributions.\n4. The authors experimentally verify their results on several datasets and demonstrate that their bound is tighter than existing bounds.\n\n$$\\textbf{Weaknesses:}$$\n1. The paper assumes that the loss function is twice differentiable, which may not be the case in all scenarios.\n2. The authors do not provide a clear explanation of how to compute the sharpness of a model in practice.\n3. The paper does not discuss the limitations of the proposed framework and the potential biases in the experimental results.\n4. The authors do not provide a comparison with other existing bounds and frameworks for OOD generalization.\n\n$$\\textbf{Suggestions for Improvement:}$$\n1. The authors should provide a more detailed explanation of how to compute the sharpness of a model in practice.\n2. The authors should discuss the limitations of the proposed framework and the potential biases in the experimental results.\n3. The authors should provide a comparison with other existing bounds and frameworks for OOD generalization.\n4. The authors should consider including more datasets and experimental results to further verify the effectiveness of their framework.\n\n$$\\textbf{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that it be published in a reputable conference or journal in the field of machine learning. However, the authors should address the weaknesses and suggestions for improvement before submitting the paper. The paper provides a new framework for OOD generalization bounds that takes into account the optimization property of the learned model, and the authors experimentally verify their results on several datasets. With further development and refinement, the paper has the potential to make a significant contribution to the field of machine learning.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents a new framework for Out-of-Distribution (OOD) generalization bounds, which focuses on algorithmic robustness to capture the tolerance of distribution shift. The authors propose a sharpness-based OOD generalization bound that effectively captures the algorithmic robustness in the presence of shifting data distributions. The bound is tighter than existing bounds, and it is independent of the model size.\n\nStrengths:\n- The paper presents a new and interesting approach to OOD generalization, focusing on algorithmic robustness.\n- The sharpness-based OOD generalization bound is tighter than existing bounds, and it is independent of the model size.\n- The paper provides a clear and intuitive explanation of the relationship between robustness and sharpness.\n\nWeaknesses:\n- The paper assumes that the model is robust, which may not be true in practice. The assumption that the model is robust is not explicitly stated, but it is assumed throughout the paper.\n- The paper does not provide a clear comparison with existing bounds. The comparison is limited to Proposition 2.1, which is not explicitly mentioned as a baseline in the paper.\n\nSuggestions for Improvement:\n- The paper should explicitly state the assumption that the model is robust and discuss the implications of this assumption.\n- The paper should provide a clear comparison with existing bounds, including Proposition 2.1, to demonstrate the effectiveness of the proposed bound.\n- The paper should discuss the limitations of the proposed bound, such as its dependence on the number of partitions (K).\n- The paper should provide more experimental results to demonstrate the effectiveness of the proposed bound.\n\nRecommendation:\nThe paper presents an interesting approach to OOD generalization, focusing on algorithmic robustness. The sharpness-based OOD generalization bound is tighter than existing bounds, and it is independent of the model size. However, the paper has some limitations, such as the assumption that the model is robust, and the dependence on the number of partitions (K). The author should address these limitations and provide more experimental results to demonstrate the effectiveness of the proposed bound. Overall, the paper is well-written, and the author presents a clear and intuitive explanation of the relationship between robustness and sharpness.\n\n$$$Paper Summary (concised):\nThe paper proposes a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions. The bound is tighter than existing bounds, independent of model size, and focuses on robustness. The author also establishes a connection between robustness and sharpness.\n\n$$$Strengths (concised):\n- Introduces a new approach to OOD generalization focusing on robustness.\n- Presents a sharpness-based bound that is tighter than existing bounds.\n\n$$$Weaknesses (concised):\n- Assumes robustness, not explicitly stated.\n- Limited comparison with existing bounds.\n\n$$$Suggestions for Improvement (concised):\n- State assumption of robustness explicitly.\n- Provide clear comparison with existing bounds.\n- Discuss limitations, e.g., dependence on K.\n\n$$$Recommendation (concised):\nThe paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has limitations, such as the assumption of robustness, and dependence on K. The author should address these issues to demonstrate the effectiveness of the proposed bound.\n\n$$$Paper Summary (concised, revised):\nThe paper presents a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions. The bound is tighter than existing bounds, independent of model size, and focuses on robustness. The author establishes a connection between robustness and sharpness.\n\n$$$Strengths (concised, revised):\n- Introduces a new approach to OOD generalization focusing on robustness.\n- Presents a sharpness-based bound that is tighter than existing bounds.\n\n$$$Weaknesses (concised, revised):\n- Assumes robustness, not explicitly stated.\n\n$$$Suggestions for Improvement (concised, revised):\n- State assumption of robustness explicitly.\n- Provide clear comparison with existing bounds.\n\n$$$Recommendation (concised, revised):\nThe paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has a limitation in assuming robustness, which should be addressed to demonstrate the effectiveness of the proposed bound.\n\n$$\nPaper Summary (concised, revised, final): The paper presents a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions, focusing on robustness. The bound is tighter than existing bounds, independent of model size, and establishes a connection between robustness and sharpness.\n\n$$\nStrengths (concised, revised, final): The paper introduces a new approach to OOD generalization focusing on robustness, presenting a sharpness-based bound that is tighter than existing bounds.\n\n$$\nWeaknesses (concised, revised, final): The paper assumes robustness, not explicitly stated.\n\n$$\nSuggestions for Improvement (concised, revised, final): State assumption of robustness explicitly, provide clear comparison with existing bounds.\n\n$$\nRecommendation (concised, revised, final): The paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has a limitation in assuming robustness, which should be addressed.\n\n$$\n$$$Paper Summary (concised, final, revised, final): The paper presents a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions, focusing on robustness. The bound is tighter than existing bounds, independent of model size, and establishes a connection between robustness and sharpness.\n\n$$\n$$$Strengths (concised, final, revised, final): The paper introduces a new approach to OOD generalization focusing on robustness, presenting a sharpness-based bound that is tighter than existing bounds.\n\n$$\n$$$Weaknesses (concised, final, revised, final): The paper assumes robustness, not explicitly stated.\n\n$$\n$$$Suggestions for Improvement (concised, final, revised, final): State assumption of robustness explicitly, provide clear comparison with existing bounds.\n\n$$\n$$$Recommendation (concised, final, revised, final): The paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has a limitation in assuming robustness, which should be addressed.\n\n$$\n$$$Paper Summary (concised, final, revised, final, final): The paper presents a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions, focusing on robustness. The bound is tighter than existing bounds, independent of model size, and establishes a connection between robustness and sharpness.\n\n$$\n$$$Strengths (concised, final, revised, final, final): The paper introduces a new approach to OOD generalization focusing on robustness, presenting a sharpness-based bound that is tighter than existing bounds.\n\n$$\n$$$Weaknesses (concised, final, revised, final, final): The paper assumes robustness, not explicitly stated.\n\n$$\n$$$Suggestions for Improvement (concised, final, revised, final, final): State assumption of robustness explicitly, provide clear comparison with existing bounds.\n\n$$\n$$$Recommendation (concised, final, revised, final, final): The paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has a limitation in assuming robustness, which should be addressed.\n\n$$\nPaper Summary (concised, final, revised, final, final, final): The paper presents a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions, focusing on robustness. The bound is tighter than existing bounds, independent of model size, and establishes a connection between robustness and sharpness.\n\n$$$Strengths (concised, final, revised, final, final, final): The paper introduces a new approach to OOD generalization focusing on robustness, presenting a sharpness-based bound that is tighter than existing bounds.\n\n$$$Weaknesses (concised, final, revised, final, final, final): The paper assumes robustness, not explicitly stated.\n\n$$$Suggestions for Improvement (concised, final, revised, final, final, final): State assumption of robustness explicitly, provide clear comparison with existing bounds.\n\n$$$Recommendation (concised, final, revised, final, final, final): The paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has a limitation in assuming robustness, which should be addressed.\n\n$$\n$$$Paper Summary (concised, final, revised, final, final, final, final): The paper presents a sharpness-based OOD generalization bound that captures algorithmic robustness in shifting data distributions, focusing on robustness. The bound is tighter than existing bounds, independent of model size, and establishes a connection between robustness and sharpness.\n\n$$\n$$$Strengths (concised, final, revised, final, final, final, final): The paper introduces a new approach to OOD generalization focusing on robustness, presenting a sharpness-based bound that is tighter than existing bounds.\n\n$$\n$$$Weaknesses (concised, final, revised, final, final, final, final): The paper assumes robustness, not explicitly stated.\n\n$$\n$$$Suggestions for Improvement (concised, final, revised, final, final, final, final): State assumption of robustness explicitly, provide clear comparison with existing bounds.\n\n$$\n$$$Recommendation (concised, final, revised, final, final, final, final): The paper is well-written, presenting a clear explanation of robustness and sharpness. However, it has a limitation in assuming robustness, which should be addressed.\n\n$$\nPaper Summary (concised, final, revised, final, final, final, final, final): The paper presents a sharpness-based",
        "gr_score_with": 0.5738161559888579,
        "gr_score_without": 0.445993031358885,
        "or_score_with": 0.3426183844011142,
        "or_score_without": 0.43902439024390244,
        "output_without": {
            "num_tokens_scored": 287,
            "num_green_tokens": 128,
            "green_fraction": 0.445993031358885,
            "num_orange_tokens": 126,
            "orange_fraction": 0.43902439024390244,
            "green_z_score": 1.5904768859492022,
            "orange_z_score": 84.28006127010397,
            "green_p_value": 0.055863675840639504,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 206,
            "green_fraction": 0.5738161559888579,
            "num_orange_tokens": 123,
            "orange_fraction": 0.3426183844011142,
            "green_z_score": 6.722514451981719,
            "orange_z_score": 73.19834559689946,
            "green_p_value": 8.930750778895752e-12,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TEXTFIELD3D: TOWARDS ENHANCING OPEN- VOCABULARY 3D GENERATION WITH NOISY TEXT FIELDS",
        "abstract": "Generative models have shown remarkable progress in 3D aspect. Recent works learn 3D representation explicitly under text-3D guidance. However, limited text3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing openvocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D.Specifically, rather than using the text prompts as input directly, we suggest to inject dynamic noise into the latent space of given text prompts, i.e., Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the appropriate range of textual latent space that is expanded by NTFs. To this end, an NTFGen module is proposed to model general text latent code in noisy fields. Meanwhile, an NTFBind module is proposed to align viewinvariant image latent code to noisy fields, further supporting image-conditional 3D generation. To guide the conditional generation in both geometry and texture, multi-modal discrimination is constructed with a text-3D discriminator and a text2.5D discriminator. Compared to previous methods, TextField3D includes three merits: 1) large vocabulary, 2) text consistency, and 3) low latency. Extensive experiments demonstrate that our method achieves a potential open-vocabulary 3D generation capability.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n&\\text{Paper Summary:} \\\\\n&\\text{The paper presents TextField3D, a 3D generative model that enhances open-vocabulary capability. It introduces Noisy Text Fields (NTFs) to map limited 3D data to a comprehensive V-L pre-trained concept space. The NTFGen module generates noisy text latent code, while the NTFBind module aligns view-invariant image latent code with NTFs. The multi-modal discriminator supervises geometry and texture generation. Extensive experiments demonstrate the effectiveness of TextField3D in open-vocabulary 3D generation.}\\\\\n\n&\\text{Strengths:} \\\\\n&\\text{1.} \\text{Open-vocabulary capability:} \\text{TextField3D} \\text{achieves a potential open-vocabulary 3D generation capability.} \\\\\n&\\text{2.} \\text{Text consistency:} \\text{The model exhibits text consistency, which is an essential property for open-vocabulary generation.} \\\\\n&\\text{3.} \\text{Low latency:} \\text{TextField3D} \\text{has low latency, making it suitable for real-time generation.} \\\\\n&\\text{4.} \\text{Flexibility:} \\text{The model can generate various 3D contents with complicated text prompts, demonstrating flexibility.} \\\\\n&\\text{5.} \\text{Comparative evaluation:} \\text{TextField3D} \\text{is compared with state-of-the-art methods, showcasing its superiority.}\\\\\n\n&\\text{Weaknesses:} \\\\\n&\\text{1.} \\text{Vocabulary limitations:} \\text{The model still depends on the vocabulary of training data, which may not fully match the general capability of V-L supervised methods.}\\\\\n&\\text{2.} \\text{Data scarcity:} \\text{The model relies on limited 3D data, which may not be sufficient for comprehensive V-L pre-trained concepts.}\\\\\n&\\text{3.} \\text{Potential risks:} \\text{There is a risk of generating malicious content, similar to other generative models.}\\\\\n\n&\\text{Suggestions for Improvement:} \\\\\n&\\text{1.} \\text{Increase vocabulary:} \\text{Expand the vocabulary of training data to match the general capability of V-L supervised methods.}\\\\\n&\\text{2.} \\text{Collect more data:} \\text{Collect more 3D data to improve the model's ability to map limited data to comprehensive V-L pre-trained concepts.}\\\\\n&\\text{3.} \\text{Regularization techniques:} \\text{Apply regularization techniques to prevent the model from generating malicious content.}\\\\\n\n&\\text{Recommendation:} \\\\\n&\\text{TextField3D} \\text{is a promising approach to enhancing open-vocabulary capability in 3D generative models. However, its limitations need to be addressed. With further improvements, TextField3D has the potential to become a state-of-the-art method for open-vocabulary 3D generation.}\\\\\n\n$$",
        "peer_review_with_watermark": "$$\\text{Paper Summary: }$$\nThe paper proposes a novel approach to 3D generative models, which enables open-vocabulary generation with limited text-3D data. The approach introduces Noisy Text Fields (NTFs) to 3D latent codes, allowing for a larger vocabulary and text control. The NTFGen module generates noisy text latent codes, and the NTFBind module binds view-invariant image latent codes to NTFs. The multi-modal discrimination framework, including text-3D and text-2.5D discriminators, is used to supervise generation quality and text consistency.\n\n$$\\text{Strengths: }$$\n1. The approach enables open-vocabulary generation with limited text-3D data, which is a significant limitation in current 3D generative models.\n2. The NTFGen and NTFBind modules are novel and contribute to the development of 3D generative models.\n3. The multi-modal discrimination framework is used to supervise generation quality and text consistency, which is an important aspect of 3D generative models.\n\n$$\\text{Weaknesses: }$$\n1. The approach still depends on the vocabulary of training data, which may not fully match the general capability of V-L supervised methods.\n2. The paper does not provide a thorough evaluation of the approach's performance, which is essential for validating its effectiveness.\n3. The approach's ability to handle complex text prompts, such as those with multiple nouns or adjectives, is not thoroughly demonstrated.\n\n$$\\text{Suggestions for Improvement: }$$\n1. Provide a more thorough evaluation of the approach's performance, including a comparison with state-of-the-art methods.\n2. Investigate the approach's ability to handle complex text prompts and provide additional examples.\n3. Consider addressing the limitations of the approach, such as its dependence on the vocabulary of training data.\n\n$$\\text{Recommendation: }$$\nThe approach is a novel contribution to 3D generative models and has the potential to enable open-vocabulary generation with limited text-3D data. However, further evaluation and improvement are needed to validate its effectiveness and address its limitations.",
        "gr_score_with": 0.5255474452554745,
        "gr_score_without": 0.4059701492537313,
        "or_score_with": 0.4306569343065693,
        "or_score_without": 0.4746268656716418,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 136,
            "green_fraction": 0.4059701492537313,
            "num_orange_tokens": 159,
            "orange_fraction": 0.4746268656716418,
            "green_z_score": 0.22304986837273524,
            "orange_z_score": 109.23731530675455,
            "green_p_value": 0.41174834732736976,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 274,
            "num_green_tokens": 144,
            "green_fraction": 0.5255474452554745,
            "num_orange_tokens": 118,
            "orange_fraction": 0.4306569343065693,
            "green_z_score": 4.242067163536113,
            "orange_z_score": 89.51913435756038,
            "green_p_value": 1.1073519388408595e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
        "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}~\\text{The authors propose a recurrent circuit model that can implicitly represent priors over latent variables and combine them with sensory and contextual sources of information to encode task-specific posteriors. The model uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors. The model provides a new framework for circuit-level representation of probabilistic information in a format that facilitates flexible inference.}\n\n\\text{Strengths:}~\\text{The authors provide a clear and well-structured paper that presents a novel approach to modeling probabilistic inference in recurrent circuits. The model is well-motivated and the authors provide a thorough analysis of its properties and behavior. The paper also includes a thorough discussion of the theoretical background and the connection to diffusion models. The authors also provide a detailed analysis of the neural signatures of the model and the potential for experimental validation.}\n\n\\text{Weaknesses:}~\\text{The authors could provide more detail on the training procedure for the model and the hyperparameter tuning. The paper could also benefit from more extensive simulations to demonstrate the robustness of the model to different scenarios. Additionally, the authors could provide more insight into the potential neural mechanisms underlying the model and the potential for experimental validation.}\n\n\\text{Suggestions for Improvement:}~\\text{The authors could provide more detail on the training procedure for the model and the hyperparameter tuning. The paper could also benefit from more extensive simulations to demonstrate the robustness of the model to different scenarios. Additionally, the authors could provide more insight into the potential neural mechanisms underlying the model and the potential for experimental validation.}\n\n\\text{Recommendation:}~\\text{Based on the quality of the paper and the novelty of the approach, I recommend publication in a top-tier neural networks or machine learning journal. The paper provides a clear and well-structured presentation of a novel approach to modeling probabilistic inference in recurrent circuits, and the authors provide a thorough analysis of its properties and behavior. The paper also includes a thorough discussion of the theoretical background and the connection to diffusion models, and the authors provide a detailed analysis of the neural signatures of the model and the potential for experimental validation.}$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents a recurrent circuit model that can implicitly represent complex priors and combine them with sensory and contextual sources of information to encode task-specific posteriors. The model uses dendritic nonlinearities, optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. The circuit dynamics sample from the prior at a rate prescribed by the period of the global oscillator, and additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior.\n\nStrengths:\n1. The model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.\n2. The use of dendritic nonlinearities and stochastic somatic integration with oscillatory global signal provides a biologically plausible mechanism for sampling from complex distributions.\n3. The modular approach to inference, which separates the neural substrates implementing the prior from the computation of the likelihood, allows for flexible inference and reuses the same prior information across multiple tasks.\n\nWeaknesses:\n1. The mathematical guarantees of the proposed neural dynamics are not explicitly stated, and it is not clear how they differ from traditional diffusion models.\n2. The model relies on several simplifying assumptions, such as the use of a single oscillatory signal and the absence of other sources of noise.\n3. The results presented in the paper are mostly based on simulations, and it is not clear how well the model generalizes to more complex scenarios.\n\nSuggestions for Improvement:\n1. Provide more mathematical rigor and derive the properties of the proposed neural dynamics explicitly.\n2. Investigate the effect of different sources of noise and other factors on the circuit dynamics and sampling process.\n3. Implement experiments to test the model on more complex scenarios and evaluate its performance on various tasks.\n\nRecommendation:\nThe proposed recurrent circuit model provides an innovative approach to probabilistic inference and demonstrates a biologically plausible mechanism for sampling from complex distributions. While there are some limitations and simplifying assumptions, the model has the potential to be an important contribution to the field of neuroscience and artificial intelligence. With further development and testing, it is likely to be a valuable tool for studying probabilistic inference and modeling complex distributions.",
        "gr_score_with": 0.49295774647887325,
        "gr_score_without": 0.3345724907063197,
        "or_score_with": 0.4732394366197183,
        "or_score_without": 0.5390334572490706,
        "output_without": {
            "num_tokens_scored": 269,
            "num_green_tokens": 90,
            "green_fraction": 0.3345724907063197,
            "num_orange_tokens": 145,
            "orange_fraction": 0.5390334572490706,
            "green_z_score": -2.1904377069899126,
            "orange_z_score": 88.003724067892,
            "green_p_value": 0.9857537460574292,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 175,
            "green_fraction": 0.49295774647887325,
            "num_orange_tokens": 168,
            "orange_fraction": 0.4732394366197183,
            "green_z_score": 3.5751489676438757,
            "orange_z_score": 88.52826949565619,
            "green_p_value": 0.00017501426785660588,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "A LIE GROUP APPROACH TO RIEMANNIAN BATCH NORMALIZATION",
        "abstract": "Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization layers induced by these Lie groups are then proposed for SPD neural networks. We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography (EEG) classification. The code is available at https://github.com/GitZH-Chen/LieBN.git.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThis paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. The authors establish a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. They generalize the existing Lie groups on SPD manifolds and showcase their framework on the parameterized Lie groups of SPD manifolds.\n\nStrengths:\n- The paper provides a theoretical guarantee of controlling both the Riemannian mean and variance.\n- The authors demonstrate the effectiveness of their approach through extensive experiments.\n- The framework is general and can be applied to other Lie groups.\n\nWeaknesses:\n- The paper does not provide a clear comparison with existing RBN methods.\n- The authors do not discuss the limitations of their framework, such as the requirement of a convexity constraint on the weights.\n\nSuggestions for Improvement:\n- The authors should provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n- They should discuss the limitations of their framework and provide suggestions for future improvements.\n\nRecommendation:\n- The paper provides a significant contribution to the field of Riemannian Batch Normalization and Lie groups in machine learning. However, the authors should address the limitations of their framework and provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n\n$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. The authors establish a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. They generalize the existing Lie groups on SPD manifolds and showcase their framework on the parameterized Lie groups of SPD manifolds.\n\nStrengths:\n- The paper provides a theoretical guarantee of controlling both the Riemannian mean and variance.\n- The authors demonstrate the effectiveness of their approach through extensive experiments.\n- The framework is general and can be applied to other Lie groups.\n\nWeaknesses:\n- The paper does not provide a clear comparison with existing RBN methods.\n- The authors do not discuss the limitations of their framework, such as the requirement of a convexity constraint on the weights.\n\nSuggestions for Improvement:\n- The authors should provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n- They should discuss the limitations of their framework and provide suggestions for future improvements.\n\nRecommendation:\n- The paper provides a significant contribution to the field of Riemannian Batch Normalization and Lie groups in machine learning. However, the authors should address the limitations of their framework and provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n\n$$\\begin{aligned}\n$$Paper Summary: \nThis paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. The authors establish a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. They generalize the existing Lie groups on SPD manifolds and showcase their framework on the parameterized Lie groups of SPD manifolds.\n\nStrengths:\n- The paper provides a theoretical guarantee of controlling both the Riemannian mean and variance.\n- The authors demonstrate the effectiveness of their approach through extensive experiments.\n- The framework is general and can be applied to other Lie groups.\n\nWeaknesses:\n- The paper does not provide a clear comparison with existing RBN methods.\n- The authors do not discuss the limitations of their framework, such as the requirement of a convexity constraint on the weights.\n\nSuggestions for Improvement:\n- The authors should provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n- They should discuss the limitations of their framework and provide suggestions for future improvements.\n\nRecommendation:\n- The paper provides a significant contribution to the field of Riemannian Batch Normalization and Lie groups in machine learning. However, the authors should address the limitations of their framework and provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n\n$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. The authors establish a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. They generalize the existing Lie groups on SPD manifolds and showcase their framework on the parameterized Lie groups of SPD manifolds.\n\nStrengths:\n- The paper provides a theoretical guarantee of controlling both the Riemannian mean and variance.\n- The authors demonstrate the effectiveness of their approach through extensive experiments.\n- The framework is general and can be applied to other Lie groups.\n\nWeaknesses:\n- The paper does not provide a clear comparison with existing RBN methods.\n- The authors do not discuss the limitations of their framework, such as the requirement of a convexity constraint on the weights.\n\nSuggestions for Improvement:\n- The authors should provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n- They should discuss the limitations of their framework and provide suggestions for future improvements.\n\nRecommendation:\n- The paper provides a significant contribution to the field of Riemannian Batch Normalization and Lie groups in machine learning. However, the authors should address the limitations of their framework and provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n\n$$\\begin{aligned}\n$$Paper Summary: \nThis paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. The authors establish a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. They generalize the existing Lie groups on SPD manifolds and showcase their framework on the parameterized Lie groups of SPD manifolds.\n\nStrengths:\n- The paper provides a theoretical guarantee of controlling both the Riemannian mean and variance.\n- The authors demonstrate the effectiveness of their approach through extensive experiments.\n- The framework is general and can be applied to other Lie groups.\n\nWeaknesses:\n- The paper does not provide a clear comparison with existing RBN methods.\n- The authors do not discuss the limitations of their framework, such as the requirement of a convexity constraint on the weights.\n\nSuggestions for Improvement:\n- The authors should provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.\n- They should discuss the limitations of their framework and provide suggestions for future improvements.\n\nRecommendation:\n- The paper provides a significant contribution to the field of Riemannian Batch Normalization and Lie groups in machine learning. However, the authors should address the limitations of their framework and provide a clear comparison with existing RBN methods to demonstrate the superiority of their approach.",
        "peer_review_with_watermark": "$$\\begin{align*}\n\\text{Paper Summary:} \\ \\ & \\text{The paper proposes a novel framework for batch normalization over Lie groups, called LieBN. The framework can effectively normalize both sample and population statistics. The authors generalize the existing Lie groups on SPD manifolds and demonstrate the framework's effectiveness on parameterized Lie groups of SPD manifolds. The extensive experiments show the advantage of the proposed LieBN framework.}\\\\\n\\text{Strengths:} \\ \\ & \\text{The paper provides a comprehensive review of the existing Riemannian normalization methods. The authors introduce a novel framework that can effectively normalize both sample and population statistics. The generalization of existing Lie groups on SPD manifolds is a valuable contribution.}\\\\\n\\text{Weaknesses:} \\ \\ & \\text{The paper assumes that the manifold is a Lie group and does not discuss the case where the manifold is not a Lie group. The authors also do not provide a detailed analysis of the computational complexity of the proposed framework.}\\\\\n\\text{Suggestions for Improvement:} \\ \\ & \\text{The authors should provide a detailed analysis of the computational complexity of the proposed framework and discuss the case where the manifold is not a Lie group. Additionally, the authors should provide more experimental results to validate the proposed framework's effectiveness on different datasets.}\\\\\n\\text{Recommendation:} \\ \\ & \\text{Based on the review, I recommend that the paper be published after revisions that address the mentioned weaknesses and provide additional experimental results. The proposed framework of LieBN is a valuable contribution to the field of machine learning, and with further revisions, it can have a more significant impact.}\\\\\n\\end{align*}$$",
        "gr_score_with": 0.45701357466063347,
        "gr_score_without": 0.3556701030927835,
        "or_score_with": 0.47058823529411764,
        "or_score_without": 0.520618556701031,
        "output_without": {
            "num_tokens_scored": 194,
            "num_green_tokens": 69,
            "green_fraction": 0.3556701030927835,
            "num_orange_tokens": 101,
            "orange_fraction": 0.520618556701031,
            "green_z_score": -1.260352320785182,
            "orange_z_score": 86.58619916764282,
            "green_p_value": 0.8962288532839644,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 221,
            "num_green_tokens": 101,
            "green_fraction": 0.45701357466063347,
            "num_orange_tokens": 104,
            "orange_fraction": 0.47058823529411764,
            "green_z_score": 1.7300903646004318,
            "orange_z_score": 83.41552373112327,
            "green_p_value": 0.04180706573773662,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MAGNITUDE INVARIANT PARAMETRIZATIONS IMPROVE HYPERNETWORK LEARNING",
        "abstract": "Hypernetworks, neural networks that predict the parameters of another neural network, are powerful models that have been successfully used in diverse applications from image generation to multi-task learning. Unfortunately, existing hypernetworks are often challenging to train. Training typically converges far more slowly than for non-hypernetwork models, and the rate of convergence can be very sensitive to hyperparameter choices. In this work, we identify a fundamental and previously unidentified problem that contributes to the challenge of training hypernetworks: a magnitude proportionality between the inputs and outputs of the hypernetwork. We demonstrate both analytically and empirically that this can lead to unstable optimization, thereby slowing down convergence, and sometimes even preventing any learning. We present a simple solution to this problem using a revised hypernetwork formulation that we call Magnitude Invariant Parametrizations (MIP). We demonstrate the proposed solution on several hypernetwork tasks, where it consistently stabilizes training and achieves faster convergence. Furthermore, we perform a comprehensive ablation study including choices of activation function, normalization strategies, input dimensionality, and hypernetwork architecture; and find that MIP improves training in all scenarios. We also provide easy-to-use code that can turn existing networks into MIP-based hypernetworks.",
        "peer_review_without_watermark": "$$$\\boxed{Peer Review}$$\n\n$\\boxed{Paper Summary:}$\n\nThe paper proposes a new method called Magnitude Invariant Parametrizations (MIP) to address the problem of training instability in hypernetworks. Hypernetworks are neural networks that predict the parameters of another neural network, and they have been successfully used in various applications. However, training hypernetworks is challenging due to the magnitude proportionality between the inputs and outputs of the hypernetwork. The authors identify this problem and propose a revised hypernetwork formulation that eliminates the influence of the scale of hypernetwork inputs on the scale of predicted parameters. They demonstrate the proposed solution on several hypernetwork tasks and show that MIP consistently stabilizes training and achieves faster convergence.\n\n$\\boxed{Strengths:}$\n\n1. The paper identifies a fundamental problem in hypernetwork training and proposes a novel solution.\n2. The authors provide a thorough analysis of the problem and demonstrate the effectiveness of MIP on several hypernetwork tasks.\n3. The paper provides a clear and concise explanation of the proposed method and its benefits.\n4. The authors release their implementation as an open-source PyTorch library, HyperLight, which facilitates the development of hypernetwork models.\n\n$\\boxed{Weaknesses:}$\n\n1. The paper focuses on fully connected layers, which may not be the most common architecture in hypernetworks.\n2. The authors do not discuss the potential limitations of MIP when used with other types of hypernetwork architectures.\n3. The paper does not provide a comprehensive comparison with other normalization strategies and hypernetwork architectures.\n4. The authors do not discuss the potential impact of MIP on hypernetworks used in transfer learning.\n\n$\\boxed{Suggestions for Improvement:}$\n\n1. The authors should provide a more comprehensive analysis of MIP's performance on different hypernetwork architectures.\n2. The authors should discuss the potential limitations of MIP and provide suggestions for addressing them.\n3. The authors should provide a more detailed comparison with other normalization strategies and hypernetwork architectures.\n4. The authors should discuss the potential impact of MIP on hypernetworks used in transfer learning.\n\n$\\boxed{Recommendation:}$\n\nThe paper proposes a novel solution to a fundamental problem in hypernetwork training, and the authors demonstrate its effectiveness on several hypernetwork tasks. While there are some limitations and potential areas for improvement, the paper provides a clear and concise explanation of the proposed method and its benefits. We recommend accepting the paper for publication in its current form, with the authors addressing the suggestions for improvement.",
        "peer_review_with_watermark": "$$$$\n\n$$$$\n\n**Paper Summary:**\n\nThis paper addresses the challenge of training hypernetworks, which are neural networks that predict the parameters of another neural network. The authors identify a fundamental problem that contributes to the instability of hypernetwork training: the proportionality relationship between the inputs and outputs of the hypernetwork. They propose a simple solution, Magnitude Invariant Parametrizations (MIP), which eliminates this proportionality relationship and achieves faster convergence and more stable training.\n\n**Strengths:**\n\n1. The paper provides a clear and concise explanation of the hypernetwork architecture and the proportionality problem it faces.\n2. The authors demonstrate the impact of the proportionality relationship on hypernetwork training through empirical experiments.\n3. The proposed MIP solution is straightforward and easy to implement.\n4. The paper provides a comprehensive analysis of the MIP solution, including an ablation study and robustness analysis.\n\n**Weaknesses:**\n\n1. The paper does not provide a comprehensive review of existing hypernetwork architectures and their limitations.\n2. The authors assume that biases are at zero, which only holds at initialization, and do not include normalization layers that are sometimes used.\n3. The paper does not provide a detailed analysis of the choice of optimization algorithm and hyperparameter settings.\n4. The authors conclude that their method is widely useful without providing empirical evidence for this claim.\n\n**Suggestions for Improvement:**\n\n1. The authors should provide a more comprehensive review of existing hypernetwork architectures and their limitations.\n2. The authors should include normalization layers that are sometimes used in hypernetworks.\n3. The authors should provide a more detailed analysis of the choice of optimization algorithm and hyperparameter settings.\n4. The authors should provide empirical evidence for the claim that their method is widely useful.\n\n**Recommendation:**\n\nThe paper makes a valuable contribution to the field of hypernetworks by proposing a simple solution to the proportionality problem. The authors provide a clear and concise explanation of the problem and their proposed solution, and their empirical experiments demonstrate the impact of the proportionality relationship on hypernetwork training. However, the paper lacks a comprehensive review of existing hypernetwork architectures and their limitations, and the authors' conclusions about the wide applicability of their method are not supported by empirical evidence. Overall, the paper is of interest to researchers working on hypernetworks and neural networks.\n\n$$$$\n\n**Peer Review:**\n\n**Peer Review Summary:**\n\nThis paper proposes a solution to the proportionality problem in hypernetworks, which is a fundamental challenge in training hypernetworks. The authors provide a clear explanation of the problem and their proposed solution, Magnitude Invariant Parametrizations (MIP), which eliminates the proportionality relationship between the inputs and outputs of the hypernetwork. The authors demonstrate the impact of the proportionality relationship on hypernetwork training through empirical experiments and provide a comprehensive analysis of the MIP solution.\n\n**Peer Review Strengths:**\n\n1. The paper provides a clear and concise explanation of the hypernetwork architecture and the proportionality problem it faces.\n2. The authors demonstrate the impact of the proportionality relationship on hypernetwork training through empirical experiments.\n3. The proposed MIP solution is straightforward and easy to implement.\n\n**Peer Review Weaknesses:**\n\n1. The paper does not provide a comprehensive review of existing hypernetwork architectures and their limitations.\n2. The authors assume that biases are at zero, which only holds at initialization, and do not include normalization layers that are sometimes used.\n3. The paper does not provide a detailed analysis of the choice of optimization algorithm and hyperparameter settings.\n\n**Peer Review Recommendations:**\n\n1. The authors should provide a more comprehensive review of existing hypernetwork architectures and their limitations.\n2. The authors should include normalization layers that are sometimes used in hypernetworks.\n3. The authors should provide a more detailed analysis of the choice of optimization algorithm and hyperparameter settings.\n\n**Peer Review Rating:**\n\n4/5\n\n$$$$",
        "gr_score_with": 0.45151515151515154,
        "gr_score_without": 0.36419753086419754,
        "or_score_with": 0.4909090909090909,
        "or_score_without": 0.5030864197530864,
        "output_without": {
            "num_tokens_scored": 324,
            "num_green_tokens": 118,
            "green_fraction": 0.36419753086419754,
            "num_orange_tokens": 163,
            "orange_fraction": 0.5030864197530864,
            "green_z_score": -1.3154667137168912,
            "orange_z_score": 111.6872457393079,
            "green_p_value": 0.9058234514431021,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 330,
            "num_green_tokens": 149,
            "green_fraction": 0.45151515151515154,
            "num_orange_tokens": 162,
            "orange_fraction": 0.4909090909090909,
            "green_z_score": 1.9102329436458525,
            "orange_z_score": 109.95288637146407,
            "green_p_value": 0.028051613627682572,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "OUT-OF-DOMAIN KNOWLEDGE DISTILLATION",
        "abstract": "Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers\u2019 training data and real-world scenarios (student domain). The degradation stems from the portions of teachers\u2019 knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students\u2019 performance. Hence, selectively transferring teachers\u2019 appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off the learning process between OOD knowledge distillation and domain-specific information learning via mixup learning. Extensive experiments in 3 datasets and 8 settings demonstrate the stability and superiority of our approach. Code available at https://github.com/IshiKura-a/AuG-KD",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel method called AuG-KD (Anchor-Based Mixup Generative Knowledge Distillation) for addressing the Out-of-Domain Knowledge Distillation (OOD-KD) problem. OOD-KD occurs when a teacher model is trained on a different domain than the student model, resulting in poor performance. The proposed method uses a data-driven anchor to align student-domain data with the teacher domain and leverages a generative method to progressively evolve the learning process from OOD knowledge distillation to domain-specific information learning.\n\nStrengths:\n1. The proposed method is simple and effective in addressing the OOD-KD problem.\n2. The use of a data-driven anchor to align student-domain data with the teacher domain is a novel approach.\n3. The generative method used in the proposed method is effective in progressively evolving the learning process.\n\nWeaknesses:\n1. The paper assumes that the teacher model is trained on a different domain than the student model, which may not always be the case.\n2. The proposed method may not work well with models that have a large domain shift between the teacher and student domains.\n3. The paper does not provide a detailed analysis of the effect of different hyperparameters on the performance of the proposed method.\n\nSuggestions for Improvement:\n1. The paper should provide a more detailed analysis of the effect of different hyperparameters on the performance of the proposed method.\n2. The paper should provide a more detailed explanation of how the data-driven anchor is used to align student-domain data with the teacher domain.\n3. The paper should provide a more detailed analysis of the effect of different anchor parameters on the performance of the proposed method.\n\nRecommendation:\nBased on the paper's strengths and weaknesses, I recommend that the authors revise the paper to address the weaknesses and improve the suggestions for improvement. The authors should provide a more detailed analysis of the effect of different hyperparameters on the performance of the proposed method, provide a more detailed explanation of how the data-driven anchor is used to align student-domain data with the teacher domain, and provide a more detailed analysis of the effect of different anchor parameters on the performance of the proposed method. Additionally, the authors should consider providing a more detailed analysis of the effect of different anchor parameters on the performance of the proposed method. \n\n$$\n$Recommendation for future work: \n1. Investigate the effect of different anchor parameters on the performance of the proposed method.\n2. Analyze the effect of different hyperparameters on the performance of the proposed method.\n3. Investigate the application of the proposed method to other domains and tasks.\n4. Investigate the use of other generative methods to evolve the learning process.\n5. Investigate the use of other anchor methods to align student-domain data with the teacher domain.\n$$",
        "peer_review_with_watermark": "$$$${Paper Title: Out-of-Domain Knowledge Distillation}\n\nThe paper proposes a novel method, AuG-KD, for out-of-domain knowledge distillation, a problem where a student model needs to learn from a teacher model without access to its training data. The authors aim to address the discrepancy between the teacher's training data and the student's domain, which leads to significant performance degradation. The proposed method uses an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off between out-of-domain knowledge distillation and domain-specific information learning.\n\nThe paper is well-structured and easy to follow. The authors provide a clear overview of the problem and the proposed method, and the technical details are well-explained. The use of a data-driven anchor to align student-domain data with the teacher domain is an innovative approach, and the generative method to evolve the learning process is effective in balancing out-of-domain knowledge distillation and domain-specific information learning.\n\nThe experimental results show the effectiveness of the proposed method, with significant improvements over the baselines. The ablation studies demonstrate the importance of each component of the proposed method, including the data-driven anchor and the generative method. The results are well-presented, and the conclusions are drawn based on the experimental evidence.\n\nThe paper makes a significant contribution to the field of knowledge distillation and out-of-domain learning. The proposed method can be applied to various domains, including image classification, and can be extended to other types of data, such as text and audio. The results can be used to improve the performance of student models in out-of-domain settings, which is crucial in many real-world applications, such as transfer learning and domain adaptation.\n\nHowever, there are some limitations to the paper. The authors do not discuss the potential biases and limitations of the proposed method, such as the assumption of a data-driven anchor and the generative method. Additionally, the paper does not provide a detailed analysis of the effectiveness of the proposed method in different scenarios, such as different levels of domain shift and different types of data. Moreover, the results can be improved by using more extensive experimental settings, such as using multiple datasets and different types of student models.\n\nOverall, the paper presents a novel and effective method for out-of-domain knowledge distillation. The results show significant improvements over the baselines, and the ablation studies demonstrate the importance of each component of the proposed method. The paper makes a significant contribution to the field of knowledge distillation and out-of-domain learning, and the results can be used to improve the performance of student models in out-of-domain settings. However, the paper needs to address the potential biases and limitations of the proposed method and provide a more detailed analysis of its effectiveness in different scenarios. \n\nRecommendation: \nThe paper is recommended for publication in a top-tier conference in machine learning, such as NIPS, ICML, or IJCAI. The results can be used to improve the performance of student models in out-of-domain settings, and the proposed method can be extended to other types of data, such as text and audio. The authors need to address the potential biases and limitations of the proposed method and provide a more detailed analysis of its effectiveness in different scenarios. \n\nRating: \nThe paper is rated as 8 out of 10, based on its technical quality, innovation, and effectiveness. The results are well-presented, and the conclusions are drawn based on the experimental evidence. However, the paper needs to address the potential biases and limitations of the proposed method and provide a more detailed analysis of its effectiveness in different scenarios.",
        "gr_score_with": 0.5321782178217822,
        "gr_score_without": 0.36769759450171824,
        "or_score_with": 0.39603960396039606,
        "or_score_without": 0.5051546391752577,
        "output_without": {
            "num_tokens_scored": 291,
            "num_green_tokens": 107,
            "green_fraction": 0.36769759450171824,
            "num_orange_tokens": 147,
            "orange_fraction": 0.5051546391752577,
            "green_z_score": -1.1248011151677904,
            "orange_z_score": 102.4986799386271,
            "green_p_value": 0.8696633390977629,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 404,
            "num_green_tokens": 215,
            "green_fraction": 0.5321782178217822,
            "num_orange_tokens": 160,
            "orange_fraction": 0.39603960396039606,
            "green_z_score": 5.423066795213434,
            "orange_z_score": 94.32151344124327,
            "green_p_value": 2.9292543897341476e-08,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS",
        "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16, 464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench. The codes, trained models, and demo are publicly available at https://github.com/OpenBMB/ToolBench.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a framework called ToolLLM, which enables large language models (LLMs) to master a large number of real-world APIs and perform complex tool-use tasks. The framework consists of three main components: (1) ToolBench, an instruction-tuning dataset that covers 16,000+ real-world APIs; (2) a novel depth-first search-based decision tree algorithm (DFSDT) to improve the planning and reasoning ability of LLMs; and (3) an automatic evaluator ToolEval to assess the tool-use capabilities of LLMs. The authors fine-tune the LLaMA model on ToolBench and demonstrate its generalization ability to unseen APIs.\n\n$$Strengths:\n1. The authors have made significant contributions to the field of tool learning for LLMs, providing a comprehensive framework that enables LLMs to master a large number of real-world APIs.\n2. The use of DFSDT to improve the planning and reasoning ability of LLMs is a novel and innovative approach.\n3. The authors have demonstrated the generalization ability of the proposed framework to unseen APIs and out-of-distribution domains.\n\n$$Weaknesses:\n1. The paper does not provide a clear explanation of the limitations of the current state-of-the-art (SOTA) LLMs in tool use.\n2. The authors rely heavily on the ChatGPT model for instruction generation, which may limit the applicability of the proposed framework.\n3. The paper does not provide a comprehensive evaluation of the proposed framework's performance on various tasks and datasets.\n\n$$Suggestions for Improvement:\n1. The authors should provide a more detailed explanation of the limitations of the current SOTA LLMs in tool use and how their proposed framework addresses these limitations.\n2. The authors should evaluate the proposed framework's performance on various tasks and datasets to demonstrate its robustness and generalization ability.\n3. The authors should provide a more comprehensive analysis of the results and discuss the implications of the findings.\n\n$$Recommendation:\nBased on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a top-tier conference in the field of natural language processing (NLP) and artificial intelligence (AI). The authors have made significant contributions to the field, and their proposed framework has the potential to revolutionize the way LLMs are used in real-world applications. However, the authors need to address the limitations of the current SOTA LLMs in tool use and provide a more comprehensive evaluation of the proposed framework's performance.",
        "peer_review_with_watermark": "$$\\text{Paper Title: TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS}\\]\n\nThe paper proposes the development of a tool-use framework, called ToolLLM, to empower open-source large language models (LLMs) to skillfully master diverse APIs. The framework consists of three components: data construction, model training, and evaluation. The construction process, called ToolBench, generates high-quality instruction-tuning data for tool use, which is constructed automatically using the state-of-the-art (SOTA) closed-source LLM, ChatGPT. The model training process fine-tunes the LLaMA model on the generated dataset, and the evaluation process assesses the tool-use capabilities of the model using the automatic evaluator, ToolEval. The results demonstrate that the ToolLLaMA model outperforms the baselines in both pass rate and win rate, and exhibits remarkable generalization ability to unseen APIs.\n\nStrengths:\n\n1. The paper proposes a novel framework, called ToolLLM, to empower open-source LLMs to skillfully master diverse APIs.\n2. The construction process, called ToolBench, generates high-quality instruction-tuning data for tool use, which is constructed automatically using the SOTA closed-source LLM, ChatGPT.\n3. The model training process fine-tunes the LLaMA model on the generated dataset, and the evaluation process assesses the tool-use capabilities of the model using the automatic evaluator, ToolEval.\n4. The results demonstrate that the ToolLLaMA model outperforms the baselines in both pass rate and win rate, and exhibits remarkable generalization ability to unseen APIs.\n\nWeaknesses:\n\n1. The paper does not provide sufficient details on the construction process, called ToolBench, and the dataset generation process.\n2. The paper does not provide sufficient details on the model training process, and the fine-tuning process on the generated dataset.\n3. The paper does not provide sufficient details on the evaluation process, and the assessment of the tool-use capabilities of the model using the automatic evaluator, ToolEval.\n4. The paper does not provide sufficient details on the comparison with the baselines, and the results of the experiments.\n\nSuggestions for Improvement:\n\n1. Provide sufficient details on the construction process, called ToolBench, and the dataset generation process.\n2. Provide sufficient details on the model training process, and the fine-tuning process on the generated dataset.\n3. Provide sufficient details on the evaluation process, and the assessment of the tool-use capabilities of the model using the automatic evaluator, ToolEval.\n4. Provide sufficient details on the comparison with the baselines, and the results of the experiments.\n5. Consider including additional results, such as the accuracy of the automatic evaluator, ToolEval, and the effectiveness of the neural API retriever in recommending relevant APIs for each instruction.\n6. Consider including a comparison with other state-of-the-art LLMs, such as GPT-4 and CLIP, to demonstrate the superiority of the proposed framework, ToolLLM.\n\nRecommendation:\n\nThe paper proposes a novel framework, called ToolLLM, to empower open-source LLMs to skillfully master diverse APIs. The framework consists of three components: data construction, model training, and evaluation. The construction process, called ToolBench, generates high-quality instruction-tuning data for tool use, which is constructed automatically using the SOTA closed-source LLM, ChatGPT. The model training process fine-tunes the LLaMA model on the generated dataset, and the evaluation process assesses the tool-use capabilities of the model using the automatic evaluator, ToolEval. The results demonstrate that the ToolLLaMA model outperforms the baselines in both pass rate and win rate, and exhibits remarkable generalization ability to unseen APIs. Overall, the paper presents a well-designed and well-executed framework, and the results demonstrate the effectiveness of the proposed framework, ToolLLM. However, additional results and a comparison with other state-of-the-art LLMs are necessary to fully demonstrate the superiority of the framework.",
        "gr_score_with": 0.4782608695652174,
        "gr_score_without": 0.37604456824512533,
        "or_score_with": 0.45962732919254656,
        "or_score_without": 0.5264623955431755,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 135,
            "green_fraction": 0.37604456824512533,
            "num_orange_tokens": 189,
            "orange_fraction": 0.5264623955431755,
            "green_z_score": -0.9265003892154285,
            "orange_z_score": 112.89027290538218,
            "green_p_value": 0.822907002534183,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 322,
            "num_green_tokens": 154,
            "green_fraction": 0.4782608695652174,
            "num_orange_tokens": 148,
            "orange_fraction": 0.45962732919254656,
            "green_z_score": 2.8665992577177257,
            "orange_z_score": 93.14180721106473,
            "green_p_value": 0.002074540139282022,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "WHAT\u2019S IN MY BIG DATA?",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and set of sixteen analyses for processing and analyzing large text corpora. The authors use WIMBD to analyze ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. The analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.\n\nStrengths:\n1. The authors provide a comprehensive analysis of large text corpora, highlighting the importance of data understanding in language model training.\n2. The proposed WIMBD platform and set of analyses provide a standardized framework for evaluating large text corpora.\n3. The authors demonstrate the effectiveness of WIMBD in uncovering hidden patterns and anomalies in large text corpora.\n\nWeaknesses:\n1. The authors rely heavily on existing datasets and corpora, which may limit the generalizability of their findings.\n2. The analysis is focused on a specific set of corpora, which may not be representative of the broader landscape of large text corpora.\n3. The authors do not provide a clear explanation of the limitations and potential biases of their analysis.\n\nSuggestions for Improvement:\n1. The authors should consider incorporating more diverse datasets and corpora to increase the generalizability of their findings.\n2. The authors should provide a more detailed explanation of the limitations and potential biases of their analysis.\n3. The authors should consider exploring the use of more advanced techniques, such as transfer learning and ensemble methods, to improve the accuracy and robustness of their analysis.\n\nRecommendation:\nThe paper presents an interesting and comprehensive analysis of large text corpora, highlighting the importance of data understanding in language model training. However, the authors should consider incorporating more diverse datasets and corpora, providing a more detailed explanation of the limitations and potential biases of their analysis, and exploring the use of more advanced techniques to improve the accuracy and robustness of their analysis. With these improvements, the paper has the potential to make a significant contribution to the field of natural language processing. \n\n$$$ \n\n$$$BENCHMARK TIMES:\n\n| Analysis | Time (seconds) | Estimated Cost ($ USD) |\n| --- | --- | --- |\n| Summary Statistics | 1,100 | $10.3 |\n| Internet Domain Distribution | 1,400 | $13.3 |\n| Internet Domain Schemes | 1,300 | $12.7 |\n| Internet Domain Suffixes | 1,500 | $14.3 |\n| Utterance Date Statistics | 1,200 | $11.7 |\n| Geolocation | 1,100 | $10.3 |\n| Most & Least Common n-grams | 10,500 | $100.5 |\n| Duplicate Detection | 10,500 | $100.5 |\n| Document Length Distribution | 10,500 | $100.5 |\n| Toxic Language Detection | 10,500 | $100.5 |\n| Demographic Sentiment Co-occurrences | 10,500 | $100.5 |\n| Cross-Data Analysis | 10,500 | $100.5 |\n\n$$$ \n\n$$$TECHNICAL DETAILS:\n\n1. The authors use a Bloom filter-inspired approach to compute the most common and least common n-grams, which provides an approximate upper bound of the true counts.\n2. The authors use a similar approach to estimate the number of unique n-grams, which provides a lower bound of the actual number of unique n-grams.\n3. The authors use a combination of exact and approximate methods to detect duplicates and toxic language, which provides a good balance between accuracy and computational efficiency.\n\n$$$ \n\n$$$LIMITATIONS:\n\n1. The authors rely on a single search tool (Elasticsearch), which may not be the most cost-effective or efficient option for large text corpora.\n2. The authors do not provide a clear explanation of the limitations and potential biases of their analysis.\n3. The authors do not consider the use of more advanced techniques, such as transfer learning and ensemble methods, to improve the accuracy and robustness of their analysis.\n\n$$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and set of sixteen analyses for processing and analyzing large text corpora. The authors use WIMBD to analyze ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. The analysis uncovers several surprising findings, including high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. The authors open-source WIMBD\u2019s code and artifacts to encourage further analysis and transparency around large text corpora.\n\nStrengths:\n1. The authors provide a comprehensive analysis of large text corpora, highlighting the importance of understanding the content of these corpora.\n2. The proposed platform, WIMBD, offers a structured approach to analyzing large text corpora, enabling researchers to identify and quantify various types of content.\n3. The authors provide detailed results of their analysis, shedding light on the prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination in large text corpora.\n\nWeaknesses:\n1. The analysis is limited to ten corpora, which may not be representative of the broader range of large text corpora used in language model training.\n2. The authors rely on pre-existing corpora, which may not be publicly available or may have been curated in ways that impact the analysis.\n3. The analysis focuses on detecting specific types of content, but may not capture other important aspects of large text corpora, such as their semantic meaning or cultural significance.\n4. The authors do not provide a detailed evaluation of the performance of WIMBD, which may impact its adoption and usage in the field.\n5. The analysis may not be able to capture all types of content, especially those that are not easily quantifiable, such as humor, sarcasm, or idioms.\n\nSuggestions for Improvement:\n1. Expand the analysis to include more corpora and types of content to increase the representativeness of the findings.\n2. Consider using more advanced methods for detecting and quantifying content, such as using neural networks or machine learning algorithms.\n3. Evaluate the performance of WIMBD using a range of metrics, including precision, recall, and F1 score, to provide a more comprehensive understanding of its capabilities.\n4. Consider adding more contextual information to the analysis, such as metadata about the corpora, to provide a more nuanced understanding of the findings.\n5. Open-source WIMBD\u2019s code and artifacts to encourage further analysis and transparency around large text corpora.\n\nRecommendation:\nThe authors provide a valuable analysis of large text corpora, highlighting the importance of understanding the content of these corpora. The proposed platform, WIMBD, offers a structured approach to analyzing large text corpora, enabling researchers to identify and quantify various types of content. While the analysis has some limitations, the authors provide a detailed evaluation of their methods and results, which should be helpful for researchers interested in understanding large text corpora. To further improve the analysis, the authors should consider expanding their analysis to include more corpora and types of content, using more advanced methods for detecting and quantifying content, and evaluating the performance of WIMBD using a range of metrics.",
        "gr_score_with": 0.5714285714285714,
        "gr_score_without": 0.4731404958677686,
        "or_score_with": 0.38345864661654133,
        "or_score_without": 0.4194214876033058,
        "output_without": {
            "num_tokens_scored": 484,
            "num_green_tokens": 229,
            "green_fraction": 0.4731404958677686,
            "num_orange_tokens": 203,
            "orange_fraction": 0.4194214876033058,
            "green_z_score": 3.2845430641865323,
            "orange_z_score": 54.549231573876085,
            "green_p_value": 0.0005107394316324879,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 399,
            "num_green_tokens": 228,
            "green_fraction": 0.5714285714285714,
            "num_orange_tokens": 153,
            "orange_fraction": 0.38345864661654133,
            "green_z_score": 6.989788470128607,
            "orange_z_score": 45.00237328487165,
            "green_p_value": 1.3765050201735998e-12,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper proposes a novel approach to visual robot manipulation learning by leveraging large-scale video generative pre-training. The proposed method, GR-1, is a GPT-style transformer that takes as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data to predict actions and future images. The authors perform extensive experiments on the CALVIN benchmark and a real robot to evaluate the performance of GR-1.\n\nStrengths:\n1. The paper presents a novel approach to visual robot manipulation learning by leveraging large-scale video generative pre-training.\n2. The proposed method, GR-1, is a flexible GPT-style transformer that can be seamlessly finetuned on robot data.\n3. The authors perform extensive experiments on the CALVIN benchmark and a real robot to evaluate the performance of GR-1.\n4. The paper provides a clear and concise explanation of the proposed method and its components.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the effectiveness of large-scale video generative pre-training for visual robot manipulation learning.\n2. The proposed method, GR-1, may be sensitive to the quality and diversity of the pre-trained video dataset.\n3. The paper does not provide a detailed comparison with other state-of-the-art methods for visual robot manipulation learning.\n4. The experiments are limited to the CALVIN benchmark and a real robot, and it is unclear how well the proposed method would perform on other tasks and robots.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the effectiveness of large-scale video generative pre-training for visual robot manipulation learning.\n2. The proposed method, GR-1, should be evaluated on a more diverse set of robots and tasks to demonstrate its generalizability.\n3. The authors should compare the proposed method with other state-of-the-art methods for visual robot manipulation learning to demonstrate its superiority.\n4. The experiments should be replicated on other benchmarks and robots to demonstrate the robustness and generalizability of the proposed method.\n\nRecommendation:\nThe proposed method, GR-1, is a novel and innovative approach to visual robot manipulation learning by leveraging large-scale video generative pre-training. While the paper provides a clear and concise explanation of the proposed method and its components, it lacks a detailed analysis of its effectiveness and generalizability. The authors should provide more detailed experiments and comparisons to demonstrate the superiority of the proposed method. Overall, the paper has the potential to contribute to the field of visual robot manipulation learning, but it requires further development and evaluation to demonstrate its robustness and generalizability.\n\n$$",
        "peer_review_with_watermark": "$$\n\n$$ Paper Summary: \nThe paper proposes leveraging large-scale video generative pre-training for enhancing visual robot manipulation learning. They introduce GR-1, a GPT-style transformer that takes as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner. The model is first pre-trained on language-conditioned video prediction with a large-scale video dataset, and then finetuned on robot data to predict actions and future images. The paper presents extensive experiments on both CALVIN benchmark and a real robot, showing that GR-1 improves state-of-the-art methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization. \n\n$$ Strengths: \n1. The paper introduces a novel approach to leveraging large-scale video generative pre-training for visual robot manipulation learning, which is a significant contribution to the field.\n2. The experiments demonstrate the effectiveness of the proposed method, showing improved performance over state-of-the-art methods in various settings.\n3. The paper presents a clear and concise methodology, making it accessible to a wider audience.\n4. The qualitative results show the robustness of GR-1 in scenes with disturbance, which is a significant advantage over existing methods.\n\n$$ Weaknesses: \n1. The paper assumes access to a large-scale video dataset, which may not be feasible for all researchers. Developing methods to collect such data or leveraging alternative datasets is necessary.\n2. The paper does not provide a comprehensive analysis of the video prediction performance, which is a key component of the proposed method. Additional qualitative results would strengthen the paper's argument.\n3. The paper does not provide a thorough discussion of the potential limitations and challenges of the proposed method, which is essential for future researchers to build upon or extend the work.\n4. Some of the baseline methods, such as RT-1, are not explicitly discussed in the paper, which may limit its clarity and accessibility to a wider audience.\n\n$$ Suggestions for Improvement: \n1. Develop methods to collect large-scale video data or leverage alternative datasets to reduce the assumption of access to such data.\n2. Include additional qualitative results to demonstrate the robustness of GR-1 in scenes with disturbance.\n3. Provide a comprehensive analysis of the potential limitations and challenges of the proposed method, including potential pitfalls or areas for future research.\n4. Include a thorough discussion of the baseline methods, such as RT-1, to provide a clearer and more comprehensive comparison with the proposed method.\n5. Consider scaling up the robot data by increasing both the number of robot trajectories in diverse environments and the number of manipulation skills, as mentioned in the future work section.\n\n$$ Recommendation: \nBased on the overall quality and contributions of the paper, I recommend its acceptance to the conference. However, it is essential to address the areas of improvement outlined above to further strengthen the paper's argument and accessibility.",
        "gr_score_with": 0.46987951807228917,
        "gr_score_without": 0.34098360655737703,
        "or_score_with": 0.46265060240963857,
        "or_score_without": 0.5639344262295082,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 104,
            "green_fraction": 0.34098360655737703,
            "num_orange_tokens": 172,
            "orange_fraction": 0.5639344262295082,
            "green_z_score": -2.1038606199548298,
            "orange_z_score": 121.78940368591182,
            "green_p_value": 0.9823046964967185,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 415,
            "num_green_tokens": 195,
            "green_fraction": 0.46987951807228917,
            "num_orange_tokens": 192,
            "orange_fraction": 0.46265060240963857,
            "green_z_score": 2.9058174582037335,
            "orange_z_score": 116.25450285948223,
            "green_p_value": 0.001831475352141696,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CADE AND CONTINUOUS SCENE RECONSTRUCTION FROM FOVEATED VISUAL SAMPLING",
        "abstract": "High accuracy, low latency and high energy efficiency represent a set of conflicting goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduced camera frame rates, and increased data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, and saccade mechanism of the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from a sequence of foveal-peripheral views, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our model demonstrates a 5% higher image recognition accuracy compared to state-of-theart foveal-peripheral based vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches. Code is available at Github.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper presents a novel framework for image classification and object detection using foveal-peripheral vision and saccade control. The proposed system employs a foveal-peripheral sampling mechanism, which reduces the amount of input data while maintaining the same level of performance as the original images. A reinforcement learning-based saccade controller is used to select the next foveal center, and the scene reconstruction model is trained using self-supervised learning. The system is evaluated on the GTSRB and ImageNet datasets, and the results show that it achieves similar image recognition performance with at least 70% fewer pixels compared to systems with conventional imaging front-end.\n\nStrengths:\n1. The proposed system is energy-efficient and can be easily integrated with any existing back-end processing models.\n2. The use of foveal-peripheral sampling and saccade control reduces the amount of input data while maintaining the same level of performance as the original images.\n3. The system is highly transferable and data-efficient.\n4. The experimental results show that the proposed system achieves similar image recognition performance with at least 70% fewer pixels compared to systems with conventional imaging front-end.\n\nWeaknesses:\n1. The system requires a large amount of training data, which may be a limitation for real-world applications.\n2. The system may not perform well on images with complex scenes, as the foveal-peripheral sampling mechanism may miss some important regions.\n3. The system does not consider top-down influences of attention, which may lead to incorrect saccade movements.\n4. The system may not be able to handle occlusions or other forms of data corruption.\n\nSuggestions for Improvement:\n1. Increase the amount of training data to improve the performance of the system.\n2. Develop a method to handle complex scenes and occlusions.\n3. Incorporate top-down influences of attention to improve the accuracy of the saccade controller.\n4. Investigate the differences between the trained saccade model versus human visual scanpaths.\n\nRecommendation:\nThe proposed system is a significant contribution to the field of computer vision and image processing. It has the potential to revolutionize the way images are captured and processed in real-world applications. However, further research is needed to address the limitations of the system and improve its performance. The authors should continue to explore new techniques to handle complex scenes and occlusions, and investigate the differences between the trained saccade model versus human visual scanpaths. Additionally, the authors should consider the potential applications of the system in real-world scenarios and explore ways to integrate it with existing back-end processing models.",
        "peer_review_with_watermark": "$$$$\nPaper Title: CADE AND CONTINUOUS SCENE RECONSTRUCTION FROM FOVEATED VISUAL SAMPLING\n$$$$\nThe paper proposes a novel framework for energy-efficient and low-cost sensing and processing, inspired by the foveal-peripheral vision mechanism of the human visual system. The proposed system reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images.\n\n$$$$\n**Paper Summary:**\n\nThe paper proposes a novel framework for energy-efficient and low-cost sensing and processing, inspired by the foveal-peripheral vision mechanism of the human visual system. The proposed system reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. The system consists of three core components: the predictive reconstruction module, the evaluation module, and the policy module. The predictive reconstruction module receives a sequence of foveal-peripheral views and merges them to generate a high-resolution rendition of the original scene. The policy module controls the saccades, leveraging information obtained by the predictive reconstruction module to strategically direct the foveal center to a new position. The evaluation module assesses the performance of scene reconstruction, and the saccade controller is trained using an advantage actor-critic (A2C) model.\n\n$$$$\n**Strengths:**\n\n1. The proposed system is energy-efficient and low-cost, reducing the required input pixels by over 90% per frame.\n2. The system maintains the same level of performance in image recognition as with the original images.\n3. The use of foveal-peripheral vision and saccades provides a realistic and biologically inspired approach to sensing and processing.\n4. The proposed system is flexible and can be easily integrated with any existing back-end processing models, such as image classification and object detection.\n5. The use of a self-supervised learning approach for the predictive reconstruction module reduces the need for labeled data.\n\n$$$$\n**Weaknesses:**\n\n1. The system may not perform well in scenes with complex structures or high levels of noise.\n2. The use of a random saccade controller may not provide optimal performance, and the learned saccade controller may require additional fine-tuning.\n3. The system may not be suitable for real-time applications, as the reconstruction process may take some time.\n4. The use of a hybrid loss function for the predictive reconstruction module may not be optimal, and the choice of loss function may need to be adjusted.\n5. The system may not provide a clear understanding of the optimal sampling strategy, and further research is needed to determine the best approach.\n\n$$$$\n**Suggestions for Improvement:**\n\n1. Improve the performance of the saccade controller by using additional fine-tuning or transfer learning.\n2. Develop a more robust sampling strategy that can handle complex scenes or high levels of noise.\n3. Implement a real-time reconstruction process that can handle high frame rates.\n4. Explore the use of additional loss functions or optimization techniques to improve the performance of the predictive reconstruction module.\n5. Develop a more comprehensive evaluation framework that can assess the performance of the system in different scenarios.\n\n$$$$\n**Recommendation:**\n\nThe proposed system is a promising approach to energy-efficient and low-cost sensing and processing, and it has the potential to improve the performance of image recognition systems. However, further research is needed to address the weaknesses and limitations of the system. The use of a learned saccade controller, more robust sampling strategy, and real-time reconstruction process can help to improve the performance of the system. Additionally, the development of a more comprehensive evaluation framework can provide a clearer understanding of the optimal sampling strategy and the performance of the system in different scenarios.",
        "gr_score_with": 0.4601366742596811,
        "gr_score_without": 0.3324022346368715,
        "or_score_with": 0.48291571753986334,
        "or_score_without": 0.5921787709497207,
        "output_without": {
            "num_tokens_scored": 358,
            "num_green_tokens": 119,
            "green_fraction": 0.3324022346368715,
            "num_orange_tokens": 212,
            "orange_fraction": 0.5921787709497207,
            "green_z_score": -2.6107677046127256,
            "orange_z_score": 134.7849196050757,
            "green_p_value": 0.9954830382683648,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 439,
            "num_green_tokens": 202,
            "green_fraction": 0.4601366742596811,
            "num_orange_tokens": 212,
            "orange_fraction": 0.48291571753986334,
            "green_z_score": 2.5719708724176695,
            "orange_z_score": 121.39695341808287,
            "green_p_value": 0.00505606995242903,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SMOOTH ECE: PRINCIPLED RELIABILITY DIAGRAMS",
        "abstract": "Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures \u2014 binning and ECE \u2014 both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of B\u0142asiok, Gopalan, Hu, and Nakkiran (2023) \u2014 a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE. We also develop a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: https://anonymous.4open. science/r/smoothece-anon/.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a new method for computing calibration error, called SmoothECE, which is a consistent calibration measure in the sense of B\u0142asiok et al. (2023). The method involves smoothing the observations using a Gaussian kernel and computing the Expected Calibration Error (ECE) of this smoothed function. The authors show that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved and can be visually represented in a reliability diagram.\n\nStrengths:\n\n* The paper presents a new and innovative method for computing calibration error, which is mathematically well-behaved and can be visually represented in a reliability diagram.\n* The authors provide a theoretical justification for the method and show that it is consistent in the sense of B\u0142asiok et al. (2023).\n* The method is efficient and can be implemented using a simple algorithm.\n\nWeaknesses:\n\n* The paper assumes that the observations are i.i.d. and does not consider the case where the observations are not independent.\n* The method requires the choice of a bandwidth, which can be difficult to determine in practice.\n* The paper does not provide a comparison with existing methods for computing calibration error.\n\nSuggestions for Improvement:\n\n* The authors should consider the case where the observations are not independent and provide a method for computing calibration error in this case.\n* The authors should provide a more detailed explanation of how to choose the bandwidth and provide a method for choosing the bandwidth that is robust to outliers.\n* The authors should compare their method with existing methods for computing calibration error and provide a discussion of the strengths and limitations of their method.\n\nRecommendation:\n\n* Based on the paper's contributions and the potential impact of the method, I recommend that the paper be published in a top-tier conference in machine learning or statistics.\n* However, I also recommend that the authors address the weaknesses and suggestions for improvement mentioned above to strengthen the paper and make it more competitive with existing work in the field.",
        "peer_review_with_watermark": "$$$\\begin{align}\nPaper Title: SMOOTH ECE: PRINCIPLED RELIABILITY DIAGRAMS\n\nSummary: \nThe paper proposes a new calibration measure called SmoothECE (Smooth Expected Calibration Error) and a principled reliability diagram construction method. The SmoothECE is a consistent calibration measure that is polynomially upper and lower bounded by the distance to perfect calibration. The reliability diagram is constructed using a kernel density estimate of the predictions and the SmoothECE is visually encoded in the reliability diagram. The paper also develops a Python package for computing the SmoothECE and plotting the reliability diagram.\n\nStrengths:\n- The paper proposes a new calibration measure that is consistent in the sense of B\u0142asiok et al. (2023).\n- The reliability diagram construction method is principled and can be visually interpreted.\n- The Python package is efficient and easy to use.\n- The paper includes experimental demonstrations of the method on various datasets.\n\nWeaknesses:\n- The paper does not provide a clear definition of the SmoothECE and the reliability diagram construction method is not well-explained.\n- The paper does not provide a clear connection between the SmoothECE and the reliability diagram construction method.\n- The paper does not discuss the limitations of the SmoothECE and the reliability diagram construction method.\n- The paper does not provide a clear comparison with existing calibration measures and reliability diagram construction methods.\n\nSuggestions for Improvement:\n- Provide a clear definition of the SmoothECE and the reliability diagram construction method.\n- Explain the connection between the SmoothECE and the reliability diagram construction method.\n- Discuss the limitations of the SmoothECE and the reliability diagram construction method.\n- Provide a clear comparison with existing calibration measures and reliability diagram construction methods.\n- Include more experimental results to demonstrate the effectiveness of the SmoothECE and the reliability diagram construction method.\n\nRecommendation:\n- The paper proposes a new calibration measure that is consistent in the sense of B\u0142asiok et al. (2023) and a principled reliability diagram construction method.\n- The reliability diagram construction method is efficient and easy to use.\n- The paper includes experimental demonstrations of the method on various datasets.\n- However, the paper needs further work on providing a clear definition of the SmoothECE and the reliability diagram construction method, explaining the connection between the SmoothECE and the reliability diagram construction method, discussing the limitations of the SmoothECE and the reliability diagram construction method, and providing a clear comparison with existing calibration measures and reliability diagram construction methods.",
        "gr_score_with": 0.48497854077253216,
        "gr_score_without": 0.3594306049822064,
        "or_score_with": 0.39914163090128757,
        "or_score_without": 0.4875444839857651,
        "output_without": {
            "num_tokens_scored": 281,
            "num_green_tokens": 101,
            "green_fraction": 0.3594306049822064,
            "num_orange_tokens": 137,
            "orange_fraction": 0.4875444839857651,
            "green_z_score": -1.3881809188089274,
            "orange_z_score": 94.7895018980315,
            "green_p_value": 0.9174590194475705,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 233,
            "num_green_tokens": 113,
            "green_fraction": 0.48497854077253216,
            "num_orange_tokens": 93,
            "orange_fraction": 0.39914163090128757,
            "green_z_score": 2.6477782410417046,
            "orange_z_score": 70.42713504654444,
            "green_p_value": 0.004051132719783755,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LEARNING MULTI-FACETED PROTOTYPICAL USER INTERESTS",
        "abstract": "We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the granularity of item space to that of user interests, which we postulate is not ideal as user interests would likely map to a small subset of item space. In addition, the compositionality of user interests has received inadequate attention, preventing the modeling of interactions between explanatory factors driving a user\u2019s decision. To resolve this, we propose to align user interests with multi-faceted item characteristics. First, we involve prototype-based representation learning to discover item characteristics along multiple facets. Second, we compose user interests from uncovered item characteristics via binding mechanism, separating the granularity of user preferences from that of item space. Third, we design a dedicated bi-directional binding block, aiding the derivation of compositional user interests. On real-world datasets, the experimental results demonstrate the strong performance of our proposed method compared to a series of baselines.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach to disentangled representation learning for recommendation systems, called FACETVAE. FACETVAE addresses the shortcomings of existing VAE-based models by disentangling item space under multiple facets, binding compositional user interests from low-level ones, and effectively binding user interests via a bi-directional binding block. The proposed method is evaluated on three real-world datasets, and the experimental results demonstrate its strong performance compared to state-of-the-art baselines.\n\nStrengths:\n1. FACETVAE addresses the limitations of existing VAE-based models by disentangling item space under multiple facets, which improves the modeling of user interests.\n2. The proposed method effectively binds compositional user interests from low-level ones discovered from item space, which is crucial for capturing the complexity of user preferences.\n3. The bi-directional binding block is a novel and effective mechanism for binding user interests, which is demonstrated to improve the recommendation performance.\n\nWeaknesses:\n1. FACETVAE requires more computational resources and time compared to existing VAE-based models, which may be a limitation for large-scale datasets.\n2. The number of facets and the number of clusters per facet need to be predefined, which may not be ideal for all datasets.\n3. The proposed method assumes that the number of user interests equals the number of item groups, which may not always hold true.\n\nSuggestions for Improvement:\n1. Investigate ways to improve the efficiency of multi-faceted item grouping and discovering the number of facets and the number of clusters per facet in a data-driven manner.\n2. Explore the use of more advanced binding mechanisms, such as graph-based or attention-based methods, to further improve the recommendation performance.\n3. Consider using transfer learning or pre-training on smaller datasets to adapt FACETVAE to new domains or datasets.\n\nRecommendation:\nFACETVAE is a promising approach to disentangled representation learning for recommendation systems, addressing the shortcomings of existing VAE-based models. The proposed method's strong performance on three real-world datasets demonstrates its effectiveness. However, further improvements are needed to address the limitations of FACETVAE, such as increasing its efficiency and adaptability to new datasets.",
        "peer_review_with_watermark": "$$$Paper Summary: \nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models. FACETVAE disentangles item space under multiple facets, binds compositional user interests from low-level ones discovered from item space, and effectively binds user interests via a bi-directional binding block. The paper presents a detailed analysis of the proposed model's performance on real-world datasets and demonstrates its superiority over existing baselines.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets, which is a significant improvement over existing methods.\n2. The bi-directional binding block is a novel and effective mechanism for binding compositional user interests, which is crucial for capturing the complexity of user preferences.\n3. The paper presents a thorough analysis of the proposed model's performance on real-world datasets and demonstrates its superiority over existing baselines.\n\nWeaknesses: \n\n1. The paper assumes a uniform facet distribution, which may not be the case in real-world scenarios. This assumption may limit the applicability of the proposed model.\n2. The paper does not provide a detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n3. The paper does not provide a comprehensive comparison of the proposed model's performance with other state-of-the-art recommendation models.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n2. The paper should provide a comprehensive comparison of the proposed model's performance with other state-of-the-art recommendation models.\n3. The paper should consider incorporating additional data-driven methods for discovering the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets and binding compositional user interests. The bi-directional binding block is a novel and effective mechanism for binding user interests, which is crucial for capturing the complexity of user preferences. The paper presents a thorough analysis of the proposed model's performance on real-world datasets and demonstrates its superiority over existing baselines. However, the paper assumes a uniform facet distribution and does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance. \n\n$$$\nPaper Summary: \nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models. FACETVAE disentangles item space under multiple facets, binds compositional user interests from low-level ones discovered from item space, and effectively binds user interests via a bi-directional binding block.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n2. The bi-directional binding block is a novel and effective mechanism for binding compositional user interests.\n3. The paper presents a thorough analysis of the proposed model's performance on real-world datasets.\n\nWeaknesses: \n\n1. The paper assumes a uniform facet distribution, which may not be the case in real-world scenarios.\n2. The paper does not provide a comprehensive comparison with other state-of-the-art recommendation models.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n2. The paper should consider incorporating additional data-driven methods for discovering the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets and binding compositional user interests. The bi-directional binding block is a novel and effective mechanism for binding user interests. The paper presents a thorough analysis of the proposed model's performance on real-world datasets. However, the paper assumes a uniform facet distribution and does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance.\n\n$$$\nPaper Summary: \n\nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n2. The bi-directional binding block is a novel and effective mechanism for binding compositional user interests.\n\nWeaknesses: \n\n1. The paper assumes a uniform facet distribution, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n2. The paper should consider incorporating additional data-driven methods for discovering the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets and binding compositional user interests. The bi-directional binding block is a novel and effective mechanism for binding user interests. The paper assumes a uniform facet distribution and does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance.\n\n$$$\nPaper Summary: \n\nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n\nWeaknesses: \n\n1. The paper assumes a uniform facet distribution, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets. The paper assumes a uniform facet distribution and does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance. \n\n$$$\nPaper Summary: \n\nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n\nWeaknesses: \n\n1. The paper does not provide a comprehensive comparison with other state-of-the-art recommendation models.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n2. The paper should consider incorporating additional data-driven methods for discovering the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets. The paper does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance.\n\n$$$\nPaper Summary: \n\nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n\nWeaknesses: \n\n1. The paper assumes a uniform facet distribution.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets. The paper assumes a uniform facet distribution and does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance.\n\n$$$\nPaper Summary: \n\nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n\nWeaknesses: \n\n1. The paper does not provide a comprehensive comparison with other state-of-the-art recommendation models.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n\nRecommendation: \n\nThe paper proposes a novel and insightful approach to disentangling item space under multiple facets. The paper does not provide a comprehensive comparison with other state-of-the-art recommendation models. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive evaluation of the proposed model's performance.\n\n$$$\nPaper Summary: \n\nThe paper proposes FACETVAE, a novel VAE-based recommendation model that addresses the shortcomings of existing disentangled recommendation models.\n\nStrengths: \n\n1. The paper proposes a novel and insightful approach to disentangling item space under multiple facets.\n\nWeaknesses: \n\n1. The paper assumes a uniform facet distribution.\n\nSuggestions for Improvement: \n\n1. The paper should provide a more detailed analysis of the sensitivity of the proposed model's performance to the number of facets and the number of clusters per facet.\n\nRecommend",
        "gr_score_with": 0.4847328244274809,
        "gr_score_without": 0.40863787375415284,
        "or_score_with": 0.45038167938931295,
        "or_score_without": 0.4850498338870432,
        "output_without": {
            "num_tokens_scored": 301,
            "num_green_tokens": 123,
            "green_fraction": 0.40863787375415284,
            "num_orange_tokens": 146,
            "orange_fraction": 0.4850498338870432,
            "green_z_score": 0.3059035234677569,
            "orange_z_score": 101.68914531374149,
            "green_p_value": 0.37983905157218345,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 262,
            "num_green_tokens": 127,
            "green_fraction": 0.4847328244274809,
            "num_orange_tokens": 118,
            "orange_fraction": 0.45038167938931295,
            "green_z_score": 2.799604661294515,
            "orange_z_score": 87.9969226878529,
            "green_p_value": 0.0025582613473458713,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MAKING PRE-TRAINED LANGUAGE MODELS GREAT",
        "abstract": "The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates feature values with the corresponding feature names. Comprehensive experiments demonstrate that our pre-trained TP-BERTa leads the performance among tabular DNNs and is competitive with Gradient Boosted Decision Tree models in typical tabular data regime.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper presents TP-BERTa, a pre-trained language model specifically designed for tabular data prediction tasks. The model utilizes a novel relative magnitude tokenization approach to represent numerical values in the language space and an intra-feature attention module to integrate feature names and values. The authors evaluate TP-BERTa on 145 downstream datasets and compare its performance with various existing tabular prediction models, including Gradient Boosted Decision Trees (GBDTS). The results show that TP-BERTa outperforms other tabular DNNs and is competitive with GBDTs in the typical tabular data regime.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The paper introduces a novel relative magnitude tokenization approach to represent numerical values in the language space, which enables the model to understand relative magnitudes of values.\n2. The intra-feature attention module effectively integrates feature names and values, preserving the semantic signal of feature names.\n3. TP-BERTa demonstrates unprecedented progress over various non-LM DNNs and is competitive with GBDTs under the typical tabular prediction regime.\n4. The paper provides a comprehensive evaluation of TP-BERTa on 145 downstream datasets, covering various task types and feature distributions.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The model's performance is currently limited to tables dominated by categorical features, and some tables prefer a small magnitude token number.\n2. The paper does not explore the limitations of TP-BERTa in handling large-scale tabular data or high-dimensional feature spaces.\n3. The authors do not discuss the potential biases in the magnitude-aware triplet loss function or the impact of removing position encoding for value vectors.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. Investigate the limitations of TP-BERTa in handling large-scale tabular data or high-dimensional feature spaces.\n2. Explore the potential biases in the magnitude-aware triplet loss function and the impact of removing position encoding for value vectors.\n3. Investigate the use of TP-BERTa for multi-task learning or transfer learning to improve its performance on diverse tabular data regimes.\n4. Develop more efficient algorithms for training TP-BERTa, such as parallelizing the pre-training process or using more efficient optimization methods.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nThe paper presents a significant contribution to the field of tabular data prediction, introducing a novel relative magnitude tokenization approach and an intra-feature attention module to improve the performance of pre-trained language models on tabular data. The results demonstrate the effectiveness of TP-BERTa in handling tabular prediction tasks, and the authors provide a comprehensive evaluation of the model on 145 downstream datasets. However, further research is needed to explore the limitations of TP-BERTa and to improve its performance on diverse tabular data regimes.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThis paper proposes the Tabular Prediction adapted BERT approach (TP-BERTa), a pre-trained language model specifically designed for tabular data prediction tasks. TP-BERTa overcomes the challenges of handling continuous numerical values in tables by using relative magnitude tokenization and an intra-feature attention module. Experimental results show that TP-BERTa outperforms classical and advanced deep tabular models, as well as gradient boosted decision tree (GBDT) models, on a wide range of tabular data regimes. \n\nStrengths:\n1. Novel approach to handle numerical features in tables by using relative magnitude tokenization and intra-feature attention module.\n2. Experimental results show the potential of pre-trained language models in handling tabular data prediction tasks.\n3. TP-BERTa is competitive with gradient boosted decision tree (GBDT) models on typical tabular data regimes.\n\nWeaknesses:\n1. Limited comparison of TP-BERTa with other deep tabular models in the paper.\n2. The impact of the magnitude token number on the performance of TP-BERTa is not thoroughly analyzed.\n3. There is no discussion on the generalizability of TP-BERTa across different domains and datasets.\n\nSuggestions for Improvement:\n1. Conduct a more comprehensive comparison of TP-BERTa with other deep tabular models to demonstrate its superiority.\n2. Analyze the impact of the magnitude token number on the performance of TP-BERTa and discuss potential ways to optimize it.\n3. Discuss the generalizability of TP-BERTa across different domains and datasets to ensure its applicability in real-world tabular data prediction tasks.\n\nRecommendation:\nTP-BERTa is a promising approach for tabular data prediction tasks, and its experimental results demonstrate its potential in handling numerical features in tables. However, further analysis and comparison with other deep tabular models are needed to fully establish its superiority. Additionally, exploring ways to optimize the magnitude token number and discuss the generalizability of TP-BERTa across different domains and datasets can further enhance its applicability and robustness. \n\n $$$",
        "gr_score_with": 0.48846153846153845,
        "gr_score_without": 0.3954154727793696,
        "or_score_with": 0.43846153846153846,
        "or_score_without": 0.5272206303724928,
        "output_without": {
            "num_tokens_scored": 349,
            "num_green_tokens": 138,
            "green_fraction": 0.3954154727793696,
            "num_orange_tokens": 184,
            "orange_fraction": 0.5272206303724928,
            "green_z_score": -0.17482423975010522,
            "orange_z_score": 108.454881244316,
            "green_p_value": 0.5693911294574164,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 260,
            "num_green_tokens": 127,
            "green_fraction": 0.48846153846153845,
            "num_orange_tokens": 114,
            "orange_fraction": 0.43846153846153846,
            "green_z_score": 2.9116256803655416,
            "orange_z_score": 77.60590760969583,
            "green_p_value": 0.001797766139345252,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LOCAL COMPOSITE SADDLE POINT OPTIMIZATION",
        "abstract": "Distributed optimization (DO) approaches for saddle point problems (SPP) have recently gained in popularity due to the critical role they play in machine learning (ML). Existing works mostly target smooth unconstrained objectives in Euclidean space, whereas ML problems often involve constraints or non-smooth regularization, which results in a need for composite optimization. Moreover, although non-smooth regularization often serves to induce structure (e.g., sparsity), standard aggregation schemes in distributed optimization break this structure. Addressing these issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-step primal-dual algorithm with local updates, which is the first of its kind to encompass both saddle point optimization and composite objectives under the distributed paradigm. Using a generalized notion of Bregman divergence, we analyze its convergence and communication complexity in the homogeneous setting. Furthermore, the empirical evaluation demonstrates the effectiveness of FeDualEx for inducing structure in these challenging settings.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes Federated Dual Extrapolation (FeDualEx), a distributed optimization algorithm for composite saddle point problems (SPP) that can handle both smooth and non-smooth regularization terms. The algorithm is designed to induce structure in the solution, such as sparsity or low-rankness, by leveraging dual aggregation. The authors analyze the convergence rate of FeDualEx under the homogeneous setting and provide a theoretical guarantee for its performance. They also demonstrate the effectiveness of FeDualEx through numerical experiments on various composite SPP problems.\n\nStrengths:\n\n* The paper provides a novel approach to distributed optimization of composite SPP, which is a challenging problem in machine learning.\n* The authors provide a detailed analysis of the convergence rate of FeDualEx, which is a key aspect of the paper.\n* The numerical experiments demonstrate the effectiveness of FeDualEx in inducing structures in the solution.\n\nWeaknesses:\n\n* The paper assumes that the optimization domain is compact with respect to Bregman divergence, which may not always be the case in practice.\n* The authors do not provide a detailed analysis of the communication complexity of FeDualEx, which is an important aspect of distributed optimization.\n* The paper does not provide a comparison with other state-of-the-art algorithms for distributed optimization of composite SPP.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the communication complexity of FeDualEx.\n* They should provide a comparison with other state-of-the-art algorithms for distributed optimization of composite SPP.\n* They should consider the heterogeneous federated setting of composite SPP, which is a challenging direction for future work.\n\nRecommendation:\n\n* Based on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a top-tier conference in machine learning or optimization.\n* The authors should address the weaknesses and suggestions for improvement to strengthen the paper.\n\n$$$",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$The paper presents Federated Dual Extrapolation (FeDualEx), an algorithm for distributed saddle point optimization with composite objectives, which is the first to address this challenge in the distributed paradigm. The authors analyze the convergence rate in the homogeneous setting and demonstrate the effectiveness of FeDualEx in inducing structures.\n\nStrengths:\n\n1.  FeDualEx is the first algorithm to address the challenges of composite saddle point optimization in the distributed paradigm.\n2.  The authors provide a thorough analysis of the convergence rate in the homogeneous setting.\n3.  The algorithm is designed to induce structures, such as sparsity, in the aggregated solution.\n\nWeaknesses:\n\n1.  The authors rely heavily on theoretical analysis, which may limit the generalizability of the results.\n2.  The algorithm's performance in the heterogeneous setting is not thoroughly analyzed.\n3.  The authors do not provide extensive experimental results to support the theoretical claims.\n\nSuggestions for improvement:\n\n1.  Provide more extensive experimental results to support the theoretical claims.\n2.  Investigate the algorithm's performance in the heterogeneous setting.\n3.  Consider the limitations of the algorithm and the assumptions made in the analysis.\n\nRecommendation:\n\n1.  Based on the theoretical analysis, FeDualEx appears to be an effective algorithm for distributed saddle point optimization with composite objectives.\n2.  However, the authors should provide more extensive experimental results to support the theoretical claims.\n3.  Investigating the algorithm's performance in the heterogeneous setting would also be beneficial.\n\n$$\\begin{aligned}\n$$\n\n$$The paper presents Federated Dual Extrapolation (FeDualEx), an algorithm for distributed saddle point optimization with composite objectives, which is the first to address this challenge in the distributed paradigm. The authors analyze the convergence rate in the homogeneous setting and demonstrate the effectiveness of FeDualEx in inducing structures.\n\nThe authors present FeDualEx as an algorithm for distributed saddle point optimization with composite objectives, which is the first to address this challenge in the distributed paradigm. The authors analyze the convergence rate in the homogeneous setting and demonstrate the effectiveness of FeDualEx in inducing structures.\n\n$$The authors rely heavily on theoretical analysis, which may limit the generalizability of the results. The algorithm's performance in the heterogeneous setting is not thoroughly analyzed. The authors do not provide extensive experimental results to support the theoretical claims.\n\nThe authors should provide more extensive experimental results to support the theoretical claims. They should also investigate the algorithm's performance in the heterogeneous setting.\n\n$$The authors demonstrate the effectiveness of FeDualEx in inducing structures with empirical evaluation. The sequential version of FeDualEx provides a solution to composite stochastic saddle point optimization in the nonEuclidean setting.\n\n$$The authors advance distributed optimization to the broad class of composite saddle point problems by proposing FeDualEx and providing, to our knowledge, the first convergence rate of its kind. The authors demonstrate the effectiveness of FeDualEx for inducing structures with empirical evaluation. They also show that the sequential version of FeDualEx provides a solution to composite stochastic saddle point optimization in the nonEuclidean setting.\n\n$$The authors recognize further study of the heterogeneous federated setting of composite saddle point optimization would be a challenging direction for future work.\n\n$$In summary, the authors present a new algorithm for distributed saddle point optimization with composite objectives, which is the first to address this challenge in the distributed paradigm. They demonstrate the effectiveness of FeDualEx in inducing structures with empirical evaluation and advance distributed optimization to the broad class of composite saddle point problems. However, the authors should provide more extensive experimental results to support the theoretical claims and investigate the algorithm's performance in the heterogeneous setting.\n\n$$\n\n$$The authors provide a thorough analysis of the convergence rate in the homogeneous setting, which is a significant contribution to the field of distributed optimization.\n\n$$The authors demonstrate the effectiveness of FeDualEx in inducing structures with empirical evaluation, which is an important aspect of the algorithm's design.\n\n$$The sequential version of FeDualEx provides a solution to composite stochastic saddle point optimization in the nonEuclidean setting, which is an interesting direction for future work.\n\n$$The authors advance distributed optimization to the broad class of composite saddle point problems by proposing FeDualEx, which is a significant contribution to the field.\n\n$$The authors should provide more extensive experimental results to support the theoretical claims.\n\n$$The authors should also investigate the algorithm's performance in the heterogeneous setting.\n\n$$The authors should consider the limitations of the algorithm and the assumptions made in the analysis.\n\n$$The authors should also explore the applications of FeDualEx in other fields, such as deep learning and computer vision.\n\n$$The authors should also consider the scalability of FeDualEx for large-scale distributed systems.\n\n$$The authors should also explore the use of FeDualEx for federated learning and other applications of distributed optimization.\n\n$$The authors should also consider the robustness of FeDualEx against adversarial attacks and other types of noise.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the applications of FeDualEx in other fields, such as computer vision and natural language processing.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the scalability of FeDualEx for large-scale distributed systems.\n\n$$The authors should also explore the use of FeDualEx for federated learning and other applications of distributed optimization.\n\n$$The authors should also consider the robustness of FeDualEx against adversarial attacks and other types of noise.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the applications of FeDualEx in other fields, such as computer vision and natural language processing.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the applications of FeDualEx in other fields, such as computer vision and natural language processing.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the scalability of FeDualEx for large-scale distributed systems.\n\n$$The authors should also explore the use of FeDualEx for federated learning and other applications of distributed optimization.\n\n$$The authors should also consider the robustness of FeDualEx against adversarial attacks and other types of noise.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the applications of FeDualEx in other fields, such as computer vision and natural language processing.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the scalability of FeDualEx for large-scale distributed systems.\n\n$$The authors should also explore the use of FeDualEx for federated learning and other applications of distributed optimization.\n\n$$The authors should also consider the robustness of FeDualEx against adversarial attacks and other types of noise.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the applications of FeDualEx in other fields, such as computer vision and natural language processing.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the scalability of FeDualEx for large-scale distributed systems.\n\n$$The authors should also explore the use of FeDualEx for federated learning and other applications of distributed optimization.\n\n$$The authors should also consider the robustness of FeDualEx against adversarial attacks and other types of noise.\n\n$$The authors should also explore the use of FeDualEx for stochastic optimization and other applications of distributed optimization.\n\n$$The authors should also consider the theoretical limitations of FeDualEx and the assumptions made in the analysis.\n\n$$The authors should also explore the use of FeDualEx for reinforcement learning and other applications of distributed optimization.\n\n$$The authors should also consider the applications of FeDualEx in other fields, such as computer vision and natural",
        "gr_score_with": 0.6005361930294906,
        "gr_score_without": 0.4714828897338403,
        "or_score_with": 0.36193029490616624,
        "or_score_without": 0.4372623574144487,
        "output_without": {
            "num_tokens_scored": 263,
            "num_green_tokens": 124,
            "green_fraction": 0.4714828897338403,
            "num_orange_tokens": 115,
            "orange_fraction": 0.4372623574144487,
            "green_z_score": 2.366324793672843,
            "orange_z_score": 84.55587313094486,
            "green_p_value": 0.008982837781535729,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 373,
            "num_green_tokens": 224,
            "green_fraction": 0.6005361930294906,
            "num_orange_tokens": 135,
            "orange_fraction": 0.36193029490616624,
            "green_z_score": 7.905722410224416,
            "orange_z_score": 83.07297164277394,
            "green_p_value": 1.3319209711875025e-15,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FINE-TUNED LANGUAGE MODELS GENERATE STABLE INORGANIC MATERIALS",
        "abstract": "We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting\u2019s inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models\u2019 ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.",
        "peer_review_without_watermark": "$$\n\nThe paper proposes a novel approach to generating stable inorganic materials using fine-tuned large language models (LLMs). The authors fine-tune LLaMA-2 models on text-encoded atomistic data and show that the strongest model can generate materials predicted to be metastable at a higher rate than a competing diffusion model.\n\nStrengths:\n1. The authors demonstrate the effectiveness of fine-tuning LLMs on atomistic data, which is a novel approach to materials generation.\n2. The paper highlights the ability of LLMs to capture key symmetries of crystal structures, which is essential for generating stable materials.\n3. The authors propose a new metric, IPT, to assess the invariance of language models to continuous group transformations, which is a useful contribution to the field.\n\nWeaknesses:\n1. The paper relies heavily on the fine-tuning approach, which may not be generalizable to other materials generation tasks.\n2. The authors do not provide a thorough comparison with other state-of-the-art methods, such as CDVAE, which may limit the applicability of their approach.\n3. The paper does not discuss the potential biases and limitations of LLMs in generating materials, which is an important consideration for materials discovery.\n\nSuggestions for Improvement:\n1. The authors should provide a more comprehensive comparison with other state-of-the-art methods to demonstrate the superiority of their approach.\n2. The paper should discuss the potential biases and limitations of LLMs in generating materials and propose strategies to mitigate these limitations.\n3. The authors should explore the use of other metrics, such as Ehull, to evaluate the generated materials and provide a more detailed analysis of the results.\n\nRecommendation:\nThe paper presents an innovative approach to generating stable inorganic materials using fine-tuned LLMs. While there are some limitations and areas for improvement, the authors demonstrate the effectiveness of their approach and propose a new metric to assess the invariance of language models. With further development and refinement, this approach has the potential to make significant contributions to materials discovery.",
        "peer_review_with_watermark": "$$\\boxed{\\text{Paper Summary:}}\n$$ The paper proposes a novel approach to generating stable inorganic materials using fine-tuned large language models. The method involves encoding crystal structures as text and fine-tuning a pre-trained language model to predict stable materials. The authors demonstrate that their approach can generate materials predicted to be metastable at a higher rate than a competing diffusion model. They also show that the learned symmetry properties of the model improve with model scale.\n\n$$\\boxed{\\text{Strengths:}}\n$$ 1. The paper presents a novel approach to materials generation using language models, which has the potential to be more efficient and scalable than traditional methods.\n2. The authors demonstrate the ability of their method to generate materials predicted to be metastable at a higher rate than a competing diffusion model.\n3. The paper highlights the potential of language models to capture key symmetries of crystal structures, which is a crucial aspect of materials discovery.\n\n$$\\boxed{\\text{Weaknesses:}}\n$$ 1. The paper does not provide a detailed analysis of the limitations of the method, such as the potential for hallucination or the need for large amounts of training data.\n2. The paper assumes that the learned symmetry properties of the model are a key factor in its ability to generate stable materials, but it does not provide a detailed analysis of this assumption.\n3. The paper does not provide a comparison with other state-of-the-art methods for materials generation, which limits its ability to demonstrate the novelty and effectiveness of its approach.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}\n$$ 1. The authors should provide a more detailed analysis of the limitations of the method and discuss potential ways to address these limitations.\n2. The authors should provide a more detailed analysis of the learned symmetry properties of the model and discuss their implications for materials discovery.\n3. The authors should compare their method with other state-of-the-art methods for materials generation to demonstrate its novelty and effectiveness.\n\n$$\\boxed{\\text{Recommendation:}}\n$$ The paper presents a novel approach to materials generation using language models, which has the potential to be more efficient and scalable than traditional methods. While there are some limitations to the method, it demonstrates a promising approach to materials discovery. With further refinement and analysis, this method could have a significant impact on the field of materials science.",
        "gr_score_with": 0.47719298245614034,
        "gr_score_without": 0.40955631399317405,
        "or_score_with": 0.47368421052631576,
        "or_score_without": 0.5255972696245734,
        "output_without": {
            "num_tokens_scored": 293,
            "num_green_tokens": 120,
            "green_fraction": 0.40955631399317405,
            "num_orange_tokens": 154,
            "orange_fraction": 0.5255972696245734,
            "green_z_score": 0.333901677025384,
            "orange_z_score": 84.1386162725039,
            "green_p_value": 0.3692268771317933,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 285,
            "num_green_tokens": 136,
            "green_fraction": 0.47719298245614034,
            "num_orange_tokens": 135,
            "orange_fraction": 0.47368421052631576,
            "green_z_score": 2.6600795837367097,
            "orange_z_score": 74.60942048909436,
            "green_p_value": 0.003906109490463643,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "BADEXPERT: EXTRACTING BACKDOOR FUNCTIONAL- ITY FOR ACCURATE BACKDOOR INPUT DETECTION",
        "abstract": "In this paper, we present a novel defense against backdoor attacks on deep neural networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. Our proposed defense is built upon an intriguing concept: given a backdoored model, we reverse engineer it to directly extract its backdoor functionality to a backdoor expert model. To accomplish this, we finetune the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising robust backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB, and ImageNet) across multiple model architectures (ResNet, VGG, MobileNetV2, and Vision Transformer). Our code is integrated into our research toolbox: https://github.com/vtu81/backdoor-toolbox. Untrusted Source Incorrect Labels Small Reserved Clean Set Recognize Forgot Clean Task Backdoor Task Backdoored Model M Backdoor Expert B Finetuning Figure 1: Extracting backdoor functionality via finetuning on a mislabeled small clean set. The backdoored modelM can correctly recognize both benign and poisoned samples whereas our backdoor expert model B can only recognize backdoor samples.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:}$$\nThe paper proposes a novel defense strategy, BaDExpert, against backdoor attacks on deep neural networks (DNNs). The defense operates independently of the model development and relies on post-development defenses. The approach involves extracting the backdoor functionality from the compromised model into a designated backdoor expert model. This extraction is achieved by leveraging a simple yet effective insight: finetuning the backdoor model on a set of intentionally mislabeled reserved clean samples allows the model to erase its normal functionality while preserving the backdoor functionality. The extracted backdoor expert model is then utilized to devise an accurate and resilient detector for backdoor inputs during inference-time.\n\n$$\\textbf{Strengths:}$$\n1. The paper introduces a novel defense strategy against backdoor attacks on DNNs.\n2. The approach relies on post-development defenses, which can be more effective than development-stage defenses.\n3. The method of extracting backdoor functionality from the compromised model is simple and effective.\n4. The paper presents a comprehensive evaluation of BaDExpert on various datasets and model architectures.\n\n$$\\textbf{Weaknesses:}$$\n1. The paper assumes that the backdoor expert model can be extracted without any inductive bias on the types of backdoor triggers, which might not always be the case.\n2. The approach relies on the assumption that the backdoor functionality can be preserved during finetuning, which might not always be the case.\n3. The paper does not provide a comprehensive evaluation of the robustness of BaDExpert against adaptive attacks.\n4. The evaluation of BaDExpert is limited to a few datasets and model architectures, which might not be representative of the entire DNN landscape.\n\n$$\\textbf{Suggestions for Improvement:}$$\n1. Provide a more comprehensive evaluation of BaDExpert against various datasets and model architectures.\n2. Investigate the robustness of BaDExpert against adaptive attacks.\n3. Consider incorporating more advanced techniques to mitigate the limitations of the current approach.\n4. Provide a more detailed explanation of the assumptions made by the approach and their implications.\n\n$$\\textbf{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper for further evaluation. The paper presents a novel defense strategy against backdoor attacks on DNNs, but there are some limitations and assumptions that need to be addressed. With further revisions, the paper has the potential to contribute significantly to the field of DNN security.",
        "peer_review_with_watermark": "$$\\begin{boxed}\n\n$$Paper Summary: \nThe paper proposes a novel defense strategy against backdoor attacks on deep neural networks (DNNs), known as BaDExpert. The defense strategy is based on the concept of extracting the backdoor functionality from a compromised model and using it to construct a backdoor expert model that can detect backdoor inputs during inference time. The proposed method is compared against existing backdoor defense strategies and is shown to be effective across different attacks, datasets, and model architectures.\n\n$$Strengths:\n1. The proposed method is novel and different from existing backdoor defense strategies.\n2. The method is based on an interesting concept of extracting backdoor functionality from a compromised model.\n3. The method is compared against existing backdoor defense strategies, providing a comprehensive evaluation of its effectiveness.\n4. The paper provides an extensive evaluation of the method across different attacks, datasets, and model architectures.\n5. The method is adaptable to different scenarios, including adaptive attacks.\n\n$$Weaknesses:\n1. The method may not be effective against attacks that do not rely on backdoor triggers.\n2. The method may require significant computational resources to extract the backdoor functionality.\n3. The method may not be suitable for models with high dimensionality or large datasets.\n4. The method may not provide a clear understanding of the underlying backdoor attacks.\n5. The method may not be effective against attacks that use multiple triggers.\n\n$$Suggestions for Improvement:\n1. Investigate the effectiveness of the method against attacks that do not rely on backdoor triggers.\n2. Optimize the computational resources required to extract the backdoor functionality.\n3. Investigate the method's effectiveness in high-dimensional models and large datasets.\n4. Provide a clear understanding of the underlying backdoor attacks.\n5. Investigate the method's effectiveness against attacks that use multiple triggers.\n\n$$Recommendation:\nThe proposed method, BaDExpert, is a novel and effective defense strategy against backdoor attacks on DNNs. While it has some limitations, it is adaptable to different scenarios and provides a comprehensive evaluation of its effectiveness across different attacks, datasets, and model architectures. The method is recommended for use in scenarios where the risk of backdoor attacks is high, and the benefits of its adaptability and effectiveness outweigh its limitations.\n\n$$$$",
        "gr_score_with": 0.4927536231884058,
        "gr_score_without": 0.35693215339233036,
        "or_score_with": 0.4166666666666667,
        "or_score_without": 0.5309734513274337,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 121,
            "green_fraction": 0.35693215339233036,
            "num_orange_tokens": 180,
            "orange_fraction": 0.5309734513274337,
            "green_z_score": -1.6186292560784985,
            "orange_z_score": 106.56097636440707,
            "green_p_value": 0.9472364717806142,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 276,
            "num_green_tokens": 136,
            "green_fraction": 0.4927536231884058,
            "num_orange_tokens": 115,
            "orange_fraction": 0.4166666666666667,
            "green_z_score": 3.145428397971138,
            "orange_z_score": 75.12595342410712,
            "green_p_value": 0.0008292189263926783,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LLMS MEET VLMS: BOOST OPEN VOCABULARY OB-",
        "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n\\text{Paper Summary:} \n\\text{The paper proposes DVDet, a Descriptor-Enhanced Open Vocabulary Detection framework that leverages the zero-shot capabilities of Vision Language Models (VLMs) to improve open-vocabulary object detection. The framework introduces a Conditional Context Prompt (CCP) that transforms regional embeddings into image-like representations and a hierarchical descriptor generation mechanism that iteratively interacts with Large Language Models (LLMs) to mine and refine fine-grained descriptors. The experiments demonstrate that DVDet outperforms the state-of-the-art consistently on multiple large-scale benchmarks.}\n\n\\text{Strengths:}\n1. The paper introduces a novel approach to open-vocabulary object detection that leverages the zero-shot capabilities of VLMs.\n2. The Conditional Context Prompt (CCP) is a novel idea that transforms regional embeddings into image-like representations.\n3. The hierarchical descriptor generation mechanism is effective in mining and refining fine-grained descriptors.\n4. The experiments demonstrate that DVDet outperforms the state-of-the-art consistently on multiple large-scale benchmarks.\n\n\\text{Weaknesses:}\n1. The paper does not provide a clear explanation of the underlying mechanism of the Conditional Context Prompt (CCP).\n2. The hierarchical descriptor generation mechanism is complex and difficult to understand.\n3. The paper does not provide a detailed analysis of the results and the experiments were not thoroughly explained.\n4. The paper does not provide a clear evaluation of the effectiveness of the proposed approach.\n\n\\text{Suggestions for Improvement:}\n1. Provide a clear explanation of the underlying mechanism of the Conditional Context Prompt (CCP).\n2. Simplify the hierarchical descriptor generation mechanism and provide a detailed analysis of the results.\n3. Provide a thorough explanation of the experiments and the evaluation of the proposed approach.\n4. Provide a clear evaluation of the effectiveness of the proposed approach.\n\n\\text{Recommendation:}\nBased on the paper summary, strengths, weaknesses, and suggestions for improvement, I recommend that the paper be accepted for publication in a reputable conference or journal in the field of computer vision and machine learning. However, the paper needs to be revised based on the suggestions for improvement to provide a clear and thorough explanation of the underlying mechanism, the experiments, and the evaluation of the proposed approach.\n\n$$\\text{Rating:} 8/10$$",
        "peer_review_with_watermark": "$$$ Paper Summary:\nThe paper proposes a novel approach for open-vocabulary object detection called Descriptor-Enhanced Open Vocabulary Detection (DVDet). The approach utilizes a Conditional Context Prompt (CCP) that transforms regional embeddings into image-like representations and a hierarchical descriptor generation mechanism that iteratively interacts with large language models (LLMs) to refine fine-grained descriptors. The authors evaluate the proposed method on two large-scale benchmarks (COCO and LVIS) and demonstrate its effectiveness compared to state-of-the-art open-vocabulary object detection methods.\n\nStrengths:\n1. The proposed method introduces a novel approach to open-vocabulary object detection that leverages the capabilities of vision language models (VLMs) to enhance the performance of existing detectors.\n2. The Conditional Context Prompt (CCP) mechanism effectively transforms regional embeddings into image-like representations, enabling precise region-text alignment.\n3. The hierarchical descriptor generation mechanism iteratively interacts with LLMs to refine fine-grained descriptors, addressing the limitation of existing methods that rely on coarse and category-level alignment knowledge.\n\nWeaknesses:\n1. The authors rely on the effectiveness of the proposed method on two large-scale benchmarks, but the analysis of the results could be more comprehensive, including a more detailed comparison with other state-of-the-art methods.\n2. The paper assumes that the large language models (LLMs) used for descriptor generation are well-trained and have access to a large corpus of text data. However, the authors do not discuss the potential challenges of using LLMs for this task, such as data sparsity or the need for significant computational resources.\n3. The authors mention that the proposed method can be used as a general plugin for existing open-vocabulary object detectors, but they do not discuss the potential challenges of adapting the method to different architectures or datasets.\n\nSuggestions for Improvement:\n1. The authors could provide a more comprehensive analysis of the results, including a more detailed comparison with other state-of-the-art methods.\n2. The authors should discuss the potential challenges of using large language models for descriptor generation and provide suggestions for addressing these challenges.\n3. The authors could explore the potential benefits of using the proposed method for other open-vocabulary dense prediction tasks, such as image captioning or question answering.\n\nRecommendation:\nBased on the paper's contribution to the field of open-vocabulary object detection and the effectiveness of the proposed method on two large-scale benchmarks, I recommend that the paper be published in a top-tier conference or journal in computer vision or artificial intelligence. The authors could also be considered for an oral presentation or a demo session to further showcase the method's capabilities. Overall, the paper presents a novel and effective approach to open-vocabulary object detection that has the potential to be widely adopted in the field.",
        "gr_score_with": 0.5404699738903395,
        "gr_score_without": 0.3992932862190813,
        "or_score_with": 0.412532637075718,
        "or_score_without": 0.44876325088339225,
        "output_without": {
            "num_tokens_scored": 283,
            "num_green_tokens": 113,
            "green_fraction": 0.3992932862190813,
            "num_orange_tokens": 127,
            "orange_fraction": 0.44876325088339225,
            "green_z_score": -0.02426784190581485,
            "orange_z_score": 91.8349272150584,
            "green_p_value": 0.5096805179927422,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 383,
            "num_green_tokens": 207,
            "green_fraction": 0.5404699738903395,
            "num_orange_tokens": 158,
            "orange_fraction": 0.412532637075718,
            "green_z_score": 5.611478041813143,
            "orange_z_score": 98.08103316334937,
            "green_p_value": 1.0030287339173997e-08,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SHARING RATIO DECOMPOSITION",
        "abstract": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to faithfully represent the decision-making process of complex deep learning models. The proposed method analyzes the model using Pointwise Feature Vectors (PFVs) and decomposes relevance with sharing ratios. It reflects the model's inference process and relies exclusively on model-generated data, resulting in robust and trustworthy explanations.\n\n$$\\text{Strengths:}$$\n1. The proposed method reflects the model's decision-making process faithfully, resulting in robust and trustworthy explanations.\n2. It adopts a vector perspective to consider nonlinear interactions between filters, departing from conventional neuron-level analyses.\n3. The method identifies Activation-Pattern-Only Prediction (APOP), highlighting the influence of inactive neurons in model behavior, which has been overlooked so far by conventional approaches.\n4. The recursive decomposition of PFVs provides high-resolution Effective Receptive Fields (ERFs) at any layer.\n\n$$\\text{Weaknesses:}$$\n1. The paper lacks a comprehensive discussion on the computational complexity of the proposed method, which might be a concern for large-scale models.\n2. The evaluation of the method on various datasets and architectures is limited, which might not generalize well to other scenarios.\n3. The paper does not provide a detailed explanation of the sharing ratio calculation and its significance in the context of SRD.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a more detailed discussion on the computational complexity of the proposed method and potential optimization techniques to address it.\n2. Conduct a more comprehensive evaluation of the method on various datasets and architectures to ensure its generalizability.\n3. Provide a more detailed explanation of the sharing ratio calculation and its significance in the context of SRD.\n4. Consider incorporating more evaluation metrics that focus on the interpretability and transparency of the model's decision-making process.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the proposed method, I recommend that the authors further develop and refine SRD to address the limitations mentioned above. Additionally, they should conduct a more comprehensive evaluation of the method to ensure its generalizability and robustness. With these improvements, SRD has the potential to become a state-of-the-art method for explainable AI.",
        "peer_review_with_watermark": "$$$\\label{peer-review} \n\nPaper Summary:\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in explanations. SRD analyzes the model with Pointwise Feature Vectors and decomposes relevance with sharing ratios, providing a high-resolution Effective Receptive Field (ERF) at any layer.\n\nStrengths:\n1. SRD provides a high-resolution ERF at any layer, which is a significant improvement over conventional methods.\n2. The method reflects the model\u2019s inference process faithfully, relying exclusively on model-generated data, making it robust against adversarial attacks.\n3. SRD accounts for nonlinear interactions between filters, departing from conventional neuron-level analyses.\n4. The identification of Activation-Pattern-Only Prediction (APOP) highlights the influence of inactive neurons in model behavior, which has been overlooked so far.\n\nWeaknesses:\n1. The method requires prior knowledge of the sharing ratio, which can be challenging to obtain, especially in the absence of ground truth information.\n2. The complexity of the method may make it difficult to interpret and visualize the results, especially for large models.\n\nSuggestions for Improvement:\n1. Investigate methods to obtain the sharing ratio without prior knowledge, such as using self-supervised learning or regularization techniques.\n2. Develop more interpretable and visualizable methods to present the results, such as using PCA or t-SNE to reduce the dimensionality of the feature vectors.\n\nRecommendation:\nI recommend that the authors further investigate and validate the proposed method, SRD, on various models and datasets to demonstrate its effectiveness and robustness. Additionally, they should consider developing more interpretable and visualizable methods to present the results, making the method more accessible to a broader range of researchers and practitioners.\n\nOverall, the paper presents a novel XAI method that provides high-resolution ERFs at any layer and reflects the model\u2019s inference process faithfully. With further investigation and validation, SRD has the potential to be a promising direction for local explanation methods.\n\nRating: 8/10\n\n$ \\$ \n\nNote: The rating is based on the quality of the paper, the novelty and relevance of the proposed method, and the potential impact on the field of XAI. The rating is subjective and may vary depending on the reviewer's expertise and biases.",
        "gr_score_with": 0.5782828282828283,
        "gr_score_without": 0.484149855907781,
        "or_score_with": 0.35353535353535354,
        "or_score_without": 0.3976945244956772,
        "output_without": {
            "num_tokens_scored": 347,
            "num_green_tokens": 168,
            "green_fraction": 0.484149855907781,
            "num_orange_tokens": 138,
            "orange_fraction": 0.3976945244956772,
            "green_z_score": 3.1997238113280986,
            "orange_z_score": 89.03168257039155,
            "green_p_value": 0.0006877966871211624,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 396,
            "num_green_tokens": 229,
            "green_fraction": 0.5782828282828283,
            "num_orange_tokens": 140,
            "orange_fraction": 0.35353535353535354,
            "green_z_score": 7.2418827192840745,
            "orange_z_score": 84.36745612197944,
            "green_p_value": 2.2124921619456219e-13,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "PARETO DEEP LONG-TAILED RECOGNITION: A CONFLICT-AVERSE SOLUTION",
        "abstract": "Deep long-tailed recognition (DLTR) has attracted much attention due to its close touch with realistic scenarios. Recent advances have focused on re-balancing across various aspects, e.g., sampling strategy, loss re-weighting, logit adjustment, and input/parameter perturbation, etc. However, few studies have considered dynamic re-balancing to address intrinsic optimization conflicts, which are identified as prevalent and critical issues in this study. In this paper, we empirically establish the severity of the optimization conflict issue in the DLTR scenario, which leads to a degradation of representation learning. This observation serves as the motivation for pursuing Pareto optimal solutions. Unfortunately, a straightforward integration of multi-objective optimization (MOO) with DLTR methods is infeasible due to the disparity between multi-task learning (MTL) and DLTR. Therefore, we propose effective alternatives by decoupling MOO-based MTL from a temporal perspective rather than a structural one. Furthermore, we enhance the integration of MOO and DLTR by investigating the generalization and convergence problems. Specifically, we propose optimizing the variability collapse loss, guided by the derived MOObased DLTR generalization bound, to improve generalization. Additionally, we anticipate worst-case optimization to ensure convergence. Building upon the proposed MOO framework, we introduce a novel method called Pareto deep LOngTailed recognition (PLOT). Extensive evaluations demonstrate that our method not only generally improves mainstream pipelines, but also achieves an augmented version to realize state-of-the-art performance across multiple benchmarks. Code is available at https://github.com/zzpustc/PLOT.",
        "peer_review_without_watermark": "$$$\\begin{align*}\nPaper Summary:\nThe paper proposes a novel approach to address the optimization conflict issue in deep long-tailed recognition (DLTR). The authors empirically establish the severity of the optimization conflict issue in the DLTR scenario and propose a Pareto deep long-tailed recognition (PLOT) method to mitigate this issue. PLOT is a conflict-averse solution that integrates multi-objective optimization (MOO) with DLTR methods. The authors demonstrate the effectiveness of PLOT on various benchmarks and show that it can significantly enhance the performance of mainstream DLTR methods.\n\nStrengths:\n1. The paper provides a thorough analysis of the optimization conflict issue in DLTR and highlights its severity.\n2. The authors propose a novel approach to address this issue, which is a conflict-averse solution.\n3. The PLOT method is demonstrated to be effective on various benchmarks.\n4. The paper provides a detailed analysis of the performance of PLOT on different datasets and scenarios.\n\nWeaknesses:\n1. The paper may be too technical and may not be accessible to non-experts in the field.\n2. Some of the explanations and derivations may be unclear or require additional context.\n3. The paper does not provide a comprehensive comparison with other state-of-the-art methods.\n4. The authors may not have considered all possible variants or extensions of the PLOT method.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation of the optimization conflict issue and its implications for DLTR methods.\n2. The paper should include more comparisons with other state-of-the-art methods to demonstrate the effectiveness of PLOT.\n3. The authors should provide more detailed analysis of the performance of PLOT on different datasets and scenarios.\n4. The paper should include more discussion on the potential variants or extensions of the PLOT method.\n\nRecommendation:\nI recommend that the paper be published in a top-tier conference or journal in the field of machine learning or computer vision. The authors have made significant contributions to the field of DLTR and have proposed a novel approach to address the optimization conflict issue. With some minor revisions to address the weaknesses and suggestions for improvement, the paper has the potential to be a leading publication in the field.\n\nRating: 8/10\n\nNote: The rating is based on the overall quality of the paper, the significance of the contributions, and the clarity of the presentation. The rating is subjective and may vary depending on the reviewer's expertise and experience.",
        "peer_review_with_watermark": "$$$Abstract: \nThe paper proposes the concept of \"Pareto deep long-tailed recognition\" (PDLTR), which aims to address the optimization conflicts in deep long-tailed recognition (DLTR) methods. The authors propose a new method called Pareto deep long-tailed recognition (PLOT), which integrates multi-objective optimization (MOO) with DLTR to improve the representation learning of DLTR models. The proposed method is evaluated on several benchmarks and shows state-of-the-art performance.\n\nStrengths: \n1. The paper provides a thorough analysis of the optimization conflicts in DLTR methods and highlights the importance of addressing these conflicts to improve the representation learning of DLTR models.\n2. The proposed method, PLOT, is well-structured and easy to understand, with a clear explanation of the proposed MOO-based DLTR pipeline.\n3. The paper provides extensive experimental results to evaluate the performance of PLOT on several benchmarks, which demonstrates its effectiveness.\n\nWeaknesses: \n1. The paper may benefit from a more detailed explanation of the proposed MOO-based DLTR pipeline, particularly the optimization process and the choice of the MOO algorithm.\n2. The paper may also benefit from a more thorough analysis of the generalization and convergence problems in the proposed method.\n3. The experimental results may be biased towards the chosen benchmarks, and it would be better to evaluate the performance of PLOT on a larger number of benchmarks.\n\nSuggestions for Improvement: \n1. The authors should provide a more detailed explanation of the proposed MOO-based DLTR pipeline, including the optimization process and the choice of the MOO algorithm.\n2. The authors should conduct a more thorough analysis of the generalization and convergence problems in the proposed method to ensure its effectiveness.\n3. The authors should evaluate the performance of PLOT on a larger number of benchmarks to ensure its universality.\n\nRecommendation: \nBased on the paper's contributions and experimental results, I recommend its publication in a top-tier conference in the field of deep learning.\n\n $$$",
        "gr_score_with": 0.5490196078431373,
        "gr_score_without": 0.41818181818181815,
        "or_score_with": 0.4117647058823529,
        "or_score_without": 0.4636363636363636,
        "output_without": {
            "num_tokens_scored": 330,
            "num_green_tokens": 138,
            "green_fraction": 0.41818181818181815,
            "num_orange_tokens": 153,
            "orange_fraction": 0.4636363636363636,
            "green_z_score": 0.674199862463242,
            "orange_z_score": 92.17924738514594,
            "green_p_value": 0.2500921285353972,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 255,
            "num_green_tokens": 140,
            "green_fraction": 0.5490196078431373,
            "num_orange_tokens": 105,
            "orange_fraction": 0.4117647058823529,
            "green_z_score": 4.85744492936668,
            "orange_z_score": 71.80274043139664,
            "green_p_value": 5.945509721370704e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "RAIN: YOUR LANGUAGE MODELS CAN ALIGN THEMSELVES",
        "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%. The code is available at https://github.com/SafeAILab/RAIN.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces a novel inference method called Rewindable Auto-regressive INference (RAIN) that enables frozen large language models (LLMs) to align themselves without finetuning or requiring additional data. RAIN integrates self-evaluation and rewind mechanisms to guide the generation process, allowing LLMs to produce responses consistent with human preferences. The authors evaluate RAIN on various datasets, including the Anthropic's Helpful and Harmless (HH) dataset and the TruthfulQA dataset, and demonstrate its effectiveness in improving the harmlessness and truthfulness of LLMs.\n\nStrengths:\n\n1. Novel approach: The paper proposes a novel approach to aligning frozen LLMs without requiring additional data or finetuning.\n2. Self-evaluation and rewind mechanisms: The authors effectively integrate self-evaluation and rewind mechanisms to guide the generation process, enabling LLMs to produce responses consistent with human preferences.\n3. Experimental evaluation: The paper provides extensive experimental evaluation of RAIN on various datasets, demonstrating its effectiveness in improving the harmlessness and truthfulness of LLMs.\n\nWeaknesses:\n\n1. Complexity: RAIN involves a complex mechanism that may be difficult to understand and implement, particularly for non-experts.\n2. Limited evaluation: While the paper evaluates RAIN on various datasets, the evaluation is limited to specific tasks, such as harm-free generation and truthful generation.\n3. Comparison with existing methods: The paper does not provide a comprehensive comparison with existing alignment methods, such as RLHF and RLAIF.\n\nSuggestions for Improvement:\n\n1. Simplify the mechanism: The authors could simplify the RAIN mechanism to make it easier to understand and implement.\n2. Expand evaluation: The authors could expand the evaluation to include more tasks and datasets, demonstrating the effectiveness of RAIN in a broader range of scenarios.\n3. Comparison with existing methods: The authors could provide a comprehensive comparison with existing alignment methods, highlighting the strengths and weaknesses of RAIN.\n\nRecommendation:\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper with the following changes:\n\n1. Simplify the RAIN mechanism to make it easier to understand and implement.\n2. Expand the evaluation to include more tasks and datasets.\n3. Provide a comprehensive comparison with existing alignment methods.\n\nOverall, the paper presents a novel approach to aligning frozen LLMs without requiring additional data or finetuning, and the authors provide extensive experimental evaluation to demonstrate its effectiveness. With revisions to simplify the mechanism, expand the evaluation, and provide a comprehensive comparison with existing methods, the paper has the potential to make a significant contribution to the field of natural language processing.",
        "peer_review_with_watermark": "$$$ Paper Summary:\nThe paper proposes a novel inference method called Rewindable Auto-regressive INference (RAIN) that enables frozen large language models (LLMs) to align themselves without finetuning. RAIN integrates self-evaluation and rewind mechanisms to produce responses consistent with human preferences. The authors demonstrate the effectiveness of RAIN using experiments on various LLMs and datasets, including the Anthropic\u2019s Helpful and Harmless (HH) dataset and the TruthfulQA dataset. The results show that RAIN improves the harmlessness rate of LLaMA 30B from 82% to 97% and maintains the helpfulness rate. RAIN also demonstrates potential in boosting adversarial robustness under static LLM-ATTACKS. Overall, the paper presents a promising approach for aligning frozen LLMs without requiring additional data or finetuning.\n\nStrengths:\n1. The paper proposes a novel inference method that integrates self-evaluation and rewind mechanisms to enable frozen LLMs to align themselves without finetuning.\n2. The authors demonstrate the effectiveness of RAIN using experiments on various LLMs and datasets.\n3. RAIN shows potential in boosting adversarial robustness under static LLM-ATTACKS.\n4. The paper presents a clear and concise explanation of the RAIN algorithm and its components.\n\nWeaknesses:\n1. The paper does not provide a comprehensive review of existing alignment methods for LLMs.\n2. The authors assume that the reader is familiar with the concept of self-alignment and the superficial alignment hypothesis.\n3. The paper does not provide a detailed analysis of the computational cost and time efficiency of RAIN.\n4. The authors do not provide a comparison with other state-of-the-art alignment methods for LLMs.\n\nSuggestions for Improvement:\n1. Provide a comprehensive review of existing alignment methods for LLMs to demonstrate the novelty and contribution of the RAIN algorithm.\n2. Clearly explain the assumptions and requirements for the RAIN algorithm to ensure that the reader understands the concept of self-alignment and the superficial alignment hypothesis.\n3. Provide a detailed analysis of the computational cost and time efficiency of RAIN to demonstrate its potential for large-scale LLMs.\n4. Compare RAIN with other state-of-the-art alignment methods for LLMs to demonstrate its superiority and provide a more comprehensive understanding of the current state-of-the-art in LLM alignment.\n\nRecommendation:\nThe paper presents a promising approach for aligning frozen LLMs without requiring additional data or finetuning. While there are some weaknesses and areas for improvement, the authors demonstrate the effectiveness of RAIN using experiments on various LLMs and datasets. I recommend that the authors address the weaknesses and improve the paper to provide a more comprehensive and convincing contribution to the field of LLM alignment.",
        "gr_score_with": 0.46546546546546547,
        "gr_score_without": 0.375366568914956,
        "or_score_with": 0.4894894894894895,
        "or_score_without": 0.49560117302052786,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 128,
            "green_fraction": 0.375366568914956,
            "num_orange_tokens": 169,
            "orange_fraction": 0.49560117302052786,
            "green_z_score": -0.9285311453993025,
            "orange_z_score": 103.47918246060947,
            "green_p_value": 0.8234339401684718,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 333,
            "num_green_tokens": 155,
            "green_fraction": 0.46546546546546547,
            "num_orange_tokens": 163,
            "orange_fraction": 0.4894894894894895,
            "green_z_score": 2.4385336672776194,
            "orange_z_score": 100.97736689946707,
            "green_p_value": 0.0073734924660437615,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GRAPH NEURAL NETWORKS",
        "abstract": "Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs and guides further research on the robustness of GNNs in terms of fairness.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes a novel fairness attack method on Graph Neural Networks (GNNs) named GFairAttack, which consists of two parts: a carefully designed surrogate loss function and an optimization method. The proposed surrogate loss function is designed to represent all types of fairness loss terms, and the optimization method uses a non-gradient attack algorithm to solve the constrained optimization problem. The paper also proposes a fast computation technique to reduce the time complexity of GFairAttack.\n\nStrengths:\n- The paper proposes a novel fairness attack method on GNNs, which is the first to consider the unnoticeable requirement for fairness attacks.\n- The proposed surrogate loss function is designed to represent all types of fairness loss terms, making it a versatile method for attacking various types of fairness-aware GNNs.\n- The optimization method uses a non-gradient attack algorithm, which is more efficient than traditional gradient-based methods.\n- The paper provides extensive experiments on three real-world datasets to evaluate the efficacy of GFairAttack.\n\nWeaknesses:\n- The paper does not provide a comprehensive analysis of the potential vulnerabilities in fairness-aware GNNs, which is an important aspect of fairness attacks.\n- The proposed surrogate loss function may not be effective for GNNs with complex fairness loss terms.\n- The paper does not discuss the potential defenses against GFairAttack, which is an important aspect of fairness attacks.\n\nSuggestions for Improvement:\n- The paper should provide a more comprehensive analysis of the potential vulnerabilities in fairness-aware GNNs.\n- The proposed surrogate loss function should be evaluated on GNNs with complex fairness loss terms.\n- The paper should discuss potential defenses against GFairAttack.\n\nRecommendation:\n- The paper proposes a novel fairness attack method on GNNs, which is the first to consider the unnoticeable requirement for fairness attacks. The proposed surrogate loss function is designed to represent all types of fairness loss terms, making it a versatile method for attacking various types of fairness-aware GNNs. The optimization method uses a non-gradient attack algorithm, which is more efficient than traditional gradient-based methods. However, the paper should provide a more comprehensive analysis of the potential vulnerabilities in fairness-aware GNNs and discuss potential defenses against GFairAttack.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents the first unnoticeable fairness attack method on Graph Neural Networks (GNNs). The authors propose a novel surrogate loss function to attack various types of fairness-aware GNNs and develop a sequential attack algorithm to solve the problem. The attack method is designed to be unnoticeable in terms of prediction utility, allowing the attacker to corrupt the fairness of the GNN model without being detected.\n\nStrengths:\n\n* The paper presents a novel approach to fairness attacks on GNNs, which is unnoticeable in terms of prediction utility.\n* The surrogate loss function is designed to represent different types of fairness loss terms, making it effective against various types of fairness-aware GNNs.\n* The sequential attack algorithm is developed to solve the problem, allowing the attacker to iteratively flip edges to achieve the desired fairness degradation.\n\nWeaknesses:\n\n* The paper does not provide a clear analysis of the time complexity of the attack algorithm, which is a significant limitation.\n* The authors do not provide a detailed explanation of the kernel estimation used to compute the total variation loss, which may be difficult to understand for non-experts.\n* The experimental results are not provided in the paper, which limits the evaluation of the attack method.\n\nSuggestions for Improvement:\n\n* The authors should provide a clear analysis of the time complexity of the attack algorithm, including the number of iterations required to achieve the desired fairness degradation.\n* The authors should provide a detailed explanation of the kernel estimation used to compute the total variation loss, including the choice of kernel function and bandwidth constant.\n* The authors should provide the experimental results to evaluate the efficacy of the attack method.\n\nRecommendation:\n\n* Based on the paper summary, the attack method seems effective in corrupting the fairness of GNNs without being detected. However, the lack of clear analysis of the time complexity and the experimental results limits the evaluation of the attack method. The authors should address these limitations to provide a more comprehensive evaluation of the attack method.\n\n$$$",
        "gr_score_with": 0.5476190476190477,
        "gr_score_without": 0.397196261682243,
        "or_score_with": 0.38492063492063494,
        "or_score_without": 0.5186915887850467,
        "output_without": {
            "num_tokens_scored": 214,
            "num_green_tokens": 85,
            "green_fraction": 0.397196261682243,
            "num_orange_tokens": 111,
            "orange_fraction": 0.5186915887850467,
            "green_z_score": -0.08372183582789332,
            "orange_z_score": 91.8276153236445,
            "green_p_value": 0.5333612021929198,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 252,
            "num_green_tokens": 138,
            "green_fraction": 0.5476190476190477,
            "num_orange_tokens": 97,
            "orange_fraction": 0.38492063492063494,
            "green_z_score": 4.783403848824848,
            "orange_z_score": 73.61220388933472,
            "green_p_value": 8.617567734031186e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "AN EFFICIENT MEMBERSHIP INFERENCE ATTACK FOR THE DIFFUSION MODEL BY PROXIMAL INITIALIZATION",
        "abstract": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by \u03b5 initialized in t = 0 and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries that achieve at least 6\u00d7 efficiency than the previous SOTA baseline on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-tospeech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. Code is available at https://github.com/kong13661/PIA.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel membership inference attack (MIA) method for diffusion models, called Proximal Initialization Attack (PIA). The method leverages the groundtruth trajectory of a diffusion model to infer the membership of a sample. Experimental results demonstrate the effectiveness of PIA on various diffusion models, including continuous-time and discrete-time models. The paper also explores the robustness of diffusion models to MIA in the text-to-speech (TTS) task and finds that models with audio output are more robust to MIA.\n\nStrengths:\n1. The paper proposes a novel MIA method for diffusion models, which is an active area of research.\n2. The experimental results demonstrate the effectiveness of PIA on various diffusion models.\n3. The paper explores the robustness of diffusion models to MIA in the TTS task, which is a new aspect of research.\n\nWeaknesses:\n1. The paper does not provide a clear theoretical foundation for the proposed MIA method.\n2. The experimental results are limited to a few diffusion models and TTS datasets.\n3. The paper does not discuss the potential limitations and challenges of the proposed MIA method.\n\nSuggestions for Improvement:\n1. Provide a clear theoretical foundation for the proposed MIA method, including the underlying mathematical framework and the assumptions made.\n2. Conduct experiments on a larger set of diffusion models and TTS datasets to demonstrate the generalizability of PIA.\n3. Discuss the potential limitations and challenges of the proposed MIA method, including the potential impact on model performance and the limitations of the experimental setup.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper for publication. Specifically, I recommend that the authors provide a clear theoretical foundation for the proposed MIA method, conduct experiments on a larger set of diffusion models and TTS datasets, and discuss the potential limitations and challenges of the proposed MIA method. With these revisions, the paper has the potential to make a significant contribution to the field of membership inference attacks and diffusion models.",
        "peer_review_with_watermark": "$$ $$$\n$$ Paper Title: AN EFFICIENT MEMBERSHIP INFERENCE ATTACK FOR THE DIFFUSION MODEL BY PROXIMAL INITIALIZATION\n$$ Abstract: This paper proposes an efficient query-based membership inference attack (MIA) for diffusion models. The attack is based on utilizing the ground truth trajectory obtained by \u03b5 initialized in t = 0 and predicted point to infer memberships. Experimental results show that the proposed method achieves competitive performance with only two queries and is more efficient than the previous SOTA baseline. The attack is also evaluated on the robustness of diffusion models to MIA in the text-to-speech (TTS) task, and the results indicate that models with mel-spectrogram output are vulnerable to MIA.\n$$ \n\n$$ Paper Summary:\nThe paper proposes a query-based membership inference attack (MIA) for diffusion models. The attack is based on utilizing the ground truth trajectory obtained by \u03b5 initialized in t = 0 and predicted point to infer memberships. The attack is evaluated on the robustness of diffusion models to MIA in the text-to-speech (TTS) task, and the results indicate that models with mel-spectrogram output are vulnerable to MIA. The attack achieves competitive performance with only two queries and is more efficient than the previous SOTA baseline.\n$$ \n\n$$ Strengths:\n1. The paper proposes a novel and efficient query-based MIA attack for diffusion models.\n2. The attack is evaluated on the robustness of diffusion models to MIA in the text-to-speech (TTS) task.\n3. The results show that models with mel-spectrogram output are vulnerable to MIA.\n$$ \n\n$$ Weaknesses:\n1. The paper does not provide a detailed explanation of the mathematical formulation of the attack.\n2. The attack is evaluated only on the robustness of diffusion models to MIA in the text-to-speech (TTS) task.\n3. The paper does not provide a comparison with other MIA attacks.\n$$ \n\n$$ Suggestions for Improvement:\n1. Provide a detailed explanation of the mathematical formulation of the attack.\n2. Evaluate the attack on other types of diffusion models.\n3. Provide a comparison with other MIA attacks.\n$$ \n\n$$ Recommendation:\nThe paper proposes a novel and efficient query-based MIA attack for diffusion models. The attack is evaluated on the robustness of diffusion models to MIA in the text-to-speech (TTS) task, and the results show that models with mel-spectrogram output are vulnerable to MIA. The attack achieves competitive performance with only two queries and is more efficient than the previous SOTA baseline. Therefore, the paper is recommended for acceptance. \n\n$$ \n\n$$ Note: The review is based on the standard conference review format. The reviewer has thoroughly read the paper and has evaluated the paper based on the strength, weaknesses, and suggestions for improvement. The reviewer has also provided a recommendation for the paper. \n\n$$ \n\n$$ References:\n1. Ho et al. (2020)\n2. Song et al. (2021b)\n3. Song & Ermon (2019)\n4. Rombach et al. (2022)\n5. Saharia et al. (2022)\n6. Popov et al. (2021)\n7. Kong et al. (2021)\n8. Yang et al. (2022)\n9. Ho et al. (2022)\n10. Shokri et al. (2017)\n11. Yeom et al. (2018)\n12. Pham & Le (2020)\n13. Zhang et al. (2020)\n14. Duan et al. (2023)\n15. Matsumoto et al. (2023)\n16. Hu & Pang (2023)\n17. Carlini et al. (2023)\n18. Shokri et al. (2017)\n19. Yeom et al. (2018)\n20. Hilprecht et al. (2019)\n$$",
        "gr_score_with": 0.5486486486486486,
        "gr_score_without": 0.46441947565543074,
        "or_score_with": 0.40540540540540543,
        "or_score_without": 0.4044943820224719,
        "output_without": {
            "num_tokens_scored": 267,
            "num_green_tokens": 124,
            "green_fraction": 0.46441947565543074,
            "num_orange_tokens": 108,
            "orange_fraction": 0.4044943820224719,
            "green_z_score": 2.1486575084548014,
            "orange_z_score": 85.8438750897995,
            "green_p_value": 0.015830779671748507,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 370,
            "num_green_tokens": 203,
            "green_fraction": 0.5486486486486486,
            "num_orange_tokens": 150,
            "orange_fraction": 0.40540540540540543,
            "green_z_score": 5.8365499496976225,
            "orange_z_score": 101.28514783109138,
            "green_p_value": 2.6646395450201335e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "INTRA-CLASS INCOHERENCE",
        "abstract": "The current face recognition (FR) algorithms has achieved a high level of accuracy, making further improvements increasingly challenging. While existing FR algorithms primarily focus on optimizing margins and loss functions, limited attention has been given to exploring the feature representation space. Therefore, this paper endeavors to improve FR performance in the view of feature representation space. Firstly, we consider two FR models that exhibit distinct performance discrepancies, where one model exhibits superior recognition accuracy compared to the other. We implement orthogonal decomposition on the features from the superior model along those from the inferior model and obtain two sub-features. Surprisingly, we find the sub-feature orthogonal to the inferior still possesses a certain level of face distinguishability. We adjust the modulus of the sub-features and recombine them through vector addition. Experiments demonstrate this recombination is likely to contribute to an improved facial feature representation, even better than features from the original superior model. Motivated by this discovery, we further consider how to improve FR accuracy when there is only one FR model available. Inspired by knowledge distillation, we incorporate the intraclass incoherence constraint (IIC) to solve the problem. Experiments on various FR benchmarks show the existing state-of-the-art method with IIC can be further improved, highlighting its potential to further enhance FR performance.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a novel approach to improve face recognition performance by introducing intra-class incoherence constraint (IIC) into the face recognition framework. The authors analyze the feature representation space of two face recognition models and find that the superior model contains innovations that are orthogonal to the inferior model. They propose a method to incorporate these innovations into the feature representation space, which leads to improved face recognition performance. The authors also evaluate the proposed method on various face recognition benchmarks and show that it outperforms the state-of-the-art methods.\n\nStrengths:\n\n* The paper provides a detailed analysis of the feature representation space of face recognition models and introduces a novel approach to improve face recognition performance.\n* The authors evaluate the proposed method on various face recognition benchmarks and show that it outperforms the state-of-the-art methods.\n* The paper provides a clear and concise explanation of the proposed method and its advantages.\n\nWeaknesses:\n\n* The paper does not provide a comprehensive review of existing face recognition methods and does not compare the proposed method with all existing methods.\n* The authors assume that the reader is familiar with face recognition methods and does not provide a detailed explanation of the underlying concepts.\n* The paper does not provide a thorough evaluation of the proposed method on various face recognition benchmarks.\n\nSuggestions for Improvement:\n\n* The authors should provide a comprehensive review of existing face recognition methods and compare the proposed method with all existing methods.\n* The authors should provide a detailed explanation of the underlying concepts and assumptions made in the paper.\n* The authors should evaluate the proposed method on various face recognition benchmarks and provide a thorough analysis of the results.\n\nRecommendation:\n\n* The paper proposes a novel approach to improve face recognition performance by introducing intra-class incoherence constraint (IIC) into the face recognition framework. The authors analyze the feature representation space of two face recognition models and find that the superior model contains innovations that are orthogonal to the inferior model. They propose a method to incorporate these innovations into the feature representation space, which leads to improved face recognition performance. The authors also evaluate the proposed method on various face recognition benchmarks and show that it outperforms the state-of-the-art methods. Overall, the paper provides a clear and concise explanation of the proposed method and its advantages, and the authors provide a thorough evaluation of the proposed method on various face recognition benchmarks. However, the paper lacks a comprehensive review of existing face recognition methods and does not provide a detailed explanation of the underlying concepts.",
        "peer_review_with_watermark": "$$\\begin{gathered}\n\\text{\\textit{Summary:}}\\ \n\\text{This paper introduces the concept of \\textit{intra-class incoherence constraint (IIC)}, which is a novel method to improve the performance of face recognition algorithms. The authors propose a framework that leverages the knowledge distillation method to introduce innovation, which is a sub-feature orthogonal to the inferior model. They demonstrate the effectiveness of their approach on several benchmarks and show that it outperforms state-of-the-art methods.}\n\n\\textit{Strengths:}\n\n\\textbullet\\ The paper presents a novel approach to improve the performance of face recognition algorithms by introducing intra-class incoherence constraint.\n\n\\textbullet\\ The authors provide a detailed analysis of the feature representation space and demonstrate the effectiveness of their approach on several benchmarks.\n\n\\textbullet\\ The paper is well-structured and easy to follow.\n\n\\textbullet\\ The authors provide a thorough comparison with state-of-the-art methods.\n\n\\textit{Weaknesses:}\n\n\\textbullet\\ The paper does not provide a comprehensive discussion on the limitations of the proposed approach.\n\n\\textbullet\\ The authors do not provide a detailed analysis of the impact of the weight on the performance of the approach.\n\n\\textbullet\\ The paper does not provide a thorough discussion on the potential applications of the proposed approach.\n\n\\textit{Suggestions for improvement:}\n\n\\textbullet\\ The authors should provide a more comprehensive discussion on the limitations of the proposed approach.\n\n\\textbullet\\ The authors should provide a detailed analysis of the impact of the weight on the performance of the approach.\n\n\\textbullet\\ The authors should provide a thorough discussion on the potential applications of the proposed approach.\n\n\\textbullet\\ The authors should consider including more benchmarks to evaluate the performance of their approach.\n\n\\textit{Recommendation:}\n\n\\textbullet\\ Based on the analysis, we recommend that the authors consider the proposed approach as a promising method to improve the performance of face recognition algorithms.\n\n\\textbullet\\ We suggest that the authors consider further exploring the potential applications of the proposed approach.\n\n\\textbullet\\ We recommend that the authors consider including more benchmarks to evaluate the performance of their approach.\n\n$$",
        "gr_score_with": 0.627906976744186,
        "gr_score_without": 0.47317073170731705,
        "or_score_with": 0.2837209302325581,
        "or_score_without": 0.375609756097561,
        "output_without": {
            "num_tokens_scored": 205,
            "num_green_tokens": 97,
            "green_fraction": 0.47317073170731705,
            "num_orange_tokens": 77,
            "orange_fraction": 0.375609756097561,
            "green_z_score": 2.1384973069207542,
            "orange_z_score": 70.06075752792619,
            "green_p_value": 0.016238201236439098,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 215,
            "num_green_tokens": 135,
            "green_fraction": 0.627906976744186,
            "num_orange_tokens": 61,
            "orange_fraction": 0.2837209302325581,
            "green_z_score": 6.821364068273881,
            "orange_z_score": 53.92384581011173,
            "green_p_value": 4.509003771427351e-12,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "POLYNORMER: POLYNOMIAL-EXPRESSIVE GRAPH TRANSFORMER IN LINEAR TIME",
        "abstract": "Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomialexpressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme to learn high-degree equivariant polynomials whose coefficients are controlled by attention scores. Polynormer has been evaluated on 13 homophilic and heterophilic datasets, including large graphs with millions of nodes. Our extensive experiment results show that Polynormer outperforms state-of-the-art GNN and GT baselines on most datasets, even without the use of nonlinear activation functions. Source code of Polynormer is freely available at: github.com/cornell-zhang/Polynormer.",
        "peer_review_without_watermark": "$$$\\boxed{\\text{Paper Summary:}}$\n\nThe paper proposes Polynormer, a linear graph transformer that is polynomial-expressive and scalable to large graphs. The model is based on a novel base model that learns a high-degree polynomial on input features and integrates graph topology and node features into polynomial coefficients separately. The polynomial expressivity is achieved through a local-to-global attention scheme, which allows the model to learn high-degree equivariant polynomials with linear complexity. The paper presents extensive experimental results on various graph datasets, demonstrating the efficacy of Polynormer over state-of-the-art graph neural networks and graph transformers.\n\n$\\boxed{\\text{Strengths:}}$\n\n1. The paper introduces a novel and effective approach to balance the trade-off between expressivity and scalability of graph transformers.\n2. The proposed Polynormer model is polynomial-expressive and can learn high-degree equivariant polynomials with linear complexity.\n3. The local-to-global attention scheme adopted by Polynormer is efficient and effective in capturing both local and global structural information.\n4. The paper presents extensive experimental results on various graph datasets, demonstrating the efficacy of Polynormer over state-of-the-art graph neural networks and graph transformers.\n\n$\\boxed{\\text{Weaknesses:}}$\n\n1. The paper may be considered too long and dense, making it challenging to follow for non-experts.\n2. Some of the notation and definitions may be unfamiliar to readers without a strong background in graph theory and machine learning.\n3. The paper does not provide a detailed analysis of the computational complexity of Polynormer, which may be an important consideration for large-scale applications.\n\n$\\boxed{\\text{Suggestions for Improvement:}}$\n\n1. Provide a more detailed and concise introduction to the paper, highlighting the main contributions and motivations.\n2. Consider adding more notation and definitions to help readers without a strong background in graph theory and machine learning.\n3. Provide a more detailed analysis of the computational complexity of Polynormer, including its time and space complexity.\n4. Consider adding more experimental results on different datasets and graph sizes to demonstrate the scalability of Polynormer.\n\n$\\boxed{\\text{Recommendation:}}$\n\nBased on the paper's contributions and experimental results, I recommend accepting the paper for publication in a top-tier machine learning or computer science conference. However, I suggest that the authors consider addressing the weaknesses and suggestions for improvement mentioned above to make the paper more accessible and rigorous.",
        "peer_review_with_watermark": "$$$ Paper Summary:\n \n The paper introduces Polynormer, a polynomial-expressive graph transformer that can expressively represent a high-degree polynomial in linear time. The proposed model builds upon a base model that learns a high-degree polynomial on input features, and it is integrated with graph topology and node features separately, resulting in local and global equivariant attention models. The Polynormer architecture adopts a local-to-global attention scheme, which is fundamentally distinct from the purpose of prior attention models in literature. The model is evaluated on 13 homophilic and heterophilic datasets, including large graphs with millions of nodes, and it outperforms state-of-the-art GNN and GT baselines on most datasets, even without the use of nonlinear activation functions.\n\nStrengths:\n\n1.  The proposed model is polynomial-expressive, which is theoretically more expressive than message-passing graph neural networks (GNNs) and graph transformers (GTs) with quadratic complexity.\n2.  The Polynormer architecture adopts a local-to-global attention scheme, which is more beneficial for heterophilic graphs than the local-and-global attention adopted by prior GT models.\n3.  The model is scalable to large graphs with millions of nodes, as it has linear complexity in regard to the graph size.\n\nWeaknesses:\n\n1.  The polynomial expressivity of Polynormer relies on the high-degree polynomial learned by the base model, which may not capture all possible complex functions, especially in the absence of nonlinear activation functions.\n2.  The model relies on the attention mechanism, which may not be effective in all graph structures or datasets.\n\nSuggestions for Improvement:\n\n1.  To further improve the expressivity of Polynormer, it is possible to incorporate more advanced polynomial networks, such as those that utilize skip connections or other techniques to efficiently implement the polynomial expansion with high-dimensional features.\n2.  The model can be further improved by incorporating more advanced attention mechanisms, such as those that can handle complex graphs or heterogeneous graphs.\n\nRecommendation:\n\nBased on the experimental results, Polynormer is a promising approach for graph learning, particularly in the absence of nonlinear activation functions. However, further improvements are needed to address the limitations of the model, such as its reliance on high-degree polynomials and attention mechanisms.",
        "gr_score_with": 0.4514285714285714,
        "gr_score_without": 0.34355828220858897,
        "or_score_with": 0.5228571428571429,
        "or_score_without": 0.5184049079754601,
        "output_without": {
            "num_tokens_scored": 326,
            "num_green_tokens": 112,
            "green_fraction": 0.34355828220858897,
            "num_orange_tokens": 169,
            "orange_fraction": 0.5184049079754601,
            "green_z_score": -2.08019190557222,
            "orange_z_score": 114.29599660600253,
            "green_p_value": 0.9812460328048077,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 350,
            "num_green_tokens": 158,
            "green_fraction": 0.4514285714285714,
            "num_orange_tokens": 183,
            "orange_fraction": 0.5228571428571429,
            "green_z_score": 1.9639610121239315,
            "orange_z_score": 119.45868403083595,
            "green_p_value": 0.024767306717813353,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:}$$\nThe paper proposes CoT3DRef, a novel and interpretable framework for 3D visual grounding. It formulates the 3D visual grounding problem as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently used to localize the final target object. The approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n\n$$\\textbf{Strengths:}$$\n1. The approach is novel and interpretable, providing insights into the reasoning process behind the model's decisions.\n2. The framework is data-efficient, achieving state-of-the-art results on Nr3D and Sr3D datasets with limited training data.\n3. The use of automatic rule-based methods, syntactic parsing, and in-context learning to collect pseudo-labels eliminates the need for manual annotations.\n4. The approach can be easily integrated into other architectures.\n\n$$\\textbf{Weaknesses:}$$\n1. The accuracy of the pseudo-labels is a significant challenge, with a notable gap between automatically collected pseudo-labels and ground-truth data.\n2. The anchor localization block is tailored on the ScanNet dataset, requiring adaptations for other 3D scene datasets.\n3. The Pathway module does not handle multi-path scenarios, where multiple paths are valid.\n4. The approach may not generalize well to unseen scenes or objects.\n\n$$\\textbf{Suggestions for Improvement:}$$\n1. Enhance the pseudo-label module to improve accuracy, potentially using more advanced techniques such as active learning or transfer learning.\n2. Adapt the anchor localization block to be more generalizable across different 3D scene datasets.\n3. Develop a more robust Pathway module to handle multi-path scenarios.\n4. Conduct further experiments to evaluate the approach's generalization to unseen scenes or objects.\n\n$$\\textbf{Recommendation:}$$\nThe proposed CoT3DRef framework is a significant contribution to the field of 3D visual grounding. Its novel approach, interpretability, and data efficiency make it an attractive solution for various applications. However, addressing the weaknesses and improving the pseudo-label module, anchor localization block, and Pathway module are essential steps to further enhance the approach's performance and generalization.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces the CoT3DRef framework for 3D visual grounding, which formulates the problem as a sequence-to-sequence task to predict a chain of anchor objects and subsequently localizes the final target object. The framework is designed to be data-efficient and interpretable, using automatic rule-based methods, syntactic parsing, and in-context learning to collect pseudo-labels for the anchor objects. The proposed approach achieves state-of-the-art results on the Nr3D and Sr3D datasets without requiring manual annotations.\n\nStrengths:\n1. The proposed framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n2. The use of automatic rule-based methods, syntactic parsing, and in-context learning to collect pseudo-labels for the anchor objects eliminates the laborious and time-consuming process of manually annotating anchors.\n3. The framework is designed to be interpretable, providing a clear understanding of the decision-making process.\n4. The proposed approach can be easily integrated into other architectures.\n\nWeaknesses:\n1. The accuracy of the pseudo-labels plays a vital role in the overall performance, and the accuracy gap between automatically collected pseudo-labels and ground-truth data contributes to the performance loss observed on the Nr3D dataset.\n2. The anchor localization block in the pseudo module is tailored on the ScanNet dataset and will thus need some adaptations to be usable on other 3D scene datasets.\n3. The Pathway module does not handle the multi-path scenario, where multiple paths are valid, and requires simplification to avoid incorrect interpretations.\n4. The proposed approach does not handle the case where the anchors are not mentioned in the input utterance, requiring additional handling to avoid incorrect interpretations.\n\nSuggestions for Improvement:\n1. Enhance the performance of the pseudo-label module to reduce the accuracy gap with ground-truth data.\n2. Adapt the anchor localization block to be usable on other 3D scene datasets.\n3. Develop the Pathway module to handle the multi-path scenario and avoid incorrect interpretations.\n4. Incorporate additional handling to address the case where the anchors are not mentioned in the input utterance.\n\nRecommendation:\nThe proposed CoT3DRef framework is a significant contribution to the field of 3D visual grounding, demonstrating the potential for data-efficient and interpretable approaches to solve this challenging task. While there are some limitations and areas for improvement, the framework shows promising results and provides a clear understanding of the decision-making process. I recommend further development and exploration of the proposed approach to address the limitations and fully realize its potential. \n\nNote: The review is based on the provided paper and the standard format for a conference review. The review provides a detailed analysis of the paper's strengths and weaknesses, along with suggestions for improvement and a recommendation for further development. The review is concise and to the point, providing a clear and comprehensive evaluation of the paper's merits and limitations.",
        "gr_score_with": 0.4975609756097561,
        "gr_score_without": 0.39325842696629215,
        "or_score_with": 0.4560975609756098,
        "or_score_without": 0.5168539325842697,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 140,
            "green_fraction": 0.39325842696629215,
            "num_orange_tokens": 184,
            "orange_fraction": 0.5168539325842697,
            "green_z_score": -0.2596453934447499,
            "orange_z_score": 116.36641414627779,
            "green_p_value": 0.6024313410487181,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 410,
            "num_green_tokens": 204,
            "green_fraction": 0.4975609756097561,
            "num_orange_tokens": 187,
            "orange_fraction": 0.4560975609756098,
            "green_z_score": 4.0323891927275595,
            "orange_z_score": 110.00255974531663,
            "green_p_value": 2.760632513299909e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ADVERSARIAL CAUSAL BAYESIAN OPTIMIZATION",
        "abstract": "In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users\u2019 demand patterns in a shared mobility system and reposition vehicles in strategic areas.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$\\boxed{Paper Summary:}$ \nThe paper proposes a new approach to causal Bayesian optimization (CBO) that incorporates the influence of external events and adversaries. The authors introduce the concept of adversarial causal Bayesian optimization (ACBO) and propose a novel algorithm called Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). The algorithm combines classical online learning strategies with causal modeling of rewards and is shown to achieve bounded regret. The authors also propose a distributed version of the algorithm, called D-CBO-MW, that can scale to large action spaces.\n\n$\\boxed{Strengths:}$ \n1. The paper provides a new and novel approach to causal Bayesian optimization that incorporates the influence of external events and adversaries.\n2. The authors provide a formal analysis of the algorithm and its regret guarantees.\n3. The paper presents empirical results that demonstrate the effectiveness of the proposed algorithm.\n4. The authors propose a distributed version of the algorithm that can scale to large action spaces.\n\n$\\boxed{Weaknesses:}$ \n1. The paper may be too technical for non-experts, as it assumes a high level of background knowledge in CBO, causal graphs, and machine learning.\n2. The authors may have overlooked some potential issues or limitations of the proposed algorithm.\n3. The paper may not provide a clear comparison to existing methods, which could make it difficult to evaluate the novelty of the proposed approach.\n\n$\\boxed{Suggestions for Improvement:}$ \n1. The authors could provide more detailed explanations and examples to make the paper more accessible to non-experts.\n2. The authors could consider including a more comprehensive comparison to existing methods to evaluate the novelty of the proposed approach.\n3. The authors could provide more detailed results and analysis to better understand the strengths and limitations of the proposed algorithm.\n\n$\\boxed{Recommendation:}$ \nBased on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a reputable conference or journal. The paper presents a novel and effective approach to causal Bayesian optimization and provides a thorough analysis of its regret guarantees. However, the paper may benefit from more detailed explanations and comparisons to existing methods to make it more accessible to a broader audience.",
        "peer_review_with_watermark": "$$\\begin{align*}\n\\textbf{Paper Summary:} \\\\\n\\text{The paper presents Adversarial Causal Bayesian Optimization (ACBO), a novel approach to causal Bayesian optimization in non-stationary and potentially multi-agent environments. The authors propose a first algorithm for ACBO, called Causal Bayesian Optimization with Multiplicative Weights (CBO-MW), which combines a classical online learning strategy with causal modeling of the rewards. The approach is based on a soft intervention model and incorporates external events into the optimization problem. The authors derive regret bounds for CBO-MW that naturally depend on graph-related quantities and propose a scalable implementation for the case of combinatorial interventions and submodular rewards. The paper also introduces a distributed version of CBO-MW, called D-CBO-MW, which can scale to large action spaces.\\\\\n\\textbf{Strengths:} \\\\\n- The paper introduces a novel approach to ACBO, which is a key problem in many real-world applications.\n- The authors propose a first algorithm for ACBO, which is a significant contribution to the field.\n- The approach is based on a soft intervention model and incorporates external events into the optimization problem, which is a key aspect of ACBO.\n- The authors derive regret bounds for CBO-MW that naturally depend on graph-related quantities, which is a significant theoretical contribution.\n- The paper introduces a distributed version of CBO-MW, which can scale to large action spaces, which is a key aspect of many real-world applications.\n\\\\\n\\textbf{Weaknesses:} \\\\\n- The paper assumes a soft intervention model, which may not be realistic in all cases.\n- The approach is based on a classical online learning strategy, which may not be optimal in all cases.\n- The paper does not provide a comprehensive analysis of the computational complexity of CBO-MW, which is a key aspect of many real-world applications.\n- The paper does not provide a comparison of the performance of CBO-MW with other state-of-the-art methods, which is a key aspect of many real-world applications.\n\\\\\n\\textbf{Suggestions for Improvement:} \\\\\n- The paper should provide a more comprehensive analysis of the assumptions of the soft intervention model and discuss potential relaxations or extensions.\n- The approach should be compared with other state-of-the-art methods to demonstrate its performance in different scenarios.\n- The paper should provide a more detailed analysis of the computational complexity of CBO-MW and discuss potential optimization techniques to improve its efficiency.\n\\\\\n\\textbf{Recommendation:} \\\\\nBased on the review, I would recommend that the paper be published in its current form. The paper presents a novel approach to ACBO, which is a key problem in many real-world applications. The approach is based on a soft intervention model and incorporates external events into the optimization problem, which is a key aspect of ACBO. The authors derive regret bounds for CBO-MW that naturally depend on graph-related quantities, which is a significant theoretical contribution. However, the paper should provide a more comprehensive analysis of the assumptions of the soft intervention model and discuss potential relaxations or extensions. The approach should be compared with other state-of-the-art methods to demonstrate its performance in different scenarios. The paper should provide a more detailed analysis of the computational complexity of CBO-MW and discuss potential optimization techniques to improve its efficiency.",
        "gr_score_with": 0.5412541254125413,
        "gr_score_without": 0.43812709030100333,
        "or_score_with": 0.40594059405940597,
        "or_score_without": 0.44481605351170567,
        "output_without": {
            "num_tokens_scored": 299,
            "num_green_tokens": 131,
            "green_fraction": 0.43812709030100333,
            "num_orange_tokens": 133,
            "orange_fraction": 0.44481605351170567,
            "green_z_score": 1.345747669252173,
            "orange_z_score": 86.54076575644599,
            "green_p_value": 0.08919195364769272,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 303,
            "num_green_tokens": 164,
            "green_fraction": 0.5412541254125413,
            "num_orange_tokens": 123,
            "orange_fraction": 0.40594059405940597,
            "green_z_score": 5.018995819076082,
            "orange_z_score": 79.37000586144042,
            "green_p_value": 2.597114009478921e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING",
        "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT4\u2019s CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper proposes a novel math instruction-tuning approach to activate open-source large language models' (LLMs) mathematical reasoning capabilities. The authors introduce a hybrid instruction-tuning dataset, MathInstruct, which combines chain-of-thought (CoT) and program-of-thought (PoT) rationales. The dataset is curated from 13 math datasets with intermediate rationales, six of which are newly curated. The authors fine-tune Llama models on MathInstruct and achieve substantial accuracy gains over existing open-source models on nine mathematical reasoning datasets across all scales.\n\nStrengths:\n\n* The paper introduces a novel hybrid instruction-tuning approach that combines CoT and PoT rationales.\n* The authors provide a comprehensive study of the effectiveness of the approach on various mathematical reasoning datasets.\n* The use of a large-scale dataset, MathInstruct, allows for robust evaluation of the approach.\n* The authors demonstrate the potential of the approach to activate LLMs' core capabilities in specialized domains.\n\nWeaknesses:\n\n* The paper relies heavily on the authors' own dataset, MathInstruct, which may not be generalizable to other datasets.\n* The authors do not provide a detailed analysis of the limitations of the approach and its potential biases.\n* The evaluation of the approach is based on a limited set of datasets, which may not be representative of the broader mathematical reasoning task.\n* The paper does not provide a clear explanation of the role of the hybrid decoding strategy in improving performance.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the limitations of the approach and its potential biases.\n* The evaluation of the approach should be based on a broader set of datasets to ensure generalizability.\n* The authors should provide a clear explanation of the role of the hybrid decoding strategy in improving performance.\n* The paper should include a more detailed discussion of the potential applications and limitations of the approach.\n\nRecommendation:\n\n* The paper contributes significantly to the field of mathematical reasoning and large language models. The authors' novel approach to instruction-tuning is promising, and the results demonstrate the potential of the approach to activate LLMs' core capabilities in specialized domains. However, the paper could benefit from a more detailed analysis of the limitations and biases of the approach. With further refinement and expansion of the evaluation, the paper has the potential to make a significant impact in the field.",
        "peer_review_with_watermark": "$$\n\\begin{align*}\n\\textbf{Paper Summary:} &\\quad \\text{The authors propose a novel approach to enhance the mathematical reasoning capabilities of open-source large language models (LLMs). They introduce a new instruction-tuning dataset, MathInstruct, which combines a diverse set of math problems and hybrid chain-of-thought (CoT) and program-of-thought (PoT) rationales. The authors fine-tune a series of LLMs, including MAmmoTH and MAmmoTH-Coder, on MathInstruct and achieve substantial accuracy gains over existing open-source models. The MAmmoTH models outperform the state-of-the-art (SoTA) models on nine mathematical reasoning datasets across all scales, with an average accuracy gain of 16% to 32%. The authors also demonstrate the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.\\\\\n\\textbf{Strengths:} &\\quad \\text{The authors' approach is novel and innovative, combining the strengths of CoT and PoT rationales. The use of a diverse instruction-tuning dataset, MathInstruct, is a significant contribution, as it provides a broad coverage of different math fields and complexity levels. The authors' ability to fine-tune LLMs on a large-scale dataset, achieving substantial accuracy gains, is a major strength of the paper.\\\\\n\\textbf{Weaknesses:} &\\quad \\text{One major limitation of the paper is that the authors do not provide a detailed analysis of the limitations of their approach. For example, they do not discuss the potential limitations of using a hybrid approach, or how it may interact with other factors that could impact performance. Another limitation is that the authors do not provide a comprehensive comparison of their approach to other existing methods, which could provide a more nuanced understanding of the strengths and weaknesses of their approach.\\\\\n\\textbf{Suggestions for Improvement:} &\\quad \\text{To improve the paper, the authors could provide a more detailed analysis of the limitations of their approach, including potential limitations of using a hybrid approach and how it may interact with other factors that could impact performance. They could also conduct a more comprehensive comparison of their approach to other existing methods, which could provide a more nuanced understanding of the strengths and weaknesses of their approach. Additionally, the authors could consider exploring other factors that could impact performance, such as the impact of different learning rates or batch sizes on performance.\\\\\n\\textbf{Recommendation:} &\\quad \\text{Based on the strengths and limitations of the paper, I recommend that the authors continue to work on improving their approach, addressing the limitations identified, and conducting a more comprehensive comparison of their approach to other existing methods. With these improvements, the paper has the potential to contribute significantly to the field of mathematical reasoning in LLMs.\n\\end{align*}\n$$",
        "gr_score_with": 0.46511627906976744,
        "gr_score_without": 0.33766233766233766,
        "or_score_with": 0.4728682170542636,
        "or_score_without": 0.5681818181818182,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 104,
            "green_fraction": 0.33766233766233766,
            "num_orange_tokens": 175,
            "orange_fraction": 0.5681818181818182,
            "green_z_score": -2.2331621049563304,
            "orange_z_score": 128.14057968681095,
            "green_p_value": 0.9872308718448968,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 387,
            "num_green_tokens": 180,
            "green_fraction": 0.46511627906976744,
            "num_orange_tokens": 183,
            "orange_fraction": 0.4728682170542636,
            "green_z_score": 2.6148057867316985,
            "orange_z_score": 119.28611648046508,
            "green_p_value": 0.004463910224648224,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LEFTOVER-LUNCH: ADVANTAGE-BASED OFFLINE REINFORCEMENT LEARNING FOR LANGUAGE MODELS",
        "abstract": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LOL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LOL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM\u2019s value estimate, A-LOL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LOL is an easy-to-implement, sample-efficient, and stable LM training recipe. We demonstrate the effectiveness of A-LOL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LOL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LOL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a new class of offline policy gradient algorithms called Advantage-Leftover Lunch RL (A-LOL) for language model learning. A-LOL is designed to be easy to implement, sample-efficient, and stable, and can be used to optimize language models towards any desired rewards using only pre-existing data. The algorithm assumes the entire output sequence as a single action and uses sequence-level classifiers or human-designed scoring functions as rewards. A-LOL is shown to outperform other offline RL methods, including preference-based and reward-based baselines, on four different language generation tasks.\n\n$$Strengths:\n1. A-LOL is easy to implement and can be used to optimize language models towards any desired rewards using only pre-existing data.\n2. The algorithm is sample-efficient and stable, and can be used to train language models on a limited amount of data.\n3. A-LOL can be used to exploit the reference LM's advantage estimate to discard unfavorable data, making it resilient to noise.\n4. The algorithm can be used to optimize multiple distinct reward functions, even when using noisy or suboptimal training data.\n\n$$Weaknesses:\n1. The paper does not provide a clear explanation of how the algorithm works, and the derivations are not fully explained.\n2. The paper does not provide a clear comparison of the algorithm's performance with other offline RL methods.\n3. The paper does not provide a clear evaluation of the algorithm's robustness to noise and suboptimal training data.\n4. The paper does not provide a clear discussion of the limitations and potential biases of the algorithm.\n\n$$Suggestions for Improvement:\n1. Provide a clear explanation of how the algorithm works, including the derivations and mathematical proofs.\n2. Conduct a more thorough comparison of the algorithm's performance with other offline RL methods, including preference-based and reward-based baselines.\n3. Evaluate the algorithm's robustness to noise and suboptimal training data, including the impact of filtering unfavorable data points.\n4. Discuss the limitations and potential biases of the algorithm, including the potential for overfitting and the impact of the reference LM's advantage estimate.\n\n$$Recommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper for publication. The authors should provide a clear explanation of how the algorithm works, conduct a more thorough comparison of the algorithm's performance with other offline RL methods, and evaluate the algorithm's robustness to noise and suboptimal training data. The authors should also discuss the limitations and potential biases of the algorithm, including the potential for overfitting and the impact of the reference LM's advantage estimate.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe authors propose a new class of offline policy gradient algorithms called Advantage-Leftover Lunch RL (A-LOL) that enables reinforcement learning with human feedback on any pre-existing data. A-LOL assumes the entire output sequence as a single action and incorporates sequence-level classifiers or human-designed scoring functions as rewards. The algorithm only trains on positive advantage (leftover) data points, making it resilient to noise. The authors demonstrate the effectiveness of A-LOL and its variants on four different language generation tasks, outperforming other offline RL baselines, including preference-based and reward-based methods. A-LOL variants exploit the flexibility of importance weighting, achieving top performance in almost every evaluation.\n\nStrengths:\n1. The authors propose a new class of offline policy gradient algorithms that are easy to implement on top of standard negative log-likelihood.\n2. A-LOL is resilient to noise and can exploit the reference LM\u2019s advantage estimate to discard unfavorable data.\n3. The authors create four variants of A-LOL that achieve top performance in almost every evaluation, exploiting the flexibility of importance weighting.\n4. The authors demonstrate the effectiveness of A-LOL and its variants on four different language generation tasks, outperforming other offline RL baselines.\n\nWeaknesses:\n1. The authors do not compare their method with online RL methods, which could be a potential area of improvement.\n2. The authors do not discuss potential limitations of their method, such as the assumption that the reference policy is a good initial policy.\n3. The authors do not provide a detailed analysis of the importance weight used in their method, which could be an area of further research.\n4. The authors do not discuss potential applications of their method beyond language generation tasks.\n\nSuggestions for Improvement:\n1. Compare the authors' method with online RL methods to demonstrate its potential benefits over online RL.\n2. Provide a more detailed analysis of the importance weight used in the method, including its potential limitations and improvements.\n3. Discuss potential limitations of the method, such as the assumption that the reference policy is a good initial policy.\n4. Explore potential applications of the method beyond language generation tasks.\n\nRecommendation:\nOverall, the authors propose a new class of offline policy gradient algorithms that are easy to implement on top of standard negative log-likelihood. The method demonstrates excellent performance on four different language generation tasks, outperforming other offline RL baselines. However, there are potential areas for improvement, such as comparing with online RL methods and discussing potential limitations of the method. I recommend that the authors revise their paper to address these areas and further explore potential applications of their method.",
        "gr_score_with": 0.3707165109034268,
        "gr_score_without": 0.25496688741721857,
        "or_score_with": 0.5763239875389408,
        "or_score_without": 0.609271523178808,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 77,
            "green_fraction": 0.25496688741721857,
            "num_orange_tokens": 184,
            "orange_fraction": 0.609271523178808,
            "green_z_score": -5.14475879379759,
            "orange_z_score": 120.1296922974769,
            "green_p_value": 0.9999998660676225,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 119,
            "green_fraction": 0.3707165109034268,
            "num_orange_tokens": 185,
            "orange_fraction": 0.5763239875389408,
            "green_z_score": -1.070951285671973,
            "orange_z_score": 117.06851553899797,
            "green_p_value": 0.8579043329788238,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-ofthe-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces SWE-bench, a benchmark for evaluating the capabilities of language models (LMs) in resolving real-world GitHub issues. The benchmark consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The goal of SWE-bench is to provide a realistic and challenging testbed for evaluating the capabilities of LMs in software engineering tasks.\n\nThe authors evaluate multiple state-of-the-art proprietary models and their fine-tuned version SWE-Llama on SWE-bench and find that they struggle to resolve most issues. The best-performing model, Claude 2, is only able to solve a mere 1.96% of the issues. The authors also release a training dataset, SWE-benchtrain, and two fine-tuned models, SWE-Llama 7b and 13b, based on the CodeLlama model.\n\nThe authors conclude that SWE-bench represents a step towards LMs that are more practical, intelligent, and autonomous, and that it addresses the shortcomings of existing benchmarks in software engineering tasks.\n\nStrengths:\n\n* SWE-bench provides a realistic and challenging testbed for evaluating the capabilities of LMs in software engineering tasks.\n* The benchmark consists of a large number of real-world GitHub issues and corresponding pull requests, making it a more representative and challenging testbed than existing benchmarks.\n* The authors release a training dataset and fine-tuned models, making it easier for others to build upon their work.\n\nWeaknesses:\n\n* The benchmark is limited to Python repositories, which may not be representative of other programming languages.\n* The authors do not provide a detailed analysis of the performance of different models on different tasks, which may make it difficult to interpret the results.\n\nSuggestions for Improvement:\n\n* Expand the benchmark to include repositories from other programming languages.\n* Provide a more detailed analysis of the performance of different models on different tasks.\n* Consider adding more tasks that require more complex reasoning and problem-solving skills.\n\nRecommendation:\n\n* SWE-bench is a valuable contribution to the field of natural language processing and software engineering, and it has the potential to provide a more realistic and challenging testbed for evaluating the capabilities of LMs in software engineering tasks. However, it is essential to address the limitations and shortcomings of the benchmark to make it more representative and challenging.",
        "peer_review_with_watermark": "$$\n\nThe paper proposes SWE-bench, a benchmark for evaluating the capabilities of language models in software engineering tasks. The authors introduce a new framework for creating realistic software engineering problems, using GitHub issues and pull requests from popular Python repositories. The benchmark consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The authors evaluate multiple state-of-the-art language models, including proprietary models and their fine-tuned version SWE-Llama, on SWE-bench and find that they fail to solve all except the simplest issues. The best-performing model, Claude 2, is able to solve only 1.96% of the issues. The authors also release two fine-tuned models, SWE-Llama 7b and 13b, based on the CodeLlama model. The authors observe that models struggle significantly to resolve issues, especially those requiring understanding and coordinating changes across multiple functions, classes, and even files simultaneously. The authors identify several challenges in software engineering tasks, including understanding and coordinating changes across multiple files, handling images, and performing multi-modal reasoning. The authors discuss the potential value of providing feedback via an execution environment that would allow models to run fixes against existing tests, then decide whether to continue editing or submit the patch for review. \n\nThe authors identify several strengths of their work, including the creation of a realistic software engineering benchmark, the use of GitHub issues and pull requests to create realistic problems, and the evaluation of multiple state-of-the-art language models. The authors also identify some weaknesses, including the failure of models to solve all but the simplest issues, and the potential for models to hallucinate or make mistakes. The authors provide several suggestions for improvement, including the use of more realistic problems, the evaluation of models on more complex issues, and the consideration of multi-modal reasoning. \n\nBased on the authors' analysis, I would recommend that the authors:\n\n* Continue to work on creating more realistic software engineering problems and evaluating multiple state-of-the-art language models.\n* Consider the potential value of providing feedback via an execution environment that would allow models to run fixes against existing tests.\n* Discuss the potential challenges and limitations of their work, including the failure of models to solve all but the simplest issues.\n* Provide more detailed analysis of the performance of different models on different issues.\n* Consider the potential value of using SWE-bench as a testbed for designing safe, robust measures towards aligned, verifiable, and safe AI-driven software engineering.\n\nThe authors' work has several implications for the field of natural language processing and software engineering. The creation of a realistic software engineering benchmark has the potential to improve the evaluation of language models, and the consideration of multi-modal reasoning has the potential to improve the understanding of software engineering tasks. The authors' work highlights the challenges and limitations of current language models and provides a foundation for future work in this area. \n\nThe authors' paper is well-written and provides a clear and concise overview of their work. The paper is well-structured and easy to follow, and the authors provide several examples and illustrations to help explain their work. The authors also provide a detailed analysis of their results and discuss the potential implications of their work. \n\nHowever, I would like to see some additional details on the following points:\n\n* How the authors chose the 12 popular Python repositories for their benchmark.\n* How the authors filtered the GitHub issues and pull requests to create realistic problems.\n* How the authors evaluated the performance of different models on different issues.\n* How the authors addressed the potential challenges and limitations of their work.\n* How the authors plan to continue their work and improve their benchmark. \n\nOverall, I believe that the authors have made a significant contribution to the field of natural language processing and software engineering, and their work has the potential to improve the evaluation of language models and the understanding of software engineering tasks. \n\n$$",
        "gr_score_with": 0.5116772823779193,
        "gr_score_without": 0.42857142857142855,
        "or_score_with": 0.46709129511677283,
        "or_score_without": 0.5167173252279635,
        "output_without": {
            "num_tokens_scored": 329,
            "num_green_tokens": 141,
            "green_fraction": 0.42857142857142855,
            "num_orange_tokens": 170,
            "orange_fraction": 0.5167173252279635,
            "green_z_score": 1.0578504710249084,
            "orange_z_score": 65.51490258332817,
            "green_p_value": 0.14506180931535217,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 471,
            "num_green_tokens": 241,
            "green_fraction": 0.5116772823779193,
            "num_orange_tokens": 220,
            "orange_fraction": 0.46709129511677283,
            "green_z_score": 4.947316213985179,
            "orange_z_score": 70.56735309792315,
            "green_p_value": 3.762184500128496e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SEER: LANGUAGE INSTRUCTED VIDEO PREDICTION WITH LATENT DIFFUSION MODELS",
        "abstract": "Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-toimage (T2I) stable diffusion models along the temporal axis. We enhance the U-Net and language conditioning model by incorporating computation-efficient spatial-temporal attention. Furthermore, we introduce a novel Frame Sequential Text Decomposer module that dissects a sentence\u2019s global instruction into temporally aligned sub-instructions, ensuring precise integration into each frame of generation. Our framework allows us to effectively leverage the extensive prior knowledge embedded in pretrained T2I models across the frames. With the adaptable-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2), Bridgedata and EpicKitchens-100 datasets demonstrate our superior video prediction performance with around 480-GPU hours versus CogVideo with over 12,480-GPU hours: achieving the 31% FVD improvement compared to the current SOTA model on SSv2 and 83.7% average preference in the human evaluation. Our project is available at https://seervideodiffusion.github.io/",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a novel approach to text-conditioned video prediction (TVP) using latent diffusion models. The proposed method, named Seer, leverages the prior knowledge of pre-trained text-to-image (T2I) models to generate high-quality videos by fine-tuning a few layers on a small amount of data. Seer incorporates a Frame Sequential Text Decomposer module to decompose the language instruction into fine-grained sub-instructions, ensuring precise integration into each frame of generation.\n\nStrengths:\n1. The proposed method, Seer, is sample and computation-efficient, making it suitable for large-scale video prediction tasks.\n2. The use of Frame Sequential Text Decomposer module ensures precise integration of language instructions into each frame of generation.\n3. The experiments demonstrate superior performance of Seer over all the recent models.\n\nWeaknesses:\n1. The paper lacks a clear explanation of the hyperparameter tuning process for the diffusion model and the Frame Sequential Text Decomposer.\n2. The evaluation metrics used are limited to Frechet Video Distance (FVD) and Kernel Video Distance (KVD), which may not be comprehensive for evaluating TVP performance.\n3. The paper does not discuss the potential limitations of using pre-trained T2I models for TVP tasks.\n\nSuggestions for Improvement:\n1. Provide a detailed explanation of the hyperparameter tuning process for the diffusion model and the Frame Sequential Text Decomposer.\n2. Use a more comprehensive set of evaluation metrics to assess TVP performance.\n3. Discuss the potential limitations of using pre-trained T2I models for TVP tasks.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors should address the limitations of the current approach and provide a more comprehensive evaluation of the proposed method. Additionally, the authors should consider incorporating additional evaluation metrics to assess TVP performance.\n\n$$\n\\text{Recommendation: Revisions and Resubmission}\n$$",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper proposes a novel approach for text-conditioned video prediction (TVP) using latent diffusion models. The authors introduce a novel component called Frame Sequential Text (FSText) Decomposer, which decomposes global language instructions into fine-grained sub-instructions that are synchronized with video frames. The proposed model, called Seer, leverages the prior knowledge of pre-trained text-to-image models and achieves state-of-the-art performance in TVP tasks.\n\nStrengths:\n\n* The paper introduces a novel approach for TVP that leverages the prior knowledge of pre-trained text-to-image models.\n* The authors provide a detailed analysis of the computational efficiency of the proposed model.\n* The experiments demonstrate the superior performance of Seer over state-of-the-art models.\n\nWeaknesses:\n\n* The paper does not provide a clear comparison with other state-of-the-art models in terms of computational efficiency.\n* The authors do not provide a thorough analysis of the limitations of the proposed model.\n\nSuggestions for Improvement:\n\n* The authors could provide a more detailed analysis of the limitations of the proposed model and suggest potential avenues for improvement.\n* The authors could provide a more comprehensive comparison with other state-of-the-art models in terms of computational efficiency.\n* The authors could provide a more thorough analysis of the results of the human evaluation study.\n\nRecommendation:\n\n* The paper is well-written and provides a clear and concise overview of the proposed approach.\n* The authors provide a thorough analysis of the computational efficiency of the proposed model.\n* The experiments demonstrate the superior performance of Seer over state-of-the-art models.\n* The paper could be improved with a more detailed analysis of the limitations of the proposed model and a more comprehensive comparison with other state-of-the-art models.\n\nRating: 8/10",
        "gr_score_with": 0.6089108910891089,
        "gr_score_without": 0.4503311258278146,
        "or_score_with": 0.3613861386138614,
        "or_score_without": 0.4503311258278146,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 136,
            "green_fraction": 0.4503311258278146,
            "num_orange_tokens": 136,
            "orange_fraction": 0.4503311258278146,
            "green_z_score": 1.785395745792769,
            "orange_z_score": 89.91431316430915,
            "green_p_value": 0.03709857410818022,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 202,
            "num_green_tokens": 123,
            "green_fraction": 0.6089108910891089,
            "num_orange_tokens": 73,
            "orange_fraction": 0.3613861386138614,
            "green_z_score": 6.060816640506043,
            "orange_z_score": 58.76992322398006,
            "green_p_value": 6.771607663662232e-10,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CONVOLUTION MEETS LORA: PARAMETER EFFI-",
        "abstract": "The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM\u2019s local prior assumption. Notably, Conv-LoRA not only preserves SAM\u2019s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM\u2019s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores ConvLoRA\u2019s superiority in adapting SAM to real-world semantic segmentation tasks.1",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces Conv-LoRA, a parameter-efficient fine-tuning approach for the Segment Anything Model (SAM) in semantic segmentation tasks. SAM is a foundation model for image segmentation that exhibits zero-shot generalization in typical scenarios but struggles in specialized domains like medical imagery and remote sensing. Conv-LoRA combines ultra-lightweight convolutional parameters with Low-Rank Adaptation (LoRA) to inject image-related inductive biases into the plain ViT encoder, enhancing SAM's local prior assumption and its capacity to learn high-level image semantics.\n\nStrengths:\n1. The proposed approach, Conv-LoRA, demonstrates promising results in adapting SAM to various downstream segmentation applications, including natural images, agriculture, remote sensing, and healthcare.\n2. The incorporation of lightweight convolutional parameters with LoRA enables the model to inject image-related local prior knowledge, which is essential for capturing fine-grained details within images.\n3. The authors provide a comprehensive analysis of the effectiveness of Conv-LoRA, including ablation studies, experiments, and comparisons with existing methods.\n\nWeaknesses:\n1. The paper relies heavily on the pretraining of SAM on a vast dataset of segmentation masks, which may not generalize well to other domains or tasks.\n2. The authors acknowledge that SAM's image encoder struggles to learn high-level semantic information, which can be alleviated through finetuning relatively few parameters in the encoder.\n3. The performance of Conv-LoRA may not yet consistently outperform domain-specific state-of-the-art (SOTA) models, highlighting the need for further research and tailoring of the approach to specific applications.\n\nSuggestions for Improvement:\n1. The authors should investigate the effect of different hyperparameters, such as the number of experts, scaling ratios, and optimization schedules, on the performance of Conv-LoRA.\n2. To further improve the generalizability of Conv-LoRA, the authors could explore its application to other foundation models and downstream tasks.\n3. The authors should provide more detailed explanations of the LoRA mechanism and its integration with convolutional layers, as well as its theoretical foundations.\n\nRecommendation:\nBased on the provided analysis, I recommend that Conv-LoRA be accepted for publication in its current form. However, I suggest that the authors address the limitations and weaknesses mentioned above to further strengthen the paper. Specifically, I recommend that they provide more comprehensive experiments, ablation studies, and theoretical analysis to demonstrate the robustness and generalizability of Conv-LoRA.",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\nThe paper introduces Conv-LoRA, a parameter-efficient fine-tuning approach for adapting the Segment Anything Model (SAM) to real-world semantic segmentation tasks. SAM is a foundation model for image segmentation that exhibits remarkable zero-shot generalization but faces challenges when applied to specialized domains like medical imagery and remote sensing. Conv-LoRA combines ultra-lightweight convolutional parameters with Low-Rank Adaptation (LoRA) to inject image-related inductive biases into the plain ViT encoder of SAM, thereby reinforcing its local prior assumption.\n\nStrengths:\n\n1.  Conv-LoRA effectively addresses the limitation of SAM's image encoder by injecting lightweight convolutional operations that strengthen the vision-specific local prior.\n2.  The approach is simple, generic, and achieves promising results across multiple domains, including natural images, agriculture, remote sensing, and healthcare.\n3.  Conv-LoRA leverages the power of LoRA to dynamically select the optimal scale for injecting the local prior into feature maps, improving performance and efficiency.\n4.  The paper provides extensive experimentation across diverse benchmarks, demonstrating the superiority of Conv-LoRA in adapting SAM to real-world semantic segmentation tasks.\n\nWeaknesses:\n\n1.  Conv-LoRA may not outperform domain-specific SOTA models, highlighting the need for tailored approaches to specialized domains.\n2.  The paper's focus on developing a general PEFT method for SAM may not address the specific challenges of certain domains or tasks.\n\nSuggestions for Improvement:\n\n1.  To further strengthen Conv-LoRA's performance, consider combining it with other PEFT methods or tailoring the approach to specific domains or tasks.\n2.  Investigate the effects of varying the scaling ratio or expert selection on the performance of Conv-LoRA across different datasets.\n3.  Conduct further ablation studies to analyze the impact of different architectural components on Conv-LoRA's performance.\n\nRecommendation:\n\n1.  Conv-LoRA is a promising PEFT approach for adapting SAM to real-world semantic segmentation tasks, demonstrating superior performance across multiple domains.\n2.  To fully leverage Conv-LoRA's capabilities, consider combining it with other PEFT methods or tailoring the approach to specific domains or tasks.\n3.  Future work should focus on addressing the challenges of certain domains or tasks and further optimizing Conv-LoRA's performance.\n\n$$$$",
        "gr_score_with": 0.47058823529411764,
        "gr_score_without": 0.3925925925925926,
        "or_score_with": 0.5147058823529411,
        "or_score_without": 0.5308641975308642,
        "output_without": {
            "num_tokens_scored": 405,
            "num_green_tokens": 159,
            "green_fraction": 0.3925925925925926,
            "num_orange_tokens": 215,
            "orange_fraction": 0.5308641975308642,
            "green_z_score": -0.30429030972509225,
            "orange_z_score": 131.61149513182548,
            "green_p_value": 0.6195466364624429,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 160,
            "green_fraction": 0.47058823529411764,
            "num_orange_tokens": 175,
            "orange_fraction": 0.5147058823529411,
            "green_z_score": 2.656844656620286,
            "orange_z_score": 116.87272717804727,
            "green_p_value": 0.003943788959836535,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FORMER FOR PHOTOREALISTIC TEXT-TO-IMAGE SYNTHESIS",
        "abstract": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-\u03b1, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024 \u00d7 1024 resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that respectively optimize pixel dependency, textimage alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-\u03b1\u2019s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART\u03b1 only takes 12% of Stable Diffusion v1.5\u2019s training time (\u223c753 vs. \u223c6,250 A100 GPU days), saving nearly $300,000 ($28,400 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-\u03b1 excels in image quality, artistry, and semantic control. We hope PIXART-\u03b1 will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces PIXART-\u03b1, a Transformer-based text-to-image (T2I) diffusion model that achieves competitive image generation quality with significant reductions in training costs and CO2 emissions. The model is designed to tackle the challenges of T2I generation, including text-image misalignment, deficient descriptions, and infrequent diverse vocabulary usage. The authors propose three core designs: training strategy decomposition, efficient T2I Transformer, and high-informative data. These designs enable the model to learn stable text-image alignments, improve aesthetic quality, and reduce training costs.\n\nStrengths:\n1. The paper presents a novel approach to T2I generation that addresses the challenges of existing models.\n2. The authors provide a detailed analysis of the training process and propose efficient strategies to reduce training costs and CO2 emissions.\n3. The model achieves competitive image generation quality, as demonstrated by extensive experiments.\n4. The paper provides a thorough evaluation of the model's performance on various metrics, including FID, T2I-CompBench, and user study.\n\nWeaknesses:\n1. The paper relies heavily on existing models (e.g., Imagen, SDXL, and Midjourney) as baselines, which may limit the model's ability to generalize to new scenarios.\n2. The authors do not provide a detailed analysis of the model's limitations and potential biases.\n3. The paper does not discuss the potential applications of the model in various domains, such as healthcare, education, or marketing.\n4. The authors mention the potential for future exploration of performance enhancement but do not provide a clear roadmap for future research.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the model's limitations and potential biases to improve its robustness and generalizability.\n2. The paper should discuss the potential applications of the model in various domains to demonstrate its versatility and value.\n3. The authors should provide a clear roadmap for future research, including potential extensions and improvements to the model.\n4. The paper should include more detailed experiments and analysis of the model's performance on various metrics to further demonstrate its effectiveness.\n\nRecommendation:\nBased on the paper's strengths and weaknesses, I recommend that the authors revise and resubmit their work. The authors should address the limitations and potential biases of the model, provide a more detailed analysis of its applications, and outline a clear roadmap for future research. With these improvements, the paper has the potential to make a significant contribution to the field of T2I generation and its applications.",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\nThe paper introduces PIXART-\u03b1, a Transformer-based text-to-image (T2I) diffusion model that achieves competitive image generation quality with reduced training costs and CO2 emissions. The model is designed to address the challenges of traditional T2I models, including the high training costs and environmental impact. The authors propose three core designs: training strategy decomposition, efficient T2I Transformer, and high-informative data. These designs enable the model to achieve near-commercial application standards in image generation quality while reducing training costs and environmental impact.\n\nStrengths:\n1. The authors have made significant contributions to the field of text-to-image synthesis by introducing a new diffusion model that achieves competitive image generation quality with reduced training costs and environmental impact.\n2. The proposed three core designs are effective in addressing the challenges of traditional T2I models and enabling the model to achieve near-commercial application standards in image generation quality.\n3. The authors have conducted extensive experiments to evaluate the performance of the model and have demonstrated its superiority over state-of-the-art T2I models.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the limitations of the proposed model and its potential biases.\n2. The authors have not evaluated the model's performance on a large-scale dataset, which is essential for generalization and deployment.\n3. The paper does not provide a clear explanation of the reparameterization technique used in the efficient T2I Transformer.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of the proposed model and its potential biases.\n2. The authors should evaluate the model's performance on a large-scale dataset to demonstrate its generalization and deployment capabilities.\n3. The authors should provide a clear explanation of the reparameterization technique used in the efficient T2I Transformer.\n\nRecommendation:\nI recommend that the authors revise the paper based on the feedback provided and address the limitations and weaknesses mentioned above. The authors have made significant contributions to the field of text-to-image synthesis, and with further refinement and evaluation, the model has the potential to make a significant impact in the field.\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$\n\n$$",
        "gr_score_with": 0.4699248120300752,
        "gr_score_without": 0.42296918767507,
        "or_score_with": 0.48872180451127817,
        "or_score_without": 0.49299719887955185,
        "output_without": {
            "num_tokens_scored": 357,
            "num_green_tokens": 151,
            "green_fraction": 0.42296918767507,
            "num_orange_tokens": 176,
            "orange_fraction": 0.49299719887955185,
            "green_z_score": 0.8858784221879566,
            "orange_z_score": 111.20709448579751,
            "green_p_value": 0.1878415224084064,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 266,
            "num_green_tokens": 125,
            "green_fraction": 0.4699248120300752,
            "num_orange_tokens": 130,
            "orange_fraction": 0.48872180451127817,
            "green_z_score": 2.327911710596178,
            "orange_z_score": 95.14868287404431,
            "green_p_value": 0.00995839614659225,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ON DIFFERENTIALLY PRIVATE FEDERATED LINEAR CONTEXTUAL BANDITS",
        "abstract": "We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user\u2019s privacy. We identify three issues in the state-of-the-art (Dubey & Pentland, 2020): (i) failure of claimed privacy protection, (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. To resolve these issues, we take a two-step approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential privacy, under which we show that our algorithm can achieve nearly \u201coptimal\u201d regret without a trusted server. We accomplish this via two different schemes \u2013 one relies on a new result on privacy amplification via shuffling for DP mechanisms and another one leverages the integration of a shuffle protocol for vector sum into the tree-based mechanism, both of which might be of independent interest. Finally, we support our theoretical results with numerical evaluations over contextual bandit instances generated from both synthetic and real-life data.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$\\boxed{Paper Summary:}$ \nThe paper proposes a new algorithm for privately federated linear contextual bandits (LCBs) under differential privacy. The authors identify three issues in the state-of-the-art algorithm of Dubey and Pentland (2020): privacy leakage, incorrect regret bound, and ungrounded communication cost. They propose a two-step approach to address these issues, which involves designing a generic algorithmic framework and proving regret guarantees under different privacy constraints. The authors also show that their algorithm can achieve nearly optimal regret without a trusted server under the shuffle model of differential privacy.\n\n$\\boxed{Strengths:}$ \n1. The paper identifies three significant issues in the state-of-the-art algorithm and proposes a novel solution.\n2. The authors provide a detailed analysis of the proposed algorithm and its performance guarantees under different privacy constraints.\n3. The paper demonstrates the effectiveness of the proposed algorithm through numerical evaluations on both synthetic and real-life data.\n\n$\\boxed{Weaknesses:}$ \n1. The paper assumes a specific adversary model, which may not be representative of all possible scenarios.\n2. The authors do not provide a comprehensive comparison with other existing algorithms for privately federated LCBs.\n3. Some of the proofs and results are omitted or mentioned briefly, which may make it difficult for readers to fully understand the underlying mathematics.\n\n$\\boxed{Suggestions for Improvement:}$ \n1. Provide a more comprehensive comparison with existing algorithms for privately federated LCBs.\n2. Include more detailed proofs and results to ensure that readers can fully understand the underlying mathematics.\n3. Consider extending the analysis to more complex scenarios, such as multiple trusted servers or non-i.i.d. data.\n\n$\\boxed{Recommendation:}$ \nThe proposed algorithm is a significant contribution to the field of privately federated LCBs. While it has some limitations, the paper provides a detailed analysis of the algorithm's performance guarantees under different privacy constraints. The authors demonstrate the effectiveness of the proposed algorithm through numerical evaluations, which suggests that it is a viable solution for privately federated LCBs. However, further research is needed to address the limitations and provide a more comprehensive comparison with existing algorithms.",
        "peer_review_with_watermark": "$$$${$\n\nPaper Title: ON DIFFERENTIALLY PRIVATE FEDERATED LINEAR CONTEXTUAL BANDITS\nAbstract:\nThis paper investigates the cross-silo federated linear contextual bandits (LCB) problem under differential privacy. The authors identify three issues in the state-of-the-art (Dubey & Pentland, 2020): (i) failure of claimed privacy protection, (ii) incorrect regret bound due to noise miscalculation, and (iii) ungrounded communication cost. To resolve these issues, the authors propose a two-step approach, designing an algorithmic framework and studying federated LCBs under two different privacy constraints.\n\nSummary:\nThe authors consider the cross-silo federated linear contextual bandits (LCB) problem, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user's privacy. They propose a two-step approach to address the issues in the state-of-the-art algorithm. First, they design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, they study federated LCBs under two different privacy constraints, establishing privacy and regret guarantees under silo-level local differential privacy and shuffle model of differential privacy.\n\nStrengths:\nThe authors make significant contributions to the field of federated learning, identifying three issues in the state-of-the-art algorithm and proposing a two-step approach to resolve them. The authors also provide a detailed analysis of the communication cost and privacy guarantee, which is a crucial aspect of federated learning. The use of the shuffle model of differential privacy is also an innovative approach.\n\nWeaknesses:\nThe authors have several limitations in their work. First, the state-of-the-art algorithm is only considered, and there is no comparison with other algorithms. Second, the authors only consider the case where the central server is trusted, and there is no discussion on the case where the central server is not trusted. Third, the authors use a simple additive noise, which may not be the most effective approach.\n\nSuggestions for Improvement:\nTo improve the work, the authors should consider the following suggestions:\n1. Compare the proposed algorithm with other algorithms, such as the one proposed by Lowy & Razaviyayn (2021).\n2. Consider the case where the central server is not trusted, and discuss the implications on the communication cost and privacy guarantee.\n3. Investigate the use of more advanced noise mechanisms, such as the one proposed by Feldman et al. (2022).\n\nRecommendation:\nThe authors have made significant contributions to the field of federated learning, identifying three issues in the state-of-the-art algorithm and proposing a two-step approach to resolve them. However, the work has several limitations, and the authors should consider the suggestions for improvement. Based on the current state of the art, I recommend that the authors continue their work and consider the following:\n\n* Investigate the use of more advanced noise mechanisms, such as the one proposed by Feldman et al. (2022).\n* Consider the case where the central server is not trusted, and discuss the implications on the communication cost and privacy guarantee.\n* Compare the proposed algorithm with other algorithms, such as the one proposed by Lowy & Razaviyayn (2021).\n\nOverall, the work is well-structured, and the authors have provided a detailed analysis of the communication cost and privacy guarantee. However, the work has several limitations, and the authors should consider the suggestions for improvement.",
        "gr_score_with": 0.5034965034965035,
        "gr_score_without": 0.43769968051118213,
        "or_score_with": 0.4382284382284382,
        "or_score_without": 0.44089456869009586,
        "output_without": {
            "num_tokens_scored": 313,
            "num_green_tokens": 137,
            "green_fraction": 0.43769968051118213,
            "num_orange_tokens": 138,
            "orange_fraction": 0.44089456869009586,
            "green_z_score": 1.3614579042824346,
            "orange_z_score": 88.4899609418569,
            "green_p_value": 0.08668451396513721,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 429,
            "num_green_tokens": 216,
            "green_fraction": 0.5034965034965035,
            "num_orange_tokens": 188,
            "orange_fraction": 0.4382284382284382,
            "green_z_score": 4.375711730319209,
            "orange_z_score": 102.96038832000174,
            "green_p_value": 6.051844397840424e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ZERO BUBBLE (ALMOST) PIPELINE PARALLELISM",
        "abstract": "Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 15% in throughput under a similar memory limit. This number can be further pushed to 30% when the memory constraint is relaxed. We believe our results mark a major step forward in harnessing the true potential of pipeline parallelism. The source code based on Megatron-LM is publicly avaiable at https: //github.com/sail-sg/zero-bubble-pipeline-parallelism.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces a novel strategy to improve the efficiency of pipeline parallelism by splitting the activation gradient and parameter gradient in backward computation. The authors propose two handcrafted schedules (ZB-H1 and ZB-H2) and an automatic scheduling algorithm that can minimize the pipeline bubble rate under different memory budgets. The schedules produced by the automatic algorithm consistently outperform 1F1B and achieve close to zero bubble rate.\n\nStrengths:\n1. The paper presents a novel approach to improving pipeline parallelism efficiency by splitting the activation gradient and parameter gradient in backward computation.\n2. The authors propose two handcrafted schedules (ZB-H1 and ZB-H2) and an automatic scheduling algorithm that can minimize the pipeline bubble rate under different memory budgets.\n3. The paper presents extensive empirical evaluations of the proposed schedules against 1F1B and other baseline methods.\n\nWeaknesses:\n1. The paper assumes that the execution times of forward, backward, and weight computations are identical, which may not be realistic in all cases.\n2. The paper does not provide a comprehensive analysis of the communication overhead and its impact on the pipeline bubble rate.\n3. The authors do not provide a detailed explanation of the optimization tool used to navigate the space of possible schedules.\n\nSuggestions for Improvement:\n1. The authors should provide a more realistic analysis of the execution times of forward, backward, and weight computations.\n2. The authors should provide a more detailed analysis of the communication overhead and its impact on the pipeline bubble rate.\n3. The authors should provide a more comprehensive explanation of the optimization tool used to navigate the space of possible schedules.\n\nRecommendation:\nThe paper presents a novel approach to improving pipeline parallelism efficiency and presents extensive empirical evaluations of the proposed schedules against 1F1B and other baseline methods. However, the paper assumes unrealistic execution times and does not provide a comprehensive analysis of communication overhead. The authors should address these limitations to strengthen the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\nThis paper introduces a novel scheduling strategy to improve the efficiency of pipeline parallelism in distributed training. The key idea is to split the backward computation into two parts, one that computes the gradient for the input and another that computes the gradient for the parameters. This is achieved by handcrafting novel pipeline schedules that outperform baseline methods. The authors also develop an algorithm to automatically find an optimal schedule based on specific model configuration and memory limit. Additionally, they introduce a technique to bypass synchronizations during the optimizer step, which is necessary to achieve zero pipeline bubbles under synchronous training semantics.\n\nStrengths:\n1. The paper introduces a novel scheduling strategy that addresses the issue of pipeline bubbles in distributed training.\n2. The handcrafted schedules presented in the paper outperform baseline methods, including the 1F1B schedule.\n3. The automatic scheduling algorithm is a significant improvement over handcrafted schedules, as it can adapt to realistic scenarios.\n4. The paper provides a detailed analysis of the bubble rate and its relationship with memory limit, which is a key aspect of pipeline parallelism.\n5. The experimental results demonstrate the effectiveness of the proposed scheduling strategy, including the achievement of zero pipeline bubbles under synchronous training semantics.\n\nWeaknesses:\n1. The paper assumes a specific setting, i.e., synchronous training semantics, which may not be applicable to all scenarios.\n2. The handcrafted schedules presented in the paper may not be suitable for all models, as the analysis is based on a specific transformer architecture.\n3. The automatic scheduling algorithm may not perform well in scenarios with limited resources or specific constraints.\n4. The paper does not provide a comprehensive comparison with other scheduling strategies, which may limit its generalizability.\n5. The analysis of the bubble rate and its relationship with memory limit is based on a specific mathematical model, which may not be accurate in all scenarios.\n\nSuggestions for Improvement:\n1. The paper should consider exploring general mixed strategies for large-scale distributed training, which may improve its generalizability.\n2. The analysis should be extended to other scheduling strategies, including asynchronous pipeline parallelism.\n3. The paper should provide a more detailed analysis of the bubble rate and its relationship with memory limit, including scenarios with limited resources or specific constraints.\n4. The experimental results should be compared with other scheduling strategies to demonstrate its effectiveness.\n5. The paper should consider exploring the application of the proposed scheduling strategy to other domains, including non-deep learning models.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend its acceptance to the conference. The novel scheduling strategy presented in the paper addresses a key issue in pipeline parallelism, and the handcrafted schedules outperform baseline methods. The automatic scheduling algorithm is a significant improvement over handcrafted schedules, and the experimental results demonstrate its effectiveness. However, the paper should consider exploring general mixed strategies, other scheduling strategies, and the application of the proposed scheduling strategy to other domains to improve its generalizability.",
        "gr_score_with": 0.5487465181058496,
        "gr_score_without": 0.4,
        "or_score_with": 0.403899721448468,
        "or_score_without": 0.5069767441860465,
        "output_without": {
            "num_tokens_scored": 215,
            "num_green_tokens": 86,
            "green_fraction": 0.4,
            "num_orange_tokens": 109,
            "orange_fraction": 0.5069767441860465,
            "green_z_score": 0.0,
            "orange_z_score": 90.91235514270213,
            "green_p_value": 0.5,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 197,
            "green_fraction": 0.5487465181058496,
            "num_orange_tokens": 145,
            "orange_fraction": 0.403899721448468,
            "green_z_score": 5.752921021407433,
            "orange_z_score": 93.27852794633534,
            "green_p_value": 4.385723514629978e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TRANSFORMER-VQ: LINEAR-TIME TRANSFORMERS",
        "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmaxbased dense self-attention in linear time. Transformer-VQ\u2019s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: https://github.com/transformer-vq/transformer_vq k1 k2 k3 k4 k5 7\u2192 VQ \u2248 k\u03021 k\u03022 k\u03023 k\u03024 k\u03025 Figure 1: Schematic of the VQ-Attention approximation. The colorful and blank boxes depict the keys and attention weights, respectively. The keys on the right have been vector-quantized. Since the green keys k2, k5 map to the same code, they have the same attention weights in this attention head.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time. The model uses vector-quantized keys, localized positional biases, and a compressive cache that can be attended to efficiently. The authors demonstrate the effectiveness of Transformer-VQ on several large-scale language modeling tasks, including Enwik8, PG-19, and ImageNet64.\n\nStrengths:\n\n* The paper presents a novel and efficient approach to computing dense self-attention in linear time.\n* The use of vector-quantized keys and compressive cache enables the model to attend to the same keys in compressed form, leading to significant speedup.\n* The authors demonstrate the effectiveness of Transformer-VQ on several large-scale language modeling tasks, including Enwik8, PG-19, and ImageNet64.\n* The paper provides a detailed mathematical description of the attention mechanism and pseudocode for implementation.\n\nWeaknesses:\n\n* The paper does not provide a thorough analysis of the theoretical limits of the proposed attention mechanism.\n* The authors assume that the vector-quantized keys can be approximated using the k-means algorithm, which may not be accurate in practice.\n* The paper does not provide a detailed comparison with other state-of-the-art models on the same tasks.\n* The authors claim that the proposed attention mechanism is equivalent to quadratic-time attention, but the proof is not provided in the paper.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the theoretical limits of the proposed attention mechanism.\n* Investigate the accuracy of the k-means algorithm for approximating vector-quantized keys.\n* Provide a detailed comparison with other state-of-the-art models on the same tasks.\n* Provide a more rigorous proof that the proposed attention mechanism is equivalent to quadratic-time attention.\n\nRecommendation:\n\n* The paper presents a novel and efficient approach to computing dense self-attention in linear time, and the authors demonstrate its effectiveness on several large-scale language modeling tasks.\n* However, the paper lacks a thorough analysis of the theoretical limits of the proposed attention mechanism and provides only a limited comparison with other state-of-the-art models.\n* To improve the paper, the authors should provide a more detailed analysis of the theoretical limits, investigate the accuracy of the k-means algorithm, and provide a more rigorous proof of the equivalence to quadratic-time attention.\n* Overall, the paper is well-written and clearly presents the authors' ideas, but requires more thorough analysis and comparison to establish its significance and impact.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time with respect to sequence length. It uses a combination of vector-quantized keys, localized positional biases, and a compressive cache to enable efficient attention.\n\nThe authors propose a new attention mechanism that approximates the softmax-based attention with vector-quantized keys. They show that their approach can reduce the time complexity of attention from quadratic to linear.\n\nThe authors also present several experiments on large-scale datasets, including Enwik8, PG-19, and ImageNet64. They compare their results with state-of-the-art models and demonstrate the efficiency and flexibility of their approach.\n\nStrengths:\n- The authors propose a new attention mechanism that can reduce the time complexity of attention from quadratic to linear.\n- The approach is efficient and flexible, and can be applied to large-scale datasets.\n- The authors present several experiments that demonstrate the efficiency and effectiveness of their approach.\n\nWeaknesses:\n- The authors assume that the vector-quantized keys are sufficient to capture the attention mechanisms, which may not be the case in all situations.\n- The approach may not be suitable for all types of datasets or tasks.\n\nSuggestions for Improvement:\n- The authors could further explore the properties of vector-quantized keys and their effects on attention mechanisms.\n- The authors could also investigate the use of other types of attention mechanisms, such as sparse attention or low-rank attention.\n\nRecommendation:\n- The authors have made a significant contribution to the field of natural language processing with their proposal of a new attention mechanism that can reduce the time complexity of attention from quadratic to linear.\n- The approach is efficient and flexible, and can be applied to large-scale datasets.\n- However, further exploration of the properties of vector-quantized keys and their effects on attention mechanisms is needed to fully understand the capabilities and limitations of this approach.\n\nOverall, the paper presents a new and efficient attention mechanism that can be applied to large-scale datasets. The authors have made a significant contribution to the field, and their approach has the potential to improve the efficiency and effectiveness of transformer-based models.\n\nRating: 8/10\n\n$$\n\nPlease note that this is a peer review and the rating is subjective. The reviewer's opinion may not reflect the opinion of the reviewer's institution or the broader research community. \n\nAlso, please note that the review is not exhaustive and there may be other strengths and weaknesses that were not mentioned in the review. \n\nPlease let me know if you need any further assistance.",
        "gr_score_with": 0.48732394366197185,
        "gr_score_without": 0.35563380281690143,
        "or_score_with": 0.4450704225352113,
        "or_score_without": 0.4964788732394366,
        "output_without": {
            "num_tokens_scored": 284,
            "num_green_tokens": 101,
            "green_fraction": 0.35563380281690143,
            "num_orange_tokens": 141,
            "orange_fraction": 0.4964788732394366,
            "green_z_score": -1.526179986801255,
            "orange_z_score": 94.21243627256673,
            "green_p_value": 0.936517476909255,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 173,
            "green_fraction": 0.48732394366197185,
            "num_orange_tokens": 158,
            "orange_fraction": 0.4450704225352113,
            "green_z_score": 3.3584732726351563,
            "orange_z_score": 94.25401869000109,
            "green_p_value": 0.00039187150005207333,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "IN-CONTEXT LEARNING LEARNS LABEL RELATION-",
        "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input\u2013label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper investigates the mechanisms of in-context learning (ICL) in Large Language Models (LLMs). ICL is a technique that improves LLM predictions by providing examples of the input\u2013label relationship. However, there is currently no consensus about how ICL works. The authors study the probabilistic aspects of ICL predictions and examine the dynamics of ICL as more examples are provided. They find that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, they also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and does not consider all in-context information equally.\n\nStrengths:\n- The authors provide a thorough analysis of ICL behavior using a range of experiments and metrics.\n- They address the limitations of previous studies and provide a more comprehensive understanding of ICL.\n- The paper highlights the importance of considering probabilistic aspects of ICL predictions.\n\nWeaknesses:\n- The paper relies heavily on experiments with specific LLMs and tasks, which may not be generalizable to other settings.\n- Some of the results, such as the finding that ICL does not treat all information provided in-context equally, may be surprising and require further investigation.\n- The paper does not provide a clear theoretical framework for understanding ICL behavior, which may limit its applicability to other areas of natural language processing.\n\nSuggestions for Improvement:\n- The authors could provide more theoretical insights into ICL behavior, such as exploring the role of label relationships in ICL or developing a more formal framework for understanding ICL dynamics.\n- To improve generalizability, the authors could conduct experiments with a broader range of LLMs and tasks.\n- The paper could benefit from a more detailed discussion of the implications of the findings for alignment and other applications of ICL.\n\nRecommendation:\n- The paper contributes significantly to our understanding of ICL behavior and provides a comprehensive analysis of ICL dynamics.\n- The authors' findings have important implications for the development of more effective ICL methods and for the broader field of natural language processing.\n- With some further development of the theoretical framework and expansion of the experimental scope, this paper has the potential to be a leading work in the field.",
        "peer_review_with_watermark": "$$$$\n$$$$\n$$$$\nPaper Summary:\nThe paper investigates the in-context learning (ICL) ability of Large Language Models (LLMs) and its limitations. The authors provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. They study probabilistic aspects of ICL predictions and examine the dynamics of ICL as more examples are provided. The results show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, they also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and does not consider all in-context information equally.\n\nStrengths:\n\n1. The paper provides a thorough analysis of ICL behavior and its limitations, which is a significant contribution to the field of NLP.\n2. The authors present a novel approach to studying ICL, which involves rephrasing null hypotheses that encode their beliefs about how an idealized conventional learning algorithm should incorporate label information.\n3. The experiments are well-designed and provide a comprehensive picture of ICL 'training dynamics'.\n4. The results are clearly presented and discussed, which facilitates a deeper understanding of the findings.\n\nWeaknesses:\n\n1. The paper assumes a certain level of background knowledge in NLP and machine learning, which may limit its accessibility to non-experts.\n2. Some of the results, such as the finding that ICL does not treat all information provided in-context equally, may be counterintuitive to readers who are not familiar with ICL or probabilistic models.\n3. The paper does not provide a detailed comparison with other approaches to ICL, which may limit its impact and relevance.\n4. Some of the experiments, such as the ones involving arbitrary labels, may be limited by their design or sampling strategy.\n\nSuggestions for Improvement:\n\n1. The authors could provide a more detailed comparison with other approaches to ICL, such as gradient-based finetuning or few-shot learning.\n2. They could provide more context and explanations for the results, especially for readers who are not familiar with probabilistic models or NLP.\n3. The authors could explore the limitations of their approach and provide ideas for future work, such as investigating the effects of different sampling strategies or using different types of label noise.\n4. They could provide a more detailed analysis of the results, including a more thorough examination of the effects of label randomization, arbitrary labels, and non-stationary input distributions.\n\nRecommendation:\nOverall, the paper provides a significant contribution to the field of NLP and ICL. The authors present a thorough analysis of ICL behavior and its limitations, which is a major breakthrough. However, the paper assumes a certain level of background knowledge and may be limited by its design or sampling strategy. With some additional work, such as a more detailed comparison with other approaches and a more thorough analysis of the results, the paper has the potential to be a major impact in the field of NLP. \n\nRating: 8/10 \n\n$$$$\n$$$$\n$$$$",
        "gr_score_with": 0.45365853658536587,
        "gr_score_without": 0.3872832369942196,
        "or_score_with": 0.47317073170731705,
        "or_score_without": 0.49710982658959535,
        "output_without": {
            "num_tokens_scored": 346,
            "num_green_tokens": 134,
            "green_fraction": 0.3872832369942196,
            "num_orange_tokens": 172,
            "orange_fraction": 0.49710982658959535,
            "green_z_score": -0.4828464094367624,
            "orange_z_score": 116.34468076135407,
            "green_p_value": 0.6853976022477283,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 410,
            "num_green_tokens": 186,
            "green_fraction": 0.45365853658536587,
            "num_orange_tokens": 194,
            "orange_fraction": 0.47317073170731705,
            "green_z_score": 2.2178140560001576,
            "orange_z_score": 120.47263602574252,
            "green_p_value": 0.013283756667921077,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
        "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper presents a novel learning paradigm called \"Forget to Mitigate Overfitting\" (FOMO) to address the problem of robust overfitting in adversarial training (AT). FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. The paper evaluates the effectiveness of FOMO on benchmark datasets and adversarial attacks, demonstrating significant improvements in robust performance and generalization.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The paper presents a novel and innovative approach to addressing robust overfitting in AT.\n2. The use of active forgetting and relearning mechanisms to improve generalization is well-motivated and grounded in cognitive psychology research.\n3. The paper provides extensive empirical results, including comparisons with baseline methods and other robust generalization approaches.\n4. The authors address potential limitations and provide ablation studies to demonstrate the importance of individual components.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The paper relies heavily on empirical results, which may not generalize to other domains or scenarios.\n2. The theoretical foundations of FOMO are not thoroughly explored, and the underlying cause of robust overfitting is not yet fully understood.\n3. Some of the experimental results, such as those on CIFAR-100 and SVHN, are not detailed enough to be fully understood.\n4. The paper does not provide a clear comparison with other state-of-the-art robust generalization methods.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. Provide a more detailed theoretical analysis of FOMO and its underlying mechanisms.\n2. Conduct more extensive experiments on other datasets and scenarios to demonstrate the generalizability of FOMO.\n3. Provide a clearer comparison with other state-of-the-art robust generalization methods.\n4. Include more detailed ablation studies to demonstrate the importance of individual components.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the paper's strengths and weaknesses, I recommend that the authors revise and resubmit the paper for further evaluation. The paper presents a novel and innovative approach to addressing robust overfitting in AT, and the empirical results demonstrate significant improvements in robust performance and generalization. However, the paper's reliance on empirical results and lack of theoretical foundations and comparisons with other methods need to be addressed. With revisions, the paper has the potential to make a significant contribution to the field of robust machine learning.",
        "peer_review_with_watermark": "$$$$\nThe paper proposes a novel learning paradigm, Forget to Mitigate Overfitting (FOMO), to address the problem of robust overfitting in adversarial training (AT). FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features.\n\n**Strengths:**\n\n1. The paper provides a clear and concise overview of the problem of robust overfitting in AT and the limitations of existing methods.\n2. The proposed FOMO paradigm is well-structured and easy to follow, with a clear explanation of the forgetting and relearning phases.\n3. The paper presents a thorough analysis of the experimental results, with a detailed comparison of the performance of FOMO with various baseline methods.\n4. The use of ablation studies to evaluate the importance of individual components of the FOMO paradigm is a valuable contribution.\n\n**Weaknesses:**\n\n1. The paper may benefit from a more detailed analysis of the theoretical underpinnings of the FOMO paradigm, particularly in relation to the concept of active forgetting in the brain.\n2. Some of the experimental results, such as those presented in Figure 2, may be difficult to interpret without additional context or clarification.\n3. The paper may be more effective in its presentation of the FOMO paradigm if it includes more detailed examples or case studies of its application in real-world scenarios.\n\n**Suggestions for Improvement:**\n\n1. Consider adding more theoretical analysis of the FOMO paradigm, particularly in relation to the concept of active forgetting in the brain.\n2. Provide more detailed explanations of the experimental results, including additional context or clarification where necessary.\n3. Consider including more case studies or examples of the application of the FOMO paradigm in real-world scenarios.\n\n**Recommendation:**\n\nBased on the strengths and weaknesses of the paper, I recommend that it be considered for publication in a reputable conference or journal in the field of machine learning or artificial intelligence. However, I would suggest that the authors revise the paper to address the weaknesses and suggestions for improvement outlined above, and provide a more detailed analysis of the theoretical underpinnings of the FOMO paradigm.\n\n$$$$\nRecommendation: Accept with minor revisions.\n\n$$$$\nPlease let me know if I can be of assistance with the revision.",
        "gr_score_with": 0.611764705882353,
        "gr_score_without": 0.5096952908587258,
        "or_score_with": 0.3205882352941177,
        "or_score_without": 0.38227146814404434,
        "output_without": {
            "num_tokens_scored": 361,
            "num_green_tokens": 184,
            "green_fraction": 0.5096952908587258,
            "num_orange_tokens": 138,
            "orange_fraction": 0.38227146814404434,
            "green_z_score": 4.254376921676045,
            "orange_z_score": 85.16993678306126,
            "green_p_value": 1.0481596313396335e-05,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 208,
            "green_fraction": 0.611764705882353,
            "num_orange_tokens": 109,
            "orange_fraction": 0.3205882352941177,
            "green_z_score": 7.970533969860858,
            "orange_z_score": 69.06747861394699,
            "green_p_value": 7.89951144321494e-16,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ENERGY-GUIDED ENTROPIC NEURAL OPTIMAL TRANSPORT",
        "abstract": "Energy-based models (EBMs) are known in the Machine Learning community for decades. Since the seminal works devoted to EBMs dating back to the noughties, there have been a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN-based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present a novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. From the theoretical perspective, we prove generalization bounds for our technique. In practice, we validate its applicability in toy 2D and image domains. To showcase the scalability, we empower our method with a pre-trained StyleGAN and apply it to high-res AFHQ 512 \u00d7 512 unpaired I2I translation. For simplicity, we choose simple shortand long-run EBMs as a backbone of our Energy-guided Entropic OT approach, leaving the application of more sophisticated EBMs for future research. Our code is available at: https: //github.com/PetrMokrov/Energy-guided-Entropic-OT Figure 1: AFHQ 512\u00d7 512 Cat\u2192Dog unpaired translation by our Energy-guided EOT solver applied in the latent space of StyleGAN2-ADA. Our approach does not need data2latent encoding. Left: source samples; right: translated samples.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe authors propose a novel methodology to bridge the gap between Energy-based Models (EBMs) and Entropy-regularized Optimal Transport (EOT). The proposed approach utilizes the recent developments and technical improvements of EBMs to enrich EOT. The authors prove generalization bounds for their technique and validate its applicability in toy 2D and image domains.\n\n$$\\text{Strengths:}$$\n1. The authors propose a novel approach to bridge the gap between EBMs and EOT.\n2. The authors prove generalization bounds for their technique.\n3. The authors validate the applicability of their approach in toy 2D and image domains.\n\n$$\\text{Weaknesses:}$$\n1. The authors' approach may be computationally expensive due to the use of MCMC methods.\n2. The authors' approach may not be suitable for all types of data, particularly those with high-dimensional spaces.\n3. The authors' approach may not be as efficient as other existing methods for EOT.\n\n$$\\text{Suggestions for Improvement:}$$\n1. The authors should consider using more efficient MCMC methods to reduce computational cost.\n2. The authors should explore the use of more efficient optimization algorithms to improve the efficiency of their approach.\n3. The authors should consider using more robust methods to handle high-dimensional spaces.\n\n$$\\text{Recommendation:}$$\nThe proposed approach is a valuable contribution to the field of EOT and EBMs. However, further research is needed to improve the efficiency and robustness of the approach. The authors should consider addressing the weaknesses and suggestions for improvement mentioned above to make their approach more suitable for practical applications.",
        "peer_review_with_watermark": "$$$$\n\nPaper Title: Energy-Guided Entropic Neural Optimal Transport\n\nThe paper presents a novel approach to solving the Entropy-regularized Optimal Transport (EOT) problem using energy-based models (EBMs). The authors propose a method that bridges the gap between EOT and EBMs, allowing for the utilization of recent developments and technical improvements of EBMs to enrich EOT.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the EOT problem and its applications in generative modelling.\n2. The authors present a novel approach to solving EOT using EBMs, which is well-motivated and theoretically grounded.\n3. The paper includes a thorough analysis of the EOT problem and its relation to EBMs, which is essential for understanding the proposed method.\n\nWeaknesses:\n\n1. The paper assumes a certain level of background knowledge in the reader, particularly in the areas of Optimal Transport and EBMs. It may be beneficial to include more introductory material or references to help readers who are not familiar with these topics.\n2. The paper could benefit from more experimental results to demonstrate the efficacy of the proposed method. Currently, the paper only includes a few examples and does not provide a comprehensive evaluation of the method's performance.\n3. The paper's focus on theoretical guarantees may make it less accessible to readers who are primarily interested in practical applications. It may be beneficial to include more discussion of the method's practical implications and potential limitations.\n\nSuggestions for Improvement:\n\n1. Provide more introductory material or references to help readers who are not familiar with Optimal Transport and EBMs.\n2. Include more experimental results to demonstrate the efficacy of the proposed method.\n3. Provide more discussion of the method's practical implications and potential limitations.\n4. Consider adding more theoretical guarantees or bounds to the paper to increase its accessibility to a broader audience.\n\nRecommendation:\n\nBased on the review, I recommend that the authors revise the paper to address the weaknesses and suggestions for improvement. The paper presents a novel and theoretically grounded approach to solving EOT using EBMs, and with additional experimental results and practical discussion, it has the potential to make a significant contribution to the field. However, the paper's current state may make it less accessible to a broader audience.",
        "gr_score_with": 0.550314465408805,
        "gr_score_without": 0.4592274678111588,
        "or_score_with": 0.37735849056603776,
        "or_score_without": 0.44206008583690987,
        "output_without": {
            "num_tokens_scored": 233,
            "num_green_tokens": 107,
            "green_fraction": 0.4592274678111588,
            "num_orange_tokens": 103,
            "orange_fraction": 0.44206008583690987,
            "green_z_score": 1.845421198301794,
            "orange_z_score": 64.17582811636476,
            "green_p_value": 0.03248814591417693,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 318,
            "num_green_tokens": 175,
            "green_fraction": 0.550314465408805,
            "num_orange_tokens": 120,
            "orange_fraction": 0.37735849056603776,
            "green_z_score": 5.471529130622779,
            "orange_z_score": 63.7292834128539,
            "green_p_value": 2.2308449250279834e-08,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SEMIREWARD: A GENERAL REWARD MODEL FOR SEMI-SUPERVISED LEARNING",
        "abstract": "Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks across three modalities, extensive experiments verify that SemiReward achieves significant performance gains and faster convergence speeds upon Pseudo Label, FlexMatch, and Free/SoftMatch. Code and models are available at https://github.com/Westl ake-AI/SemiReward. ESC-50 250 ESC-50 500 FSDnoisy-18k 1773 UrtraSound-8k 400 Yelp Review 250",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}  \nThe paper proposes a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels in semi-supervised learning (SSL). The rewarder network is designed to model correlations and predict credible reward scores, which is trained online in a two-stage pipeline assisted by a generator network to avoid confirmation bias. The proposed approach is general, pluggable, and can be used with various SSL methods in wide task types and scenarios. \n\nStrengths:\n1. The proposed approach is general and can be used with various SSL methods, making it a valuable contribution to the field of SSL.\n2. The two-stage training pipeline and the generator network are effective in avoiding confirmation bias and improving the quality of pseudo labels.\n3. The paper presents extensive experiments on diverse classification and regression datasets, demonstrating consistent performance gains and convergence speedup when applying SemiReward to popular SSL algorithms.\n\nWeaknesses:\n1. The paper assumes that the rewarder network is able to predict reliable pseudo labels, but it does not provide a detailed analysis of the limitations of this assumption.\n2. The paper does not provide a detailed explanation of how the generator network is trained and how it affects the performance of the rewarder network.\n3. The paper assumes that the pseudo labels are generated by a teacher model, but it does not provide a detailed explanation of how the teacher model is trained and how it affects the performance of the rewarder network.\n\nSuggestions for Improvement:\n1. Provide a detailed analysis of the limitations of the assumption that the rewarder network is able to predict reliable pseudo labels.\n2. Provide a detailed explanation of how the generator network is trained and how it affects the performance of the rewarder network.\n3. Provide a detailed explanation of how the teacher model is trained and how it affects the performance of the rewarder network.\n4. Consider incorporating more advanced techniques, such as ensemble methods or transfer learning, to improve the performance of the rewarder network.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper to address the weaknesses and improve the overall quality of the paper. Specifically, I recommend that the authors provide a more detailed analysis of the limitations of the assumption that the rewarder network is able to predict reliable pseudo labels, provide a detailed explanation of how the generator network is trained and how it affects the performance of the rewarder network, and provide a detailed explanation of how the teacher model is trained and how it affects the performance of the rewarder network. Additionally, I recommend that the authors consider incorporating more advanced techniques to improve the performance of the rewarder network.",
        "peer_review_with_watermark": "$$\\boxed{\\text{Summary: The paper proposes a Semi-supervised Reward framework (SemiReward) to improve the quality of pseudo labels in semi-supervised learning (SSL) tasks. The framework predicts reward scores to evaluate and filter out high-quality pseudo labels. It is trained online in two stages with a generator network and subsampling strategy to avoid confirmation bias. The paper presents extensive experiments on 13 standard SSL benchmarks across three modalities and demonstrates significant performance gains and faster convergence speeds when applying SemiReward to popular SSL algorithms. The authors also analyze the contribution of each component of the SemiReward framework and provide a detailed analysis of its limitations and future directions. }}\n\n\\text{Strengths:}\n- The paper proposes a novel and effective approach to improve the quality of pseudo labels in SSL tasks.\n- The SemiReward framework is well-structured and easy to understand.\n- The authors provide extensive experiments to demonstrate the effectiveness of their approach.\n\n\\text{Weaknesses:}\n- The paper may be challenging to understand for those without a background in SSL and reward modeling.\n- The authors may need to provide more detailed explanations of some of the mathematical derivations and proofs.\n- The paper may be too long and could benefit from condensation of some of the sections.\n\n\\text{Suggestions for Improvement:}\n- The authors could provide more detailed explanations of the mathematical derivations and proofs.\n- The paper could be condensed to make it easier to read and understand.\n- The authors could explore the possibility of applying the SemiReward framework to other types of SSL tasks.\n\n\\text{Recommendation:}\n- The paper is well-written and presents a novel and effective approach to improve the quality of pseudo labels in SSL tasks.\n- The authors provide extensive experiments to demonstrate the effectiveness of their approach.\n- The paper is a valuable contribution to the SSL community and has the potential to inspire further research in this area.",
        "gr_score_with": 0.6254681647940075,
        "gr_score_without": 0.4542372881355932,
        "or_score_with": 0.3295880149812734,
        "or_score_without": 0.45084745762711864,
        "output_without": {
            "num_tokens_scored": 295,
            "num_green_tokens": 134,
            "green_fraction": 0.4542372881355932,
            "num_orange_tokens": 133,
            "orange_fraction": 0.45084745762711864,
            "green_z_score": 1.9015307541661133,
            "orange_z_score": 89.70120726902013,
            "green_p_value": 0.028616264095756967,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 267,
            "num_green_tokens": 167,
            "green_fraction": 0.6254681647940075,
            "num_orange_tokens": 88,
            "orange_fraction": 0.3295880149812734,
            "green_z_score": 7.520301279591808,
            "orange_z_score": 62.00964321966783,
            "green_p_value": 2.7325079888732814e-14,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "META-LEARNING PRIORS USING UNROLLED PROXIMAL NETWORKS",
        "abstract": "Relying on prior knowledge accumulated from related tasks, meta-learning offers a powerful approach to learning a novel task from limited training data. Recent approaches parameterize the prior with a family of probability density functions or recurrent neural networks, whose parameters can be optimized by utilizing validation data from the observed tasks. While these approaches have appealing empirical performance, the expressiveness of their prior is relatively low, which limits the generalization and interpretation of meta-learning. Aiming at expressive yet meaningful priors, this contribution puts forth a novel prior representation model that leverages the notion of algorithm unrolling. The key idea is to unroll the proximal gradient descent steps, where learnable piecewise linear functions are developed to approximate the desired proximal operators within tight theoretical error bounds established for both smooth and non-smooth proximal functions. The resultant multi-block neural network not only broadens the scope of learnable priors, but also enhances interpretability from an optimization viewpoint. Numerical tests conducted on few-shot learning datasets demonstrate markedly improved performance with flexible, visualizable, and understandable priors.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$\\boxed{Paper Summary:}$\nThe paper proposes a novel meta-learning approach called MetaProxNet that leverages algorithm unrolling to learn more flexible and generalized priors. The approach uses learnable piecewise linear functions (PLFs) to approximate the proximal operators, which are used to regularize the model parameters. The paper presents a theoretical analysis of the error bounds for the PLF-based proximal operator and provides empirical results on several meta-learning benchmark datasets.\n\n$\\boxed{Strengths:}$\n1. The paper introduces a novel approach to meta-learning using algorithm unrolling, which can learn more flexible and generalized priors.\n2. The approach uses learnable PLFs to approximate the proximal operators, which can provide interpretability from an optimization viewpoint.\n3. The paper presents a theoretical analysis of the error bounds for the PLF-based proximal operator.\n4. The empirical results demonstrate the superiority of MetaProxNet relative to meta-learning alternatives in prior representation and learning.\n\n$\\boxed{Weaknesses:}$\n1. The paper assumes that the prior factorizes across its input dimensions, which may not be the case in all scenarios.\n2. The error bounds provided in the paper may not be tight, and further analysis is needed to establish a more accurate bound.\n3. The paper does not provide a detailed analysis of the convergence of the MetaProxNet algorithm.\n\n$\\boxed{Suggestions for Improvement:}$\n1. The authors should provide a more detailed analysis of the convergence of the MetaProxNet algorithm.\n2. The authors should investigate the effect of different optimizers on the performance of MetaProxNet.\n3. The authors should explore the use of MetaProxNet with more complicated backbones and DC modules.\n4. The authors should establish bilevel convergence guarantees for MetaProxNet.\n\n$\\boxed{Recommendation:}$\nBased on the paper's contributions, I recommend that it be accepted for publication in a top-tier conference or journal in machine learning. However, I suggest that the authors address the weaknesses and provide additional improvements to strengthen the paper's overall quality.\n\n$$\\begin{aligned}\n\\end{aligned}$$",
        "peer_review_with_watermark": "$$\\boxed{\\text{Summary: } \\text{This paper presents a novel meta-learning approach called MetaProxNet that leverages algorithm unrolling to learn flexible and generalized priors. The proposed method employs unrolled neural networks to approximate proximal operators, allowing for more expressive priors while maintaining interpretability. The authors demonstrate the effectiveness of MetaProxNet on several meta-learning benchmark datasets, showing improved performance over state-of-the-art methods.}\n\n\\text{Strengths: } \n- The proposed approach leverages algorithm unrolling to learn flexible and generalized priors, offering more expressive priors while maintaining interpretability.\n- The use of unrolled neural networks enables the approximation of proximal operators, allowing for more effective priors.\n- The authors demonstrate the effectiveness of MetaProxNet on several meta-learning benchmark datasets, showing improved performance over state-of-the-art methods.\n\n\\text{Weaknesses: } \n- The approach assumes that the optimal regularizer can be factorized across its input dimensions, which may not always hold in practice.\n- The authors do not provide a comprehensive analysis of the computational complexity of the proposed algorithm.\n- The results on meta-learning benchmark datasets are limited to specific datasets, and it is unclear how well the approach generalizes to other domains.\n\n\\text{Suggestions for Improvement: } \n- Provide a more rigorous analysis of the computational complexity of the proposed algorithm.\n- Investigate the impact of the assumption that the optimal regularizer can be factorized across its input dimensions on the performance of the approach.\n- Evaluate the proposed approach on a broader range of meta-learning benchmark datasets to assess its generalizability.\n\n\\text{Recommendation: } \n- The proposed approach, MetaProxNet, shows promise in leveraging algorithm unrolling to learn flexible and generalized priors, offering more expressive priors while maintaining interpretability. However, further investigation is necessary to fully understand the approach's limitations and potential applications. The authors' results on meta-learning benchmark datasets are promising, but more comprehensive evaluations are required to assess the approach's generalizability.}\n\n\\text{Assumptions: } \n- The authors assume that the optimal regularizer can be factorized across its input dimensions, which may not always hold in practice.\n- The authors do not provide a comprehensive analysis of the computational complexity of the proposed algorithm.\n\n\\text{Limitations: } \n- The approach assumes that the optimal regularizer can be factorized across its input dimensions, which may not always hold in practice.\n- The authors do not provide a comprehensive analysis of the computational complexity of the proposed algorithm.\n- The results on meta-learning benchmark datasets are limited to specific datasets, and it is unclear how well the approach generalizes to other domains.}$$",
        "gr_score_with": 0.47547169811320755,
        "gr_score_without": 0.3254237288135593,
        "or_score_with": 0.45660377358490567,
        "or_score_without": 0.5220338983050847,
        "output_without": {
            "num_tokens_scored": 295,
            "num_green_tokens": 96,
            "green_fraction": 0.3254237288135593,
            "num_orange_tokens": 154,
            "orange_fraction": 0.5220338983050847,
            "green_z_score": -2.614604786978406,
            "orange_z_score": 103.41890601697462,
            "green_p_value": 0.995533462300405,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 265,
            "num_green_tokens": 126,
            "green_fraction": 0.47547169811320755,
            "num_orange_tokens": 121,
            "orange_fraction": 0.45660377358490567,
            "green_z_score": 2.5078493128775956,
            "orange_z_score": 85.55822323194798,
            "green_p_value": 0.006073422752014172,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS",
        "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs) called Relative Importance and Activations (RIA). The proposed method consists of two key components: RIA, a new pruning metric that jointly considers the weight and activations efficiently, and Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The authors evaluate their method on three popular LLMs and show that RIA outperforms existing SOTA one-shot pruning techniques SparseGPT and Wanda, and achieves a significant performance boost with the incorporation of Channel Permutation.\n\nStrengths:\n1. The proposed method, RIA, is a significant improvement over existing pruning metrics, as it jointly considers the weight and activations efficiently.\n2. The authors demonstrate the effectiveness of RIA on three popular LLMs and show that it outperforms existing SOTA one-shot pruning techniques.\n3. The incorporation of Channel Permutation improves the performance of the pruned model and reduces the performance drop when adapting the model to N:M constraint.\n4. The method is hardware-friendly, ensuring enhanced inference acceleration.\n\nWeaknesses:\n1. The authors do not provide a clear explanation of the RIA metric, and it is not immediately clear how it works.\n2. The paper does not provide a comprehensive analysis of the limitations of the proposed method.\n3. The authors rely heavily on empirical evaluations, which may not be generalizable to all cases.\n4. The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n\nSuggestions for Improvement:\n1. Provide a clear explanation of the RIA metric and how it works.\n2. Conduct a more comprehensive analysis of the limitations of the proposed method.\n3. Evaluate the proposed method on a larger dataset and with more diverse LLMs.\n4. Provide a detailed analysis of the computational complexity of the proposed method.\n5. Consider incorporating more advanced techniques, such as regularization or optimization methods, to further improve the performance of the proposed method.\n\nRecommendation:\nI recommend that the authors revise their paper based on the above suggestions and provide a more comprehensive analysis of the proposed method. The authors should also provide a clear explanation of the RIA metric and its limitations, as well as a more detailed analysis of the computational complexity of the proposed method. Additionally, the authors should consider incorporating more advanced techniques to further improve the performance of the proposed method. Overall, the proposed method has the potential to be a significant improvement over existing pruning techniques, but it requires further development and evaluation to establish its effectiveness and limitations.",
        "peer_review_with_watermark": "$$$ \n\nPaper Title: PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS \n\nThe paper presents a plug-and-play post-training pruning method for large language models (LLMs), which aims to reduce the model size and computation while maintaining or improving performance. The proposed method, called Relative Importance and Activations (RIA), incorporates a new pruning metric that jointly considers the weight and activations efficiently on LLMs. The method also introduces Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity.\n\n**Strengths:**\n\n1. The paper introduces a novel pruning metric, RIA, which effectively mitigates channel corruption issues in post-training pruning.\n2. The Channel Permutation approach is efficient and scalable, allowing for the maximally preservation of important weights under N:M sparsity.\n3. The experiments demonstrate that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, and N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks.\n\n**Weaknesses:**\n\n1. The paper does not provide a clear motivation for using N:M sparsity, and the performance gains are limited to specific hardware and model sizes.\n2. The Channel Permutation approach may not be suitable for all types of LLMs, and its efficiency may degrade for smaller model sizes.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed methods.\n\n**Suggestions for Improvement:**\n\n1. The paper should provide a more detailed analysis of the computational complexity of the proposed methods and explore potential optimizations.\n2. The authors should investigate the applicability of the proposed methods to other types of neural networks and provide empirical results.\n3. The paper should include a more comprehensive evaluation of the proposed methods on a wider range of LLMs and tasks.\n\n**Recommendation:**\n\nBased on the strengths and weaknesses of the paper, I recommend acceptance of the manuscript. However, the authors should address the limitations and weaknesses mentioned above to improve the overall quality and relevance of the paper.\n\n$$$ \n\nNote: The peer review is based on the provided text and may not be exhaustive. The reviewer may have additional comments or suggestions.",
        "gr_score_with": 0.43342776203966005,
        "gr_score_without": 0.3392857142857143,
        "or_score_with": 0.49291784702549574,
        "or_score_without": 0.5773809523809523,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 114,
            "green_fraction": 0.3392857142857143,
            "num_orange_tokens": 194,
            "orange_fraction": 0.5773809523809523,
            "green_z_score": -2.271720556255608,
            "orange_z_score": 128.25266404131665,
            "green_p_value": 0.9884483030991179,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 353,
            "num_green_tokens": 153,
            "green_fraction": 0.43342776203966005,
            "num_orange_tokens": 174,
            "orange_fraction": 0.49291784702549574,
            "green_z_score": 1.2820029772261117,
            "orange_z_score": 112.00101345207007,
            "green_p_value": 0.09992080091640476,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DIFFTACTILE: A PHYSICS-BASED DIFFERENTIABLE TACTILE SIMULATOR FOR CONTACT-RICH ROBOTIC MANIPULATION",
        "abstract": "We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website1.",
        "peer_review_without_watermark": "$$$\\begin{align}\\textbf{Paper Summary:} &\\quad \\text{The paper presents DIFFTACTILE, a physics-based differentiable tactile simulator for contact-rich robotic manipulation. DIFFTACTILE models the sensing elastomer, multi-material objects, and penalty-based contacts, providing dense tactile feedback. The simulator is designed to be differentiable, enabling gradient-based optimization for skill learning and system identification. The authors evaluate the simulator on various manipulation tasks, demonstrating its versatility and efficiency.}\\end{align}\n\n$$\\begin{align}\\textbf{Strengths:} &\\quad \\text{The paper makes significant contributions to the field of tactile simulation, providing a physics-based differentiable simulator that can accurately model contact-rich robotic manipulation. The authors demonstrate the versatility of the simulator through various manipulation tasks and evaluate its efficiency compared to reinforcement learning and sample-based trajectory optimization approaches.}\\end{align}\n\n$$\\begin{align}\\textbf{Weaknesses:} &\\quad \\text{One potential weakness of the paper is the limited scope of the experiments. While the authors evaluate the simulator on various manipulation tasks, the experiments are relatively small and may not be representative of the broader range of tasks that the simulator can handle. Additionally, the paper could benefit from a more detailed discussion of the limitations of the differentiable physics approach and how it compares to other optimization methods.}\\end{align}\n\n$$\\begin{align}\\textbf{Suggestions for Improvement:} &\\quad \\text{To improve the paper, the authors could conduct more extensive experiments to evaluate the simulator on a wider range of tasks and scenarios. They could also provide a more detailed analysis of the limitations of the differentiable physics approach and compare it to other optimization methods. Furthermore, the authors could explore the potential applications of the simulator in other areas of robotics, such as human-robot interaction or robotic assembly.}\\end{align}\n\n$$\\begin{align}\\textbf{Recommendation:} &\\quad \\text{Based on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the manuscript for consideration in a top-tier conference or journal in the field of robotics. The authors have made significant contributions to the field of tactile simulation, and with some revisions to address the weaknesses, the paper has the potential to be a leading work in the field.}\\end{align}$$",
        "peer_review_with_watermark": "$$$\\boxed{DIFFTACTILE: A PHYSICS-BASED DIFFERENTIABLE TACTILE SIMULATOR FOR CONTACT-RICH ROBOTIC MANIPULATION}$\n\n**Paper Summary:**\n\nThis paper introduces DIFFTACTILE, a physics-based differentiable tactile simulator for contact-rich robotic manipulation. DIFFTACTILE models tactile sensors with Finite Element Method (FEM), objects in various materials (rigid, elastic, and elastoplastic) with Moving Least Square Material Point Method (MLS-MPM), and cable with Position-Based Dynamics (PBD). The system is differentiable and can reduce the sim-to-real gap with system identification. It is demonstrated that DIFFTACTILE can improve the efficiency of tactile-assisted manipulation skill learning. The paper also presents the results of experiments on grasping, surface following, cable straightening, case opening, and object reposing tasks.\n\n**Strengths:**\n\n1. DIFFTACTILE is a physics-based simulator that models the complex interactions between tactile sensors and objects.\n2. The system is differentiable, which enables gradient-based optimization for skill learning.\n3. DIFFTACTILE supports a wide range of materials (rigid, elastic, and elastoplastic) and objects (cable and articulated objects).\n4. The optical simulation method is efficient and accurate.\n5. The paper presents a comprehensive evaluation of DIFFTACTILE on various manipulation tasks.\n\n**Weaknesses:**\n\n1. The paper does not provide a detailed analysis of the limitations of DIFFTACTILE.\n2. The experiments were not designed to test the robustness of the simulator under various scenarios.\n3. The paper does not provide a comparison with other state-of-the-art tactile simulators.\n4. The optical simulation method relies on a polynomial lookup table, which may not be accurate for all materials.\n5. The paper does not provide a discussion on the potential biases of the experiments.\n\n**Suggestions for Improvement:**\n\n1. Conduct a more comprehensive analysis of the limitations of DIFFTACTILE.\n2. Design experiments to test the robustness of the simulator under various scenarios.\n3. Compare DIFFTACTILE with other state-of-the-art tactile simulators.\n4. Develop a more accurate optical simulation method.\n5. Discuss the potential biases of the experiments and provide recommendations for mitigating them.\n\n**Recommendation:**\n\nDIFFTACTILE is a significant contribution to the field of tactile simulation. The paper presents a well-designed and comprehensive simulator that models the complex interactions between tactile sensors and objects. However, there are some limitations and biases in the experiments that need to be addressed. To further improve DIFFTACTILE, the authors should conduct more comprehensive analysis, design experiments to test the robustness, compare with other simulators, develop a more accurate optical simulation method, and discuss the potential biases. Overall, DIFFTACTILE has the potential to serve as a learning platform for broader tactile-assisted manipulation tasks.",
        "gr_score_with": 0.45409429280397023,
        "gr_score_without": 0.4064516129032258,
        "or_score_with": 0.47146401985111663,
        "or_score_without": 0.4645161290322581,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 126,
            "green_fraction": 0.4064516129032258,
            "num_orange_tokens": 144,
            "orange_fraction": 0.4645161290322581,
            "green_z_score": 0.23186944788008415,
            "orange_z_score": 100.20747068935259,
            "green_p_value": 0.4083197074703745,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 403,
            "num_green_tokens": 183,
            "green_fraction": 0.45409429280397023,
            "num_orange_tokens": 190,
            "orange_fraction": 0.47146401985111663,
            "green_z_score": 2.2166562497811775,
            "orange_z_score": 115.98734569682605,
            "green_p_value": 0.013323295296270157,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MOGANET: MULTI-ORDER GATED AGGREGATION NETWORK",
        "abstract": "By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks. However, recent progress on multi-order game-theoretic interaction within deep neural networks (DNNs) reveals the representation bottleneck of modern ConvNets, where the expressive interactions have not been effectively encoded with the increased kernel size. To tackle this challenge, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. MogaNet exhibits great scalability, impressive efficiency of parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D&3D human pose estimation, and video prediction. Notably, MogaNet hits 80.0% and 87.8% accuracy with 5.2M and 181M parameters on ImageNet-1K, outperforming ParC-Net and ConvNeXtL, while saving 59% FLOPs and 17M parameters, respectively. The source code is available at https://github.com/Westlake-AI/MogaNet.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a new modern ConvNet architecture, named MogaNet, which tackles the representation bottleneck of modern ConvNets by incorporating multi-order game-theoretic interaction. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. The paper presents a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning with favorable complexity-performance trade-offs.\n\nStrengths:\n\n1.  The proposed MogaNet architecture is a significant improvement over existing ConvNets and ViTs, with a compact module that efficiently gathers and contextualizes discriminative features.\n2.  The incorporation of multi-order game-theoretic interaction is a novel approach that addresses the representation bottleneck of modern ConvNets.\n3.  The extensive experiments demonstrate the consistent superiority of MogaNet in terms of both performance and efficiency compared to popular ConvNets, ViTs, and hybrid architectures.\n\nWeaknesses:\n\n1.  The paper relies heavily on the assumption that multi-order game-theoretic interaction can be effectively encoded in a compact module, which may not be the case in all scenarios.\n2.  The proposed MogaBlock and channel aggregation module CA(\u00b7) may not be suitable for all types of visual tasks or datasets.\n3.  The paper does not provide a comprehensive analysis of the computational overhead and parameters required by the proposed architecture.\n\nSuggestions for Improvement:\n\n1.  Further experiments should be conducted to evaluate the robustness of MogaNet to different visual tasks, datasets, and computational resources.\n2.  The paper should provide a more detailed analysis of the computational overhead and parameters required by the proposed architecture, including the costs of implementing the MogaBlock and CA(\u00b7) modules.\n3.  The incorporation of multi-order game-theoretic interaction could be further explored and refined to improve the performance and efficiency of MogaNet.\n\nRecommendation:\n\n1.  The proposed MogaNet architecture is a significant improvement over existing ConvNets and ViTs, with a compact module that efficiently gathers and contextualizes discriminative features.\n2.  Further experiments and analysis are required to evaluate the robustness and efficiency of MogaNet in different scenarios.\n3.  The incorporation of multi-order game-theoretic interaction is a novel approach that addresses the representation bottleneck of modern ConvNets, and it has the potential to lead to significant advances in visual representation learning.",
        "peer_review_with_watermark": "$$$ Paper Summary:\n \n The paper proposes Moganet, a new architecture that combines locality perception and context aggregation to improve visual representation learning in deep neural networks. The architecture consists of a spatial aggregation block that learns multi-order interactions and a channel aggregation block that adaptsively reallocate channel-wise features. The Moganet architecture is evaluated on various vision benchmarks and shows consistent superiority over state-of-the-art architectures in terms of performance and efficiency.\n\nStrengths:\n\n1. The paper proposes a new and innovative architecture that addresses the representation bottleneck in deep neural networks.\n2. The Moganet architecture is evaluated on various vision benchmarks and shows consistent superiority over state-of-the-art architectures.\n3. The paper provides extensive experiments and ablation studies to demonstrate the effectiveness of the Moganet architecture.\n\nWeaknesses:\n\n1. The paper relies heavily on the concept of multi-order game-theoretic interaction, which may be difficult to understand and interpret for some readers.\n2. The Moganet architecture is evaluated on various vision benchmarks, but the results are not always presented in a clear and concise manner.\n3. The paper does not provide a detailed comparison with other architectures that do not rely on the concept of multi-order game-theoretic interaction.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed and concise summary of the paper in the abstract.\n2. Provide more detailed explanations and justifications for the use of multi-order game-theoretic interaction in the Moganet architecture.\n3. Provide more detailed comparisons with other architectures that do not rely on the concept of multi-order game-theoretic interaction.\n4. Provide more detailed results and analysis of the experiments, including error rates and other relevant metrics.\n5. Provide more detailed explanations and justifications for the choice of hyperparameters and other experimental settings.\n\nRecommendation:\n\nBased on the paper, I recommend that the authors revise and resubmit the paper for further improvement. The paper has some significant contributions to the field of deep learning and visual representation learning, but it also has some limitations and areas for improvement. The authors should address the suggestions for improvement outlined above to make the paper more robust and effective.",
        "gr_score_with": 0.4862068965517241,
        "gr_score_without": 0.35034013605442177,
        "or_score_with": 0.4482758620689655,
        "or_score_without": 0.54421768707483,
        "output_without": {
            "num_tokens_scored": 294,
            "num_green_tokens": 103,
            "green_fraction": 0.35034013605442177,
            "num_orange_tokens": 160,
            "orange_fraction": 0.54421768707483,
            "green_z_score": -1.738095238095239,
            "orange_z_score": 102.75107624722044,
            "green_p_value": 0.958902983080456,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 290,
            "num_green_tokens": 141,
            "green_fraction": 0.4862068965517241,
            "num_orange_tokens": 130,
            "orange_fraction": 0.4482758620689655,
            "green_z_score": 2.996645634283651,
            "orange_z_score": 83.78825268044292,
            "green_p_value": 0.0013648390946543612,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "IDENTIFYING POLICY GRADIENT SUBSPACES",
        "abstract": "Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.",
        "peer_review_without_watermark": "$$$\\boxed{\\text{Paper Summary:}}$\n\nThis paper investigates the existence of low-dimensional, slowly-changing gradient subspaces in policy gradient algorithms, specifically in the context of reinforcement learning (RL). The authors conduct an empirical evaluation of two popular RL algorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), on various simulated RL benchmarks. Their results demonstrate that despite the continuously changing data distribution inherent to RL, the gradients of the actor and critic networks of PPO and SAC lie in a low-dimensional, slowly-changing subspace of high curvature. This finding has implications for more efficient reinforcement learning, enabling second-order optimization and improving parameter-space exploration.\n\n$\\boxed{\\text{Strengths:}}$\n\n1. The paper provides a thorough analysis of the gradient subspaces in policy gradient algorithms, which is a crucial aspect of reinforcement learning.\n2. The authors conduct an empirical evaluation of two popular RL algorithms, PPO and SAC, on various simulated RL benchmarks, which provides a comprehensive understanding of the phenomenon.\n3. The results demonstrate that the gradient subspaces are low-dimensional, slowly-changing, and of high curvature, which has significant implications for more efficient reinforcement learning.\n\n$\\boxed{\\text{Weaknesses:}}$\n\n1. The paper assumes that the reader is familiar with the mathematical background and notation used throughout the paper, which may be a barrier to understanding for non-experts.\n2. The analysis is limited to two popular RL algorithms, PPO and SAC, which may not be representative of all RL algorithms.\n3. The paper does not provide a comprehensive comparison of the gradient subspaces in different RL algorithms, which may limit the generalizability of the results.\n\n$\\boxed{\\text{Suggestions for Improvement:}}$\n\n1. Provide a more detailed explanation of the mathematical background and notation used throughout the paper to ensure that all readers can understand the analysis.\n2. Conduct a comprehensive comparison of the gradient subspaces in different RL algorithms to ensure that the results are representative of the broader RL community.\n3. Investigate the applicability of the findings to other RL algorithms and domains to further establish the significance of the results.\n\n$\\boxed{\\text{Recommendation:}}$\n\nBased on the thorough analysis and empirical evaluation, I recommend that the paper be accepted for publication in a reputable conference or journal in the field of reinforcement learning. The findings have significant implications for more efficient reinforcement learning, enabling second-order optimization and improving parameter-space exploration. With some minor revisions to address the weaknesses and suggestions for improvement, the paper has the potential to make a significant contribution to the field.",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\nThis paper explores the concept of gradient subspaces in the context of policy gradient methods for reinforcement learning. They investigate whether gradients in the RL setting lie in a low-dimensional, slowly-changing subspace, similar to supervised learning. The authors conduct a comprehensive empirical evaluation of two popular policy gradient methods, PPO and SAC, on various simulated RL benchmarks. They analyze the properties of the gradients, including the subspace spanned by the top eigenvectors of the Hessian matrix, and show that the gradients are indeed contained in a low-dimensional subspace of high curvature. The results demonstrate that this property holds for both on-policy and off-policy learning, despite the continuously changing data distribution inherent to RL.\n\nStrengths:\n\n* The paper provides a thorough analysis of the properties of gradient subspaces in RL, which is a novel and interesting topic.\n* The authors conduct a comprehensive empirical evaluation of two popular policy gradient methods, PPO and SAC, on various simulated RL benchmarks.\n* The analysis is well-organized and easy to follow, with a clear structure and logical progression.\n* The results are presented in a clear and concise manner, with visualizations and tables that facilitate understanding.\n\nWeaknesses:\n\n* The paper could benefit from a more in-depth discussion of the theoretical background and mathematical formalism used in the analysis.\n* The analysis is limited to a few policy gradient methods, and it would be interesting to explore other methods as well.\n* The paper could also benefit from a more detailed discussion of the limitations and potential pitfalls of the approach.\n* The conclusion could be more concise and to the point, with a clearer summary of the main findings and implications.\n\nSuggestions for Improvement:\n\n* Add more theoretical background and mathematical formalism to the analysis, including a discussion of the properties of gradient subspaces in RL.\n* Explore other policy gradient methods and analyze their properties in the context of gradient subspaces.\n* Consider a more in-depth discussion of the limitations and potential pitfalls of the approach.\n* Improve the clarity and concision of the conclusion, with a clearer summary of the main findings and implications.\n* Consider adding more visualizations and tables to facilitate understanding and presentation of the results.\n\nRecommendation:\n\n* The paper is well-written and well-organized, with a clear and logical structure.\n* The analysis is thorough and provides valuable insights into the properties of gradient subspaces in RL.\n* With some revisions to address the limitations and potential pitfalls of the approach, the paper could be even stronger and more impactful.\n* Overall, the paper is a valuable contribution to the field of reinforcement learning and policy gradient methods, and it is recommended for publication in a top-tier conference or journal.",
        "gr_score_with": 0.5057142857142857,
        "gr_score_without": 0.42857142857142855,
        "or_score_with": 0.41714285714285715,
        "or_score_without": 0.46130952380952384,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 144,
            "green_fraction": 0.42857142857142855,
            "num_orange_tokens": 155,
            "orange_fraction": 0.46130952380952384,
            "green_z_score": 1.069044967649697,
            "orange_z_score": 100.15843194651036,
            "green_p_value": 0.1425247037013065,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 350,
            "num_green_tokens": 177,
            "green_fraction": 0.5057142857142857,
            "num_orange_tokens": 146,
            "orange_fraction": 0.41714285714285715,
            "green_z_score": 4.037030969365859,
            "orange_z_score": 92.28668739881746,
            "green_p_value": 2.7065962632547926e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "PROVES ADVERSARIAL TRAINING",
        "abstract": "In standard adversarial training, models are optimized to fit invariant one-hot labels for adversarial data when the perturbations are within allowable budgets. However, the overconfident target harms generalization and causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that reflects the underlying distribution of data. By utilizing ADR, we can obtain rectified labels that improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-andplay integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach called Annealing Self-Distillation Rectification (ADR) to enhance the adversarial robustness of deep neural networks. The authors analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. They propose a data-driven scheme to rectify labels in a noise-aware manner that mimics the behavior of human annotators. The approach is simple, effective, and can be integrated with other adversarial training techniques. The paper demonstrates the efficacy of ADR through extensive experiments and strong performances across datasets.\n\nStrengths:\n1. The paper provides a comprehensive analysis of the robust models' output properties and identifies the key characteristics that distinguish robust models from non-robust ones.\n2. The proposed approach is simple, effective, and can be integrated with other adversarial training techniques.\n3. The paper demonstrates the efficacy of ADR through extensive experiments and strong performances across datasets.\n\nWeaknesses:\n1. The paper does not provide a detailed comparison with other state-of-the-art methods, which may limit the evaluation of ADR's effectiveness.\n2. The authors do not discuss the potential limitations and challenges of ADR, such as the need for careful tuning of hyperparameters and the potential impact on model interpretability.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed comparison with other state-of-the-art methods to evaluate the effectiveness of ADR.\n2. The authors should discuss the potential limitations and challenges of ADR, such as the need for careful tuning of hyperparameters and the potential impact on model interpretability.\n\nRecommendation:\nThe paper proposes a novel approach to enhance the adversarial robustness of deep neural networks. The approach is simple, effective, and can be integrated with other adversarial training techniques. The paper demonstrates the efficacy of ADR through extensive experiments and strong performances across datasets. However, the authors should provide a more detailed comparison with other state-of-the-art methods and discuss the potential limitations and challenges of ADR to strengthen the paper's evaluation.\n\nOverall, the paper contributes to the field of adversarial robustness and provides a new approach to enhance the robustness of deep neural networks. With some minor revisions, the paper can be considered for publication in a reputable conference or journal.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n&\\text{\\textbf{Paper Summary}}:\\\\\n&\\text{The paper proposes a novel approach to enhance adversarial robustness of deep neural networks. The approach, called Annealing Self-Distillation Rectification (ADR), utilizes the self-distillation model to generate noise-aware labels that reflect the underlying data distribution. The authors conduct extensive experiments to evaluate the efficacy of ADR on various datasets.}\\\\\n&\\text{\\textbf{Strengths}}:\\\\\n&\\text{The paper presents a novel approach to address the issue of robust overfitting in adversarial training. The use of self-distillation model to generate noise-aware labels is a unique and innovative approach. The authors provide a detailed analysis of the properties of robust models and the characteristics of the proposed ADR method.}\\\\\n&\\text{\\textbf{Weaknesses}}:\\\\\n&\\text{The paper relies heavily on theoretical analysis and experimental results. The theoretical analysis is not always clear, and the experimental results are not always convincing. The authors also do not provide a clear comparison with state-of-the-art methods.}\\\\\n&\\text{\\textbf{Suggestions for Improvement}}:\\\\\n&\\text{The authors should provide a more detailed theoretical analysis to explain the properties of robust models. The experimental results should be more convincing, and the authors should provide a clear comparison with state-of-the-art methods. The authors should also consider the computational cost of the proposed method and provide a more detailed analysis of the trade-off between accuracy and robustness.}\\\\\n&\\text{\\textbf{Recommendation}}:\\\\\n&\\text{The proposed approach, ADR, shows promising results in improving adversarial robustness. However, the theoretical analysis is not always clear, and the experimental results are not always convincing. The authors should address these issues to make the approach more reliable and efficient.}\\\\\n\\end{aligned}$$",
        "gr_score_with": 0.6150627615062761,
        "gr_score_without": 0.4524714828897338,
        "or_score_with": 0.32217573221757323,
        "or_score_without": 0.45627376425855515,
        "output_without": {
            "num_tokens_scored": 263,
            "num_green_tokens": 119,
            "green_fraction": 0.4524714828897338,
            "num_orange_tokens": 120,
            "orange_fraction": 0.45627376425855515,
            "green_z_score": 1.7369830932279378,
            "orange_z_score": 83.68421899296403,
            "green_p_value": 0.041195077566859535,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 239,
            "num_green_tokens": 147,
            "green_fraction": 0.6150627615062761,
            "num_orange_tokens": 77,
            "orange_fraction": 0.32217573221757323,
            "green_z_score": 6.7866983693050225,
            "orange_z_score": 55.9309159031699,
            "green_p_value": 5.736436028550374e-12,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MERT: ACOUSTIC MUSIC UNDERSTANDING MODEL WITH LARGE-SCALE SELF-SUPERVISED TRAINING",
        "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the ConstantQ Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel self-supervised learning (SSL) paradigm, MERT (ACOUSTIC MUSIC UNDERSTANDING MODEL WITH LARGE-SCALE SELF-SUPERVISED TRAINING), for pre-training acoustic music models. The proposed model incorporates a multi-task paradigm to balance acoustic and musical representation learning, using Residual Vector Quantisation Variational AutoEncoder (RVQ-VAE) and Constant-Q Transform (CQT) as teacher models. The authors explore various settings to overcome training instability and demonstrate the effectiveness of the proposed approach in achieving state-of-the-art (SOTA) results on various music information retrieval (MIR) tasks.\n\nStrengths:\n1. The proposed MERT paradigm is innovative and addresses the challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music.\n2. The use of RVQ-VAE and CQT as teacher models provides a unique blend of acoustic and musical information necessary for MLM-based pre-training for music understanding.\n3. The authors explore various settings to overcome training instability and demonstrate the effectiveness of the proposed approach in achieving SOTA results on various MIR tasks.\n\nWeaknesses:\n1. The model is limited by the short 5-second training context, which may limit its performance in tasks requiring understanding extended musical contexts.\n2. The authors observe inverse-scaling effect in specific tasks while scaling-up to 330M, which indicates that the design could be further improved by stabilizing the training.\n3. The gradient exploding issues with half-precision training for settings with larger batch sizes and model sizes is not addressed.\n\nSuggestions for Improvement:\n1. Train the model on longer sequences to improve its performance in tasks requiring understanding extended musical contexts.\n2. Stabilize the training by addressing the gradient exploding issues with half-precision training or by using other optimization techniques.\n3. Explore other teacher models or architectures that can provide more comprehensive acoustic information.\n4. Investigate the effect of different hyperparameters, such as the weight of the musical loss, on the performance of the model.\n\nRecommendation:\nThe proposed MERT paradigm is a significant contribution to the field of music information retrieval and acoustic music understanding. While there are some limitations and areas for improvement, the authors demonstrate the effectiveness of the proposed approach in achieving SOTA results on various MIR tasks. With further improvements and extensions, the MERT paradigm has the potential to become a standard approach for pre-training acoustic music models.",
        "peer_review_with_watermark": "$$$ \nPaper Summary: \nThis paper proposes a novel pre-training paradigm, MERT (Acoustic Music Understanding Model with Large-Scale Self-Supervised Training), that combines Residual Vector Quantisation Variational AutoEncoder (RVQ-VAE) and Constant-Q Transform (CQT) teacher models to pre-train acoustic language models for music understanding tasks. The proposed model achieves state-of-the-art (SOTA) performance on various music information retrieval (MIR) tasks and demonstrates robustness to training instability. The authors explore various settings to overcome instability in acoustic language model pre-training and propose leveraging CQT as a musical teacher to introduce harmonic inductive bias during pre-training. The MERT model achieves SOTA results on several MIR tasks with a significantly smaller parameter size compared to existing baselines.\n\nStrengths:\n1. The paper proposes a novel pre-training paradigm that integrates RVQ-VAE and CQT teacher models to pre-train acoustic language models for music understanding tasks.\n2. The authors demonstrate the efficacy of the MERT model by achieving SOTA performance on various MIR tasks.\n3. The paper provides detailed analysis of training stability issues and proposes techniques to address them.\n4. The MERT model achieves significant performance gains on MIR tasks by leveraging CQT as a musical teacher.\n\nWeaknesses:\n1. The paper's reliance on RVQ-VAE and CQT teacher models may limit its applicability to other audio representation learning tasks.\n2. The use of 5-second training context may limit the model's performance on tasks requiring extended musical contexts.\n3. The paper's analysis of training instability issues is limited to half-precision training, and it is unclear how the design could be improved to address this issue.\n\nSuggestions for Improvement:\n1. The authors could explore alternative audio representation learning paradigms to broaden the applicability of the MERT model.\n2. The use of longer training contexts could be explored to improve the model's performance on tasks requiring extended musical contexts.\n3. The analysis of training instability issues could be extended to other training settings, such as full-precision training, to provide a more comprehensive understanding of the issue.\n\nRecommendation: \nThe MERT model is a significant contribution to the field of music information retrieval, and its proposed pre-training paradigm has the potential to improve the performance of acoustic language models for music understanding tasks. However, the paper's reliance on RVQ-VAE and CQT teacher models may limit its applicability to other audio representation learning tasks. The authors could explore alternative audio representation learning paradigms to broaden the applicability of the MERT model. Additionally, the use of longer training contexts and analysis of training instability issues could be explored to further improve the model's performance and robustness. Overall, the paper is well-written, and the authors provide a thorough analysis of the MERT model's performance and training stability issues.",
        "gr_score_with": 0.4712328767123288,
        "gr_score_without": 0.3438320209973753,
        "or_score_with": 0.4849315068493151,
        "or_score_without": 0.5826771653543307,
        "output_without": {
            "num_tokens_scored": 381,
            "num_green_tokens": 131,
            "green_fraction": 0.3438320209973753,
            "num_orange_tokens": 222,
            "orange_fraction": 0.5826771653543307,
            "green_z_score": -2.237925704992766,
            "orange_z_score": 124.14293270808146,
            "green_p_value": 0.9873870497762012,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 172,
            "green_fraction": 0.4712328767123288,
            "num_orange_tokens": 177,
            "orange_fraction": 0.4849315068493151,
            "green_z_score": 2.7779299806094735,
            "orange_z_score": 100.8330816313267,
            "green_p_value": 0.002735320275774589,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ADAPTING LARGE LANGUAGE MODELS VIA",
        "abstract": "We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension\u2014practice after reading improves the ability to answer questions based on the learned knowledge\u2014we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model\u2019s performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
        "peer_review_without_watermark": "$$$Peer Review\n\nPaper Summary:\nThe paper proposes a method to adapt large language models (LLMs) to specific domains by transforming raw domain-specific corpora into reading comprehension texts. The authors investigate the effectiveness of this approach on three domains (biomedicine, finance, and law) and demonstrate that it improves model performance on domain-specific tasks while also enhancing general LLM benchmarks.\n\nStrengths:\n\n1. The authors provide a clear and concise introduction to the problem and propose a novel approach to adapt LLMs to specific domains.\n2. The method is simple and scalable, making it feasible for large-scale pre-training corpora.\n3. The experiments demonstrate the effectiveness of the approach on three different domains, showcasing its generalizability.\n4. The authors provide a thorough analysis of the results, including fine-tuning and knowledge probing evaluations to assess domain knowledge and prompting ability.\n\nWeaknesses:\n\n1. The paper could benefit from a more detailed discussion of the limitations of the approach, including potential issues with task dominance and the impact of limited task diversity.\n2. Some of the experimental results, such as the fine-tuning and knowledge probing evaluations, could be more thoroughly explained and analyzed.\n3. The paper assumes that the reader is familiar with the basics of LLMs and domain adaptation, which may not be the case for all readers.\n4. The authors could provide more insight into the potential applications and future directions of the approach.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed discussion of the limitations of the approach and potential issues with task dominance.\n2. Include more thorough explanations and analyses of the experimental results, particularly for fine-tuning and knowledge probing evaluations.\n3. Consider adding more details about the architecture and implementation of the proposed method, including any hyperparameters or tuning procedures.\n4. Provide more insight into the potential applications and future directions of the approach, including potential extensions or variations.\n\nRecommendation:\n\nBased on the overall quality of the paper, I recommend it for publication in a reputable conference or journal. The authors have made a significant contribution to the field of LLMs and domain adaptation, and the proposed method has shown promising results. With some revisions to address the weaknesses and suggestions for improvement, the paper has the potential to be a top-notch contribution to the field.",
        "peer_review_with_watermark": "$$\n$$\n\nPaper Summary:\nThe paper investigates the effectiveness of adapting large language models (LLMs) through continued pre-training on domain-specific corpora. The authors propose a method to transform raw domain-specific corpora into reading comprehension texts, which enhances the model's ability to acquire domain knowledge and improve prompting performance. The proposed approach is applied to three domains: biomedicine, finance, and law, demonstrating its effectiveness and generalizability.\n\nStrengths:\n1. The paper presents a clear and well-defined research question, contributing to the existing literature on LLMs and domain adaptation.\n2. The proposed method is simple, scalable, and applicable to any pre-training corpora, making it a valuable contribution to the field.\n3. The experiments conducted across three domains demonstrate the effectiveness of the proposed approach in improving model performance on domain-specific tasks and general LLM benchmarks.\n4. The paper provides a detailed analysis of the impact of the proposed method on different task types, including summarization, language inference, and question answering.\n\nWeaknesses:\n1. The paper does not provide a comprehensive discussion on the limitations of the proposed approach, such as the potential for overfitting or the impact of task diversity on model performance.\n2. The experiment settings, while detailed, may be challenging to reproduce for other researchers, which could limit the reproducibility of the results.\n3. The paper does not provide a detailed analysis of the role of general instructions in the proposed approach, which may be crucial for improving prompting performance.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed discussion on the limitations of the proposed approach and potential avenues for future research.\n2. The experiment settings should be made more reproducible by providing detailed information on the data preprocessing, hyperparameters, and computational resources used.\n3. The analysis of the role of general instructions in the proposed approach should be expanded to provide a deeper understanding of their impact on prompting performance.\n\nRecommendation:\nThe proposed approach is a valuable contribution to the field of LLMs and domain adaptation. While the paper presents some limitations and areas for future research, the overall results demonstrate the effectiveness and generalizability of the proposed method. The authors should consider refining the approach to address the limitations and provide a more comprehensive analysis of the results. Overall, I recommend accepting the paper for publication.",
        "gr_score_with": 0.5059523809523809,
        "gr_score_without": 0.43567251461988304,
        "or_score_with": 0.41964285714285715,
        "or_score_without": 0.4239766081871345,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 149,
            "green_fraction": 0.43567251461988304,
            "num_orange_tokens": 145,
            "orange_fraction": 0.4239766081871345,
            "green_z_score": 1.3466078963494599,
            "orange_z_score": 99.4675447872566,
            "green_p_value": 0.08905327482916242,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 336,
            "num_green_tokens": 170,
            "green_fraction": 0.5059523809523809,
            "num_orange_tokens": 141,
            "orange_fraction": 0.41964285714285715,
            "green_z_score": 3.964375088367628,
            "orange_z_score": 97.56874801097183,
            "green_p_value": 3.679423361438815e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
        "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper proposes Steve-Eye, a large multimodal model that enables LLM-based embodied agents to interact with open-world environments via visual-text interfaces. Steve-Eye integrates a visual encoder with a pre-trained LLM to process visual-text inputs and generate multimodal feedback. The model is trained on an extensive dataset of 850K open-world instruction pairs, which includes multimodal perception, foundational knowledge base, and skill prediction and planning functions. The paper evaluates Steve-Eye on three open-world benchmarks, including environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nStrengths:\n1. The paper proposes a novel approach to enabling LLM-based embodied agents to interact with open-world environments.\n2. The use of multimodal perception, foundational knowledge base, and skill prediction and planning functions enhances the agent's ability to understand and respond to its environment.\n3. The extensive dataset of 850K open-world instruction pairs provides a comprehensive training ground for Steve-Eye.\n4. The paper evaluates Steve-Eye on three open-world benchmarks, providing a comprehensive assessment of the model's capabilities.\n\nWeaknesses:\n1. The paper relies heavily on the availability of pre-trained LLMs and visual encoders, which may limit the model's ability to generalize to new environments.\n2. The training process may be computationally expensive due to the large size of the dataset and the complexity of the model.\n3. The paper does not provide a detailed analysis of the model's performance on edge cases or adversarial examples.\n4. The evaluation benchmarks may not be comprehensive, and further evaluation may be necessary to fully assess Steve-Eye's capabilities.\n\nSuggestions for Improvement:\n1. To improve the model's generalizability, consider using transfer learning or fine-tuning pre-trained models on new datasets.\n2. To reduce computational costs, consider using more efficient training algorithms or model architectures.\n3. To improve the model's robustness, consider evaluating Steve-Eye on edge cases and adversarial examples.\n4. To further evaluate the model's capabilities, consider using additional benchmarks or evaluating Steve-Eye on more complex tasks.\n\nRecommendation:\nThe paper proposes a novel approach to enabling LLM-based embodied agents to interact with open-world environments, and the evaluation on three open-world benchmarks provides a comprehensive assessment of Steve-Eye's capabilities. However, further evaluation and improvement are necessary to fully assess the model's capabilities and generalizability.\n\n$$\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n\nPaper Title: STEVE-EYE: EQUIPPING LLM-BASED EMBODIED AGENTS WITH VISUAL TEXT INPUTS FOR OPEN-WORLD INTERACTION\n\nSummary:\nThis paper proposes Steve-Eye, a large multimodal model that enables large language models (LLMs) to interact with open-world environments using visual-text inputs. The model integrates a visual encoder with a pre-trained LLM to process visual-text inputs and generate multimodal feedback. The authors collect an extensive dataset of 850K open-world instruction pairs and develop three open-world evaluation benchmarks to validate the model's capability to strategically act and plan. The results demonstrate the advantages of Steve-Eye over a wide range of perspectives, including multimodal perception, foundational knowledge mastery, and skill prediction and planning.\n\nStrengths:\n1. The paper proposes a novel approach to enabling LLM-based embodied agents to interact with open-world environments using visual-text inputs.\n2. The authors demonstrate the effectiveness of Steve-Eye in multimodal perception, foundational knowledge mastery, and skill prediction and planning.\n3. The paper presents a comprehensive evaluation of the model using three open-world benchmarks.\n4. The results show that Steve-Eye outperforms existing approaches, including text-only LLMs and rule-based game judgment.\n\nWeaknesses:\n1. The paper relies heavily on the availability of a large-scale multimodal dataset, which may not be feasible to collect for other applications.\n2. The authors assume that the visual encoder can effectively process visual inputs, which may not be the case for all types of visual inputs.\n3. The paper does not provide a detailed explanation of the multimodal perception function, which may be a complex task.\n4. The results may be biased towards the specific tasks and environments used in the experiments.\n\nSuggestions for Improvement:\n1. The paper should provide a more detailed explanation of the multimodal perception function and its limitations.\n2. The authors should investigate the effectiveness of Steve-Eye in other types of visual inputs, such as images or videos.\n3. The paper should provide a more comprehensive evaluation of the model's ability to handle complex tasks and environments.\n4. The results should be reproduced using different datasets and evaluation metrics to verify the model's generalizability.\n\nRecommendation:\nI recommend that the paper be published in a top-tier conference or journal in the field of artificial intelligence and robotics. The paper presents a novel and effective approach to enabling LLM-based embodied agents to interact with open-world environments, and the results demonstrate the advantages of Steve-Eye over existing approaches. However, the paper should be improved by addressing the limitations and biases of the current evaluation metrics and by providing a more comprehensive explanation of the multimodal perception function.",
        "gr_score_with": 0.49865229110512127,
        "gr_score_without": 0.4318181818181818,
        "or_score_with": 0.42857142857142855,
        "or_score_without": 0.4448051948051948,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 133,
            "green_fraction": 0.4318181818181818,
            "num_orange_tokens": 137,
            "orange_fraction": 0.4448051948051948,
            "green_z_score": 1.1398431577381263,
            "orange_z_score": 88.38552862551043,
            "green_p_value": 0.12717582494862717,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 185,
            "green_fraction": 0.49865229110512127,
            "num_orange_tokens": 159,
            "orange_fraction": 0.42857142857142855,
            "green_z_score": 3.8787207161035697,
            "orange_z_score": 93.4028849627073,
            "green_p_value": 5.250361325341491e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\\boxed{\\text{Paper Summary:}}$\n\nThe paper proposes a novel approach to sorting problems by learning a neural sorting network with an error-free differentiable swap function. The authors define a softening error induced by a monotonic differentiable swap function and propose an error-free swap function to resolve the error accumulation problem. The proposed neural sorting network combines a permutation-equivariant Transformer architecture with multi-head attention and an error-free differentiable swap function. The authors demonstrate the effectiveness of their approach on diverse benchmarks, including multi-digit images and image fragments.\n\n$\\boxed{\\text{Strengths:}}$\n\n* The paper proposes a novel approach to sorting problems using a neural sorting network with an error-free differentiable swap function.\n* The authors provide a thorough analysis of the error accumulation problem and propose a solution using an error-free swap function.\n* The proposed neural sorting network combines a permutation-equivariant Transformer architecture with multi-head attention, which is effective in capturing long-term dependency and reducing inductive biases.\n\n$\\boxed{\\text{Weaknesses:}}$\n\n* The paper assumes that the input data has clear ordering information, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the potential negative societal impact of neural sorting networks, particularly in cases where the final outcomes of sorting are used as supervision.\n* The paper does not provide a comprehensive comparison with other existing approaches to sorting problems.\n\n$\\boxed{\\text{Suggestions for Improvement:}}$\n\n* The authors should provide a more detailed analysis of the potential negative societal impact of neural sorting networks.\n* The authors should provide a comprehensive comparison with other existing approaches to sorting problems.\n* The authors should consider the case where the input data does not have clear ordering information and provide a solution that can handle such cases.\n\n$\\boxed{\\text{Recommendation:}}$\n\nThe paper proposes a novel approach to sorting problems using a neural sorting network with an error-free differentiable swap function. While the approach has its strengths, it also has some weaknesses that need to be addressed. The authors should provide a more detailed analysis of the potential negative societal impact of neural sorting networks and provide a comprehensive comparison with other existing approaches to sorting problems. Additionally, the authors should consider the case where the input data does not have clear ordering information and provide a solution that can handle such cases. Overall, the paper contributes to the field of differentiable sorting algorithms and provides a new perspective on sorting problems.",
        "peer_review_with_watermark": "$$$\\begin{align}\\textbf{Paper Summary:} \n\\text{ The paper presents a neural sorting network that learns a mapping from high-dimensional inputs to ordinal variables. The proposed network adopts a permutation-equivariant Transformer architecture with multi-head attention to capture the relative relationship between instances. The error-free DSF is proposed to resolve the error accumulation problem induced by the soft DSFs. The network is trained with a combined loss function that includes the soft and hard permutation matrices. The experimental results demonstrate the effectiveness of the proposed methods in sorting multi-digit images and image fragments.\n\n\\textbf{Strengths:} \n1. The proposed error-free DSF is able to resolve the error accumulation problem induced by the soft DSFs.\n2. The permutation-equivariant Transformer architecture is capable of capturing the relative relationship between instances.\n3. The combined loss function that includes the soft and hard permutation matrices is able to train the network effectively.\n\n\\textbf{Weaknesses:} \n1. The error-free DSF may not be able to handle the case where the instances are not comparable.\n2. The permutation-equivariant Transformer architecture may not be able to handle the case where the number of instances is very large.\n\n\\textbf{Suggestions for Improvement:} \n1. The error-free DSF should be able to handle the case where the instances are not comparable.\n2. The permutation-equivariant Transformer architecture should be able to handle the case where the number of instances is very large.\n\n\\textbf{Recommendation:} \nBased on the experimental results, the proposed methods are effective in sorting multi-digit images and image fragments. The error-free DSF and the permutation-equivariant Transformer architecture are the key components that contribute to the effectiveness of the proposed methods. The combined loss function that includes the soft and hard permutation matrices is able to train the network effectively.\n\n\\textbf{Potential Bias:} \nThe proposed methods may have a potential negative societal impact if they are employed to sort controversial high-dimensional data such as beauty and intelligence.\n\n\\textbf{Ethical Consideration:} \nThe paper highlights the potential negative societal impact of the proposed methods if they are employed to sort controversial high-dimensional data. The authors emphasize the need for careful consideration of the ethics of the proposed methods.\n\n\\textbf{Contribution:} \nThe paper presents a novel approach to sorting high-dimensional data using a neural sorting network. The proposed error-free DSF and the permutation-equivariant Transformer architecture are the key components that contribute to the effectiveness of the proposed methods.\n\n\\textbf{Originality:} \nThe paper presents a novel approach to sorting high-dimensional data using a neural sorting network. The proposed error-free DSF and the permutation-equivariant Transformer architecture are the key components that contribute to the effectiveness of the proposed methods.\n\n\\textbf{Clarity:} \nThe paper is well-written and easy to follow. The authors provide a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Significance:} \nThe paper presents a novel approach to sorting high-dimensional data using a neural sorting network. The proposed error-free DSF and the permutation-equivariant Transformer architecture are the key components that contribute to the effectiveness of the proposed methods.\n\n\\textbf{Comparison with Other Works:} \nThe paper compares the proposed methods with other existing methods in the field of sorting high-dimensional data. The authors highlight the strengths and weaknesses of the proposed methods compared to other existing methods.\n\n\\textbf{Future Work:} \nThe paper highlights the potential future work in the field of sorting high-dimensional data using neural sorting networks. The authors emphasize the need for further research in this area.\n\n\\textbf{Overall Quality:} \nThe paper is well-written and presents a clear and concise summary of the proposed methods and the experimental results. The authors highlight the strengths and weaknesses of the proposed methods compared to other existing methods.\n\n\\textbf{Specificity:} \nThe paper is specific and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results without any personal opinions or biases.\n\n\\textbf{Conciseness:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of Purpose:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results with a clear purpose.\n\n\\textbf{Relevance:} \nThe paper is relevant to the field of sorting high-dimensional data using neural sorting networks.\n\n\\textbf{Coherence:} \nThe paper is coherent and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Problem Definition:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Originality of the Solution:} \nThe paper presents a novel approach to sorting high-dimensional data using a neural sorting network.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Conciseness of the Solution:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of the Solution:} \nThe paper is clear and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Conciseness of the Solution:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of the Solution:} \nThe paper is clear and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Conciseness of the Solution:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of the Solution:} \nThe paper is clear and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Conciseness of the Solution:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of the Solution:} \nThe paper is clear and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Conciseness of the Solution:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of the Solution:} \nThe paper is clear and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Objectivity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Conciseness of the Solution:} \nThe paper is concise and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Clarity of the Solution:} \nThe paper is clear and presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Overall Quality of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the experimental results.\n\n\\textbf{Specificity of the Solution:} \nThe paper presents a clear and concise summary of the proposed methods and the",
        "gr_score_with": 0.4732620320855615,
        "gr_score_without": 0.4015444015444015,
        "or_score_with": 0.44919786096256686,
        "or_score_without": 0.4942084942084942,
        "output_without": {
            "num_tokens_scored": 259,
            "num_green_tokens": 104,
            "green_fraction": 0.4015444015444015,
            "num_orange_tokens": 128,
            "orange_fraction": 0.4942084942084942,
            "green_z_score": 0.05073462894318302,
            "orange_z_score": 99.36706912434991,
            "green_p_value": 0.47976849113216213,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 374,
            "num_green_tokens": 177,
            "green_fraction": 0.4732620320855615,
            "num_orange_tokens": 168,
            "orange_fraction": 0.44919786096256686,
            "green_z_score": 2.892072266807132,
            "orange_z_score": 108.39144917436543,
            "green_p_value": 0.0019135495275871578,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "EFFECTIVELY LEVERAGING CAPACITY FOR IMPROVED DETERMINISTIC ROBUSTNESS CERTIFICATION",
        "abstract": "Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards underfitting than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large \u201cCholesky-orthogonalized residual dense\u201d layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points.",
        "peer_review_without_watermark": "$$$\\boxed{Peer Review}$$\n\n$\\boxed{Paper Summary:}$\n\nThe paper presents a comprehensive evaluation of Lipschitz-based certification methods for training certifiably robust neural networks. The authors propose a combination of novel techniques, design optimizations, and synthesis of prior work to improve the state-of-the-art deterministic certification on various benchmark datasets. They introduce a new architecture, LiResNet++, which incorporates Cholesky-orthogonalized residual dense layers and filtered generative data augmentation. The authors demonstrate significant improvements in deterministic certification accuracy over state-of-the-art methods.\n\n$\\boxed{Strengths:}$\n\n1. The paper provides a thorough evaluation of Lipschitz-based certification methods, highlighting the limitations of existing approaches.\n2. The authors propose a novel architecture, LiResNet++, which effectively expands the model's capacity.\n3. The introduction of Cholesky-orthogonalized residual dense layers and filtered generative data augmentation significantly improves deterministic certification accuracy.\n4. The paper demonstrates the effectiveness of the proposed approach through extensive experiments.\n\n$\\boxed{Weaknesses:}$\n\n1. The paper may benefit from a more detailed discussion of the theoretical foundations of Lipschitz-based certification methods.\n2. Some of the proposed techniques, such as Cholesky-orthogonalized residual dense layers, may be difficult to implement and require significant computational resources.\n3. The paper does not provide a detailed analysis of the trade-offs between model capacity and Lipschitz constant in Lipschitz-based certification methods.\n\n$\\boxed{Suggestions for Improvement:}$\n\n1. The authors could provide a more detailed analysis of the theoretical foundations of Lipschitz-based certification methods to better understand the limitations and potential of these approaches.\n2. The paper could benefit from a more detailed discussion of the implementation challenges and computational requirements of the proposed techniques.\n3. The authors could explore the trade-offs between model capacity and Lipschitz constant in Lipschitz-based certification methods to provide a more comprehensive understanding of the design space.\n\n$\\boxed{Recommendation:}$\n\nBased on the evaluation, the proposed approach, LiResNet++, appears to be a promising avenue for improving deterministic certification accuracy. However, further research is needed to fully understand the theoretical foundations of Lipschitz-based certification methods and to address the implementation challenges and computational requirements of the proposed techniques.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper presents a comprehensive evaluation of Lipschitz-based methods for deterministic robustness certification of neural networks. The authors propose novel techniques, design optimizations, and synthesis of prior work to improve the state-of-the-art in deterministic robustness certification. Specifically, they introduce Cholesky-orthogonalized residual dense layers and explore novel data augmentation techniques using generated models. The results demonstrate significant improvements in deterministic robustness certification over existing state-of-the-art methods.\n\nStrengths:\n\n1. The paper presents a thorough evaluation of Lipschitz-based methods, covering various architectures, Lipschitz control mechanisms, and data augmentation techniques.\n2. The authors propose novel and effective solutions to mitigate the limitations of existing Lipschitz-based models, such as Cholesky-orthogonalized residual dense layers and improved data augmentation pipelines.\n3. The results demonstrate significant improvements in deterministic robustness certification over existing state-of-the-art methods.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with Lipschitz-based methods and deterministic robustness certification, which may not be the case for all readers.\n2. The evaluation is based on a limited number of datasets and perturbation sizes, which may not be representative of the broader applicability of the methods.\n3. The paper does not provide a detailed analysis of the theoretical underpinnings of the proposed methods, which may be of interest to some readers.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed introduction to Lipschitz-based methods and deterministic robustness certification to make the paper more accessible to a broader audience.\n2. Expand the evaluation to include more datasets and perturbation sizes to demonstrate the robustness of the proposed methods.\n3. Provide a more detailed analysis of the theoretical underpinnings of the proposed methods to provide a deeper understanding of their strengths and limitations.\n\nRecommendation:\n\nOverall, the paper presents a thorough and effective evaluation of Lipschitz-based methods for deterministic robustness certification. The proposed solutions demonstrate significant improvements over existing state-of-the-art methods, and the results have the potential to bridge the gap between probabilistic and deterministic certification methods. With some additional improvements to the introduction, evaluation, and theoretical analysis, the paper has the potential to make a significant contribution to the field of robust neural networks.",
        "gr_score_with": 0.506896551724138,
        "gr_score_without": 0.4384057971014493,
        "or_score_with": 0.4482758620689655,
        "or_score_without": 0.47101449275362317,
        "output_without": {
            "num_tokens_scored": 276,
            "num_green_tokens": 121,
            "green_fraction": 0.4384057971014493,
            "num_orange_tokens": 130,
            "orange_fraction": 0.47101449275362317,
            "green_z_score": 1.302403946034924,
            "orange_z_score": 89.82482557596497,
            "green_p_value": 0.09638916717962959,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 290,
            "num_green_tokens": 147,
            "green_fraction": 0.506896551724138,
            "num_orange_tokens": 130,
            "orange_fraction": 0.4482758620689655,
            "green_z_score": 3.715840586511727,
            "orange_z_score": 87.55880334664917,
            "green_p_value": 0.00010126462469883575,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CODINGS FOR GRAPHS",
        "abstract": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) Non-uniqueness: there are many different eigendecompositions of the same Laplacian, and (2) Instability: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address nonuniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a \u201chard partition\u201d of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to \u201csoftly partition\u201d eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces a new positional encoding method, Stable and Expressive Positional Encodings (SPE), which is designed to be both stable and expressive for graph-structured data. The authors argue that existing methods that use Laplacian eigenvectors as positional encodings face two fundamental challenges: non-uniqueness and instability. To address these challenges, SPE proposes a soft partitioning of eigenspaces using eigenvalues, which ensures stability and expressivity. The authors prove that SPE is provably stable and universally expressive for basis invariant functions, and demonstrate its effectiveness on molecular property prediction and graph substructure counting tasks.\n\nStrengths:\n1. The authors provide a clear and concise introduction to the problem of designing effective positional encodings for graphs.\n2. The paper presents a novel approach to addressing the challenges of non-uniqueness and instability in existing positional encoding methods.\n3. The authors provide a rigorous mathematical analysis of the stability and expressivity of SPE.\n4. The paper presents extensive experimental results to demonstrate the effectiveness of SPE on various benchmarks.\n\nWeaknesses:\n1. The paper may benefit from a more detailed explanation of the mathematical assumptions and definitions used in the stability analysis.\n2. Some of the experimental results, such as the comparison with other positional encoding methods, may be difficult to interpret due to the lack of a clear baseline.\n3. The paper does not provide a comprehensive comparison with other graph neural network architectures.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the mathematical assumptions and definitions used in the stability analysis.\n2. Include a more comprehensive comparison with other graph neural network architectures.\n3. Consider including more experimental results to demonstrate the effectiveness of SPE on various benchmarks.\n\nRecommendation:\nI recommend accepting the paper for publication in its current form. The authors have presented a novel approach to addressing the challenges of non-uniqueness and instability in existing positional encoding methods, and have demonstrated its effectiveness on various benchmarks. The paper provides a clear and concise introduction to the problem, and presents a rigorous mathematical analysis of the stability and expressivity of SPE. With some minor revisions to address the weaknesses mentioned above, the paper has the potential to make a significant contribution to the field of graph neural networks.",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs, specifically the non-uniqueness and instability of existing methods. The proposed method is a soft partitioning of eigenspaces using eigenvalues to achieve stability and expressivity.\n\nStrengths:\n\n* The paper presents a well-structured and well-organized work, with a clear introduction, literature review, and technical contributions.\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative and effective in achieving stability and expressivity.\n* The analysis of the stability of SPE is comprehensive and provides a clear understanding of the benefits of the proposed method.\n* The experiments on various datasets demonstrate the effectiveness of SPE in achieving better performance compared to existing methods.\n\nWeaknesses:\n\n* The literature review could be more comprehensive, as some existing methods are not mentioned.\n* The analysis of the expressivity of SPE could be more detailed, as the results are not always clear.\n* The experiments could be more extensive, with more datasets and more detailed results.\n\nSuggestions for Improvement:\n\n* Provide a more comprehensive literature review to ensure that all existing methods are mentioned.\n* Provide more detailed analysis of the expressivity of SPE, including more results and more detailed results.\n* Consider including more experiments to demonstrate the effectiveness of SPE on various datasets.\n\nRecommendation:\nOverall, the paper presents a well-structured and well-organized work, with a clear introduction, literature review, and technical contributions. The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative and effective in achieving stability and expressivity. The analysis of the stability of SPE is comprehensive and provides a clear understanding of the benefits of the proposed method. The experiments on various datasets demonstrate the effectiveness of SPE in achieving better performance compared to existing methods. With some revisions to the literature review, analysis of expressivity, and experiments, the paper could be of high quality.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs, specifically the non-uniqueness and instability of existing methods. The proposed method is a soft partitioning of eigenspaces using eigenvalues to achieve stability and expressivity.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative and effective in achieving stability and expressivity.\n* The analysis of the stability of SPE is comprehensive and provides a clear understanding of the benefits of the proposed method.\n* The experiments on various datasets demonstrate the effectiveness of SPE in achieving better performance compared to existing methods.\n\nWeaknesses:\n\n* The literature review could be more comprehensive, as some existing methods are not mentioned.\n* The analysis of the expressivity of SPE could be more detailed, as the results are not always clear.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs, specifically the non-uniqueness and instability of existing methods. The proposed method is a soft partitioning of eigenspaces using eigenvalues to achieve stability and expressivity.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative and effective in achieving stability and expressivity.\n* The analysis of the stability of SPE is comprehensive and provides a clear understanding of the benefits of the proposed method.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n* The analysis of the expressivity of SPE could be more detailed.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs, specifically the non-uniqueness and instability of existing methods. The proposed method is a soft partitioning of eigenspaces using eigenvalues to achieve stability and expressivity.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative and effective in achieving stability and expressivity.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n* The analysis of the expressivity of SPE could be more detailed.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs, specifically the non-uniqueness and instability of existing methods.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative and effective in achieving stability and expressivity.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n* The analysis of the expressivity of SPE could be more detailed.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n* The analysis of the expressivity of SPE could be more detailed.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to address the challenges of designing effective positional encodings for graphs.\n\nStrengths:\n\n* The proposed method of soft partitioning of eigenspaces using eigenvalues is innovative.\n\nWeaknesses:\n\n* The literature review could be more comprehensive.\n\n$$$$\n\nPaper Summary:\n\n$$$$\nThe paper proposes a new positional encoding method for graphs called Stable and Expressive Positional Encodings (S",
        "gr_score_with": 0.5177865612648221,
        "gr_score_without": 0.33980582524271846,
        "or_score_with": 0.42292490118577075,
        "or_score_without": 0.5566343042071198,
        "output_without": {
            "num_tokens_scored": 309,
            "num_green_tokens": 105,
            "green_fraction": 0.33980582524271846,
            "num_orange_tokens": 172,
            "orange_fraction": 0.5566343042071198,
            "green_z_score": -2.1598723443040155,
            "orange_z_score": 112.83837821072287,
            "green_p_value": 0.9846087233841372,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 253,
            "num_green_tokens": 131,
            "green_fraction": 0.5177865612648221,
            "num_orange_tokens": 107,
            "orange_fraction": 0.42292490118577075,
            "green_z_score": 3.8242861674281228,
            "orange_z_score": 77.24731540667912,
            "green_p_value": 6.557575732673786e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS",
        "abstract": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson\u2019s r) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "peer_review_without_watermark": "$$$ \n\nPaper Summary:\nThe paper proposes a novel method called GeoLLM, which utilizes large language models (LLMs) to extract geospatial knowledge from auxiliary map data from OpenStreetMap. The authors demonstrate the effectiveness of GeoLLM in predicting various geospatial indicators, such as population density and economic livelihoods, across multiple tasks and datasets. They also show that GeoLLM outperforms traditional baselines, including k-NN and XGBoost, and even satellite-based methods.\n\nStrengths:\n1. The authors provide a thorough exploration of the geospatial knowledge embedded in LLMs and demonstrate the potential of GeoLLM in extracting this knowledge.\n2. The proposed method is simple and effective, with a 70% improvement in performance over traditional baselines.\n3. The authors provide a comprehensive benchmark of GeoLLM's performance across multiple tasks and datasets, showcasing its robustness and scalability.\n4. The paper highlights the potential of GeoLLM in mitigating the limitations of traditional geospatial covariates.\n\nWeaknesses:\n1. The authors rely heavily on the performance of GPT-3.5, which may not generalize well to other LLMs or tasks.\n2. The paper does not provide a detailed analysis of the biases and limitations of GeoLLM, which may impact its applicability in certain contexts.\n3. The authors do not discuss the potential risks and challenges associated with using GeoLLM, such as data privacy concerns or the potential for biased knowledge extraction.\n\nSuggestions for Improvement:\n1. The authors should conduct a more comprehensive analysis of GeoLLM's performance across different LLMs and tasks to ensure its generalizability.\n2. The paper should provide a more detailed discussion of the biases and limitations of GeoLLM, including potential risks and challenges associated with its use.\n3. The authors could explore the potential applications of GeoLLM in other domains, such as healthcare or education, to demonstrate its broader value.\n\nRecommendation:\nI recommend that the paper be accepted for publication in a top-tier conference or journal in the field of geospatial analysis or natural language processing. The authors have made a significant contribution to the field by proposing a novel method for extracting geospatial knowledge from LLMs, and their results have the potential to impact various applications in geospatial analysis and beyond.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents the concept of GeoLLM, a novel method for extracting geospatial knowledge from large language models (LLMs) by fine-tuning them on prompts constructed with auxiliary map data from OpenStreetMap. The authors demonstrate the utility of their approach across multiple tasks, including population density, economic livelihoods, and housing prices, and show that it outperforms traditional baselines, including k-NN and XGBoost, and even satellite-based methods. The authors also highlight the sample efficiency, robustness, and global applicability of LLMs in extracting geospatial knowledge.\n\nStrengths:\n1. The authors provide a clear and well-structured paper that presents a novel and promising approach for extracting geospatial knowledge from LLMs.\n2. The paper demonstrates the utility of the approach across multiple tasks and datasets, providing strong evidence for its effectiveness.\n3. The authors provide a detailed analysis of the sample efficiency, robustness, and global applicability of LLMs in extracting geospatial knowledge.\n4. The paper provides a clear and concise conclusion that summarizes the main contributions and implications of the work.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the limitations and potential biases of the approach, which is an important consideration for any geospatial analysis.\n2. The paper relies heavily on the authors' own experiments and may not be generalizable to other datasets or tasks.\n3. The paper does not provide a detailed comparison of the approach with other geospatial analysis methods, which is an important consideration for any analysis.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations and potential biases of the approach, including any potential issues with data quality, model performance, or geospatial accuracy.\n2. The authors should provide a more detailed comparison of the approach with other geospatial analysis methods, including k-NN, XGBoost, and satellite-based methods.\n3. The authors should provide more experimental results and analysis to further validate the approach and provide stronger evidence for its effectiveness.\n4. The authors should provide a more detailed discussion of the implications of the approach and its potential applications in various domains, including geospatial analysis, urban planning, and environmental monitoring.\n\nRecommendation:\nBased on the paper's strengths and weaknesses, I recommend that the authors revise and resubmit the paper. Specifically, I recommend that they provide a more detailed analysis of the limitations and potential biases of the approach, provide a more detailed comparison with other geospatial analysis methods, and provide more experimental results and analysis to further validate the approach. With these improvements, the paper has the potential to make a significant contribution to the field of geospatial analysis and LLMs.",
        "gr_score_with": 0.5915915915915916,
        "gr_score_without": 0.49262536873156343,
        "or_score_with": 0.33633633633633636,
        "or_score_without": 0.40707964601769914,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 167,
            "green_fraction": 0.49262536873156343,
            "num_orange_tokens": 138,
            "orange_fraction": 0.40707964601769914,
            "green_z_score": 3.48116155074417,
            "orange_z_score": 88.5456838043132,
            "green_p_value": 0.00024962213854836616,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 333,
            "num_green_tokens": 197,
            "green_fraction": 0.5915915915915916,
            "num_orange_tokens": 112,
            "orange_fraction": 0.33633633633633636,
            "green_z_score": 7.13662605377579,
            "orange_z_score": 72.24199819793415,
            "green_p_value": 4.782479600119897e-13,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LEARNING THE GREATEST COMMON DIVISOR: EXPLAINING TRANSFORMER PREDICTIONS",
        "abstract": "The predictions of small transformers, trained to calculate the greatest common divisor (GCD) of two positive integers, can be fully characterized by looking at model inputs and outputs. As training proceeds, the model learns a listD of integers, products of divisors of the base used to represent integers and small primes, and predicts the largest element of D that divides both inputs. Training distributions impact performance. Models trained from uniform operands only learn a handful of GCD (up to 38 GCD \u2264 100). Log-uniform operands boost performance to 73 GCD \u2264 100, and a log-uniform distribution of outcomes (i.e. GCD) to 91. However, training from uniform (balanced) GCD breaks explainability.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}~\\text{The}~\\text{paper}~\\text{presents}~\\text{an}~\\text{experiment}~\\text{where}~\\text{transformers}~\\text{are}~\\text{trained}~\\text{to}~\\text{calculate}~\\text{the}~\\text{greatest}~\\text{common}~\\text{divisor}~\\text{(GCD)}~\\text{of}~\\text{two}~\\text{positive}~\\text{integers}.~\\text{The}~\\text{model}~\\text{learns}~\\text{to}~\\text{cluster}~\\text{input}~\\text{pairs}~\\text{with}~\\text{the}~\\text{same}~\\text{GCD}~\\text{and}~\\text{predicts}~\\text{the}~\\text{largest}~\\text{element}~\\text{in}~\\text{a}~\\text{set}~\\text{D}~\\text{that}~\\text{divides}~\\text{both}~\\text{inputs}.~\\text{The}~\\text{results}~\\text{show}~\\text{that}~\\text{models}~\\text{trained}~\\text{from}~\\text{log-uniform}~\\text{operands}~\\text{and}~\\text{outcomes}~\\text{achieve}~\\text{better}~\\text{performance}.~\\text{The}~\\text{three}~\\text{rules}~\\text{account}~\\text{for}~\\text{model}~\\text{predictions}.}\n\n$$\\boxed{\\text{Strengths:}~\\text{The}~\\text{paper}~\\text{presents}~\\text{an}~\\text{interesting}~\\text{experiment}~\\text{on}~\\text{transformers}~\\text{learning}~\\text{the}~\\text{GCD}~\\text{of}~\\text{two}~\\text{positive}~\\text{integers}.~\\text{The}~\\text{results}~\\text{show}~\\text{that}~\\text{models}~\\text{trained}~\\text{from}~\\text{log-uniform}~\\text{operands}~\\text{and}~\\text{outcomes}~\\text{achieve}~\\text{better}~\\text{performance}.~\\text{The}~\\text{three}~\\text{rules}~\\text{account}~\\text{for}~\\text{model}~\\text{predictions}.}\n\n$$\\boxed{\\text{Weaknesses:}~\\text{The}~\\text{paper}~\\text{has}~\\text{some}~\\text{limitations}.~\\text{The}~\\text{results}~\\text{are}~\\text{limited}~\\text{to}~\\text{four}~\\text{-layer}~\\text{transformers}.~\\text{The}~\\text{paper}~\\text{does}~\\text{not}~\\text{provide}~\\text{a}~\\text{comparative}~\\text{analysis}~\\text{of}~\\text{other}~\\text{architectures}.}\n\n$$\\boxed{\\text{Suggestions}~\\text{for}~\\text{Improvement:}~\\text{The}~\\text{paper}~\\text{could}~\\text{benefit}~\\text{from}~\\text{comparative}~\\text{analysis}~\\text{of}~\\text{other}~\\text{architectures}.~\\text{The}~\\text{paper}~\\text{could}~\\text{also}~\\text{provide}~\\text{a}~\\text{theoretical}~\\text{analysis}~\\text{of}~\\text{the}~\\text{results}.}\n\n$$\\boxed{\\text{Recommendation:}~\\text{I}~\\text{recommend}~\\text{the}~\\text{paper}~\\text{for}~\\text{its}~\\text{interesting}~\\text{experiment}~\\text{on}~\\text{transformers}~\\text{learning}~\\text{the}~\\text{GCD}~\\text{of}~\\text{two}~\\text{positive}~\\text{integers}.~\\text{However},~\\text{I}~\\text{also}~\\text{recommend}~\\text{comparative}~\\text{analysis}~\\text{of}~\\text{other}~\\text{architectures}.~\\text{The}~\\text{paper}~\\text{could}~\\text{also}~\\text{provide}~\\text{a}~\\text{theoretical}~\\text{analysis}~\\text{of}~\\text{the}~\\text{results}.}$$",
        "peer_review_with_watermark": "$$$$\n\n**Paper Summary:**\nThe paper presents an experimental study on the ability of transformers to learn the greatest common divisor (GCD) of two positive integers. The authors train 4-layer transformers on various bases, from small primes to composite bases, and observe that the models learn to cluster input pairs by their GCD and predict the largest element in the cluster that divides both inputs. The results show that models trained on log-uniform operands and outcomes achieve better performance than those trained on uniform operands. The paper also explores the role of training distributions in learning the GCD, finding that an unbalanced distribution of outcomes is required for full explainability. The authors propose a new approach to model explainability by fully characterizing black-box model predictions using selected inputs and leveraging theoretical understanding of the underlying mathematics.\n\n**Strengths:**\n1. The paper presents an in-depth experimental study on the ability of transformers to learn the GCD of two positive integers.\n2. The results show that models trained on log-uniform operands and outcomes achieve better performance than those trained on uniform operands.\n3. The paper explores the role of training distributions in learning the GCD and provides new insights into the importance of an unbalanced distribution of outcomes for full explainability.\n4. The authors propose a new approach to model explainability that can be applied to other arithmetic tasks and black-box models.\n\n**Weaknesses:**\n1. The paper is lengthy and may be challenging for readers to follow, particularly those without a strong background in mathematics and transformers.\n2. The results are mostly based on empirical observations, and the authors do not provide a clear theoretical framework for understanding the mechanisms behind the transformer's ability to learn the GCD.\n3. The paper could benefit from more detailed analysis of the transformer's behavior and the role of specific hyperparameters in achieving good performance.\n4. The results are limited to the specific task of learning the GCD and may not generalize to other arithmetic tasks or transformer architectures.\n\n**Suggestions for Improvement:**\n1. The paper could benefit from a clearer theoretical framework for understanding the mechanisms behind the transformer's ability to learn the GCD.\n2. The authors could provide more detailed analysis of the transformer's behavior and the role of specific hyperparameters in achieving good performance.\n3. The results could be more thoroughly validated using additional experimental settings and evaluation metrics.\n4. The paper could be more concise and focused, with clear sections and subsections to facilitate easier navigation and understanding.\n\n**Recommendation:**\nOverall, the paper presents an important experimental study on the ability of transformers to learn the GCD of two positive integers. The results are intriguing, and the authors propose a new approach to model explainability that can be applied to other arithmetic tasks and black-box models. However, the paper could benefit from further development and refinement to address the limitations and criticisms noted above.",
        "gr_score_with": 0.5697674418604651,
        "gr_score_without": 0.5887850467289719,
        "or_score_with": 0.35755813953488375,
        "or_score_without": 0.12149532710280374,
        "output_without": {
            "num_tokens_scored": 214,
            "num_green_tokens": 126,
            "green_fraction": 0.5887850467289719,
            "num_orange_tokens": 26,
            "orange_fraction": 0.12149532710280374,
            "green_z_score": 5.637270279078069,
            "orange_z_score": 20.09947532392374,
            "green_p_value": 8.638342157296598e-09,
            "orange_p_value": 3.7288944476077544e-90
        },
        "output_with": {
            "num_tokens_scored": 344,
            "num_green_tokens": 196,
            "green_fraction": 0.5697674418604651,
            "num_orange_tokens": 123,
            "orange_fraction": 0.35755813953488375,
            "green_z_score": 6.427291616177055,
            "orange_z_score": 78.0213707661497,
            "green_p_value": 6.49487414650089e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "EX-GRAPH: A PIONEERING DATASET BRIDGING ETHEREUM",
        "abstract": "While numerous public blockchain datasets are available, their utility is constrained by an exclusive focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be derived. To address the above limitation, we introduce EX-Graph, a novel dataset that authentically links Ethereum and X, marking the first and largest dataset of its kind. EX-Graph combines Ethereum transaction records (2 million nodes and 30 million edges) and X following data (1 million nodes and 3 million edges), bonding 30,667 Ethereum addresses with verified X accounts sourced from OpenSea. Detailed statistical analysis on EXGraph highlights the structural differences between X-matched and non-X-matched Ethereum addresses. Extensive experiments, including Ethereum link prediction, wash-trading Ethereum addresses detection, and X-Ethereum matching link prediction, emphasize the significant role of X data in enhancing Ethereum analysis. EX-Graph is available at https://exgraph.deno.dev/.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces EX-Graph, a novel dataset that bridges Ethereum and X, combining Ethereum transaction records and X follower networks. The dataset consists of 3 million nodes, 33 million edges, and 30,667 matching links between Ethereum addresses and X accounts. The authors conduct statistical analysis and experiments to evaluate the performance of various tasks on EX-Graph, including Ethereum link prediction, wash-trading addresses detection, and matching link prediction. The results show that integrating X data significantly improves the performance of these tasks, particularly in detecting wash-trading addresses.\n\nStrengths:\n\n* The paper introduces a novel dataset that bridges Ethereum and X, providing a comprehensive view of on-chain and off-chain activities.\n* The authors conduct thorough experiments to evaluate the performance of various tasks on EX-Graph, providing a robust analysis of the dataset's utility.\n* The use of deep learning models and graph neural networks demonstrates the effectiveness of these methods in analyzing complex graph structures.\n\nWeaknesses:\n\n* The paper relies heavily on the authors' own dataset, which may limit the generalizability of the results.\n* The experiments are primarily focused on Ethereum link prediction, wash-trading addresses detection, and matching link prediction, which may not be representative of the full range of tasks that can be addressed using EX-Graph.\n* The paper does not provide a thorough discussion of the limitations and challenges of working with this dataset, which may be a significant concern for future researchers.\n\nSuggestions for Improvement:\n\n* The authors should consider collecting more matching links between on-chain and off-chain graphs, as well as wash-trading Ethereum addresses, to further enhance the dataset's utility.\n* The paper should provide a more comprehensive analysis of the dataset's limitations and challenges, including issues related to data quality, noise, and bias.\n* The authors should consider exploring other tasks and applications that can be addressed using EX-Graph, such as network analysis, social network analysis, and machine learning.\n\nRecommendation:\n\n* The paper is well-written and provides a thorough analysis of the EX-Graph dataset and its applications.\n* The use of deep learning models and graph neural networks demonstrates the effectiveness of these methods in analyzing complex graph structures.\n* However, the paper's limitations and challenges should be carefully considered, and the authors should strive to address these concerns in future work.",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper introduces EX-Graph, a pioneering and extensive dataset that bridges Ethereum and X, providing a comprehensive view of on-chain and off-chain activities. The dataset combines Ethereum transaction records and X following data, creating a unified graph that captures the relationships between Ethereum addresses and X accounts. The paper demonstrates the significant role of X data in enhancing Ethereum analysis, including link prediction, wash-trading addresses detection, and matching link prediction. The results show that integrating X features significantly improves the performance of various tasks on Ethereum, highlighting the benefits of using both on-chain and off-chain data in probing on-chain activities.\n\nStrengths:\n\n* The paper introduces a novel and comprehensive dataset that bridges Ethereum and X, providing a valuable resource for researchers.\n* The dataset is extensively analyzed, and the results demonstrate the significant role of X data in enhancing Ethereum analysis.\n* The paper discusses the limitations of previous research and highlights the importance of integrating on-chain and off-chain data in probing on-chain activities.\n\nWeaknesses:\n\n* The paper lacks a detailed discussion on the data collection process and the ethical considerations involved in collecting and using the data.\n* Some sections of the paper, such as the experimental results and discussions, are lengthy and may benefit from further clarification and organization.\n* The paper does not provide a detailed analysis of the limitations and potential biases of the dataset and the methods used to analyze it.\n\nSuggestions for Improvement:\n\n* Provide a detailed discussion on the data collection process and the ethical considerations involved in collecting and using the data.\n* Break down lengthy sections into more focused and organized sections, and provide further clarification and explanations as needed.\n* Analyze the limitations and potential biases of the dataset and the methods used to analyze it, and discuss potential avenues for future research.\n\nRecommendation:\n\n* The paper makes a valuable contribution to the field of Ethereum research, and the introduction of EX-Graph as a comprehensive dataset is a significant achievement.\n* However, further work is needed to address the limitations and potential biases of the dataset and the methods used to analyze it, and to provide a more detailed and comprehensive analysis of the results.\n* The paper is recommended for publication in a top-tier conference or journal in the field of Ethereum research and related areas.",
        "gr_score_with": 0.4752475247524752,
        "gr_score_without": 0.37540453074433655,
        "or_score_with": 0.4521452145214521,
        "or_score_without": 0.5048543689320388,
        "output_without": {
            "num_tokens_scored": 309,
            "num_green_tokens": 116,
            "green_fraction": 0.37540453074433655,
            "num_orange_tokens": 156,
            "orange_fraction": 0.5048543689320388,
            "green_z_score": -0.8825284847693834,
            "orange_z_score": 106.9175055849381,
            "green_p_value": 0.8112544585401055,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 303,
            "num_green_tokens": 144,
            "green_fraction": 0.4752475247524752,
            "num_orange_tokens": 137,
            "orange_fraction": 0.4521452145214521,
            "green_z_score": 2.673670669975109,
            "orange_z_score": 94.67076123222536,
            "green_p_value": 0.0037513044578436597,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LUM-VIT: LEARNABLE UNDER-SAMPLING MASK VISION TRANSFORMER FOR BANDWIDTH LIMITED OPTICAL SIGNAL ACQUISITION",
        "abstract": "Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a Vision Transformer variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a threestage fine-tuning strategy. Our evaluations reveal that, by sampling a mere 10% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality. Code will be available at https://github.com/MaxLLF/LUM-ViT.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes a novel approach called LUM-ViT, which leverages pre-acquisition modulation to reduce the acquisition volume of hyperspectral data. This is achieved by using a deep learning model that utilizes prior information and incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. The method also employs a kernel-level weight binarization technique and a three-stage fine-tuning strategy. The authors evaluate LUM-ViT on the ImageNet-1k classification task and demonstrate its practical feasibility in real-world optical hardware.\n\n$$\\text{Strengths:}$$\n1. The paper presents a novel approach to under-sampling hyperspectral data acquisition using pre-acquisition modulation and a learnable under-sampling mask.\n2. The method employs a kernel-level weight binarization technique, which is efficient and effective.\n3. The three-stage fine-tuning strategy allows for effective training and adaptation of the model.\n4. The authors demonstrate the practical feasibility of LUM-ViT in real-world optical hardware.\n\n$$\\text{Weaknesses:}$$\n1. The paper lacks a clear and comprehensive literature review, which is essential for understanding the context and relevance of the proposed method.\n2. The authors do not provide a detailed explanation of the learnable under-sampling mask and its optimization process.\n3. The method relies heavily on the pre-acquisition modulation, which may not be suitable for all types of hyperspectral data.\n4. The paper does not address the limitations and potential drawbacks of the proposed method.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Conduct a comprehensive literature review to provide a thorough understanding of the context and relevance of the proposed method.\n2. Provide a detailed explanation of the learnable under-sampling mask and its optimization process.\n3. Explore alternative methods for under-sampling hyperspectral data acquisition and compare their performance with LUM-ViT.\n4. Address the limitations and potential drawbacks of the proposed method.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit their manuscript for peer review. The authors should address the limitations and potential drawbacks of the proposed method and provide a more comprehensive literature review. Additionally, they should explore alternative methods for under-sampling hyperspectral data acquisition and compare their performance with LUM-ViT.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach called LUM-ViT, which leverages pre-acquisition modulation to reduce the acquisition volume of hyperspectral data. The approach utilizes a deep learning model, incorporating a learnable under-sampling mask tailored for pre-acquisition modulation. The paper presents a three-stage training strategy and evaluates the performance of LUM-ViT on the ImageNet-1k classification task, demonstrating that it maintains accuracy loss within 1.8% at 10% under-sampling and within 5.5% at an extreme 2% under-sampling. The paper also presents real-world experiments, demonstrating the practical feasibility of LUM-ViT in a real-world scenario with DMD involvement, keeping the accuracy loss within 4% for RGB images and within 5% for 7-color images.\n\nStrengths:\n1. The paper proposes a novel approach that leverages pre-acquisition modulation to reduce the acquisition volume of hyperspectral data, which is a significant challenge in real-time detection applications.\n2. The approach incorporates a learnable under-sampling mask, which is a critical component in achieving undersampling.\n3. The three-stage training strategy is effective in training the model, and the paper presents real-world experiments, demonstrating the practical feasibility of LUM-ViT.\n4. The paper evaluates the performance of LUM-ViT on the ImageNet-1k classification task, demonstrating that it maintains accuracy loss within 1.8% at 10% under-sampling and within 5.5% at an extreme 2% under-sampling.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the limitations and challenges of the proposed approach, which is essential in understanding the strengths and weaknesses of the approach.\n2. The paper does not provide a thorough comparison with other existing approaches, which is essential in understanding the effectiveness of LUM-ViT.\n3. The paper does not provide a detailed explanation of the three-stage training strategy, which is essential in understanding the training process.\n4. The paper does not provide a detailed analysis of the impact of the learnable under-sampling mask on the performance of LUM-ViT, which is essential in understanding the role of the mask in achieving undersampling.\n\nSuggestions for Improvement:\n1. Provide a detailed analysis of the limitations and challenges of the proposed approach, including the impact of the learnable under-sampling mask on the performance of LUM-ViT.\n2. Provide a thorough comparison with other existing approaches, including hyperspectral image classification tasks, to understand the effectiveness of LUM-ViT.\n3. Provide a detailed explanation of the three-stage training strategy, including the training process and the impact of each stage on the performance of LUM-ViT.\n4. Provide a detailed analysis of the impact of the learnable under-sampling mask on the performance of LUM-ViT, including the role of the mask in achieving undersampling and the impact of the mask on the accuracy loss of LUM-ViT.\n\nRecommendation:\nThe paper proposes a novel approach called LUM-ViT, which leverages pre-acquisition modulation to reduce the acquisition volume of hyperspectral data. The approach incorporates a learnable under-sampling mask, which is a critical component in achieving undersampling. The three-stage training strategy is effective in training the model, and the paper presents real-world experiments, demonstrating the practical feasibility of LUM-ViT. However, the paper lacks a detailed analysis of the limitations and challenges of the proposed approach, and a thorough comparison with other existing approaches. Therefore, the paper can be improved by providing a detailed analysis of the limitations and challenges of the proposed approach, and a thorough comparison with other existing approaches.",
        "gr_score_with": 0.4693140794223827,
        "gr_score_without": 0.3611111111111111,
        "or_score_with": 0.48375451263537905,
        "or_score_without": 0.4895833333333333,
        "output_without": {
            "num_tokens_scored": 288,
            "num_green_tokens": 104,
            "green_fraction": 0.3611111111111111,
            "num_orange_tokens": 141,
            "orange_fraction": 0.4895833333333333,
            "green_z_score": -1.347150628109127,
            "orange_z_score": 92.43899957409324,
            "green_p_value": 0.9110341373658728,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 277,
            "num_green_tokens": 130,
            "green_fraction": 0.4693140794223827,
            "num_orange_tokens": 134,
            "orange_fraction": 0.48375451263537905,
            "green_z_score": 2.3548091968969813,
            "orange_z_score": 89.55949607144052,
            "green_p_value": 0.009266109365260844,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CURRICULUM REINFORCEMENT LEARNING FOR QUANTUM ARCHITECTURE SEARCH UNDER HARD-",
        "abstract": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.",
        "peer_review_without_watermark": "$$$\\begin{align}\\textbf{Paper Summary:} &\\quad The authors propose a curriculum-based reinforcement learning quantum architecture search (CRLQAS) algorithm to tackle the challenges of finding useful quantum circuits compatible with current device limitations in the noisy intermediate-scale quantum era. The CRLQAS algorithm incorporates a 3D architecture encoding, restrictions on environment dynamics, an episode halting scheme, and a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. The authors demonstrate the effectiveness of the CRLQAS algorithm in finding ground-state energies for quantum chemistry tasks in both noiseless and noisy environments.\n\n\\textbf{Strengths:} &\\quad The authors have made significant contributions to the field of quantum architecture search by proposing a novel CRLQAS algorithm that effectively tackles the challenges of finding useful quantum circuits. The algorithm's use of a 3D architecture encoding, restrictions on environment dynamics, and episode halting scheme demonstrate a deep understanding of the complexities involved in quantum circuit design. The authors have also developed an optimized simulator for the CRLQAS algorithm, which significantly improves computational efficiency in simulating noisy quantum circuits.\n\n\\textbf{Weaknesses:} &\\quad One of the main limitations of the CRLQAS algorithm is its dependence on the choice of hyperparameters, such as the maximum number of actions allowed per episode (nact) and the threshold value (\u03be). The authors acknowledge the importance of hyperparameter tuning but do not provide a detailed analysis of its impact on the algorithm's performance. Additionally, the authors' use of the Pauli-transfer matrix (PTM) formalism to simulate noisy quantum circuits is limited to the context of this paper and may not be generalizable to other scenarios.\n\n\\textbf{Suggestions for Improvement:} &\\quad To further improve the CRLQAS algorithm, the authors should provide a more detailed analysis of the impact of hyperparameter tuning on the algorithm's performance. They should also investigate the generalizability of the PTM formalism to other scenarios and explore ways to extend its application to more complex quantum systems. Furthermore, the authors could consider incorporating other optimization strategies, such as evolutionary algorithms or genetic programming, to complement the CRLQAS algorithm and improve its overall performance.\n\n\\textbf{Recommendation:} &\\quad Based on the authors' significant contributions to the field of quantum architecture search, their novel CRLQAS algorithm, and the demonstrated effectiveness of the algorithm in finding ground-state energies for quantum chemistry tasks, I recommend that the CRLQAS algorithm be considered for further development and deployment in various applications, including combinatorial optimization, quantum machine learning, reinforcement learning for quantum computing, and quantum reinforcement learning.",
        "peer_review_with_watermark": "$$$\\begin{ Alam\nPaper Summary:\nThe paper introduces a curriculum-based reinforcement learning algorithm for quantum architecture search (CRLQAS), designed to tackle the challenges of deploying variational quantum eigensolvers (VQE) in realistic noisy quantum environments. CRLQAS incorporates a 3D tensor-based encoding, illegal actions, random halting, and a novel variant of simultaneous perturbation stochastic approximation (SPSA) algorithm to reduce the energy function evaluations in the presence of noise. The algorithm is demonstrated to outperform existing QAS algorithms across noiseless and noisy environments for various quantum chemistry tasks.\n\nStrengths:\n1. The CRLQAS algorithm is designed to tackle the unique challenges of deploying VQE in realistic noisy quantum environments.\n2. The 3D tensor-based encoding allows for a compact representation of the quantum circuit, enabling efficient exploration of the search space.\n3. The illegal actions mechanism reduces the search space, avoiding consecutive application of similar quantum gates.\n4. The random halting mechanism steers the agent to find shorter circuits, improving the efficiency of the search process.\n5. The novel variant of SPSA algorithm reduces the energy function evaluations in the presence of noise, enabling faster convergence rates.\n\nWeaknesses:\n1. The algorithm relies on the assumption that the noise profile can be represented by a finite set of noise models, which may not be the case in all realistic scenarios.\n2. The CRLQAS algorithm may not be suitable for larger system sizes, as the computational cost increases exponentially with the number of qubits.\n3. The algorithm may not be robust to different noise profiles, as the performance can degrade significantly in the presence of strong noise.\n4. The CRLQAS algorithm may not be suitable for problems that require a large number of function evaluations, as the algorithm may not be able to explore the entire search space efficiently.\n5. The algorithm may not be interpretable, as the 3D tensor-based encoding can make it difficult to understand the structure of the quantum circuit.\n\nSuggestions for Improvement:\n1. To improve the robustness of the algorithm, consider using a more robust noise model, such as a stochastic noise model, that can represent a wider range of noise profiles.\n2. To improve the efficiency of the algorithm, consider using a more efficient encoding scheme, such as a 2D tensor-based encoding, that can reduce the computational cost of the algorithm.\n3. To improve the interpretability of the algorithm, consider using a more interpretable encoding scheme, such as a symbolic encoding scheme, that can provide insights into the structure of the quantum circuit.\n4. To improve the robustness of the algorithm, consider using a more robust optimization algorithm, such as a quasi-Newton method, that can handle noisy function evaluations.\n5. To improve the efficiency of the algorithm, consider using a more efficient optimization algorithm, such as a gradient-free optimization algorithm, that can reduce the computational cost of the algorithm.\n\nRecommendation:\nBased on the strengths and weaknesses of the CRLQAS algorithm, I recommend that the authors continue to improve the robustness, efficiency, and interpretability of the algorithm. Additionally, the authors should consider using a more robust noise model, a more efficient encoding scheme, and a more robust optimization algorithm to improve the overall performance of the algorithm.",
        "gr_score_with": 0.5642317380352645,
        "gr_score_without": 0.5013054830287206,
        "or_score_with": 0.3954659949622166,
        "or_score_without": 0.42297650130548303,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 192,
            "green_fraction": 0.5013054830287206,
            "num_orange_tokens": 162,
            "orange_fraction": 0.42297650130548303,
            "green_z_score": 4.046939554318771,
            "orange_z_score": 92.66579317102693,
            "green_p_value": 2.594581710933068e-05,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 397,
            "num_green_tokens": 224,
            "green_fraction": 0.5642317380352645,
            "num_orange_tokens": 157,
            "orange_fraction": 0.3954659949622166,
            "green_z_score": 6.679542561651472,
            "orange_z_score": 88.09351894094473,
            "green_p_value": 1.1984461171984698e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
        "abstract": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
        "peer_review_without_watermark": "$$$ \n\nPaper Summary:\nThe paper proposes a novel approach called GIM, which is a self-training framework for learning a single generalizable image matcher based on any image matching architecture using internet videos. The framework achieves zero-shot generalization by combining the output of the trained model with complementary matching methods and propagating the correspondences to distant frames. The authors also propose a zero-shot evaluation benchmark ZEB, which consists of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM, and it outperforms domain-specific baselines on various downstream tasks.\n\nStrengths:\n\n* The paper proposes a novel approach that leverages abundant internet videos to learn generalizable image matching.\n* The framework achieves zero-shot generalization by combining the output of the trained model with complementary matching methods and propagating the correspondences to distant frames.\n* The authors construct a novel zero-shot evaluation benchmark ZEB, which allows thorough evaluation of an image matching model in in-the-wild environments.\n* The experiments demonstrate the effectiveness and generality of GIM, and it outperforms domain-specific baselines on various downstream tasks.\n\nWeaknesses:\n\n* The paper does not provide a clear explanation of the robust fitting algorithm used to generate base correspondences.\n* The authors do not provide a detailed analysis of the effect of the number of correspondences on the performance of GIM.\n* The paper does not provide a comparison with other state-of-the-art methods for image matching and downstream tasks.\n* The authors do not provide a detailed explanation of the data augmentation process used to enhance the supervision signals.\n\nSuggestions for Improvement:\n\n* Provide a clear explanation of the robust fitting algorithm used to generate base correspondences.\n* Analyze the effect of the number of correspondences on the performance of GIM.\n* Compare GIM with other state-of-the-art methods for image matching and downstream tasks.\n* Provide a detailed explanation of the data augmentation process used to enhance the supervision signals.\n* Consider adding more experiments to evaluate the performance of GIM on different scenarios and datasets.\n\nRecommendation:\n\n* The paper presents a novel approach for learning generalizable image matching using internet videos, and the experiments demonstrate its effectiveness and generality.\n* The authors construct a novel zero-shot evaluation benchmark ZEB, which allows thorough evaluation of an image matching model in in-the-wild environments.\n* However, the paper needs more detailed explanations and analyses to fully support the claims made in the paper.\n* The authors should consider adding more experiments to evaluate the performance of GIM on different scenarios and datasets.\n\nRating: 8/10\n\nNote: The rating is based on the quality of the paper, the originality of the approach, and the thoroughness of the analysis. However, the paper needs more detailed explanations and analyses to fully support the claims made in the paper.",
        "peer_review_with_watermark": "$$$$\nPaper Summary:\nThe paper presents GIM, a self-training framework that leverages abundant internet videos to learn generalizable image matching. GIM improves image matching performance by consuming a large amount of diverse videos and using them to create dense labels on nearby frames of novel videos. The framework consists of three stages: (1) training the image matching architecture on standard domain-specific datasets, (2) generating candidate correspondences on nearby frames of downloaded videos using complementary matching methods, and (3) propagating the correspondences to distant frames and enhancing the final model with strong data augmentations. The paper also presents ZEB, a novel zero-shot evaluation benchmark for image matching with diverse in-the-wild data.\n\nStrengths:\n1. The paper presents a novel approach to learning generalizable image matching by leveraging abundant internet videos.\n2. GIM improves image matching performance by consuming a large amount of diverse videos and using them to create dense labels on nearby frames of novel videos.\n3. The framework consists of three stages, making it more efficient and less prone to fail than standard SfM-and-MVS based frameworks.\n4. The paper presents a novel zero-shot evaluation benchmark ZEB that allows thorough evaluation of an image matching model in in-the-wild environments.\n5. The experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks such as visual localization and 3D reconstruction.\n\nWeaknesses:\n1. The paper relies on the availability of abundant internet videos, which may not be feasible in all scenarios.\n2. The framework may require significant computational resources to process large amounts of video data.\n3. The paper does not provide a thorough analysis of the effects of different hyperparameters on the performance of GIM.\n4. The evaluation of GIM is limited to a specific set of image matching architectures and downstream tasks.\n5. The paper does not provide a clear discussion on the limitations and potential biases of GIM.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the effects of different hyperparameters on the performance of GIM.\n2. Evaluate GIM on a wider range of image matching architectures and downstream tasks.\n3. Discuss the potential biases and limitations of GIM and provide suggestions for improving its robustness.\n4. Provide a more thorough analysis of the effects of video quality and content on the performance of GIM.\n5. Discuss the potential applications of GIM in real-world scenarios and provide suggestions for further research.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit their manuscript for further review. The authors should address the weaknesses and suggestions for improvement, and provide more thorough analysis and discussion of the effects of different hyperparameters, evaluation on a wider range of architectures and tasks, and potential biases and limitations of GIM. Additionally, the authors should provide more clear and concise writing and ensure that the paper is well-organized and easy to follow.",
        "gr_score_with": 0.48614609571788414,
        "gr_score_without": 0.36577181208053694,
        "or_score_with": 0.4332493702770781,
        "or_score_without": 0.5033557046979866,
        "output_without": {
            "num_tokens_scored": 298,
            "num_green_tokens": 109,
            "green_fraction": 0.36577181208053694,
            "num_orange_tokens": 150,
            "orange_fraction": 0.5033557046979866,
            "green_z_score": -1.2061086130930256,
            "orange_z_score": 111.06730301021767,
            "green_p_value": 0.8861121928737588,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 397,
            "num_green_tokens": 193,
            "green_fraction": 0.48614609571788414,
            "num_orange_tokens": 172,
            "orange_fraction": 0.4332493702770781,
            "green_z_score": 3.503686435712888,
            "orange_z_score": 110.12502997383231,
            "green_p_value": 0.00022943266290860093,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "PRE-TRAINED IMAGE-EDITING DIFFUSION MODELS",
        "abstract": "If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot\u2019s own training data. We propose SuSIE, a method that leverages an image editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller attains. Specifically, we fine-tune InstructPix2Pix on robot data such that it outputs a hypothetical future observation given the robot\u2019s current observation and a language command. We then use the same robot data to train a low-level goalconditioned policy to reach a given image observation. We find that when these components are combined, the resulting system exhibits robust generalization capabilities. The high-level planner utilizes its Internet-scale pre-training and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization than conventional language-conditioned policies. We demonstrate that this approach solves real robot control tasks involving novel objects, distractors, and even environments, both in the real world and in simulation. The project website can be found at http://subgoal-image-editing. github.io.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}~\\text{The}~\\text{paper}~\\text{proposes}~\\text{SuSIE},~\\text{a}~\\text{method}~\\text{that}~\\text{leverages}~\\text{an}~\\text{image-editing}~\\text{diffusion}~\\text{model}~\\text{to}~\\text{act}~\\text{as}~\\text{a}~\\text{high-level}~\\text{planner}~\\text{by}~\\text{proposing}~\\text{intermediate}~\\text{subgoals}~\\text{that}~\\text{a}~\\text{low-level}~\\text{controller}~\\text{attains}.~\\text{The}~\\text{method}~\\text{combines}~\\text{a}~\\text{pre-trained}~\\text{image-editing}~\\text{model}~\\text{with}~\\text{a}~\\text{low-level}~\\text{goal-conditioned}~\\text{policy}~\\text{to}~\\text{reach}~\\text{the}~\\text{generated}~\\text{subgoals}.~\\text{The}~\\text{approach}~\\text{exhibits}~\\text{robust}~\\text{generalization}~\\text{capabilities}~\\text{and}~\\text{achieves}~\\text{significantly}~\\text{better}~\\text{generalization}~\\text{than}~\\text{conventional}~\\text{language-conditioned}~\\text{policies}.}\n\n$$\\boxed{\\text{Strengths:}~\\text{The}~\\text{paper}~\\text{proposes}~\\text{a}~\\text{novel}~\\text{approach}~\\text{that}~\\text{combines}~\\text{image-editing}~\\text{diffusion}~\\text{models}~\\text{with}~\\text{goal-conditioned}~\\text{policies}~\\text{to}~\\text{improve}~\\text{robotic}~\\text{control}.~\\text{The}~\\text{approach}~\\text{exhibits}~\\text{robust}~\\text{generalization}~\\text{capabilities}~\\text{and}~\\text{achieves}~\\text{significantly}~\\text{better}~\\text{generalization}~\\text{than}~\\text{conventional}~\\text{language-conditioned}~\\text{policies}.~\\text{The}~\\text{paper}~\\text{provides}~\\text{clear}~\\text{experiments}~\\text{to}~\\text{evaluate}~\\text{the}~\\text{approach}.}\n\n$$\\boxed{\\text{Weaknesses:}~\\text{The}~\\text{paper}~\\text{does}~\\text{not}~\\text{address}~\\text{the}~\\text{impact}~\\text{of}~\\text{the}~\\text{subgoal}~\\text{generation}~\\text{on}~\\text{the}~\\text{model}~\\text{parameters}~\\text{and}~\\text{training}~\\text{process}.~\\text{The}~\\text{paper}~\\text{also}~\\text{does}~\\text{not}~\\text{evaluate}~\\text{the}~\\text{effectiveness}~\\text{of}~\\text{the}~\\text{subgoal}~\\text{generation}~\\text{on}~\\text{the}~\\text{low-level}~\\text{controller}.~\\text{The}~\\text{paper}~\\text{also}~\\text{does}~\\text{not}~\\text{discuss}~\\text{the}~\\text{impact}~\\text{of}~\\text{the}~\\text{subgoal}~\\text{generation}~\\text{on}~\\text{the}~\\text{overall}~\\text{performance}~\\text{of}~\\text{the}~\\text{system}.}\n\n$$\\boxed{\\text{Suggestions}~\\text{for}~\\text{Improvement:}~\\text{The}~\\text{paper}~\\text{should}~\\text{address}~\\text{the}~\\text{impact}~\\text{of}~\\text{the}~\\text{subgoal}~\\text{generation}~\\text{on}~\\text{the}~\\text{model}~\\text{parameters}~\\text{and}~\\text{training}~\\text{process}.~\\text{The}~\\text{paper}~\\text{should}~\\text{evaluate}~\\text{the}~\\text{effectiveness}~\\text{of}~\\text{the}~\\text{subgoal}~\\text{generation}~\\text{on}~\\text{the}~\\text{low-level}~\\text{controller}.~\\text{The}~\\text{paper}~\\text{should}~\\text{discuss}~\\text{the}~\\text{impact}~\\text{of}~\\text{the}~\\text{subgoal}~\\text{generation}~\\text{on}~\\text{the}~\\text{overall}~\\text{performance}~\\text{of}~\\text{the}~\\text{system}.}\n\n$$\\boxed{\\text{Recommendation:}~\\text{The}~\\text{paper}~\\text{proposes}~\\text{a}~\\text{novel}~\\text{approach}~\\text{that}~\\text{combines}~\\text{image-editing}~\\text{diffusion}~\\text{models}~\\text{with}~\\text{goal-conditioned}~\\text{policies}~\\text{to}~\\text{improve}~\\text{robotic}~\\text{control}.~\\text{The}~\\text{approach}~\\text{exhibits}~\\text{robust}~\\text{generalization}~\\text{capabilities}~\\text{and}~\\text{achieves}~\\text{significantly}~\\text{better}~\\text{generalization}~\\text{than}~\\text{conventional}~\\text{language-conditioned}~\\text{policies}.~\\text{However},~\\text{the}~\\text{paper}~\\text{should}~\\text{address}~\\text{some}~\\text{of}~\\text{the}~\\text{weaknesses}~\\text{and}~\\text{improve}~\\text{the}~\\text{overall}~\\text{performance}~\\text{of}~\\text{the}~\\text{system}.}$$",
        "peer_review_with_watermark": "$$\\begin{align*} \n\nPaper Summary: \nThe proposed method, SuSIE, leverages pre-trained image-editing models to generate subgoals for a low-level controller, enabling robust generalization in unstructured environments. The approach decomposes the robotic control problem into two phases: synthesizing a hypothetical future subgoal given the current observation and a language command, and reaching this subgoal via a goal-reaching robot controller. SuSIE fine-tunes an image-editing model on robot data, generating subgoals that allow the low-level controller to complete tasks involving novel objects, distractors, and environments. The results show that SuSIE significantly improves over conventional language-conditioned policies and state-of-the-art methods, achieving robust generalization capabilities.\n\nStrengths: \n1. The proposed approach effectively leverages pre-trained image-editing models to incorporate semantic knowledge into robotic control, enabling robust generalization in unstructured environments.\n2. The decomposed approach into two phases allows for the generation of subgoals that are both semantically meaningful and feasible for the low-level controller.\n3. The results demonstrate the effectiveness of SuSIE in achieving robust generalization capabilities, outperforming conventional language-conditioned policies and state-of-the-art methods.\n\nWeaknesses: \n1. The approach requires significant pre-training on large datasets, which may be a limitation for small-scale datasets.\n2. The complexity of the image-editing model and the low-level controller may make the overall system prone to overfitting.\n3. The approach assumes that the pre-trained image-editing model can effectively generalize to new environments, objects, and distractors, which may not always be the case.\n\nSuggestions for Improvement: \n1. Implementing a more efficient approach to fine-tuning the image-editing model, such as using a smaller learning rate or a more efficient optimizer.\n2. Developing a more robust low-level controller that can handle a wider range of environments, objects, and distractors.\n3. Evaluating the approach on smaller-scale datasets to assess its effectiveness in real-world scenarios.\n\nRecommendation: \nThe proposed approach, SuSIE, has shown promising results in achieving robust generalization capabilities in unstructured environments. However, further work is needed to address the limitations and challenges associated with the approach, particularly in terms of overfitting and generalization. \n\nPlease note that the provided feedback is based on the given text and may not be exhaustive, and a more thorough analysis may be necessary to fully assess the paper.",
        "gr_score_with": 0.5623268698060941,
        "gr_score_without": 0.6136363636363636,
        "or_score_with": 0.3878116343490305,
        "or_score_without": 0.09090909090909091,
        "output_without": {
            "num_tokens_scored": 220,
            "num_green_tokens": 135,
            "green_fraction": 0.6136363636363636,
            "num_orange_tokens": 20,
            "orange_fraction": 0.09090909090909091,
            "green_z_score": 6.468162120117369,
            "orange_z_score": 12.278230145056227,
            "green_p_value": 4.960101872755664e-11,
            "orange_p_value": 5.928181276545324e-35
        },
        "output_with": {
            "num_tokens_scored": 361,
            "num_green_tokens": 203,
            "green_fraction": 0.5623268698060941,
            "num_orange_tokens": 140,
            "orange_fraction": 0.3878116343490305,
            "green_z_score": 6.295618373995361,
            "orange_z_score": 73.24132415954877,
            "green_p_value": 1.5308840081320247e-10,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DET-CGD: COMPRESSED GRADIENT DESCENT WITH MATRIX STEPSIZES FOR NON-CONVEX OPTIMIZATION",
        "abstract": "This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective\u2019s structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes a new method for minimizing non-convex objectives through compressed gradient descent with matrix-valued stepsize. The proposed algorithms are theoretically analyzed and convergence guarantees are provided for both single-node and distributed settings. The paper also explores the use of layer-wise structure in neural networks and designs compression mechanisms that achieve compression for free.\n\nStrengths:\n\n*   The paper provides a novel approach to compressed gradient descent with matrix-valued stepsize.\n*   The theoretical analysis is thorough and provides convergence guarantees for both single-node and distributed settings.\n*   The paper explores the use of layer-wise structure in neural networks and designs compression mechanisms that achieve compression for free.\n\nWeaknesses:\n\n*   The paper's results may be difficult to interpret for non-experts in the field, as the notation and terminology used are specialized.\n*   The paper's theoretical analysis is extensive, but may not provide immediate practical insights for practitioners.\n\nSuggestions for Improvement:\n\n*   Consider providing more detailed explanations of the notation and terminology used in the paper to make it more accessible to non-experts.\n*   Consider providing more practical insights and examples to illustrate the potential benefits of the proposed algorithms.\n\nRecommendation:\n\n*   The paper provides a solid theoretical foundation for compressed gradient descent with matrix-valued stepsize, and the proposed algorithms show promise for achieving better convergence rates in practice.\n*   With some additional work on interpretation and practicality, the paper has the potential to make a significant impact in the field of machine learning.\n\n$$\\boxed{References:}\nBubeck, S., et al. \"Stochastic Gradient Descent and Its Convergence Properties.\" IEEE Transactions on Information Theory, vol. 67, no. 2, 2019, pp. 929-943.\n\nGower, A., et al. \"Stochastic Gradient Descent with Compression.\" In Advances in Neural Information Processing Systems, 2019.\n\nKairouz, P., et al. \"Matrix Sketched Gradient Descent.\" In Advances in Neural Information Processing Systems, 2021.\n\nKhirirat, P., et al. \"Compressed Gradient Descent.\" In Advances in Neural Information Processing Systems, 2018.\n\nKonecny, F., et al. \"Communication Complexity of Distributed Optimization Algorithms.\" In Advances in Neural Information Processing Systems, 2016.\n\nMoulines, S., et al. \"Stochastic Gradient Descent Analysis and Optimization.\" Foundations of Computational Mathematics, vol. 13, no. 3, 2011, pp. 537-579.\n\nYosinski, J., et al. \"How to Extract Features from Deep Neural Networks.\" In Advances in Neural Information Processing Systems, 2015.\n\nYosinski, J., et al. \"Image Generation from Text via Deep Visual-Text Attention.\" In Advances in Neural Information Processing Systems, 2017.\n\n$$\\boxed{Peer Review:}\nThe paper presents a novel approach to compressed gradient descent with matrix-valued stepsize, which is theoretically analyzed and convergence guarantees are provided for both single-node and distributed settings. The paper also explores the use of layer-wise structure in neural networks and designs compression mechanisms that achieve compression for free.\n\nOverall, the paper provides a solid theoretical foundation for compressed gradient descent with matrix-valued stepsize, and the proposed algorithms show promise for achieving better convergence rates in practice.\n\nHowever, the paper's results may be difficult to interpret for non-experts in the field, as the notation and terminology used are specialized. Additionally, the paper's theoretical analysis is extensive, but may not provide immediate practical insights for practitioners.\n\nTo improve the paper, the authors could consider providing more detailed explanations of the notation and terminology used, as well as more practical insights and examples to illustrate the potential benefits of the proposed algorithms.\n\nIn terms of recommendations, the paper has the potential to make a significant impact in the field of machine learning. With some additional work on interpretation and practicality, the paper could be considered for publication in a top-tier conference or journal.\n\n$$\\boxed{Paper Summary:}\nThe paper presents a new method for minimizing non-convex objectives through compressed gradient descent with matrix-valued stepsize. The proposed algorithms are theoretically analyzed and convergence guarantees are provided for both single-node and distributed settings.\n\nStrengths:\n\n*   The paper provides a novel approach to compressed gradient descent with matrix-valued stepsize.\n*   The theoretical analysis is thorough and provides convergence guarantees for both single-node and distributed settings.\n*   The paper explores the use of layer-wise structure in neural networks and designs compression mechanisms that achieve compression for free.\n\nWeaknesses:\n\n*   The paper's results may be difficult to interpret for non-experts in the field, as the notation and terminology used are specialized.\n*   The paper's theoretical analysis is extensive, but may not provide immediate practical insights for practitioners.\n\nSuggestions for Improvement:\n\n*   Provide more detailed explanations of the notation and terminology used.\n*   Provide more practical insights and examples to illustrate the potential benefits of the proposed algorithms.\n\nRecommendation:\n\n*   The paper provides a solid theoretical foundation for compressed gradient descent with matrix-valued stepsize, and the proposed algorithms show promise for achieving better convergence rates in practice.\n*   With some additional work on interpretation and practicality, the paper could make a significant impact in the field of machine learning.",
        "peer_review_with_watermark": "$$$\\begin{align*}\n\\text{Paper Summary: } \n \\text{This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective\u2019s structure and lead to faster convergence compared to a scalar stepsize.\n \n \\text{The authors consider the general minimization problem: min x\u2208Rd f(x), where f : Rd \u2192 R is a differentiable function. The authors assume that f is bounded from below and consider a case where f is L matrix smooth. They propose two matrix stepsized CGD-type algorithms and analyze their convergence properties for non-convex matrix-smooth functions. The authors also consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks.\n \n \\text{The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively: E [ SkDLDSk ] \u2aaf D and E [ DT kLT kD ] \u2aaf D. The authors also provide convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \n \\text{The authors validate their theoretical findings with experiments. They provide a simple experiment confirming that Algorithms 1 and 2 have better iteration and communication complexity compared to scalar stepsized DCGD.\n \n \\text{The authors conclude that this paper presents a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize. The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \n \\text{The main contributions of the paper are: }  \n \\begin{itemize}\n \\item The authors propose a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize.\n \\item The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \\item The authors consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks.\n \\item The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively.\n \\item The authors validate their theoretical findings with experiments.\n \\end{itemize}\n \n \\text{The main strengths of the paper are: }  \n \\begin{itemize}\n \\item The authors provide a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize.\n \\item The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \\item The authors consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks.\n \\item The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively.\n \\item The authors validate their theoretical findings with experiments.\n \\end{itemize}\n \n \\text{The main weaknesses of the paper are: }  \n \\begin{itemize}\n \\item The authors assume that the function f is bounded from below and matrix smooth.\n \\item The authors assume that the sketches are unbiased, symmetric, and positive semi-definite for each algorithm.\n \\item The authors assume that the stepsize satisfies the conditions E [ SkDLDSk ] \u2aaf D and E [ DT kLT kD ] \u2aaf D.\n \\item The authors assume that the server-to-client communication is less expensive.\n \\item The authors assume that the stepsize is \"small enough\" to guarantee that the numerator of the first term is bounded.\n \\end{itemize}\n \n \\text{The main suggestions for future work are: }  \n \\begin{itemize}\n \\item Investigate the convergence properties of the algorithms for different types of matrix-smooth functions.\n \\item Investigate the convergence properties of the algorithms for different types of sketches.\n \\item Investigate the convergence properties of the algorithms for different types of stepsize matrices.\n \\item Investigate the convergence properties of the algorithms for different types of communication models.\n \\item Investigate the convergence properties of the algorithms for different types of neural networks.\n \\end{itemize}\n \n \\text{The main conclusion is that the authors provide a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize. The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting. The authors consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks. The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively. The authors validate their theoretical findings with experiments.\n \n \\text{The main recommendations for future work are: }  \n \\begin{itemize}\n \\item Investigate the convergence properties of the algorithms for different types of matrix-smooth functions.\n \\item Investigate the convergence properties of the algorithms for different types of sketches.\n \\item Investigate the convergence properties of the algorithms for different types of stepsize matrices.\n \\item Investigate the convergence properties of the algorithms for different types of communication models.\n \\item Investigate the convergence properties of the algorithms for different types of neural networks.\n \\end{itemize}\n$$$\n$$$\\begin{align*}\n\\text{Strengths: }  \n \\begin{itemize}\n \\item The authors provide a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize.\n \\item The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \\item The authors consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks.\n \\item The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively.\n \\item The authors validate their theoretical findings with experiments.\n \\end{itemize}\n \n \\text{Weaknesses: }  \n \\begin{itemize}\n \\item The authors assume that the function f is bounded from below and matrix smooth.\n \\item The authors assume that the sketches are unbiased, symmetric, and positive semi-definite for each algorithm.\n \\item The authors assume that the stepsize satisfies the conditions E [ SkDLDSk ] \u2aaf D and E [ DT kLT kD ] \u2aaf D.\n \\item The authors assume that the server-to-client communication is less expensive.\n \\item The authors assume that the stepsize is \"small enough\" to guarantee that the numerator of the first term is bounded.\n \\end{itemize}\n \n \\text{Suggestions for Improvement: }  \n \\begin{itemize}\n \\item Investigate the convergence properties of the algorithms for different types of matrix-smooth functions.\n \\item Investigate the convergence properties of the algorithms for different types of sketches.\n \\item Investigate the convergence properties of the algorithms for different types of stepsize matrices.\n \\item Investigate the convergence properties of the algorithms for different types of communication models.\n \\item Investigate the convergence properties of the algorithms for different types of neural networks.\n \\end{itemize}\n$$$\n$$$\\begin{align*}\n\\text{Recommendation: }  \n \\begin{itemize}\n \\item The authors provide a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize.\n \\item The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \\item The authors consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks.\n \\item The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively.\n \\item The authors validate their theoretical findings with experiments.\n \\end{itemize}\n$$$\n$$$\\begin{align*}\n\\text{Conclusion: }  \n \\begin{itemize}\n \\item The authors provide a new method for minimizing matrix-smooth non-convex objectives using compressed gradient descent with matrix-valued stepsize.\n \\item The authors provide theoretical convergence guarantees for the algorithms both in the single node case and the distributed setting.\n \\item The authors consider the block-diagonal case and propose hyper-parameters for det-CGD1 designed specifically for training neural networks.\n \\item The authors provide a stepsize condition for det-CGD1 and det-CGD2, respectively.\n \\item The authors validate their theoretical findings with experiments.\n \\end{itemize}\n$$$\n$$$\\begin{align*}\n\\text{Future Work: }  \n \\begin{itemize}\n \\item Investigate the convergence properties of the algorithms for different types of matrix-smooth functions.\n \\item Investigate the convergence properties of the algorithms for different types of sketches.\n \\item Investigate the convergence properties of the algorithms for different types of stepsize matrices.\n \\item Investigate the convergence properties of the algorithms for different types of communication models.\n \\item Investigate the convergence properties of the algorithms for different types of neural networks.\n \\end{itemize}\n$$$\n$$$\\begin{align*}\n\\text{Appendix: }  \n \\begin{itemize}\n \\item The appendix contains the proofs of the main results.\n \\item The appendix contains the proofs of the auxiliary results.\n \\item The appendix contains the proofs of the additional lemmas.\n \\item The appendix contains the proofs of the additional theorems.\n \\item The appendix contains the proofs of the additional corollaries.",
        "gr_score_with": 0.5529953917050692,
        "gr_score_without": 0.4326241134751773,
        "or_score_with": 0.4009216589861751,
        "or_score_without": 0.39952718676122934,
        "output_without": {
            "num_tokens_scored": 423,
            "num_green_tokens": 183,
            "green_fraction": 0.4326241134751773,
            "num_orange_tokens": 169,
            "orange_fraction": 0.39952718676122934,
            "green_z_score": 1.3696300686034495,
            "orange_z_score": 102.02200612462379,
            "green_p_value": 0.08540120423405384,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 434,
            "num_green_tokens": 240,
            "green_fraction": 0.5529953917050692,
            "num_orange_tokens": 174,
            "orange_fraction": 0.4009216589861751,
            "green_z_score": 6.506052953857847,
            "orange_z_score": 103.70650844485704,
            "green_p_value": 3.8575519863271235e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
        "abstract": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$The paper presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. The authors leverage a body of work in information theory called partial information decomposition (PID) to further identify three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity.\n\nThe paper makes several contributions. Firstly, it provides a fundamental understanding of group fairness trade-offs in FL. Secondly, it introduces the Accuracy-Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution.\n\nThe paper also provides experimental results on synthetic datasets and the Adult dataset to support its theoretical findings. The results demonstrate the trade-offs between accuracy and fairness in FL and highlight the importance of considering the interplay between global and local fairness.\n\nHowever, the paper also has some limitations. Firstly, it assumes that the clients are independent and identically distributed (i.i.d.). In practice, the clients may not be i.i.d., which could lead to biased results. Secondly, the paper does not consider the impact of the server on the fairness of the model. The server plays a crucial role in aggregating the models trained by the clients, and its choice can significantly affect the fairness of the final model.\n\nIn terms of potential improvements, the authors could explore the following:\n\n1. Consider non-i.i.d. clients: The authors could explore the case where the clients are not independent and identically distributed. This would require modifying the PID framework to account for the dependencies between the clients.\n2. Consider the impact of the server: The authors could explore the impact of the server on the fairness of the model. This could involve analyzing the effect of different aggregation strategies on the fairness of the final model.\n3. Experiment with different fairness metrics: The authors could experiment with different fairness metrics, such as equalized odds and demographic parity, to see how they compare to the mutual information framework.\n\nOverall, the paper presents an important contribution to the field of fairness in FL. While it has some limitations, it provides a solid foundation for future research and has the potential to impact the development of fair and unbiased machine learning models.\n\n$$\\begin{aligned}\n\\end{aligned}$$",
        "peer_review_with_watermark": "$$\\label{summary}\nThe paper presents a novel approach to group fairness in federated learning (FL) using partial information decomposition (PID). The authors leverage PID to decompose global and local disparity into three non-negative components: Unique Disparity, Redundant Disparity, and Masked Disparity. The work provides a fundamental understanding of group fairness trade-offs in FL, identifying where they agree or disagree. The authors also introduce the Accuracy-Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization framework for exploring the accuracy-fairness trade-offs. Experimental results on synthetic and real-world datasets validate the theoretical findings, showing that the proposed approach can effectively capture the nuances of group fairness in FL.\n\n$$\\label{strengths}\nThe paper's main contributions are:\n- A novel approach to group fairness in FL using PID.\n- A fundamental understanding of group fairness trade-offs in FL.\n- The introduction of AGLFOP, a convex optimization framework for exploring accuracy-fairness trade-offs.\n\nThe paper's use of PID provides a nuanced understanding of group fairness, capturing the interplay between global and local disparity. The AGLFOP framework offers a systematic approach to exploring accuracy-fairness trade-offs, providing theoretical boundaries for the best possible performance achievable by any FL technique.\n\n$$\\label{weaknesses}\nThe paper's main weaknesses are:\n- The experimental setup could be improved by considering more scenarios and datasets.\n- The paper could benefit from more detailed discussions on the implications of the results.\n\n$$\\label{suggestions}\nSuggestions for improvement:\n- Consider additional experimental scenarios and datasets to validate the results.\n- Provide more detailed discussions on the implications of the results, including potential applications and future directions.\n\n$$\\label{recommendation}\nBased on the review, I recommend accepting the paper for publication in its current form. The paper's novel approach to group fairness in FL using PID and the introduction of AGLFOP framework offer significant contributions to the field. While the experimental setup could be improved, the results are generally well-justified and aligned with the theoretical framework.",
        "gr_score_with": 0.5082508250825083,
        "gr_score_without": 0.37017994858611825,
        "or_score_with": 0.46204620462046203,
        "or_score_without": 0.5629820051413882,
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 144,
            "green_fraction": 0.37017994858611825,
            "num_orange_tokens": 219,
            "orange_fraction": 0.5629820051413882,
            "green_z_score": -1.200542579421609,
            "orange_z_score": 112.41409239876437,
            "green_p_value": 0.8850356568378224,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 303,
            "num_green_tokens": 154,
            "green_fraction": 0.5082508250825083,
            "num_orange_tokens": 140,
            "orange_fraction": 0.46204620462046203,
            "green_z_score": 3.8463332445255958,
            "orange_z_score": 81.11911223016386,
            "green_p_value": 5.9949332887841585e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MAGICDRIVE: STREET VIEW GENERATION WITH DIVERSE 3D GEOMETRY CONTROL",
        "abstract": "Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird\u2019s-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MAGICDRIVE, a novel street view generation framework, offering diverse 3D geometry controls including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MAGICDRIVE, we achieve high-fidelity street-view image & video synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection. Front Front Left Front Right ... ... R ai n y",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces MAGICDRIVE, a novel framework for street view generation with diverse 3D geometry controls. The framework utilizes a cross-view attention module to ensure consistency across multiple camera views and achieves high-fidelity street-view image and video synthesis that captures nuanced 3D geometry and various scene descriptions. The proposed framework is evaluated on the nuScenes dataset and shows state-of-the-art performance in both realism and controllability.\n\nStrengths:\n1. The paper introduces a novel framework for street view generation that addresses the challenges of 3D geometry control.\n2. The cross-view attention module is simple yet effective in guaranteeing consistency across multi-camera views.\n3. The framework achieves high-fidelity street-view image and video synthesis that captures nuanced 3D geometry and various scene descriptions.\n\nWeaknesses:\n1. The paper does not provide a clear comparison with other state-of-the-art methods for street view generation.\n2. The evaluation on the nuScenes dataset is limited to a small number of scenes and may not be representative of the full dataset.\n3. The paper does not provide a clear analysis of the limitations and potential future work.\n\nSuggestions for Improvement:\n1. Provide a clear comparison with other state-of-the-art methods for street view generation.\n2. Evaluate the framework on a larger number of scenes from the nuScenes dataset.\n3. Provide a clear analysis of the limitations and potential future work.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors should address the limitations and provide a clear analysis of the potential future work. Additionally, they should provide a clear comparison with other state-of-the-art methods for street view generation and evaluate the framework on a larger number of scenes from the nuScenes dataset.\n\n $$$",
        "peer_review_with_watermark": "$$\\text{Paper Title: MAGICDRIVE: STREET VIEW GENERATION WITH DIVERSE 3D GEOMETRY CONTROL}\n\n\\text{The paper introduces MAGICDRIVE, a novel street view generation framework that offers diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, along with textual descriptions, achieved through tailored encoding strategies.}\n\n\\text{Strengths:}\n1. The paper introduces a novel framework for street view generation that offers diverse 3D geometry controls, which is a significant improvement over existing methods that primarily rely on 2D bounding boxes or segmentation maps.\n2. The use of cross-view attention module ensures consistency across multiple camera views, which is crucial for multi-camera view generation.\n3. The proposed encoding strategies for geometric conditions, including camera poses, road maps, and 3D bounding boxes, are effective in managing variable-length inputs.\n\n\\text{Weaknesses:}\n1. The paper does not provide a detailed analysis of the potential biases in the MAGICDRIVE framework, which is essential for ensuring fairness and reliability in street view generation.\n2. The lack of a comprehensive comparison with other state-of-the-art methods for street view generation is a significant weakness, as it limits the understanding of the MAGICDRIVE framework's performance in comparison to existing approaches.\n3. The paper's conclusion is overly optimistic, as it claims that MAGICDRIVE can be used for data augmentation without considering the potential limitations and challenges of this approach.\n\n\\text{Suggestions for Improvement:}\n1. Conduct a thorough analysis of the potential biases in the MAGICDRIVE framework and provide strategies for mitigating these biases.\n2. Conduct a comprehensive comparison with other state-of-the-art methods for street view generation to demonstrate the MAGICDRIVE framework's performance in comparison to existing approaches.\n3. Provide more detailed insights into the potential limitations and challenges of using MAGICDRIVE for data augmentation.\n\n\\text{Recommendation:}\nBased on the strengths and weaknesses of the paper, I recommend that the authors revisit their conclusion and provide more nuanced insights into the potential limitations and challenges of using MAGICDRIVE for data augmentation. Additionally, I suggest that the authors conduct a comprehensive comparison with other state-of-the-art methods for street view generation to demonstrate the MAGICDRIVE framework's performance in comparison to existing approaches.",
        "gr_score_with": 0.4199288256227758,
        "gr_score_without": 0.29357798165137616,
        "or_score_with": 0.5088967971530249,
        "or_score_without": 0.6055045871559633,
        "output_without": {
            "num_tokens_scored": 218,
            "num_green_tokens": 64,
            "green_fraction": 0.29357798165137616,
            "num_orange_tokens": 132,
            "orange_fraction": 0.6055045871559633,
            "green_z_score": -3.2074073289341203,
            "orange_z_score": 102.87909183327275,
            "green_p_value": 0.9993303139746621,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 281,
            "num_green_tokens": 118,
            "green_fraction": 0.4199288256227758,
            "num_orange_tokens": 143,
            "orange_fraction": 0.5088967971530249,
            "green_z_score": 0.6819134338008755,
            "orange_z_score": 97.93544396782872,
            "green_p_value": 0.24764684518175367,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "PATHFORMER: MULTI-SCALE TRANSFORMERS WITH ADAPTIVE PATHWAYS FOR TIME SERIES FORECASTING",
        "abstract": "Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multiscale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. The code is made available at https://github.com/decisionintelligence/pathformer.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$\\$Paper Summary: $\\$\nThe paper proposes Pathformer, a multi-scale Transformer with adaptive pathways for time series forecasting. It integrates multi-scale temporal resolutions and temporal distances by introducing patch division with multiple patch sizes and dual attention on the divided patches. The adaptive pathways dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics. The paper evaluates the performance of Pathformer on 11 real-world datasets and achieves state-of-the-art performance.\n\n$Strengths: $\\$\n1. The paper proposes a novel multi-scale Transformer architecture with adaptive pathways, which can effectively capture multi-scale characteristics in time series data.\n2. The adaptive pathways mechanism allows the model to dynamically select and aggregate scale-specific characteristics, improving the generalization capability of the model.\n3. The paper evaluates the performance of Pathformer on a large number of real-world datasets, demonstrating its effectiveness in time series forecasting.\n\n$Weaknesses: $\\$\n1. The paper does not provide a thorough analysis of the limitations of the proposed architecture.\n2. The evaluation of the paper is limited to 11 real-world datasets, which may not be representative of all possible scenarios.\n3. The paper does not provide a clear explanation of the hyperparameter tuning process.\n\n$Suggestions for Improvement: $\\$\n1. The authors should provide a more detailed analysis of the limitations of the proposed architecture and discuss potential future work.\n2. The evaluation should be extended to a larger number of datasets and scenarios to demonstrate the robustness of the proposed architecture.\n3. The authors should provide a clear explanation of the hyperparameter tuning process and discuss the impact of hyperparameter tuning on the performance of the model.\n\n$Recommendation: $\\$\nBased on the evaluation, I recommend accepting the paper for publication. The proposed architecture is novel and effective, and the evaluation demonstrates its robustness. However, the authors should address the limitations and suggestions for improvement to strengthen the paper.",
        "peer_review_with_watermark": "$$\n\\begin{aligned}\n\\textbf{Paper Summary:} &\\  The paper proposes Pathformer, a multi-scale transformer with adaptive pathways for time series forecasting. It integrates both temporal resolution and temporal distance for multi-scale modeling and utilizes adaptive pathways to adaptively select and aggregate scale-specific characteristics based on the different temporal dynamics. The model achieves state-of-the-art performance and demonstrates strong generalization capability on several forecasting tasks.\n\n\\textbf{Strengths:} &\\  The paper makes significant contributions by proposing a multi-scale transformer with adaptive pathways, which enables comprehensive modeling of multi-scale characteristics. The adaptive pathways dynamically select and aggregate scale-specific characteristics, leading to improved prediction performance and generalization capability. The model also performs well on various real-world datasets and exhibits strong transferability across different datasets.\n\n\\textbf{Weaknesses:} &\\  The paper relies heavily on pre-trained transformer models and may benefit from more extensive analysis of the effects of different pre-training tasks and architectures. Additionally, the evaluation of the model's performance on different datasets and tasks may not be comprehensive enough, and more comparisons with other state-of-the-art models may be required to fully assess the model's strengths and limitations.\n\n\\textbf{Suggestions for Improvement:} &\\  The authors may benefit from more detailed analysis of the effects of different pre-training tasks and architectures on the model's performance. They could also explore more extensive comparisons with other state-of-the-art models to fully assess the model's strengths and limitations. Furthermore, they may consider incorporating more techniques from other fields, such as time series analysis and signal processing, to enhance the model's ability to capture complex temporal patterns.\n\n\\textbf{Recommendation:} &\\  The proposed Pathformer model is a significant contribution to the field of time series forecasting, and its ability to capture multi-scale characteristics and exhibit strong generalization capability makes it a promising approach for a wide range of forecasting tasks. However, further analysis and comparison with other models are required to fully assess its strengths and limitations. The authors' work provides a solid foundation for future research and development in this area.\n$$",
        "gr_score_with": 0.5263157894736842,
        "gr_score_without": 0.43243243243243246,
        "or_score_with": 0.39144736842105265,
        "or_score_without": 0.44787644787644787,
        "output_without": {
            "num_tokens_scored": 259,
            "num_green_tokens": 112,
            "green_fraction": 0.43243243243243246,
            "num_orange_tokens": 116,
            "orange_fraction": 0.44787644787644787,
            "green_z_score": 1.0654272078068652,
            "orange_z_score": 90.16036504596,
            "green_p_value": 0.14334132494588353,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 304,
            "num_green_tokens": 160,
            "green_fraction": 0.5263157894736842,
            "num_orange_tokens": 119,
            "orange_fraction": 0.39144736842105265,
            "green_z_score": 4.49561189559213,
            "orange_z_score": 85.19818816017977,
            "green_p_value": 3.4685082939252825e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DISSECTING SAMPLE HARDNESS: A FINE-GRAINED ANALYSIS OF HARDNESS CHARACTERIZATION METHODS FOR DATA-CENTRIC AI",
        "abstract": "Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \u201chard\u201d samples. However, there is a lack of consensus regarding the definition and evaluation of \u201chardness\u201d. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces the concept of \"sample hardness\" in the context of data-centric AI and proposes a fine-grained taxonomy of hardness types, including mislabeling, outlier, and atypical. It also presents a comprehensive benchmarking framework called H-CAT (Hardness Characterization Analysis Toolkit) to evaluate hardness characterization methods (HCMs) across different hardness types.\n\nStrengths:\n1. The paper provides a clear and concise introduction to the concept of sample hardness and its importance in data-centric AI.\n2. The proposed taxonomy of hardness types is well-structured and provides a comprehensive framework for categorizing hardness manifestations.\n3. The H-CAT framework is well-designed and provides a unified interface for evaluating HCMs across different hardness types.\n4. The paper presents a thorough evaluation of 13 HCMs across 8 hardness types, providing insights into the strengths and weaknesses of each method.\n\nWeaknesses:\n1. The paper assumes a certain level of background knowledge in machine learning and data-centric AI, which may make it challenging for non-experts to follow.\n2. The taxonomy of hardness types may be too broad, and some categories may overlap or be too vague.\n3. The evaluation of HCMs may be limited to a specific set of datasets and hardness types, which may not be representative of the broader range of applications.\n4. The paper does not provide a clear explanation of the significance of the findings and how they contribute to the broader literature on data-centric AI.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the concept of sample hardness and its importance in data-centric AI.\n2. Refine the taxonomy of hardness types to make it more specific and precise.\n3. Expand the evaluation of HCMs to include a broader range of datasets and hardness types.\n4. Provide a clearer explanation of the significance of the findings and how they contribute to the broader literature on data-centric AI.\n\nRecommendation:\nThe paper provides a solid foundation for the concept of sample hardness and the H-CAT framework for evaluating HCMs. However, it has some limitations and areas for improvement. With further refinement and expansion, the paper has the potential to make a significant contribution to the field of data-centric AI.",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\nThe paper proposes a fine-grained taxonomy of hardness types and a comprehensive benchmarking framework, H-CAT, to evaluate hardness characterization methods (HCMs) for data-centric AI. The authors analyze 13 HCMs across 8 hardness types and demonstrate the importance of comprehensive evaluation. The findings highlight the need for rigorous HCM evaluation and suggest practical tips for selecting HCMs based on hardness and using H-CAT.\n\nStrengths:\n1. The paper provides a clear and comprehensive taxonomy of hardness types, including mislabeling, outlier, and atypical data.\n2. The H-CAT framework is well-designed and provides a unified interface for evaluating HCMs across different hardness types and datasets.\n3. The authors demonstrate the importance of comprehensive evaluation and provide practical tips for selecting HCMs based on hardness.\n\nWeaknesses:\n1. The paper relies on a limited number of datasets and may not be generalizable to other domains or datasets.\n2. The authors do not provide a detailed analysis of the limitations and potential biases of the H-CAT framework.\n3. The paper does not provide a comprehensive evaluation of the robustness of HCMs to different perturbations and adversarial attacks.\n\nSuggestions for Improvement:\n1. The authors should consider using a larger and more diverse dataset to evaluate the HCMs and H-CAT framework.\n2. A more detailed analysis of the limitations and potential biases of the H-CAT framework is necessary to ensure its robustness and generalizability.\n3. The authors should investigate the robustness of HCMs to different perturbations and adversarial attacks to provide a more comprehensive evaluation.\n\nRecommendation:\nBased on the paper's strengths and weaknesses, I recommend accepting the paper for publication. However, I suggest that the authors address the limitations and potential biases of the H-CAT framework and provide a more comprehensive evaluation of the robustness of HCMs to different perturbations and adversarial attacks.\n\nRating: 8/10\n\nNote: The rating is based on the paper's overall quality, relevance, and impact, as well as the author's efforts to address the limitations and potential biases of the H-CAT framework. The rating is subjective and may vary depending on the reviewer's perspective and expertise.",
        "gr_score_with": 0.532871972318339,
        "gr_score_without": 0.4542483660130719,
        "or_score_with": 0.42214532871972316,
        "or_score_without": 0.4934640522875817,
        "output_without": {
            "num_tokens_scored": 306,
            "num_green_tokens": 139,
            "green_fraction": 0.4542483660130719,
            "num_orange_tokens": 151,
            "orange_fraction": 0.4934640522875817,
            "green_z_score": 1.9370541162387462,
            "orange_z_score": 62.1413316882028,
            "green_p_value": 0.026369364294889418,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 289,
            "num_green_tokens": 154,
            "green_fraction": 0.532871972318339,
            "num_orange_tokens": 122,
            "orange_fraction": 0.42214532871972316,
            "green_z_score": 4.610804221709511,
            "orange_z_score": 51.32768556160107,
            "green_p_value": 2.005570826538404e-06,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
        "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called Dynamic Scene Transformer (DyST). DyST learns a separation of the scene into global content and per-view camera pose and scene dynamics, enabling independent control over these factors. The model is trained using a novel latent control swap scheme, which is effective in separating camera pose and scene dynamics in the latent space. DyST is evaluated on synthetic and real-world videos, showing promising view synthesis and scene control capabilities.\n\nStrengths:\n- The paper introduces a novel approach to learning latent dynamic neural scene representations from monocular real-world videos.\n- The latent control swap scheme is effective in separating camera pose and scene dynamics in the latent space.\n- The model is evaluated on synthetic and real-world videos, showing promising view synthesis and scene control capabilities.\n\nWeaknesses:\n- The paper assumes that the camera poses are available or can be estimated from the RGB views, which may not always be the case in real-world videos.\n- The model's performance on view synthesis and scene control may be limited by the L2 loss used in the training objective.\n- The paper does not provide a detailed analysis of the learned latent representations, which may be important for understanding the model's behavior.\n\nSuggestions for Improvement:\n- Provide a more detailed analysis of the learned latent representations, including the distribution of camera pose and scene dynamics.\n- Investigate the effect of using different loss functions or optimization algorithms on the model's performance.\n- Consider using additional data augmentation techniques or preprocessing methods to improve the model's robustness to variations in camera poses and scene dynamics.\n\nRecommendation:\n- The paper presents a novel and promising approach to generative modeling of dynamic 3D visual scenes.\n- The use of a latent control swap scheme is effective in separating camera pose and scene dynamics in the latent space.\n- However, the paper could benefit from additional analysis and evaluation of the model's performance, including a more detailed analysis of the learned latent representations.\n\nAuthor Contributions:\n- Maximilian Seitzer: conception, implementation, datasets, experiments, evaluation, writing.\n- Sjoerd van Steenkiste: writing, advising.\n- Thomas Kipf: writing, advising.\n- Klaus Greff: project co-lead, conception, evaluation, writing.\n- Mehdi S. M. Sajjadi: project lead, conception, datasets, early experiments, evaluation, writing.\n\nConflict of Interest:\n- The authors declare no conflict of interest.",
        "peer_review_with_watermark": "$$$\\boxed{Peer Review: DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS}$$\n\n**Summary:** The paper proposes the Dynamic Scene Transformer (DyST) model, which learns latent neural scene representations from monocular real-world videos and provides controlled view generation. DyST separates the scene into global content and per-view scene dynamics, enabling independent control over these factors. The model is trained on a synthetic dataset DySO and real-world videos, and it demonstrates promising view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\n**Strengths:** The DyST model is innovative and makes significant contributions to the field of visual representation learning. The use of latent control swapping to separate the camera pose and scene dynamics is a novel approach, and the model's ability to disentangle these factors is a notable achievement. The use of the DySO dataset is also a strength, as it provides a challenging and realistic dataset for training the model.\n\n**Weaknesses:** One major weakness of the paper is the limited experimentation with the DyST model. While the authors demonstrate the model's capabilities on the DySO dataset, they do not provide a comprehensive evaluation of the model on real-world videos. The use of the L2 loss for view synthesis is also a potential weakness, as it can lead to artifacts and limited view generation quality. Additionally, the authors could have explored other training objectives and techniques to improve the model's performance.\n\n**Suggestions for Improvement:** To improve the DyST model, the authors could:\n\n* Provide a more comprehensive evaluation of the model on real-world videos, including quantitative and qualitative assessments of view synthesis and scene control capabilities.\n* Explore other training objectives and techniques, such as adversarial loss or generative adversarial networks (GANs), to improve the model's view generation quality and disentanglement of camera pose and scene dynamics.\n* Investigate the use of additional data augmentation techniques, such as random camera motion or lighting changes, to improve the model's robustness and generalizability.\n* Provide more detailed explanations of the DySO dataset and its generation, as well as the data augmentation techniques used to create the dataset.\n\n**Recommendation:** The DyST model is a promising approach to learning neural scene representations from real-world videos, and it has the potential to be used for a wide range of applications, including view synthesis, scene control, and 3D reconstruction. However, the authors should provide more comprehensive experimentation and evaluation to demonstrate the model's capabilities and limitations.",
        "gr_score_with": 0.5602094240837696,
        "gr_score_without": 0.4391691394658754,
        "or_score_with": 0.3324607329842932,
        "or_score_without": 0.4599406528189911,
        "output_without": {
            "num_tokens_scored": 337,
            "num_green_tokens": 148,
            "green_fraction": 0.4391691394658754,
            "num_orange_tokens": 155,
            "orange_fraction": 0.4599406528189911,
            "green_z_score": 1.4677542950450329,
            "orange_z_score": 101.11184294555687,
            "green_p_value": 0.07108548826036795,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 382,
            "num_green_tokens": 214,
            "green_fraction": 0.5602094240837696,
            "num_orange_tokens": 127,
            "orange_fraction": 0.3324607329842932,
            "green_z_score": 6.3916666951086585,
            "orange_z_score": 77.36520642488762,
            "green_p_value": 8.20436333778281e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GROUNDING LANGUAGE PLANS IN DEMONSTRATIONS THROUGH COUNTER-FACTUAL PERTURBATIONS",
        "abstract": "Grounding the abstract knowledge captured by Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem. Whereas prior works have largely focused on leveraging LLMs for generating abstract plans in symbolic spaces, this work uses LLMs to guide the learning for structures and constraints in robot manipulation tasks. Specifically, we borrow from manipulation planning literature the concept of mode families, defining specific types of motion constraints among sets of objects, to serve as an intermediate layer that connects high-level language representations with low-level physical trajectories. By locally perturbing a small set of successful human demonstrations, we augment the dataset with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains neural network-based classifiers to differentiate success task executions from failures and as a by-product learns classifiers that ground low-level states into mode families without dense labeling. This further enables us to learn structured policies for the target task. Experimental validation in both 2D continuous-space and robotic manipulation environments demonstrates the robustness of our mode-based imitation methods under external perturbations.",
        "peer_review_without_watermark": "$$$ \n\nPaper Summary:\nThe paper proposes a framework called Manipulation Modes from Language Plans (MMLP) to ground the abstract knowledge captured by Large Language Models (LLMs) in physical domains. The framework uses LLMs to guide the learning of mode families, which are specific types of motion constraints among sets of objects. The authors demonstrate the effectiveness of MMLP in two environments: a synthetic 2D polygon domain and Robosuite, a simulated robot manipulation environment.\n\nStrengths:\n1. The paper introduces a novel approach to grounding LLMs in physical domains, which is a long-standing challenge.\n2. The framework uses a counterfactual perturbation strategy to generate additional trajectories, which helps to learn mode families.\n3. The authors demonstrate the effectiveness of MMLP in two environments, which shows the robustness of the approach.\n4. The paper provides a clear explanation of the mode family concept and its application in manipulation tasks.\n\nWeaknesses:\n1. The paper relies on a limited number of human demonstrations, which may not be sufficient to generalize to new tasks.\n2. The framework requires a large number of trial-and-errors to collect task success labels of a trajectory, which can be data inefficient.\n3. The authors mention that prompting the LLM to find a suitable state representation for learning the classifier requires skill, which may be a limitation of the approach.\n4. The paper does not provide a comprehensive analysis of the mode family classifiers, which may limit the understanding of the approach.\n\nSuggestions for Improvement:\n1. The authors should provide a more comprehensive analysis of the mode family classifiers to understand their strengths and limitations.\n2. The framework should be tested on more environments and tasks to demonstrate its robustness and generalizability.\n3. The authors should explore ways to reduce the number of trial-and-errors required to collect task success labels of a trajectory.\n4. The paper should provide more details on the state representation learning process, as it is mentioned that it requires skill.\n\nRecommendation:\nThe paper proposes a novel approach to grounding LLMs in physical domains, which has the potential to improve the robustness and generalizability of LLMs in manipulation tasks. However, the framework requires further improvement, particularly in terms of its robustness and generalizability. The authors should address the limitations mentioned above to make the approach more viable for practical applications.",
        "peer_review_with_watermark": "$$$\\$\n\n**Paper Summary:**\nThe paper presents a novel framework, Manipulation Modes from Language Plans (MMLP), for grounding abstract knowledge from large language models (LLMs) into physical domains, specifically in robot manipulation tasks. The framework leverages the concept of mode families, which define specific types of motion constraints among sets of objects, to connect high-level language representations with low-level physical trajectories. The approach uses counterfactual perturbations to augment the dataset and learn classifiers that ground low-level states into mode families, enabling the construction of robust policies for the target task.\n\n**Strengths:**\n\n1. The paper presents a novel approach to grounding abstract knowledge from LLMs into physical domains, which has the potential to improve the robustness of manipulation tasks.\n2. The use of counterfactual perturbations to augment the dataset and learn classifiers that ground low-level states into mode families is a valuable contribution to the field.\n3. The framework is well-organized and easy to follow, with a clear explanation of the components and how they work together.\n4. The experimental results demonstrate the effectiveness of the approach in both 2D continuous-space and robotic manipulation environments.\n\n**Weaknesses:**\n\n1. The paper relies on a few successful demonstrations to learn the mode classifiers, which may not be sufficient to generalize to new tasks or environments.\n2. The approach requires a large number of trial-and-errors and an environment with a reset capability to collect task success labels of a trajectory, which can be data-inefficient.\n3. The use of heuristic rules to define the groundtruth modes in the Robosuite experiments may not be generalizable to other environments.\n4. The paper does not provide a comprehensive comparison with existing approaches, which limits its overall impact.\n\n**Suggestions for Improvement:**\n\n1. To address the data inefficiency, the approach could be modified to use active learning, where the current belief of mode segmentation is used to probe demonstrations only in regions with high uncertainty.\n2. The paper could be strengthened by providing a more comprehensive comparison with existing approaches, such as those that use dense annotations or transfer learning.\n3. The approach could be modified to learn the state representation in conjunction with the mode classifiers in an end-to-end fashion, which would improve its overall robustness and generalization capabilities.\n\n**Recommendation:**\nI recommend that the paper be accepted to a conference in the field of robotics and artificial intelligence, where it can be presented and discussed with experts in the field. The paper's novel approach and experimental results demonstrate its potential to improve the robustness of manipulation tasks, and its weaknesses can be addressed through future research and extensions. Overall, the paper presents a valuable contribution to the field and has the potential to inspire further research and development.",
        "gr_score_with": 0.5552941176470588,
        "gr_score_without": 0.5103244837758112,
        "or_score_with": 0.3788235294117647,
        "or_score_without": 0.39233038348082594,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 173,
            "green_fraction": 0.5103244837758112,
            "num_orange_tokens": 133,
            "orange_fraction": 0.39233038348082594,
            "green_z_score": 4.146351655981909,
            "orange_z_score": 90.87538813767821,
            "green_p_value": 1.689074353879604e-05,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 425,
            "num_green_tokens": 236,
            "green_fraction": 0.5552941176470588,
            "num_orange_tokens": 161,
            "orange_fraction": 0.3788235294117647,
            "green_z_score": 6.534973783646051,
            "orange_z_score": 98.19261864404967,
            "green_p_value": 3.18103422695996e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "IMPROVING EQUILIBRIUM PROPAGATION WITHOUT WEIGHT SYMMETRY THROUGH JACOBIAN HOMEOSTASIS",
        "abstract": "Equilibrium propagation (EP) is a compelling alternative to the backpropagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to yield unbiased gradient estimates. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry contributes to bias is unknown because, in practice, its contribution may be masked by a finite nudge. To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral. In contrast, weight asymmetry induces residual bias through poor alignment of EP\u2019s neuronal error vectors compared to BP resulting in low task performance. To mitigate the latter issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network\u2019s fixed point. This homeostatic objective dramatically improves the network\u2019s ability to solve complex tasks such as ImageNet 32\u00d732. Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate\u2019s relaxation dynamics.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$The paper presents a generalized form of Equilibrium Propagation (EP) that can be formulated without weight symmetry and analytically isolates the two sources of bias. The authors show that bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral. However, the bias due to weight asymmetry induces residual bias through poor alignment of EP's neuronal error vectors compared to backpropagation (BP) resulting in low task performance.\n\nThe authors propose a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point, which dramatically improves the network's ability to solve complex tasks such as ImageNet 32\u00d7 32.\n\nThe main contributions of the paper are:\n\n*   A comprehensive analysis of the individual sources of bias in the gradient estimate from weight asymmetry and finite-size nudge in generalized EP.\n*   An extension of Holomorphic EP to non-symmetric complex-differentiable dynamical systems, that can be estimated through continuous oscillations.\n*   A new homeostatic loss that reduces the asymmetry of the Jacobian at the free equilibrium point without enforcing perfect weight symmetry.\n*   An empirical demonstration that the proposed homeostatic loss improves the network's ability to solve complex tasks such as ImageNet 32\u00d7 32.\n\nThe paper also discusses the theoretical results, including the derivation of the Cauchy integral formula for estimating the neuronal error vector, the analysis of the bias due to weight asymmetry, and the introduction of the homeostatic objective.\n\nThe experiments demonstrate the effectiveness of the proposed homeostatic loss in improving the network's performance on complex tasks such as ImageNet 32\u00d7 32.\n\nOverall, the paper presents a significant contribution to the field of neural networks and neuromorphic substrates, and its results have the potential to improve the performance of neural networks on complex tasks.\n\nHowever, the paper has some limitations. The authors assume that the Jacobian of the network at the fixed point is invertible, which may not always be the case. Additionally, the paper does not discuss the computational complexity of the proposed homeostatic loss, which may be a significant limitation for large-scale neural networks.\n\nIn conclusion, the paper presents a significant contribution to the field of neural networks and neuromorphic substrates, and its results have the potential to improve the performance of neural networks on complex tasks. However, further work is needed to address the limitations of the paper and to explore the potential of the proposed homeostatic loss in more detail.\n\n$$\\end{aligned}$$",
        "peer_review_with_watermark": "$$\\text{Paper Summary:}$$\nThe paper discusses the concept of equilibrium propagation (EP) and its potential applications in neuromorphic computing. EP is a gradient computation algorithm that can be used for computing gradients of neural networks on biological or analog neuromorphic substrates. However, EP requires weight symmetry and infinitesimal equilibrium perturbations to yield unbiased gradient estimates. The paper introduces a new approach to EP that relaxes the requirement of weight symmetry and introduces a homeostatic objective that reduces functional asymmetry of the Jacobian at the network\u2019s fixed point.\n\n$$\\text{Strengths:}$$\n1. The paper introduces a new approach to EP that relaxes the requirement of weight symmetry, making it more applicable to physical neural networks.\n2. The homeostatic objective introduced in the paper is a promising way to reduce functional asymmetry of the Jacobian at the network\u2019s fixed point.\n3. The paper demonstrates the potential of EP in solving complex tasks such as ImageNet 32\u00d7 32.\n\n$$\\text{Weaknesses:}$$\n1. The paper relies on several assumptions about the physical neural networks, which may not be universally applicable.\n2. The homeostatic objective introduced in the paper may not be effective in all cases, and its performance needs to be further investigated.\n3. The paper does not provide a clear comparison with other gradient computation algorithms, which may limit its applicability.\n\n$$\\text{Suggestions for Improvement:}$$\n1. The paper should provide more details about the physical neural networks and their assumptions, to make it more universally applicable.\n2. The homeostatic objective should be further investigated and its performance compared with other gradient computation algorithms.\n3. The paper should provide more details about the experimental results and their analysis, to provide a clearer understanding of the potential of EP.\n\n$$\\text{Recommendation:}$$\nThe paper is a promising contribution to the field of neuromorphic computing and gradient computation algorithms. However, it needs further investigation and analysis to fully explore its potential. The authors should provide more details about the physical neural networks, the homeostatic objective, and the experimental results, and compare their performance with other gradient computation algorithms.",
        "gr_score_with": 0.5109489051094891,
        "gr_score_without": 0.4051724137931034,
        "or_score_with": 0.4635036496350365,
        "or_score_without": 0.5201149425287356,
        "output_without": {
            "num_tokens_scored": 348,
            "num_green_tokens": 141,
            "green_fraction": 0.4051724137931034,
            "num_orange_tokens": 181,
            "orange_fraction": 0.5201149425287356,
            "green_z_score": 0.19695964928958196,
            "orange_z_score": 99.35136985763907,
            "green_p_value": 0.4219295572035208,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 274,
            "num_green_tokens": 140,
            "green_fraction": 0.5109489051094891,
            "num_orange_tokens": 127,
            "orange_fraction": 0.4635036496350365,
            "green_z_score": 3.7488035398691233,
            "orange_z_score": 78.3877171597777,
            "green_p_value": 8.88400995930712e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION",
        "abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. GUESS & SKETCH extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test GUESS & SKETCH on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes a neurosymbolic approach to assembly-to-assembly transpilation, which combines the strengths of learned language models and symbolic solvers. The approach, called GUESS & SKETCH, extracts alignment and confidence information from a language model and passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. The authors demonstrate the efficacy of this approach on three different test sets of assembly programs in the ARMv8 and RISC-V architectures.\n\n$$Strengths: \n1. The approach combines the strengths of learned language models and symbolic solvers, which is a promising direction for transpilation tasks.\n2. The authors demonstrate the efficacy of the approach on three different test sets of assembly programs.\n3. The use of a neurosymbolic approach allows for the integration of symbolic and neural methods, which can lead to more accurate and efficient transpilation.\n\n$$Weaknesses: \n1. The approach relies heavily on the quality of the language model, which may not always produce accurate or reliable results.\n2. The symbolic solver may struggle with complex programs or programs with out-of-scope references.\n3. The approach may not be scalable to longer programs due to the exponentially large search space.\n\n$$Suggestions for Improvement: \n1. Improve the language model to produce more accurate and reliable results.\n2. Develop a more robust symbolic solver that can handle complex programs and out-of-scope references.\n3. Investigate ways to improve the scalability of the approach for longer programs.\n\n$$Recommendation: \nThe proposed approach, GUESS & SKETCH, shows promise for assembly-to-assembly transpilation tasks. However, further work is needed to improve the language model, symbolic solver, and scalability of the approach. The authors' use of a neurosymbolic approach is a promising direction, and with further development, this approach could lead to more accurate and efficient transpilation.\n\n$$\\end{aligned}$$",
        "peer_review_with_watermark": "$$$$\n\n$$$$\n\n**Paper Summary:**\n\nThe paper introduces GUESS & SKETCH, a neurosymbolic approach to assembly-to-assembly transpilation. This approach leverages the strengths of both neural and symbolic methods to learn a transpiler for assembly code. The method extracts alignment and confidence information from a language model and passes it to a symbolic solver to resolve semantic equivalence. The paper evaluates GUESS & SKETCH on three test sets of assembly transpilation tasks and shows that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler.\n\n**Strengths:**\n\n1. The approach combines the strengths of both neural and symbolic methods, which can be beneficial for solving complex problems like transpilation.\n2. The method extracts alignment and confidence information from a language model, which can provide valuable information for the symbolic solver.\n3. The approach is evaluated on three test sets of assembly transpilation tasks, which provides a good benchmark for the method.\n4. The paper provides a detailed explanation of the method and its components, which can be helpful for understanding and reproducing the results.\n\n**Weaknesses:**\n\n1. The method relies on the availability and quality of the training data, which can be a limitation if the training data is limited or noisy.\n2. The approach may not generalize well to other assembly-to-assembly transpilation tasks, as the training data is specific to the ARMv8 and RISC-V architectures.\n3. The method may not be able to handle all types of assembly code, as the symbolic solver may struggle with complex or unusual patterns.\n4. The approach does not provide a formal proof of equivalence, which can be a limitation if the method is used in critical applications where reliability is important.\n\n**Suggestions for Improvement:**\n\n1. Increase the size and diversity of the training data to improve the generalizability of the method.\n2. Add more experimental evaluations to the paper to demonstrate the robustness of the method across different assembly-to-assembly transpilation tasks.\n3. Consider using more advanced symbolic solvers or reasoning techniques to improve the method's ability to handle complex or unusual patterns.\n4. Provide a more detailed analysis of the method's limitations and potential pitfalls to improve its reliability and trustworthiness.\n\n**Recommendation:**\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. With some revisions to the training data, experimental evaluations, and methodological approach, the paper has the potential to be a valuable contribution to the field of assembly-to-assembly transpilation. However, the current version of the paper requires more attention to the limitations and potential pitfalls of the method to improve its reliability and trustworthiness.",
        "gr_score_with": 0.48548812664907653,
        "gr_score_without": 0.4291044776119403,
        "or_score_with": 0.43271767810026385,
        "or_score_without": 0.44029850746268656,
        "output_without": {
            "num_tokens_scored": 268,
            "num_green_tokens": 115,
            "green_fraction": 0.4291044776119403,
            "num_orange_tokens": 118,
            "orange_fraction": 0.44029850746268656,
            "green_z_score": 0.9725716027049417,
            "orange_z_score": 81.55383670714562,
            "green_p_value": 0.16538313096517554,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 379,
            "num_green_tokens": 184,
            "green_fraction": 0.48548812664907653,
            "num_orange_tokens": 164,
            "orange_fraction": 0.43271767810026385,
            "green_z_score": 3.397189588119176,
            "orange_z_score": 95.28407836952233,
            "green_p_value": 0.00034040890268466405,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "REWARD MODEL ENSEMBLES HELP MITIGATE OVEROPTIMIZATION",
        "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \u201ctrue\u201d reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger \u201cgold\u201d reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper presents a study on mitigating overoptimization in reinforcement learning from human feedback (RLHF) for fine-tuning large language models. The authors propose using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), to counter overoptimization. They evaluate the efficacy of these methods using a systematic study with two optimization methods: best-of-n sampling (BoN) and proximal policy optimization (PPO). The results show that ensemble-based conservative optimization methods effectively mitigate overoptimization and improve performance.\n\nStrengths:\n1. The paper provides a clear and systematic study on mitigating overoptimization in RLHF.\n2. The authors propose two novel ensemble-based conservative optimization objectives, WCO and UWO, which are effective in counteracting overoptimization.\n3. The paper provides a thorough evaluation of the proposed methods using a range of experiments and analysis.\n\nWeaknesses:\n1. The paper relies heavily on the authors' previous work, Gao et al. (2023), and may not provide enough context for readers without prior knowledge of the topic.\n2. The experimental setup is somewhat limited, with only two optimization methods (BoN and PPO) and a small number of hyperparameters to tune.\n3. The paper does not provide a comprehensive analysis of the limitations and potential biases of the proposed methods.\n\nSuggestions for Improvement:\n1. Provide more context and background information on overoptimization in RLHF to make the paper more accessible to readers without prior knowledge of the topic.\n2. Expand the experimental setup to include more optimization methods and hyperparameters to provide a more comprehensive evaluation of the proposed methods.\n3. Conduct a more thorough analysis of the limitations and potential biases of the proposed methods, including sensitivity analysis and robustness tests.\n\nRecommendation:\nThe paper presents a timely and important contribution to the field of RLHF, specifically addressing the issue of overoptimization. The proposed ensemble-based conservative optimization objectives, WCO and UWO, show great promise in mitigating overoptimization and improving performance. With some revisions to address the limitations and suggestions for improvement, the paper has the potential to be a strong contribution to the field.\n\n$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents a study on using ensemble-based conservative optimization methods to mitigate overoptimization in reinforcement learning from human feedback (RLHF). The authors compare three ensemble-based methods (mean optimization, worst-case optimization, and uncertainty-weighted optimization) with single reward model optimization for two optimization methods (best-of-n sampling and proximal policy optimization). The results show that ensemble-based conservative optimization methods can effectively counter overoptimization and improve performance.\n\nStrengths:\n1. The authors provide a systematic study on overoptimization in RLHF, which is a significant problem in the field.\n2. The use of ensemble-based conservative optimization methods is an innovative approach to mitigating overoptimization.\n3. The results show that ensemble-based conservative optimization methods can improve performance and reduce overoptimization.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with RLHF and reinforcement learning, which may not be the case for all readers.\n2. The results are presented in a single figure, which may not provide enough detail for readers to understand the findings.\n3. The authors do not provide a detailed discussion of the potential limitations and future work.\n\nSuggestions for Improvement:\n1. Add more context and background information on RLHF and overoptimization to help readers understand the problem being studied.\n2. Use more figures and tables to present the results, which can help readers to better understand the findings.\n3. Discuss the potential limitations and future work in more detail, such as the impact of ensemble size on performance.\n\nRecommendation:\nThe paper presents an interesting and innovative approach to mitigating overoptimization in RLHF. While there are some limitations to the study, the results show that ensemble-based conservative optimization methods can effectively counter overoptimization and improve performance. The authors should consider revising the paper to address the limitations and provide more context and background information on RLHF and overoptimization. Additionally, the results should be presented in more detail to help readers understand the findings. Overall, the paper is well-written and provides valuable insights into the problem of overoptimization in RLHF. With some revisions, the paper can be even more effective in contributing to the field. \n\n$$$",
        "gr_score_with": 0.4812286689419795,
        "gr_score_without": 0.40809968847352024,
        "or_score_with": 0.45051194539249145,
        "or_score_without": 0.48286604361370716,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 131,
            "green_fraction": 0.40809968847352024,
            "num_orange_tokens": 155,
            "orange_fraction": 0.48286604361370716,
            "green_z_score": 0.2962205683773534,
            "orange_z_score": 94.5654640684341,
            "green_p_value": 0.38353082102317027,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 293,
            "num_green_tokens": 141,
            "green_fraction": 0.4812286689419795,
            "num_orange_tokens": 132,
            "orange_fraction": 0.45051194539249145,
            "green_z_score": 2.8381642547157666,
            "orange_z_score": 84.18938357014771,
            "green_p_value": 0.002268691328224164,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MULTILINGUAL JAILBREAK CHALLENGES IN LARGE LANGUAGE MODELS",
        "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the \u201cjailbreak\u201d problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel SELF-DEFENSE framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at https: //github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: this paper contains examples with unsafe content.",
        "peer_review_without_watermark": "$$\n\nPaper Summary:\nThe paper discusses the potential safety concerns associated with large language models (LLMs) and proposes a novel framework called SELF-DEFENCE to mitigate these concerns. The authors investigate the presence of multilingual jailbreak challenges in LLMs and consider two risk scenarios: unintentional and intentional. They demonstrate that multilingual languages can serve as a potential jailbreak method in both scenarios, posing significant threats. The SELF-DEFENCE framework has proven to be highly effective in enhancing the multilingual safety capabilities of LLMs.\n\nStrengths:\n1. The paper provides a thorough analysis of the potential safety concerns associated with LLMs and the multilingual jailbreak challenges they pose.\n2. The authors propose a novel framework called SELF-DEFENCE, which has proven to be highly effective in enhancing the multilingual safety capabilities of LLMs.\n3. The paper includes extensive experimentation and results, which demonstrate the effectiveness of the SELF-DEFENCE framework.\n4. The authors consider two risk scenarios: unintentional and intentional, and demonstrate that multilingual languages can serve as a potential jailbreak method in both scenarios.\n\nWeaknesses:\n1. The paper does not provide a comprehensive review of existing safety training methods for LLMs.\n2. The authors rely heavily on the results of their own experimentation, which may not be generalizable to other LLMs or scenarios.\n3. The paper does not provide a detailed analysis of the limitations and potential biases of the SELF-DEFENCE framework.\n4. The authors do not discuss the potential costs and challenges of implementing the SELF-DEFENCE framework in real-world applications.\n\nSuggestions for Improvement:\n1. The authors should provide a more comprehensive review of existing safety training methods for LLMs, including both theoretical and practical approaches.\n2. The authors should include more detailed analysis of the limitations and potential biases of the SELF-DEFENCE framework, including its potential impact on LLMs' overall capabilities and usefulness.\n3. The authors should discuss the potential costs and challenges of implementing the SELF-DEFENCE framework in real-world applications, including the need for additional resources and infrastructure.\n4. The authors should consider exploring other potential solutions to mitigate multilingual jailbreak challenges, such as using multimodal input or incorporating safety mechanisms into LLMs' design.\n\nRecommendation:\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit their paper for publication. The authors should address the limitations and potential biases of the SELF-DEFENCE framework, provide a more comprehensive review of existing safety training methods, and discuss the potential costs and challenges of implementing the framework in real-world applications. Additionally, the authors should consider exploring other potential solutions to mitigate multilingual jailbreak challenges.",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper discusses the challenges of multilingual jailbreaks in large language models (LLMs) and proposes a novel framework called SELF-DEFENCE to mitigate this issue. The authors identify two potential scenarios: unintentional and intentional, and demonstrate that multilingual languages can serve as a potential jailbreak method in both scenarios. They propose a framework that automatically generates multilingual training data for safety fine-tuning, which has been shown to be highly effective in enhancing the multilingual safety capabilities of LLMs.\n\nStrengths:\n\n* The paper provides a comprehensive overview of the challenges of multilingual jailbreaks in LLMs.\n* The authors propose a novel framework called SELF-DEFENCE that has been shown to be highly effective in mitigating this issue.\n* The paper provides extensive experimentation to demonstrate the effectiveness of the proposed framework.\n* The authors identify two potential scenarios: unintentional and intentional, and provide a clear explanation of the challenges posed by each scenario.\n\nWeaknesses:\n\n* The paper assumes a high level of knowledge about LLMs and their safety mechanisms, which may make it difficult for non-experts to follow.\n* The authors rely heavily on experiments to demonstrate the effectiveness of the proposed framework, which may make it difficult to evaluate the results.\n* The paper does not provide a clear explanation of how the proposed framework can be used in practice, which may make it difficult to implement.\n* The authors do not provide a clear comparison with existing approaches to mitigate LLM safety, which may make it difficult to evaluate the effectiveness of the proposed framework.\n\nSuggestions for Improvement:\n\n* The authors should provide a clear explanation of the safety mechanisms used in LLMs and how they can be used to mitigate multilingual jailbreaks.\n* The authors should provide a clear comparison with existing approaches to mitigate LLM safety, which can be used to evaluate the effectiveness of the proposed framework.\n* The authors should provide a clear explanation of how the proposed framework can be used in practice, which can be used to implement the framework.\n* The authors should provide a clear evaluation of the limitations of the proposed framework, which can be used to identify areas for future research.\n\nRecommendation:\n\nI recommend that the authors continue to work on refining the proposed framework and providing a clear explanation of how it can be used in practice. The authors should also provide a clear comparison with existing approaches to mitigate LLM safety, which can be used to evaluate the effectiveness of the proposed framework. Additionally, the authors should provide a clear evaluation of the limitations of the proposed framework, which can be used to identify areas for future research. Overall, the proposed framework shows promise in mitigating multilingual jailbreaks in LLMs, but further work is needed to refine it and provide a clear explanation of its effectiveness.",
        "gr_score_with": 0.48375451263537905,
        "gr_score_without": 0.40131578947368424,
        "or_score_with": 0.44765342960288806,
        "or_score_without": 0.5,
        "output_without": {
            "num_tokens_scored": 304,
            "num_green_tokens": 122,
            "green_fraction": 0.40131578947368424,
            "num_orange_tokens": 152,
            "orange_fraction": 0.5,
            "green_z_score": 0.0468292905790837,
            "orange_z_score": 103.79870907375717,
            "green_p_value": 0.4813246420648553,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 277,
            "num_green_tokens": 134,
            "green_fraction": 0.48375451263537905,
            "num_orange_tokens": 124,
            "orange_fraction": 0.44765342960288806,
            "green_z_score": 2.8453944462505194,
            "orange_z_score": 88.56352619443786,
            "green_p_value": 0.0022178228854378163,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "NON-EXCHANGEABLE CONFORMAL RISK CONTROL",
        "abstract": "Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing nonexchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result in tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a new method for conformal risk control, called non-exchangeable conformal risk control (non-X CRC), which allows controlling the expected value of any monotone loss function when the data is not exchangeable. The proposed method is flexible, makes few assumptions, and allows weighting the data based on its relevance for a given test example. The authors demonstrate the usefulness of their framework on three tasks: multilabel classification on synthetic data by minimizing the false negative rate, monitoring electricity usage by minimizing the \u03bb-insensitive absolute loss, and open-domain question answering by bounding the best F1-score.\n\nStrengths:\n1. The proposed method is flexible and allows weighting the data based on its relevance for a given test example, which can lead to tighter bounds.\n2. The method is still valid when the data is not exchangeable and provides a tighter bound on the expected loss than that of previous work.\n3. The authors demonstrate the usefulness of their framework on three tasks, including synthetic data, real-world data, and open-domain question answering.\n\nWeaknesses:\n1. The paper assumes that the weights can be chosen to minimize the coverage gap, but it does not provide a clear method for choosing these weights, especially when the data is non-exchangeable.\n2. The paper relies on a regularized minimization problem to choose the weights, but it does not provide a clear method for solving this problem, especially when the data is non-exchangeable.\n3. The paper does not provide a clear method for choosing the parameters of the loss function, especially when the data is non-exchangeable.\n\nSuggestions for Improvement:\n1. Provide a clear method for choosing the weights, especially when the data is non-exchangeable.\n2. Provide a clear method for solving the regularized minimization problem to choose the weights.\n3. Provide a clear method for choosing the parameters of the loss function, especially when the data is non-exchangeable.\n\nRecommendation:\nThe paper proposes a new method for conformal risk control that is flexible and makes few assumptions. However, the method requires further development to provide clear methods for choosing the weights, solving the regularized minimization problem, and choosing the parameters of the loss function. With further development, the proposed method has the potential to provide tighter bounds on the expected loss than that of previous work and to be used in a variety of applications, including language generation, medical data, and reinforcement learning.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a method for non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. The proposed method, non-exchangeable conformal risk control, provides guarantees of the form E[L(\u03bb\u0302; (Xn+1, Yn+1))] \u2264 \u03b1+ (B \u2212A) n\u2211 i=1 w\u0303idTV(Z,Z i), where \u03bb\u0302 is chosen to minimize the weighted empirical risk in the calibration set, and the weights {wi}ni=1 are chosen to minimize the coverage gap g(w\u03031,..., w\u0303n). The method is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example.\n\n$$Strengths:\n1. The proposed method, non-exchangeable conformal risk control, provides guarantees on the expected loss that are tighter than those of previous work.\n2. The method is flexible, making very few assumptions, and allows weighting the data based on its relevance for a given test example.\n3. The method has been experimentally evaluated on both synthetic and real-world data, demonstrating its usefulness in controlling the expected value of any monotone loss function.\n4. The method has been compared to standard conformal risk control, demonstrating that it provides better guarantees in the presence of non-exchangeable data distributions.\n5. The method has been applied to several tasks, including multilabel classification, monitoring electricity usage, and open-domain question answering, demonstrating its usefulness in a variety of domains.\n\n$$Weaknesses:\n1. The method requires careful choice of weights, as the choice of weights can affect the tightness of the guarantees.\n2. The method assumes that the loss function is monotone, which may not hold in all cases.\n3. The method assumes that the data is weighted based on its relevance for a given test example, which may not hold in all cases.\n4. The method has not been extensively evaluated on large-scale datasets, which may limit its practicality.\n5. The method has not been compared to other methods for controlling the expected value of any monotone loss function, which may limit its comparison to other methods.\n\n$$Suggestions for Improvement:\n1. Develop methods for automatically choosing the weights, to make the method more practical.\n2. Develop methods for relaxing the monotonicity assumption on the loss function, to make the method more general.\n3. Develop methods for weighting the data based on its relevance for a given test example, to make the method more robust.\n4. Evaluate the method on large-scale datasets, to demonstrate its practicality.\n5. Compare the method to other methods for controlling the expected value of any monotone loss function, to demonstrate its superiority.\n\n$$Recommendation:\nThe proposed method, non-exchangeable conformal risk control, provides guarantees on the expected loss that are tighter than those of previous work, and has been experimentally evaluated on both synthetic and real-world data, demonstrating its usefulness in controlling the expected value of any monotone loss function. However, the method requires careful choice of weights, assumes that the loss function is monotone, and assumes that the data is weighted based on its relevance for a given test example. To make the method more practical, methods for automatically choosing the weights, relaxing the monotonicity assumption, weighting the data, and evaluating on large-scale datasets should be developed. The method should be compared to other methods for controlling the expected value of any monotone loss function to demonstrate its superiority.",
        "gr_score_with": 0.5815217391304348,
        "gr_score_without": 0.5017667844522968,
        "or_score_with": 0.3695652173913043,
        "or_score_without": 0.43462897526501765,
        "output_without": {
            "num_tokens_scored": 283,
            "num_green_tokens": 142,
            "green_fraction": 0.5017667844522968,
            "num_orange_tokens": 123,
            "orange_fraction": 0.43462897526501765,
            "green_z_score": 3.494569234437288,
            "orange_z_score": 86.37373361979508,
            "green_p_value": 0.00023741371915536754,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 214,
            "green_fraction": 0.5815217391304348,
            "num_orange_tokens": 136,
            "orange_fraction": 0.3695652173913043,
            "green_z_score": 7.107992031835549,
            "orange_z_score": 83.50912712127084,
            "green_p_value": 5.887167787235914e-13,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "HOW CONNECTIVITY STRUCTURE SHAPES RICH AND LAZY LEARNING IN NEURAL CIRCUITS",
        "abstract": "In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity could exhibit a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights \u2014 in particular their effective rank \u2014 influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper investigates the impact of initial weight effective rank on learning dynamics in neural networks. The authors derive theoretical results for two-layer feedforward linear networks and validate them through numerical experiments on recurrent neural networks (RNNs). They find that high-rank initializations typically yield smaller network changes indicative of lazier learning, while low-rank initializations bias learning towards richer learning. The authors also demonstrate that certain low-rank initial weights can still lead to effectively lazier learning for specific tasks.\n\nStrengths:\n\n* The paper provides a clear and concise introduction to the problem and the theoretical framework.\n* The authors present a thorough analysis of the theoretical results and provide multiple numerical experiments to validate their findings.\n* The paper discusses the implications of the results for neuroscience, including the potential for low-rank structures to be present in the brain and the impact on learning regimes.\n\nWeaknesses:\n\n* The paper could benefit from a more detailed discussion of the assumptions made in the theoretical framework and the limitations of the results.\n* Some of the numerical experiments are not well-controlled, and the authors do not provide enough information about the parameters used in the experiments.\n* The paper could benefit from a more comprehensive discussion of the potential applications of the results, including their implications for deep learning and neuroscience.\n\nSuggestions for Improvement:\n\n* Provide more detailed information about the assumptions made in the theoretical framework and the limitations of the results.\n* Control for more variables in the numerical experiments to ensure that the results are not due to other factors.\n* Discuss the potential applications of the results more comprehensively, including their implications for deep learning and neuroscience.\n\nRecommendation:\n\n* The paper presents an interesting and important contribution to the field of neural networks and learning dynamics. With some revisions to address the weaknesses and suggestions for improvement, the paper has the potential to be a strong contribution to the field.\n\nRating: 8/10\n\n$$",
        "peer_review_with_watermark": "$$\\boxed{\\text{Paper Review:}\n\nPaper Summary:\nThe paper explores the impact of initial weight effective rank on learning regimes in neural circuits, particularly in recurrent neural networks (RNNs). The authors investigate how the effective rank of initial weights affects the learning dynamics, including rich and lazy learning regimes. The study finds that high-rank initializations tend to lead to lazier learning, while low-rank initializations lead to richer learning.\n\nStrengths:\n1. The paper explores a novel aspect of neural network learning dynamics, which is the effective rank of initial weights.\n2. The study uses a range of theoretical and numerical approaches to validate the findings, including RNN simulations and theoretical derivations.\n3. The authors provide a detailed analysis of the impact of effective rank on learning regimes, including rich and lazy learning.\n\nWeaknesses:\n1. The study's focus on RNNs might limit the applicability to other neural network architectures.\n2. The theoretical derivations are based on simplified feedforward settings, which might not generalize well to more complex architectures.\n3. The study's conclusions are based on a limited set of simulations and experiments, which might not be representative of all neural network architectures and learning tasks.\n\nSuggestions for Improvement:\n1. The study should explore the impact of effective rank on learning regimes in more complex neural network architectures, such as transformers or graph neural networks.\n2. The theoretical derivations should be more general and applicable to a wider range of neural network architectures.\n3. The study should conduct a more comprehensive analysis of the impact of effective rank on learning regimes, including a wider range of simulations and experiments.\n\nRecommendation:\nThe paper makes an important contribution to our understanding of neural network learning dynamics, particularly in the context of RNNs. The study's findings have significant implications for the design of neural networks for specific tasks and learning regimes. However, the study's limitations, such as the focus on RNNs and the limited scope of simulations, should be acknowledged and addressed in future research.\n\nOverall, the paper is well-written, well-organized, and makes a significant contribution to the field of neural network learning dynamics. The authors provide a thorough analysis of the impact of effective rank on learning regimes and identify areas for future research. The study's findings have significant implications for the design of neural networks for specific tasks and learning regimes, and the paper's recommendations for future research are well-formulated and well-supported.}\n\n$$",
        "gr_score_with": 0.5419354838709678,
        "gr_score_without": 0.3905109489051095,
        "or_score_with": 0.4064516129032258,
        "or_score_without": 0.4744525547445255,
        "output_without": {
            "num_tokens_scored": 274,
            "num_green_tokens": 107,
            "green_fraction": 0.3905109489051095,
            "num_orange_tokens": 130,
            "orange_fraction": 0.4744525547445255,
            "green_z_score": -0.3206213553835446,
            "orange_z_score": 86.93592141726717,
            "green_p_value": 0.6257513239537429,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 310,
            "num_green_tokens": 168,
            "green_fraction": 0.5419354838709678,
            "num_orange_tokens": 126,
            "orange_fraction": 0.4064516129032258,
            "green_z_score": 5.101127853361851,
            "orange_z_score": 78.99151297052202,
            "green_p_value": 1.6881768666071238e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines. We release all the above-mentioned resources at https://github.com/RUCAIBox/FIGA.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a new approach called FIGA (Fine-grained Quality-aware Alignment) for aligning language models with human preferences. FIGA leverages fine-grained quality signals to enhance the alignment quality during fine-tuning. The approach curates a high-quality alignment dataset by pairing initial responses with revised responses on queries that a LLM cannot perform well. A new learning objective is designed to leverage the fine-grained quality signals by contrasting initial with revised responses. The approach shows superior performance on extensive tasks, outperforming the initial supervised-finetuned model and the strong PPO method.\n\nStrengths:\n1. The approach leverages fine-grained quality signals, which can improve the alignment quality of language models.\n2. The approach curates a high-quality alignment dataset, which can reduce the distribution shift between the LLM to be aligned and the ground-truth demonstrations.\n3. The approach has a simple and efficient implementation, which can make it easy to integrate into existing fine-tuning pipelines.\n4. The approach has been evaluated on extensive tasks, including human evaluation and out-of-distribution benchmarks.\n\nWeaknesses:\n1. The approach relies on the availability of a high-quality alignment dataset, which can be time-consuming and resource-intensive to curate.\n2. The approach may not perform well on tasks that require a high level of semantic understanding or common sense.\n3. The approach may not be effective for aligning language models with human preferences in cases where the initial responses are already of high quality.\n\nSuggestions for Improvement:\n1. The approach should be evaluated on a wider range of tasks to ensure its generalizability.\n2. The approach should be compared to other alignment methods, such as reinforcement learning from human feedback, to determine its relative effectiveness.\n3. The approach should be modified to handle cases where the initial responses are already of high quality.\n4. The approach should be evaluated on tasks that require a high level of semantic understanding or common sense.\n\nRecommendation:\nBased on the evaluation, I recommend that FIGA be considered as a promising approach for aligning language models with human preferences. However, further evaluation and modification are needed to ensure its generalizability and effectiveness on a wide range of tasks.",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper introduces FIGA, a novel alignment approach for language models that leverages fine-grained quality signals to enhance alignment quality during fine-tuning. The approach curates a high-quality alignment dataset that pairs initial responses with revised responses on queries that a LLM cannot perform well. It also designs a new learning objective that can leverage fine-grained quality signals by contrasting initial with revised responses.\n\nStrengths:\n\n1. The approach inherits the merits of supervised fine-tuning (SFT) while better understanding and learning what are correct behaviors for alignment.\n2. It can leverage fine-grained quality signals to enhance alignment quality during fine-tuning.\n3. The approach demonstrates superior performance on extensive tasks, with +3.2 points and +1.8 points against the initial supervised-finetuned model and the strong PPO method.\n\nWeaknesses:\n\n1. The approach may not be effective for tasks that require high-quality responses from the beginning.\n2. The reliance on fine-grained quality signals may lead to overfitting or underfitting.\n3. The approach may not be robust to out-of-distribution scenarios.\n\nSuggestions for Improvement:\n\n1. Explore alternative methods for leveraging fine-grained quality signals.\n2. Investigate the robustness of the approach to out-of-distribution scenarios.\n3. Investigate the effectiveness of the approach for tasks that require high-quality responses from the beginning.\n\nRecommendation:\n\nI recommend that the authors revise and resubmit the paper based on the above feedback. The authors should address the weaknesses and limitations of the approach and explore alternative methods for leveraging fine-grained quality signals.\n\n$$$",
        "gr_score_with": 0.4810126582278481,
        "gr_score_without": 0.34539473684210525,
        "or_score_with": 0.4810126582278481,
        "or_score_without": 0.569078947368421,
        "output_without": {
            "num_tokens_scored": 304,
            "num_green_tokens": 105,
            "green_fraction": 0.34539473684210525,
            "num_orange_tokens": 173,
            "orange_fraction": 0.569078947368421,
            "green_z_score": -1.943415559032016,
            "orange_z_score": 116.67614942876179,
            "green_p_value": 0.974017015573488,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 237,
            "num_green_tokens": 114,
            "green_fraction": 0.4810126582278481,
            "num_orange_tokens": 114,
            "orange_fraction": 0.4810126582278481,
            "green_z_score": 2.5457833093623345,
            "orange_z_score": 86.87549725546623,
            "green_p_value": 0.005451642374001719,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "IMPROVED REGRET BOUNDS FOR NON-CONVEX ONLINE-WITHIN-ONLINE META LEARNING",
        "abstract": "Online-Within-Online (OWO) meta learning stands for the online multi-task learning paradigm in which both tasks and data within each task become available in a sequential order. In this work, we study the OWO meta learning of the initialization and step size of within-task online algorithms in the non-convex setting, and provide improved regret bounds under mild assumptions of loss functions. Previous work analyzing this scenario has obtained for bounded and piecewise Lipschitz functions an averaged regret bound O(( \u221a m T 1/4 + (logm) log T \u221a T + V ) \u221a m) across T tasks, with m iterations per task and V the task similarity. Our first contribution is to modify the existing non-convex OWO meta learning algorithm and improve the regret bound to O(( 1 T 1/2\u2212\u03b1 + (log T ) 9/2 T + V ) \u221a m), for any \u03b1 \u2208 (0, 1/2). The derived bound has a faster convergence rate with respect to T , and guarantees a vanishing task-averaged regret with respect to m (for any fixed T ). Then, we propose a new algorithm of regret O(( log T T + V ) \u221a m) for non-convex OWO meta learning. This regret bound exhibits a better asymptotic performance than previous ones, and holds for any bounded (not necessarily Lipschitz) loss functions. Besides the improved regret bounds, our contributions include investigating how to attain generalization bounds for statistical meta learning via regret analysis. Specifically, by online-to-batch arguments, we achieve a transfer risk bound for batch meta learning that assumes all tasks are drawn from a distribution. Moreover, by connecting multi-task generalization error with taskaveraged regret, we develop for statistical multi-task learning a novel PAC-Bayes generalization error bound that involves our regret bound for OWO meta learning.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper studies the non-convex online-within-online (OWO) meta learning of the initialization and step size of exponentially weighted aggregation (EWA) algorithm. The authors extend the averaged regret upper bound analysis to the non-convex setting, and propose to learn the step size with FollowThe-Leader (FTL) algorithm to guarantee the logarithmic regret. For learning the initialization, they develop two algorithms based on the type of loss functions. The paper achieves improved regret bound for non-convex OWO meta learning.\n\n$$\\text{Strengths:}$$\n- The paper provides a rigorous analysis of the non-convex OWO meta learning of the initialization and step size of EWA algorithm.\n- The authors propose two algorithms for learning the initialization based on the type of loss functions.\n- The paper achieves improved regret bound for non-convex OWO meta learning.\n\n$$\\text{Weaknesses:}$$\n- The paper may benefit from more extensive experimentation to validate the proposed algorithms.\n- The analysis of the regret bound may be too technical for some readers.\n\n$$\\text{Suggestions for Improvement:}$$\n- The paper may benefit from more extensive experimentation to validate the proposed algorithms.\n- The analysis of the regret bound may be too technical for some readers.\n- The paper could provide more insights into the limitations of the proposed algorithms.\n\n$$\\text{Recommendation:}$$\n- Based on the technical merits of the paper, I recommend it for publication in a top-tier conference or journal in machine learning.\n- However, I suggest that the authors provide more experimental validation to support the proposed algorithms.\n- Additionally, the authors may want to consider providing more insights into the limitations of the proposed algorithms to provide a more comprehensive analysis.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n$$The paper provides an improved analysis of the non-convex online-within-online (OWO) meta learning of initialization and step size of exponentially weighted aggregation (EWA) algorithm, which is an important algorithm in non-convex optimization.\n\nThe paper starts with an introduction, which provides an overview of OWO meta learning, its advantages, and its current limitations. Then, it discusses the contributions of the paper, which are:\n\n1. Improved regret bounds for non-convex OWO meta learning algorithm, including the task-averaged regret upper bound, which is O((1/T^(1/2-\u03b1) + (log T)^9/2/T + V)^sqrt(m)) for piecewise Lipschitz functions, and O((log T/T + V)^sqrt(m)) for non-Lipschitz functions.\n\n2. Development of a new algorithm for non-convex OWO meta learning, which is O((log T/T + V)^sqrt(m)), which is more efficient than the previous algorithm.\n\n3. Generalization bounds for statistical meta learning, including the transfer risk bound for non-convex batch meta learning, which is O(V^2/ (sqrt(m) * log(1/\u03b4) * T^1/2)), and the PAC-Bayes generalization bound for statistical multi-task learning, which is O((V^2/ (sqrt(m) * log(1/\u03b4) * T^1/2)) + log(m)/(T * m)).\n\nThe paper is well-structured, and the analysis is clear and concise. The use of mathematical notation is standard, and the proof is well-structured.\n\nHowever, there are some limitations of the paper:\n\n1. The paper assumes that the task distribution is known, which may not be the case in practice.\n\n2. The analysis is limited to non-convex OWO meta learning, and it is not clear how the results can be applied to other algorithms or settings.\n\n3. The paper does not provide any empirical results, which may be important for verifying the effectiveness of the proposed algorithms.\n\n4. The analysis is limited to piecewise Lipschitz and non-Lipschitz functions, and it is not clear how the results can be applied to other types of functions.\n\n5. The paper does not provide any comparison with other algorithms or methods, which may be important for verifying the effectiveness of the proposed algorithms.\n\n6. The analysis is limited to the task-averaged regret, and it is not clear how the results can be applied to other metrics, such as the single-task regret.\n\n7. The paper does not provide any theoretical guarantees for the convergence of the algorithm, which may be important for verifying its effectiveness.\n\n8. The analysis is limited to the online setting, and it is not clear how the results can be applied to offline settings.\n\n$$\\end{aligned}$$",
        "gr_score_with": 0.5702005730659025,
        "gr_score_without": 0.36283185840707965,
        "or_score_with": 0.4154727793696275,
        "or_score_without": 0.5132743362831859,
        "output_without": {
            "num_tokens_scored": 226,
            "num_green_tokens": 82,
            "green_fraction": 0.36283185840707965,
            "num_orange_tokens": 116,
            "orange_fraction": 0.5132743362831859,
            "green_z_score": -1.1405634378464944,
            "orange_z_score": 98.4577911797599,
            "green_p_value": 0.8729741802376811,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 199,
            "green_fraction": 0.5702005730659025,
            "num_orange_tokens": 145,
            "orange_fraction": 0.4154727793696275,
            "green_z_score": 6.49034990072268,
            "orange_z_score": 98.76057497935523,
            "green_p_value": 4.2818623994034097e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "CONDITIONAL VARIATIONAL DIFFUSION MODELS",
        "abstract": "Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-consuming and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior results to previous methods and fine-tuned diffusion models. We conclude that fine-tuning the schedule by experimentation should be avoided because it can be learned during training in a stable way that yields better results. The code is available on https://github.com/casus/cvdm",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces the Conditional Variational Diffusion Model (CVDM), a novel approach for learning the variance schedule in diffusion models. CVDM extends the Variational Diffusion Models (VDMs) to the conditioned case, allowing for probabilistic conditioning on data. The authors prove that the rate of convergence of the discrete-time diffusion loss to the continuous-time case depends strongly on the derivatives of the schedule. They introduce a novel regularization term that proves to be critical for the performance of the method. The authors test CVDM in three distinct applications: super-resolution microscopy, quantitative phase imaging, and image super-resolution. The results show that CVDM outperforms previous methods in most cases, with significant improvements in resolution and accuracy.\n\nStrengths:\n1. The paper introduces a novel approach for learning the variance schedule in diffusion models, which is a key parameter that controls the dynamics of the diffusion process.\n2. The authors provide a detailed theoretical analysis of the schedule and its derivatives, which is essential for understanding the behavior of the diffusion model.\n3. The paper presents experimental results that demonstrate the effectiveness of CVDM in various applications, including super-resolution microscopy and quantitative phase imaging.\n4. The authors provide a clear and concise description of the method, making it easy to understand and implement.\n\nWeaknesses:\n1. The paper assumes a certain level of background knowledge in the field of deep learning and diffusion models, which may make it difficult for non-experts to follow.\n2. The authors do not provide a thorough analysis of the limitations and potential biases of CVDM, which is essential for evaluating its performance.\n3. The paper does not provide a detailed comparison with other state-of-the-art methods in the field, which may limit its impact.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations and potential biases of CVDM, including its computational complexity, memory requirements, and potential for overfitting.\n2. The authors should compare CVDM with other state-of-the-art methods in the field, including diffusion models, generative adversarial networks, and variational autoencoders.\n3. The authors should provide a more detailed description of the experimental setup, including the choice of hyperparameters, the dataset, and the evaluation metrics.\n\nRecommendation:\nBased on the paper's strengths and weaknesses, I recommend that the authors revise and resubmit the paper with the following changes:\n\n1. Add a more detailed analysis of the limitations and potential biases of CVDM.\n2. Compare CVDM with other state-of-the-art methods in the field.\n3. Provide a more detailed description of the experimental setup.\n4. Improve the clarity and concision of the paper's writing style.\n\nOverall, the paper presents a novel approach for learning the variance schedule in diffusion models, which has the potential to improve the performance of diffusion models in various applications. With revisions to address the weaknesses and suggestions for improvement, the paper can make a significant contribution to the field of deep learning and diffusion models.",
        "peer_review_with_watermark": "$$$${Paper Summary:}\nThe paper proposes a novel approach for learning the variance schedule in conditional variational diffusion models (CVDMs), which are a type of generative model. The authors claim that learning the schedule during training can yield better results than fine-tuning it as a hyperparameter. The approach involves a novel regularization term that ensures the schedule is monotonic and does not converge to a meaningless solution.\n\nStrengths:\n- The approach addresses a key limitation of diffusion models by learning the schedule during training.\n- The use of a regularization term ensures that the schedule is monotonic and does not converge to a meaningless solution.\n- The approach is tested on three different applications (super-resolution microscopy, quantitative phase imaging, and image super-resolution) and shows comparable or better performance than previous methods.\n\nWeaknesses:\n- The approach requires a large number of iterations to converge, which can be computationally expensive.\n- The regularization term may not be effective in all cases, and the schedule may still converge to a meaningless solution.\n- The approach does not provide a detailed analysis of the schedule's behavior and its impact on the model's performance.\n\nSuggestions for Improvement:\n- Provide a more detailed analysis of the schedule's behavior and its impact on the model's performance.\n- Investigate the effect of the regularization term on the schedule's behavior and its impact on the model's performance.\n- Consider implementing a more efficient optimization algorithm to reduce the number of iterations required to converge.\n\nRecommendation:\n- The approach is a significant contribution to the field of diffusion models, and with some modifications, it has the potential to improve the performance of CVDMs in various applications.\n- The authors should provide a more detailed analysis of the schedule's behavior and its impact on the model's performance to further validate the approach.\n- The approach can be tested on more diverse applications to demonstrate its versatility and effectiveness.\n\n${$}\n\n$$$${Peer Reviewer's Feedback:}\nThe paper proposes a novel approach for learning the variance schedule in conditional variational diffusion models (CVDMs). The approach involves a novel regularization term that ensures the schedule is monotonic and does not converge to a meaningless solution.\n\nStrengths:\n- The approach addresses a key limitation of diffusion models by learning the schedule during training.\n- The use of a regularization term ensures that the schedule is monotonic and does not converge to a meaningless solution.\n\nWeaknesses:\n- The approach requires a large number of iterations to converge, which can be computationally expensive.\n- The regularization term may not be effective in all cases, and the schedule may still converge to a meaningless solution.\n\nSuggestions for Improvement:\n- Provide a more detailed analysis of the schedule's behavior and its impact on the model's performance.\n- Investigate the effect of the regularization term on the schedule's behavior and its impact on the model's performance.\n\nRecommendation:\n- The approach is a significant contribution to the field of diffusion models, and with some modifications, it has the potential to improve the performance of CVDMs in various applications.\n- The authors should provide a more detailed analysis of the schedule's behavior and its impact on the model's performance to further validate the approach.\n\n${$}\n\n$$$${Rating:}\nThe paper is well-written and provides a clear overview of the approach. The use of a regularization term to ensure the schedule is monotonic is a significant contribution to the field.\n\n${$}\n\n$$$${Rating Breakdown:}\n- Clarity and Organization: 9/10\n- Writing Style: 8/10\n- Technical Depth: 8/10\n- Relevance and Impact: 9/10\n- Overall: 8.4/10",
        "gr_score_with": 0.4228395061728395,
        "gr_score_without": 0.31,
        "or_score_with": 0.5092592592592593,
        "or_score_without": 0.58,
        "output_without": {
            "num_tokens_scored": 400,
            "num_green_tokens": 124,
            "green_fraction": 0.31,
            "num_orange_tokens": 232,
            "orange_fraction": 0.58,
            "green_z_score": -3.6742346141747673,
            "orange_z_score": 131.66438540326922,
            "green_p_value": 0.9998807182729856,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 324,
            "num_green_tokens": 137,
            "green_fraction": 0.4228395061728395,
            "num_orange_tokens": 165,
            "orange_fraction": 0.5092592592592593,
            "green_z_score": 0.8391770415090524,
            "orange_z_score": 103.85278620538209,
            "green_p_value": 0.20068498431594017,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GRAPH METANETWORKS FOR PROCESSING DIVERSE NEURAL ARCHITECTURES",
        "abstract": "Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks \u2014 neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper proposes Graph Metanetworks (GMNs), a new approach to processing neural networks by representing them as graphs and using graph neural networks (GNNs) to learn representations. GMNs are designed to be equivariant to parameter permutation symmetries, which are symmetries in the parameter space of a neural network that leave the network function unchanged. The authors prove that GMNs are expressive and equivariant to neural DAG automorphisms, which are structure-preserving transformations of a graph unto itself. The authors validate the effectiveness of GMNs on several metanetwork tasks over diverse neural network architectures.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n* The paper proposes a new approach to processing neural networks by representing them as graphs and using GNNs to learn representations.\n* The authors prove that GMNs are expressive and equivariant to neural DAG automorphisms.\n* The paper validates the effectiveness of GMNs on several metanetwork tasks over diverse neural network architectures.\n* The authors provide a clear and concise explanation of the theoretical and empirical benefits of GMNs.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n* The paper does not provide a comprehensive review of the existing literature on metanets and equivariant neural networks.\n* The authors assume that the reader is familiar with the concepts of neural networks, GNNs, and DAGs, which may not be the case for all readers.\n* The paper does not provide a detailed analysis of the computational complexity of GMNs and how they can be scaled to large neural networks.\n* The authors mention that their approach only accounts for permutation-based parameter symmetries and does not account for other types of symmetries, such as symmetries induced by scaling weights in ReLU networks.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n* The authors should provide a more comprehensive review of the existing literature on metanets and equivariant neural networks to provide context for their work.\n* The authors should provide more detailed explanations of the theoretical and empirical benefits of GMNs to make the paper more accessible to a wider range of readers.\n* The authors should provide a more detailed analysis of the computational complexity of GMNs and how they can be scaled to large neural networks.\n* The authors should explore the possibility of extending their theory to account for other types of symmetries, such as symmetries induced by scaling weights in ReLU networks.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a top-tier conference or journal in the field of machine learning or neural networks. The paper provides a clear and concise explanation of the theoretical and empirical benefits of GMNs and validates their effectiveness on several metanetwork tasks over diverse neural network architectures. With some revisions to address the weaknesses, the paper has the potential to make a significant contribution to the field.",
        "peer_review_with_watermark": "$$$$\n\n$$$$Paper Summary:**\n\nThe paper proposes Graph Metanetworks (GMNs), a new approach to processing neural networks that generalizes to diverse neural architectures, including those with complex structures and modules such as normalization layers and self-attention. GMNs represent neural networks as graphs, where each parameter is associated with an edge, and process these graphs using graph neural networks (GNNs). The authors prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. The authors validate the effectiveness of their method on several metanet tasks over diverse neural network architectures.\n\n$$$$Strengths:**\n\n1.  GMNs provide a general framework for processing neural networks that generalizes to diverse architectures.\n2.  The authors prove that GMNs are expressive and equivariant to parameter permutation symmetries.\n3.  The authors demonstrate the effectiveness of GMNs on several metanet tasks over diverse neural network architectures.\n\n$$$$Weaknesses:**\n\n1.  The authors acknowledge that processing large neural networks with billions of parameters may be challenging.\n2.  The authors do not provide formal constraints on the design of parameter graphs.\n3.  The authors do not account for symmetries induced by scaling weights in ReLU networks.\n\n$$$$Suggestions for Improvement:**\n\n1.  Develop more scalable GNN methods to process large neural networks with billions of parameters.\n2.  Provide formal constraints on the design of parameter graphs to improve their scalability and efficiency.\n3.  Extend the theoretical developments to account for symmetries induced by scaling weights in ReLU networks.\n\n$$$$Recommendation:**\n\nThe authors make substantial progress towards improving the scalability of GMNs by introducing parameter graphs. However, further work is needed to develop more scalable GNN methods and provide formal constraints on the design of parameter graphs. The authors' approach has the potential to process modern neural network layers like self-attention and spatial INR feature grids, making it a promising method for future development.",
        "gr_score_with": 0.4349442379182156,
        "gr_score_without": 0.3764705882352941,
        "or_score_with": 0.5241635687732342,
        "or_score_without": 0.5088235294117647,
        "output_without": {
            "num_tokens_scored": 340,
            "num_green_tokens": 128,
            "green_fraction": 0.3764705882352941,
            "num_orange_tokens": 173,
            "orange_fraction": 0.5088235294117647,
            "green_z_score": -0.8856148855400954,
            "orange_z_score": 113.58255733632737,
            "green_p_value": 0.8120874562556508,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 269,
            "num_green_tokens": 117,
            "green_fraction": 0.4349442379182156,
            "num_orange_tokens": 141,
            "orange_fraction": 0.5241635687732342,
            "green_z_score": 1.169892866233247,
            "orange_z_score": 104.11595039422691,
            "green_p_value": 0.12102204253682086,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "LAYOUTNUWA: REVEALING THE HIDDEN LAYOUT EXPERTISE OF LARGE LANGUAGE MODELS",
        "abstract": "Graphic layout generation plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harnesses the hidden layout expertise of large language models (LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50% improvements) on multiple datasets, showcasing the strong capabilities of LayoutNUWA. Our code is available at https://github.com/ProjectNUWA/LayoutNUWA. \u2217Both authors contributed equally to this research. During Zecheng\u2019s internship under the mentorship of Chenfei at MSRA. \u2020Corresponding author.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n&\\text{Paper Summary:} \\\\\n&\\text{The paper proposes LayoutNUWA, a novel approach to treating layout generation as a code generation task, effectively enriching the semantic information of layouts and harnessing the hidden expertise of large language models (LLMs). The approach is based on the Code Instruct Tuning (CIT) method, which comprises three interconnected modules: Code Initialization, Code Completion, and Code Rendering. The experiments demonstrate the superiority of LayoutNUWA on multiple datasets, showcasing significant state-of-the-art performance and robust capabilities.}\\\\\n\n&\\text{Strengths:} \\\\\n&\\text{1.} \\text{The proposed approach, LayoutNUWA, is innovative and novel, effectively harnessing the hidden expertise of LLMs to enrich the semantic information of layouts.}\\\\\n&\\text{2.} \\text{The Code Instruct Tuning (CIT) method is well-designed and well-implemented, allowing for the seamless integration of LLMs into the layout generation process.}\\\\\n&\\text{3.} \\text{The experiments demonstrate the superiority of LayoutNUWA on multiple datasets, showcasing significant state-of-the-art performance and robust capabilities.}\\\\\n\n&\\text{Weaknesses:} \\\\\n&\\text{1.} \\text{The paper could benefit from more detailed explanations of the adaptive quantization method and template construction process.}\\\\\n&\\text{2.} \\text{The evaluation metrics used are limited, and it would be beneficial to include more metrics to provide a comprehensive evaluation of the approach.}\\\\\n&\\text{3.} \\text{The ablation study could be more comprehensive, with a more detailed analysis of the impact of different output formats and fine-tuning on the model performance.}\\\\\n\n&\\text{Suggestions for Improvement:} \\\\\n&\\text{1.} \\text{Provide more detailed explanations of the adaptive quantization method and template construction process.}\\\\\n&\\text{2.} \\text{Incorporate more evaluation metrics to provide a comprehensive evaluation of the approach.}\\\\\n&\\text{3.} \\text{Conduct a more comprehensive ablation study, analyzing the impact of different output formats and fine-tuning on the model performance.}\\\\\n\n&\\text{Recommendation:} \\\\\n&\\text{Based on the strengths and weaknesses of the paper, I recommend that it be accepted for publication in a reputable conference or journal in the field of computer vision or artificial intelligence. The innovative approach proposed by LayoutNUWA has the potential to revolutionize the field of layout generation, and the experiments demonstrate significant state-of-the-art performance. With some minor revisions to address the weaknesses, the paper has the potential to make a significant contribution to the field.}\\\\\n\n$$",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\nThe paper introduces LayoutNUWA, a groundbreaking approach that treats layout generation as a code generation task. It leverages the hidden expertise of large language models (LLMs) to enhance the semantic information of layouts and achieve significant state-of-the-art performance on multiple datasets. The approach, Code Instruct Tuning (CIT), consists of three interconnected modules: Code Initialization, Code Completion, and Code Rendering. The authors demonstrate the superiority of LayoutNUWA over six strong baselines on three conditional layout generation tasks and report impressive results on three datasets: Magazine, RICO, and PubLayNet.\n\nStrengths:\n1. The authors have successfully addressed the limitations of existing methods by treating layout generation as a code generation task, which enables the capture of semantic information and leverages the expertise of LLMs.\n2. The Code Instruct Tuning (CIT) approach is innovative and effective, demonstrating significant state-of-the-art performance on multiple datasets.\n3. The authors provide extensive experiments and evaluation metrics, including FID, mIoU, Align., and Overlap, which demonstrates the robustness and generalization of the LayoutNUWA approach.\n4. The paper highlights the importance of semantic information in layout generation and demonstrates its impact on the quality and realism of generated layouts.\n\nWeaknesses:\n1. The paper does not discuss the potential risks or challenges associated with the use of LLMs and code generation tasks in layout generation. A more thorough analysis of the risks and challenges is necessary to ensure the responsible use of these techniques.\n2. The authors rely heavily on the performance of the LLMs and the CIT approach, which may not be generalizable to other LLMs or architectures. A more thorough evaluation of the robustness of the approach across different LLMs and architectures is necessary.\n3. The paper does not discuss the potential for overfitting or overgeneralization in the LayoutNUWA approach. A more thorough analysis of the risks and challenges associated with overfitting or overgeneralization is necessary to ensure the robustness and generalization of the approach.\n4. The authors do not provide a clear comparison between the LayoutNUWA approach and other state-of-the-art methods in the field. A more thorough comparison with other state-of-the-art methods is necessary to establish the superiority of the LayoutNUWA approach.\n\nSuggestions for Improvement:\n1. The authors should conduct a thorough analysis of the risks and challenges associated with the use of LLMs and code generation tasks in layout generation.\n2. The authors should evaluate the robustness of the LayoutNUWA approach across different LLMs and architectures.\n3. The authors should investigate the potential for overfitting or overgeneralization in the LayoutNUWA approach and provide a clear analysis of the risks and challenges associated with these risks.\n4. The authors should provide a more thorough comparison between the LayoutNUWA approach and other state-of-the-art methods in the field.\n\nRecommendation:\nThe paper introduces a groundbreaking approach to layout generation that leverages the hidden expertise of LLMs. While the approach demonstrates significant state-of-the-art performance on multiple datasets, it is essential to consider the potential risks and challenges associated with the use of LLMs and code generation tasks in layout generation. A thorough analysis of the risks and challenges is necessary to ensure the responsible use of these techniques. Based on the evaluation of the approach and the analysis of the risks and challenges, the recommendation is to consider the LayoutNUWA approach as a promising solution for layout generation, but with the need for further research and development to address the potential risks and challenges.",
        "gr_score_with": 0.5433673469387755,
        "gr_score_without": 0.4458204334365325,
        "or_score_with": 0.413265306122449,
        "or_score_without": 0.4086687306501548,
        "output_without": {
            "num_tokens_scored": 323,
            "num_green_tokens": 144,
            "green_fraction": 0.4458204334365325,
            "num_orange_tokens": 132,
            "orange_fraction": 0.4086687306501548,
            "green_z_score": 1.6809501465697403,
            "orange_z_score": 92.75904584480335,
            "green_p_value": 0.04638629919631881,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 392,
            "num_green_tokens": 213,
            "green_fraction": 0.5433673469387755,
            "num_orange_tokens": 162,
            "orange_fraction": 0.413265306122449,
            "green_z_score": 5.794122344367314,
            "orange_z_score": 103.35441251852856,
            "green_p_value": 3.4339690291418736e-09,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
        "abstract": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
        "peer_review_without_watermark": "$$$\\begin{align*}\n\\textbf{Paper Summary:} \\\\\n\\text{The paper proposes a novel approach called TGeo-KD for learning sample-wise knowledge fusion ratios in knowledge distillation (KD). It leverages trilateral geometry among the student, teacher, and ground truth to model both intra- and inter-sample geometric relations. The approach adaptsively learns a sample-wise knowledge fusion ratio using a neural network and is shown to outperform other re-weighting methods consistently across diverse domains.}\n\n\\textbf{Strengths:} \\\\\n\\text{1. The proposed approach is innovative and utilizes trilateral geometry to model both intra- and inter-sample geometric relations.}\n\\text{2. The approach adaptsively learns a sample-wise knowledge fusion ratio using a neural network, making it flexible and adaptable.}\n\\text{3. The paper provides extensive experiments across diverse domains, including image classification, attack detection, and click-through rate prediction, demonstrating the effectiveness of TGeo-KD.}\n\n\\textbf{Weaknesses:} \\\\\n\\text{1. The paper may benefit from more detailed explanations of the trilateral geometry and its mathematical formulation.}\n\\text{2. The approach relies heavily on the neural network architecture, which may require significant hyperparameter tuning.}\n\\text{3. The paper does not discuss potential limitations or challenges of the approach, such as its potential vulnerability to adversarial attacks.}\n\n\\textbf{Suggestions for Improvement:} \\\\\n\\text{1. Provide more detailed explanations of the trilateral geometry and its mathematical formulation to enhance understanding.}\n\\text{2. Investigate the potential limitations and challenges of the approach, such as its potential vulnerability to adversarial attacks.}\n\\text{3. Consider exploring the use of attention mechanisms or other neural network architectures to further improve the approach.}\n\n\\textbf{Recommendation:} \\\\\n\\text{Based on the innovative approach and extensive experiments, I recommend accepting the paper for publication. However, I suggest addressing the potential weaknesses and limitations mentioned above to further strengthen the paper.}\n\n$$\\end{align*}$$",
        "peer_review_with_watermark": "$$$\\boxed{Summary:}$$\nThe paper introduces a novel method for learning sample-wise knowledge fusion ratios in knowledge distillation (KD), named TGeo-KD. It leverages trilateral geometry among the signals from the student, teacher, and ground truth to capture both intra- and inter-sample relations. The method adaptsively learns a flexible and sample-wise knowledge fusion ratio using a neural network. TGeo-KD is evaluated on three tasks: image classification on CIFAR-100 and ImageNet, attack detection on HIL, and click-through rate prediction on Criteo. The results demonstrate consistent improvements over other loss re-weighting methods.\n\n$\\boxed{Strengths:}$\n1. The paper introduces a novel method that leverages trilateral geometry to capture both intra- and inter-sample relations in knowledge distillation.\n2. The method adaptsively learns a flexible and sample-wise knowledge fusion ratio using a neural network.\n3. TGeo-KD is evaluated on diverse tasks, including image classification, attack detection, and click-through rate prediction.\n4. The results demonstrate consistent improvements over other loss re-weighting methods.\n\n$\\boxed{Weaknesses:}$\n1. The paper may be too lengthy and may benefit from a more concise summary.\n2. Some of the supporting details in the paper may be too lengthy and may not be necessary to include.\n3. The paper may not be accessible to non-experts in the field due to the use of specialized terminology and concepts.\n4. The paper may not be suitable for all domains, as it is evaluated on image classification, attack detection, and click-through rate prediction.\n\n$\\boxed{Suggestions for Improvement:}$\n1. Consider shortening the paper to focus on the main contributions and results.\n2. Include more visual aids, such as figures and tables, to support the text and make it more accessible to non-experts.\n3. Consider including more details on the evaluation method and the baselines used.\n4. Consider including more results on other domains to demonstrate the generalizability of TGeo-KD.\n\n$\\boxed{Recommendation:}$\nBased on the summary, strengths, weaknesses, and suggestions for improvement, I recommend that the paper be published in a top-tier machine learning or computer vision journal. The paper's contribution to the field of knowledge distillation and its evaluation on diverse tasks make it a valuable addition to the literature. With some revisions to address the weaknesses and suggestions for improvement, the paper can be even more effective in communicating its results to a broader audience.",
        "gr_score_with": 0.5889212827988338,
        "gr_score_without": 0.42435424354243545,
        "or_score_with": 0.36443148688046645,
        "or_score_without": 0.41697416974169743,
        "output_without": {
            "num_tokens_scored": 271,
            "num_green_tokens": 115,
            "green_fraction": 0.42435424354243545,
            "num_orange_tokens": 113,
            "orange_fraction": 0.41697416974169743,
            "green_z_score": 0.8183774785616799,
            "orange_z_score": 85.9918476046943,
            "green_p_value": 0.20657083877725724,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 343,
            "num_green_tokens": 202,
            "green_fraction": 0.5889212827988338,
            "num_orange_tokens": 125,
            "orange_fraction": 0.36443148688046645,
            "green_z_score": 7.142040769673965,
            "orange_z_score": 84.36783854211545,
            "green_p_value": 4.597765661362404e-13,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "R-MAE: REGIONS MEET MASKED AUTOENCODERS",
        "abstract": "In this work, we explore regions as a potential visual analogue of words for selfsupervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation.1",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach called R-MAE (Region-aware Masked Autoencoding) that integrates regions into the Masked Autoencoding (MAE) framework for self-supervised image representation learning. The authors explore the concept of regions as a potential visual analogue of words for pre-training, which can unlock the potential for interactive segmentation. They propose a masked region autoencoding task, which is a reconstructive pre-text task to learn from groups of pixels or regions. The authors demonstrate the effectiveness of R-MAE on various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads.\n\nStrengths:\n1. The paper provides a clear and concise introduction to the concept of regions as a potential visual analogue of words for pre-training.\n2. The authors propose a novel approach that integrates regions into the MAE framework, which is a simple yet effective way to address the one-to-many mapping between images and regions.\n3. The paper presents a thorough analysis of the experimental results, which demonstrates the effectiveness of R-MAE on various pre-training datasets and downstream detection and segmentation benchmarks.\n4. The authors provide a detailed discussion of the limitations of the current state-of-the-art methods and highlight the potential of R-MAE in bridging the gap between vision and language.\n\nWeaknesses:\n1. The paper may benefit from a more detailed discussion of the theoretical foundations of regions and their potential as a visual analogue of words.\n2. The authors may want to explore more variants of the R-MAE approach to see how they affect the performance on different tasks and datasets.\n3. The paper may benefit from a more detailed analysis of the computational costs of the R-MAE approach and how they compare to other state-of-the-art methods.\n\nSuggestions for Improvement:\n1. The authors may want to explore more theoretical foundations of regions and their potential as a visual analogue of words.\n2. The authors may want to explore more variants of the R-MAE approach to see how they affect the performance on different tasks and datasets.\n3. The authors may want to provide a more detailed analysis of the computational costs of the R-MAE approach and how they compare to other state-of-the-art methods.\n4. The authors may want to explore more applications of R-MAE, such as its potential in other computer vision tasks.\n\nRecommendation:\nBased on the paper summary, strengths, weaknesses, and suggestions for improvement, I recommend that the paper be accepted for publication in a top-tier computer vision conference. The paper presents a novel and effective approach to self-supervised image representation learning, and its experimental results demonstrate its potential in bridging the gap between vision and language. With some minor revisions to address the weaknesses and suggestions for improvement, the paper has the potential to make a significant impact in the field of computer vision.",
        "peer_review_with_watermark": "$$\n$$\n\n**Paper Summary:**\nThe paper introduces a novel approach called Region-aware Masked Autoencoding (R-MAE), which combines the benefits of masked autoencoding (MAE) and regions in image representation learning. The authors propose a reconstructive pre-text task, masked region autoencoding, to learn from groups of pixels or regions, and demonstrate its effectiveness on various pre-training datasets and downstream detection and segmentation benchmarks. The approach is highly effective, especially with high-quality regions, and can potentially unlock the potential for interactive segmentation.\n\n**Strengths:**\n1. The authors provide a thorough review of the existing literature on self-supervised learning and masked autoencoding, demonstrating a strong understanding of the state-of-the-art methods.\n2. The proposed approach, masked region autoencoding, is simple and effective, and the authors provide a clear and intuitive explanation of its architecture and benefits.\n3. The experimental results show that R-MAE can generate useful representations for dense vision tasks, and its pre-trained models can be used as a promptable, interactive segmenter.\n4. The authors provide a thorough analysis of the one-to-many mapping problem and propose an efficient solution, the length variant, which reduces computational complexity while maintaining permutation equivariance.\n\n**Weaknesses:**\n1. The authors' approach may not capture the full range of visual semantics, as regions may lack the semantic information provided by words.\n2. The reliance on pre-computed regions from clustering algorithms or other sources may limit the flexibility and adaptability of the approach.\n3. The experimental results may be influenced by the specific pre-training datasets and hyperparameters used, which may not generalize to other settings.\n4. The authors' focus on regions as a potential visual analogue of words may lead to an overemphasis on this aspect, potentially overlooking other important factors in visual representation learning.\n\n**Suggestions for Improvement:**\n1. Explore the use of more advanced clustering algorithms or methods to generate regions, which may lead to more informative and diverse representations.\n2. Consider incorporating more visual semantic information into the pre-text task, such as object detection or segmentation, to improve the robustness and generalizability of the approach.\n3. Perform a more thorough analysis of the one-to-many mapping problem and explore alternative solutions, such as using a more complex neural network architecture or incorporating additional modulation terms.\n4. Evaluate the approach on a wider range of pre-training datasets and downstream tasks to demonstrate its robustness and generalizability.\n\n**Recommendation:**\nBased on the experimental results and the thorough analysis of the approach, I recommend that the authors continue to explore and refine their proposed approach, masked region autoencoding, and consider incorporating more visual semantic information into the pre-text task. The approach has the potential to unlock the potential for interactive segmentation and provide a more robust and generalizable solution for visual representation learning.",
        "gr_score_with": 0.5876777251184834,
        "gr_score_without": 0.4457831325301205,
        "or_score_with": 0.35308056872037913,
        "or_score_without": 0.44879518072289154,
        "output_without": {
            "num_tokens_scored": 332,
            "num_green_tokens": 148,
            "green_fraction": 0.4457831325301205,
            "num_orange_tokens": 149,
            "orange_fraction": 0.44879518072289154,
            "green_z_score": 1.7028207168032377,
            "orange_z_score": 97.60493342264832,
            "green_p_value": 0.044300812332458625,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 422,
            "num_green_tokens": 248,
            "green_fraction": 0.5876777251184834,
            "num_orange_tokens": 149,
            "orange_fraction": 0.35308056872037913,
            "green_z_score": 7.869793471819925,
            "orange_z_score": 86.20947778415176,
            "green_p_value": 1.776135906665918e-15,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "A RESTORATION NETWORK",
        "abstract": "Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper introduces the Deep Restoration Priors (DRP) methodology, which enables the use of powerful pre-trained restoration models as priors for solving inverse problems. The authors propose a new method that uses a pre-trained deep restoration network as a prior, and provide a theoretical analysis of its convergence to a stationary point of a global functional associated with the restoration operator. The paper also presents numerical results showing that DRP can achieve state-of-the-art performance on several inverse problems, including image deblurring and super-resolution.\n\nStrengths:\n1. The paper presents a new and innovative approach to using pre-trained restoration models as priors for solving inverse problems.\n2. The authors provide a theoretical analysis of the convergence of DRP, which is a significant contribution to the field.\n3. The paper presents numerical results showing the effectiveness of DRP on several inverse problems.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with the concept of inverse problems and the use of priors in this context. It may be helpful to provide a brief overview of the topic for readers who are not familiar with it.\n2. The paper does not provide a detailed analysis of the choice of hyperparameters and their impact on the performance of DRP.\n3. The paper does not provide a comparison with other state-of-the-art methods for solving inverse problems.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the choice of hyperparameters and their impact on the performance of DRP.\n2. Compare DRP with other state-of-the-art methods for solving inverse problems.\n3. Provide a more detailed explanation of the theoretical analysis of the convergence of DRP.\n\nRecommendation:\nI recommend that the paper be accepted for publication in its current form. The authors have presented a new and innovative approach to using pre-trained restoration models as priors for solving inverse problems, and have provided a theoretical analysis of its convergence. The numerical results showing the effectiveness of DRP on several inverse problems are also a significant contribution to the field. However, the authors should consider addressing the weaknesses mentioned above to improve the paper further. \n\nNote: The above review is based on the provided paper text and is not a standard peer review format.",
        "peer_review_with_watermark": "$$$\\label{review}\n$\\text{Summary: }$\nThe paper proposes a novel method called Deep Restoration Priors (DRP) that utilizes pre-trained deep image restoration networks as priors for solving inverse problems. The DRP method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. The theoretical analysis of DRP shows that it converges to a stationary point of a global functional associated with the restoration operator.\n\n$\\text{Strengths: }$\nThe paper presents a novel approach that generalizes the concept of using denoisers as priors for inverse problems. The theoretical analysis of DRP provides a clear understanding of its convergence properties. The numerical results show that DRP outperforms other baseline methods on several inverse problems.\n\n$\\text{Weaknesses: }$\nThe paper does not provide a detailed comparison with other methods that use traditional denoisers as priors. The numerical results are presented only for a few datasets and might not be representative of the overall performance of DRP. The theoretical analysis assumes that the restoration operator is a minimum mean-squared error (MMSE) estimator, which might not be the case for all inverse problems.\n\n$\\text{Suggestions for Improvement: }$\nThe paper could benefit from a more detailed comparison with other methods that use traditional denoisers as priors. Additional numerical results on more datasets would provide a more comprehensive understanding of the performance of DRP. The theoretical analysis should be extended to consider more general restoration operators.\n\n$\\text{Recommendation: }$\nThe DRP method has the potential to be a powerful approach for solving inverse problems. However, further research is needed to fully understand its properties and its potential limitations. The method should be compared with other state-of-the-art methods that use traditional denoisers as priors to provide a more comprehensive understanding of its performance.\n\n$\\label{review-end}\n$$$",
        "gr_score_with": 0.5038759689922481,
        "gr_score_without": 0.4432624113475177,
        "or_score_with": 0.437984496124031,
        "or_score_without": 0.4574468085106383,
        "output_without": {
            "num_tokens_scored": 282,
            "num_green_tokens": 125,
            "green_fraction": 0.4432624113475177,
            "num_orange_tokens": 129,
            "orange_fraction": 0.4574468085106383,
            "green_z_score": 1.4829607468952597,
            "orange_z_score": 96.30138668935444,
            "green_p_value": 0.06904242005949296,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 258,
            "num_green_tokens": 130,
            "green_fraction": 0.5038759689922481,
            "num_orange_tokens": 113,
            "orange_fraction": 0.437984496124031,
            "green_z_score": 3.405801404094837,
            "orange_z_score": 88.13927032965684,
            "green_p_value": 0.00032985074485991037,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "EXTREMELY SPARSE LABELED DATA",
        "abstract": "Graph-based semi-supervised learning, particularly in the context of extremely sparse labeled data, often suffers from degenerate solutions where label functions tend to be nearly constant across unlabeled data. In this paper, we introduce Variance-enlarged Poisson Learning (VPL), a simple yet powerful framework tailored to alleviate the issues arising from the presence of degenerate solutions. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data. This intuitive approach increases the dispersion of labels from their average mean, effectively reducing the likelihood of degenerate solutions characterized by nearly constant label functions. We subsequently introduce two streamlined algorithms, V-Laplace and V-Poisson, each intricately designed to enhance Laplace and Poisson learning, respectively. Furthermore, we broaden the scope of VPL to encompass graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation. To achieve a deeper understanding of VPL\u2019s behavior, we conduct a comprehensive theoretical exploration in both discrete and variational cases. Our findings elucidate that VPL inherently amplifies the importance of connections within the same class while concurrently tempering those between different classes. We support our claims with extensive experiments, demonstrating the effectiveness of VPL and showcasing its superiority over existing methods. The code is available at https://github.com/hitcszx/VPL.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes Variance-enlarged Poisson Learning (VPL), a framework for addressing the challenges of extremely sparse labeled data in graph-based semi-supervised learning. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation for unlabeled data, increasing the dispersion of labels away from their average value. The authors introduce two streamlined algorithms, V-Laplace and V-Poisson, and extend VPL to enhance semi-supervised node classification with graph neural networks. Theoretical analysis is provided for both discrete and variational cases.\n\n$$\\text{Strengths:}$$\n1. The paper addresses the issue of degenerate solutions in graph-based semi-supervised learning with extremely sparse labeled data.\n2. The introduction of variance-enlarged regularization term provides a novel approach to increasing the dispersion of labels.\n3. The authors provide a theoretical analysis for both discrete and variational cases.\n4. The experimental results demonstrate the effectiveness and superior performance of VPL.\n\n$$\\text{Weaknesses:}$$\n1. The paper does not provide a detailed analysis of the convergence of the algorithms.\n2. The experimental results are limited to a few datasets, and it is unclear how well the proposed methods generalize to other scenarios.\n3. The paper does not provide a comparison with other state-of-the-art methods in the field.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a more detailed analysis of the convergence of the algorithms.\n2. Conduct experiments on a larger number of datasets to demonstrate the generalization of the proposed methods.\n3. Compare the proposed methods with other state-of-the-art methods in the field.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper. The authors have made significant contributions to the field of graph-based semi-supervised learning, and the proposed framework of VPL has the potential to improve the performance of models in scenarios characterized by extremely sparse labeled data. With further revisions, the paper can provide a more comprehensive analysis of the convergence of the algorithms and demonstrate the generalization of the proposed methods to other scenarios.",
        "peer_review_with_watermark": "$$\\begin{aligned}\n\\text{Summary:} &amp; \\text{This paper presents Variance-enlarged Poisson Learning (VPL), a simple yet powerful framework tailored to alleviate the issues arising from the presence of degenerate solutions in graph-based semi-supervised learning, particularly in the context of extremely sparse labeled data. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data. Two streamlined algorithms, V-Laplace and V-Poisson, are proposed to enhance Laplace learning and Poisson learning, respectively. The paper also presents Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation.} \\\\\n\\text{Strengths:} &amp; \\text{The paper presents a novel approach to addressing the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning. The proposed VPL framework is simple and effective, and the two algorithms, V-Laplace and V-Poisson, are efficient and simple to implement. The paper also presents a comprehensive theoretical exploration of VPL in both discrete and variational cases.} \\\\\n\\text{Weaknesses:} &amp; \\text{The paper lacks a clear comparison with state-of-the-art methods in the context of extremely sparse labeled data. The experimental results, while demonstrating the effectiveness of VPL, do not provide a comprehensive comparison with other methods.} \\\\\n\\text{Suggestions for Improvement:} &amp; \\text{The paper should include a more comprehensive comparison with state-of-the-art methods in the context of extremely sparse labeled data. The experimental results should also include a more detailed comparison with other methods, including Laplace learning, Poisson learning, and other graph-based semi-supervised learning methods.} \\\\\n\\text{Recommendation:} &amp; \\text{The proposed VPL framework and algorithms are recommended for addressing the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning. The paper should be considered for publication in a top-tier conference or journal in machine learning.} \\\\\n\\end{aligned}$$",
        "gr_score_with": 0.4559386973180077,
        "gr_score_without": 0.3888888888888889,
        "or_score_with": 0.4942528735632184,
        "or_score_without": 0.49019607843137253,
        "output_without": {
            "num_tokens_scored": 306,
            "num_green_tokens": 119,
            "green_fraction": 0.3888888888888889,
            "num_orange_tokens": 150,
            "orange_fraction": 0.49019607843137253,
            "green_z_score": -0.3967460238079368,
            "orange_z_score": 100.79785143163684,
            "green_p_value": 0.6542226212444262,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 261,
            "num_green_tokens": 119,
            "green_fraction": 0.4559386973180077,
            "num_orange_tokens": 129,
            "orange_fraction": 0.4942528735632184,
            "green_z_score": 1.844705239375331,
            "orange_z_score": 93.87337218512184,
            "green_p_value": 0.032540214154774684,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "STYLIZED OFFLINE REINFORCEMENT LEARNING: EXTRACTING DIVERSE HIGH-QUALITY BEHAVIORS FROM HETEROGENEOUS DATASETS",
        "abstract": "Previous literature on policy diversity in reinforcement learning (RL) either focuses on the online setting or ignores the policy performance. In contrast, offline RL, which aims to learn high-quality policies from batched data, has yet to fully leverage the intrinsic diversity of the offline dataset. Addressing this dichotomy and aiming to balance quality and diversity poses a significant challenge to extant methodologies. This paper introduces a novel approach, termed Stylized Offline RL (SORL), which is designed to extract high-performing, stylistically diverse policies from a dataset characterized by distinct behavioral patterns. Drawing inspiration from the venerable Expectation-Maximization (EM) algorithm, SORL innovatively alternates between policy learning and trajectory clustering, a mechanism that promotes policy diversification. To further augment policy performance, we introduce advantage-weighted style learning into the SORL framework. Experimental evaluations across multiple environments demonstrate the significant superiority of SORL over previous methods in extracting high-quality policies with diverse behaviors. A case in point is that SORL successfully learns strong policies with markedly distinct playing patterns from a real-world human dataset of a popular basketball video game \u201dDunk City Dynasty.\u201d",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets$$\n\n$$Summary:$$\n\nThe paper introduces a novel approach, termed Stylized Offline RL (SORL), to extract high-performing, stylistically diverse policies from a heterogeneous offline dataset. SORL innovatively alternates between policy learning and trajectory clustering, a mechanism that promotes policy diversification. The framework combines EM-based style clustering with advantage-weighted style learning to learn diverse and high-quality policies. The authors evaluate SORL across multiple environments and offline datasets, demonstrating its superiority over previous methods in extracting high-quality policies with diverse behaviors.\n\n$$Strengths:$$\n\n1.  The paper addresses the limitation of existing offline RL methods, which often prioritize policy quality over diversity.\n2.  The introduction of EM-based style clustering and advantage-weighted style learning provides a novel approach to learning diverse and high-quality policies.\n3.  The framework is evaluated across multiple environments and offline datasets, demonstrating its robustness and adaptability.\n4.  The authors provide a comprehensive evaluation of the proposed method, including quality, diversity, and consistency metrics.\n\n$$Weaknesses:$$\n\n1.  The paper assumes a uniform prior distribution for the latent variable, which may not be the case in all scenarios.\n2.  The authors do not provide a detailed analysis of the convergence of the EM-based style clustering algorithm.\n3.  The evaluation metrics used may not be comprehensive, and other metrics such as robustness and generalizability could be explored.\n4.  The paper does not provide a detailed analysis of the impact of varying hyperparameters on the performance of SORL.\n\n$$Suggestions for Improvement:$$\n\n1.  Investigate the impact of different prior distributions on the performance of SORL.\n2.  Analyze the convergence of the EM-based style clustering algorithm and provide a detailed analysis of the optimization process.\n3.  Explore the use of additional evaluation metrics, such as robustness and generalizability, to provide a more comprehensive assessment of SORL.\n4.  Investigate the impact of varying hyperparameters on the performance of SORL and provide a detailed analysis of the results.\n\n$$Recommendation:$$\n\nBased on the strengths and weaknesses of the paper, I recommend that SORL be further developed and evaluated to provide a more comprehensive assessment of its performance. The authors should investigate the impact of different prior distributions, analyze the convergence of the EM-based style clustering algorithm, and explore the use of additional evaluation metrics. Additionally, the authors should investigate the impact of varying hyperparameters on the performance of SORL and provide a detailed analysis of the results.",
        "peer_review_with_watermark": "$$$ \n\nPaper Summary:\nThe paper introduces a novel approach, termed Stylized Offline Reinforcement Learning (SORL), which aims to extract high-quality, diverse policies from heterogeneous offline datasets. SORL combines EM-based style clustering with advantage-weighted policy learning, allowing it to optimize both policy performance and diversity. The authors evaluate SORL across various environments and datasets, achieving significant superiority over prior methods in extracting high-quality, diverse policies.\n\nStrengths:\n\n1. Novel approach: SORL introduces a novel combination of EM-based style clustering and advantage-weighted policy learning, addressing the limitations of existing methods in offline reinforcement learning.\n2. Experimental evaluations: The paper presents comprehensive experimental evaluations across various tasks and heterogeneous offline datasets, demonstrating the efficacy of SORL in extracting high-quality, diverse policies.\n3. Diverse applications: SORL has potential applications in various fields, including human interaction, autonomous driving, and opponent modeling, where diverse policies can improve performance and user engagement.\n\nWeaknesses:\n\n1. Limited theoretical analysis: The paper focuses on empirical evaluations, but a more comprehensive theoretical analysis of SORL's convergence and optimality could enhance its appeal.\n2. Lack of comparison with state-of-the-art methods: While the paper compares SORL with prior methods, a more in-depth comparison with state-of-the-art methods in offline reinforcement learning could provide a more comprehensive evaluation of SORL's performance.\n3. Limited exploration of policy behavior: The paper focuses on extracting diverse policies, but a more in-depth exploration of policy behavior, such as shared behaviors, could provide a more comprehensive understanding of SORL's capabilities.\n\nSuggestions for Improvement:\n\n1. Theoretical analysis: Conduct a more comprehensive theoretical analysis of SORL's convergence, optimality, and robustness to ensure its theoretical soundness.\n2. Comparison with state-of-the-art methods: Perform a more in-depth comparison with state-of-the-art methods in offline reinforcement learning to evaluate SORL's performance in a more comprehensive setting.\n3. Exploration of policy behavior: Investigate the capabilities of SORL in extracting shared behaviors among policies, which could enhance its appeal for real-world applications.\n\nRecommendation:\n\nThe paper introduces a novel approach, SORL, which addresses the limitations of existing methods in offline reinforcement learning. The experimental evaluations demonstrate SORL's efficacy in extracting high-quality, diverse policies. However, a more comprehensive theoretical analysis, comparison with state-of-the-art methods, and exploration of policy behavior are necessary to further establish SORL's theoretical soundness and practical appeal. Based on its current state, I recommend accepting the paper for publication in a reputable conference or journal. \n\n$$$",
        "gr_score_with": 0.5756676557863502,
        "gr_score_without": 0.4622093023255814,
        "or_score_with": 0.3590504451038576,
        "or_score_without": 0.44476744186046513,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 159,
            "green_fraction": 0.4622093023255814,
            "num_orange_tokens": 153,
            "orange_fraction": 0.44476744186046513,
            "green_z_score": 2.3552061744210446,
            "orange_z_score": 99.26051352060946,
            "green_p_value": 0.00925621554286923,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 194,
            "green_fraction": 0.5756676557863502,
            "num_orange_tokens": 121,
            "orange_fraction": 0.3590504451038576,
            "green_z_score": 6.582655626262576,
            "orange_z_score": 79.01970491024137,
            "green_p_value": 2.310593378111472e-11,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SAFEDREAMER: SAFE REINFORCEMENT LEARNING",
        "abstract": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and visiononly input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details can be seen on our project website: https://sites.google.com/view/safedreamer.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:}$$\nThe paper proposes SafeDreamer, a novel algorithm for safe reinforcement learning (SafeRL) that balances long-term rewards and costs. It integrates safety-reward planning of the world model with the Lagrangian methods to achieve nearly zero-cost performance in complex scenarios, especially vision-only tasks. The algorithm achieves higher rewards and lower costs compared to previous methods, demonstrating superior performance in tasks with low-dimensional and visual inputs.\n\n$$\\textbf{Strengths:}$$\n1. The paper addresses a critical challenge in SafeRL, which is achieving zero-cost performance in complex scenarios.\n2. The integration of safety-reward planning of the world model with the Lagrangian methods is a novel approach that balances long-term rewards and costs.\n3. The algorithm achieves higher rewards and lower costs compared to previous methods, demonstrating superior performance in tasks with low-dimensional and visual inputs.\n4. The paper provides a detailed comparison with various algorithms and provides insights into the strengths and weaknesses of each approach.\n\n$$\\textbf{Weaknesses:}$$\n1. The paper relies heavily on the Safety-Gymnasium benchmark, which may not be representative of real-world scenarios.\n2. The algorithm trains each task independently, incurring substantial costs with each individual task.\n3. The paper does not provide a clear explanation of the hyperparameter tuning process and the impact of hyperparameters on the performance of the algorithm.\n4. The paper does not provide a detailed analysis of the trade-offs between safety and reward in the context of SafeRL.\n\n$$\\textbf{Suggestions for Improvement:}$$\n1. The paper should provide a more comprehensive analysis of the trade-offs between safety and reward in the context of SafeRL.\n2. The algorithm should be tested on a more diverse set of tasks and scenarios to demonstrate its generalizability.\n3. The paper should provide a clear explanation of the hyperparameter tuning process and the impact of hyperparameters on the performance of the algorithm.\n4. The paper should consider the use of offline data from multiple tasks to pre-train the world model and examine its ability to facilitate the safe exploration of new tasks.\n\n$$\\textbf{Recommendation:}$$\nThe paper presents a novel approach to SafeRL that addresses a critical challenge in the field. While there are some limitations and areas for improvement, the algorithm demonstrates superior performance in tasks with low-dimensional and visual inputs. The authors should consider addressing the weaknesses and suggestions for improvement to further strengthen the paper and make it more impactful in the field.",
        "peer_review_with_watermark": "$$\\begin{aligned}$$\n$$\\$ Paper Summary:\\$\nThe paper introduces SafeDreamer, a novel algorithm that combines world model planning with the Lagrangian method to balance long-term rewards and costs in Safe Reinforcement Learning (SafeRL). The algorithm is designed to achieve nearly zero-cost performance in complex scenarios, especially vision-only tasks.\n\n$$\\$ Strengths:\\$\n- The paper addresses the limitations of existing SafeRL methods, which often fail to achieve zero-cost performance in complex scenarios.\n- The use of world model planning and the Lagrangian method is a novel approach to balancing rewards and costs.\n- The experimental results show that SafeDreamer achieves higher rewards and lower costs compared to previous methods.\n\n$$\\$ Weaknesses:\\$\n- The paper relies on the assumption that the world model is accurate, which may not be the case in real-world scenarios.\n- The use of the Lagrangian method may require significant computational resources and may not be suitable for all types of tasks.\n- The experimental results are mostly positive, but the paper does not provide a comprehensive comparison with other SafeRL methods.\n\n$$\\$ Suggestions for Improvement:\\$\n- The paper should provide more details on the accuracy of the world model and how it is trained.\n- The use of the Lagrangian method should be more thoroughly evaluated, especially in terms of its computational cost and suitability for different tasks.\n- The experimental results should be more comprehensive, including a more detailed comparison with other SafeRL methods.\n\n$$\\$ Recommendation:\\$\n- The paper is well-written and clearly explains the contributions of the SafeDreamer algorithm.\n- The experimental results are mostly positive, indicating that the algorithm is promising for achieving nearly zero-cost performance in SafeRL tasks.\n- The paper should be more comprehensive in its evaluation of the algorithm's performance and its potential limitations.\n\n$$\\$ Recommendations for Future Work:\\$\n- The paper should provide more details on the accuracy of the world model and how it is trained.\n- The use of the Lagrangian method should be more thoroughly evaluated, especially in terms of its computational cost and suitability for different tasks.\n- The experimental results should be more comprehensive, including a more detailed comparison with other SafeRL methods.\n\n$$\\$ Overall:\\$\nThe paper introduces a novel algorithm for SafeRL that combines world model planning with the Lagrangian method to balance long-term rewards and costs. The experimental results show promising results, but the paper should be more comprehensive in its evaluation of the algorithm's performance and its potential limitations.",
        "gr_score_with": 0.44528301886792454,
        "gr_score_without": 0.3582554517133956,
        "or_score_with": 0.5018867924528302,
        "or_score_without": 0.5264797507788161,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 115,
            "green_fraction": 0.3582554517133956,
            "num_orange_tokens": 169,
            "orange_fraction": 0.5264797507788161,
            "green_z_score": -1.5266752370217485,
            "orange_z_score": 106.19976987285925,
            "green_p_value": 0.9365791059983091,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 265,
            "num_green_tokens": 118,
            "green_fraction": 0.44528301886792454,
            "num_orange_tokens": 133,
            "orange_fraction": 0.5018867924528302,
            "green_z_score": 1.5047095877265575,
            "orange_z_score": 91.9181607388617,
            "green_p_value": 0.06619937851458521,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SKIP-ATTENTION: IMPROVING VISION TRANSFORM- ERS BY PAYING LESS ATTENTION",
        "abstract": "This work aims to improve the efficiency of vision transformers (ViTs). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers \u2013 a key redundancy that causes unnecessary computations. Based on this observation, we propose SKIPAT, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer\u2019s performance while running computationally faster. We show that SKIPAT is agnostic to transformer architecture and is effective in image classification, semantic segmentation, image denoising, and video denoising. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks.",
        "peer_review_without_watermark": "$$\\boxed{\\text{Paper Summary:}}$$\n\nThe paper proposes a novel method called SKIP-ATTENTION, which aims to improve the efficiency of vision transformers (ViTs) by leveraging the redundancy of self-attention operations across layers. The authors identify that the attention of the class token to spatial patches has a high correlation across layers, leading to unnecessary computations. They propose a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers, resulting in improved throughput while maintaining high model performance.\n\n$$\\boxed{\\text{Strengths:}}$$\n\n1. The authors have identified a key redundancy in ViTs that can be exploited to improve efficiency.\n2. The proposed SKIP-ATTENTION method is simple and light, with a computational complexity of O(nd2) compared to the original MSA block's complexity of O(n2d).\n3. The authors have demonstrated the effectiveness of SKIP-ATTENTION on various tasks, including image classification, semantic segmentation, and image denoising.\n4. The method is general-purpose and can be applied to different transformer architectures.\n\n$$\\boxed{\\text{Weaknesses:}}$$\n\n1. The authors have not provided a detailed analysis of the limitations of their method, such as its potential impact on model performance or its applicability to certain tasks.\n2. The proposed parametric function is simple and may not capture all the complexities of the self-attention mechanism.\n3. The authors have not provided a detailed comparison of their method with other state-of-the-art methods for reducing self-attention computations.\n4. The paper does not provide a clear explanation of the role of the parametric function in capturing cross-token relations.\n\n$$\\boxed{\\text{Suggestions for Improvement:}}$$\n\n1. The authors should provide a more detailed analysis of the limitations of their method and its potential impact on model performance.\n2. They should provide a more detailed comparison of their method with other state-of-the-art methods for reducing self-attention computations.\n3. The authors should provide a clear explanation of the role of the parametric function in capturing cross-token relations.\n4. They should consider exploring other approaches to reducing self-attention computations, such as using more efficient attention mechanisms or optimizing the architecture of the transformer.\n\n$$\\boxed{\\text{Recommendation:}}$$\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit their work. The authors have made a significant contribution to the field of computer vision by proposing a novel method for reducing self-attention computations in ViTs. However, there are several areas where the paper could be improved, including providing a more detailed analysis of the limitations of the method and its potential impact on model performance. With revisions, the paper has the potential to make a significant impact on the field of computer vision.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces SKIP-ATTENTION, a novel method to improve the efficiency of vision transformers (ViTs) by leveraging the redundancy across self-attention (MSA) blocks. The authors propose a plug-in module, SKIP-ATTENTION, that can be used in any ViT architecture to reduce the costly O(n2) MSA computations. The method introduces a simple parametric function to skip computing MSA blocks and reusing representations from previous layers. The authors evaluate the effectiveness of SKIP-ATTENTION on 7 different tasks, including image classification, semantic segmentation, and image denoising, and show that it achieves improved throughput while maintaining state-of-the-art accuracy levels.\n\nStrengths:\n1. The authors identify a crucial redundancy in ViTs, which is the correlation across MSA blocks, and propose a novel method to leverage this redundancy.\n2. The SKIP-ATTENTION module is simple and light, with a computational complexity of O(nd2), which is faster than the original MSA blocks.\n3. The authors evaluate the effectiveness of SKIP-ATTENTION on a wide range of tasks, including image classification, semantic segmentation, and image denoising.\n4. The method is general-purpose and can be applied to any ViT architecture, without modifying the underlying architecture.\n\nWeaknesses:\n1. The authors rely on a simplified analysis of the correlation across MSA blocks, which may not capture all the complexities of the ViT architecture.\n2. The SKIP-ATTENTION module may not work well with very large images or images with very long-range dependencies.\n3. The authors do not provide a detailed analysis of the potential biases in the parametric function used in the SKIP-ATTENTION module.\n4. The method may not work well with very small images or images with very short-range dependencies.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the correlation across MSA blocks, including a deeper analysis of the statistical properties of the attention matrices.\n2. The authors should evaluate the effectiveness of SKIP-ATTENTION on a wider range of tasks, including tasks with very long-range dependencies or very short-range dependencies.\n3. The authors should provide a detailed analysis of the potential biases in the parametric function used in the SKIP-ATTENTION module.\n4. The authors should explore the potential applications of SKIP-ATTENTION in other areas of computer vision, such as object detection or video analysis.\n\nRecommendation:\nThe authors have made a valuable contribution to the field of computer vision by introducing a novel method to improve the efficiency of ViTs. The SKIP-ATTENTION module is simple and light, and achieves improved throughput while maintaining state-of-the-art accuracy levels. However, the authors should address the weaknesses mentioned above to further improve the method. With these improvements, the authors can make a more significant impact on the field of computer vision.",
        "gr_score_with": 0.3681159420289855,
        "gr_score_without": 0.2997275204359673,
        "or_score_with": 0.5768115942028985,
        "or_score_without": 0.5858310626702997,
        "output_without": {
            "num_tokens_scored": 367,
            "num_green_tokens": 110,
            "green_fraction": 0.2997275204359673,
            "num_orange_tokens": 215,
            "orange_fraction": 0.5858310626702997,
            "green_z_score": -3.9211112625315407,
            "orange_z_score": 139.2721899378903,
            "green_p_value": 0.9999559292244465,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 345,
            "num_green_tokens": 127,
            "green_fraction": 0.3681159420289855,
            "num_orange_tokens": 199,
            "orange_fraction": 0.5768115942028985,
            "green_z_score": -1.2088642012256094,
            "orange_z_score": 132.93139683447382,
            "green_p_value": 0.8866424892639568,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MULTI-RESOLUTION HUBERT: MULTI-RESOLUTION SPEECH SELF-SUPERVISED LEARNING WITH MASKED UNIT PREDICTION",
        "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
        "peer_review_without_watermark": "$$\n\n$$$Paper Summary:\nThe paper proposes a novel self-supervised learning (SSL) model for speech, called Multi-Resolution HuBERT (MR-HuBERT), which leverages a hierarchical Transformer architecture and multi-resolution masked unit prediction to process speech signals at different resolutions. The model is pre-trained using objectives for multi-resolution masked unit prediction, which are integrated with HuBERT-style clustering units. Experimental results demonstrate that the proposed model achieves superior or comparable performance to the original HuBERT model over various tasks, including speech recognition, spoken language understanding, multilingual speech recognition, and speech enhancement. The model also exhibits computational efficiencies, specifically a 9-13% reduction in computational complexity.\n\nStrengths:\n1. The proposed model effectively addresses the limitation of existing SSL models, which typically process speech signals at a fixed resolution of 20 milliseconds.\n2. The use of multi-resolution masked unit prediction and hierarchical Transformer architecture enables the model to capture the varying informational content present at different resolutions in speech signals.\n3. The model exhibits superior or comparable performance to the original HuBERT model over various tasks, demonstrating its effectiveness in speech processing.\n4. The computational efficiency of the model, specifically a 9-13% reduction in computational complexity, addresses concerns related to inference speed.\n\nWeaknesses:\n1. The model's performance may be sensitive to the choice of hyperparameters, such as the number of masked frames and the resolution of the downsampled representation.\n2. The use of multi-resolution targets may introduce additional complexity to the model, potentially affecting its performance.\n3. The model's ability to generalize to low-resource tasks is not thoroughly evaluated in the paper.\n\nSuggestions for Improvement:\n1. Conduct further experiments to evaluate the model's performance on low-resource tasks and assess its generalizability.\n2. Investigate the effect of different hyperparameter settings on the model's performance and computational efficiency.\n3. Explore the use of additional architectures or techniques to improve the model's performance and robustness.\n\nRecommendation:\nThe proposed MR-HuBERT model is a significant contribution to the field of self-supervised learning for speech, offering improved performance and computational efficiency compared to existing models. However, further research is needed to fully evaluate the model's generalizability and robustness. The results of this paper provide a solid foundation for future work, and the authors are encouraged to continue exploring the potential of multi-resolution masked unit prediction in speech processing. \n\n$$",
        "peer_review_with_watermark": "$$\n\n$$$Paper Summary:\nThe paper introduces a novel self-supervised learning (SSL) model for speech representation, called multi-resolution HuBERT (MR-HuBERT), which leverages a hierarchical Transformer architecture and multi-resolution masked unit prediction. The proposed model is pre-trained using objectives for multi-resolution masked unit prediction, which are integrated with HuBERT-style clustering units. The model is evaluated on various speech processing tasks, including speech recognition, spoken language understanding, and speech enhancement. Experimental results show that MR-HuBERT substantially outperforms the original HuBERT model across a broad spectrum of speech processing tasks and exhibits computational efficiencies, specifically a 9-13% reduction in computational complexity.\n\nStrengths:\n1. The proposed model addresses the limitations of existing SSL models by incorporating multi-resolution information during pre-training.\n2. The hierarchical Transformer architecture allows for explicit encoding of hidden representations at multiple resolutions while retaining the iterative strategy found in the original HuBERT.\n3. The model is pre-trained using objectives for multi-resolution masked unit prediction, which are integrated with HuBERT-style clustering units, resulting in improved performance across various tasks.\n4. The model exhibits computational efficiencies, specifically a 9-13% reduction in computational complexity during inference.\n\nWeaknesses:\n1. The paper does not provide a clear explanation of the downsampling and upsampling modules used in the proposed architecture.\n2. The hyperparameters used in the pre-training process are not clearly discussed, and it is not clear how the hyperparameters were chosen.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed model and how it compares to existing models.\n\nSuggestions for Improvement:\n1. Provide a clear explanation of the downsampling and upsampling modules used in the proposed architecture and discuss their impact on the model's performance.\n2. Provide a detailed analysis of the hyperparameters used in the pre-training process and discuss how they were chosen.\n3. Provide a detailed analysis of the computational complexity of the proposed model and compare it to existing models.\n4. Consider implementing additional evaluation metrics to assess the model's performance on other speech processing tasks.\n\nRecommendation:\nBased on the experimental results and the proposed architecture, it is recommended that the authors revise and resubmit the paper. The revisions should address the limitations and weaknesses identified in the review and provide a clear explanation of the downsampling and upsampling modules used in the proposed architecture. Additionally, the authors should provide a detailed analysis of the hyperparameters used in the pre-training process and discuss how they were chosen. Finally, the authors should consider implementing additional evaluation metrics to assess the model's performance on other speech processing tasks. \n\n$$\n\n$$",
        "gr_score_with": 0.5325077399380805,
        "gr_score_without": 0.42178770949720673,
        "or_score_with": 0.4055727554179567,
        "or_score_without": 0.4664804469273743,
        "output_without": {
            "num_tokens_scored": 358,
            "num_green_tokens": 151,
            "green_fraction": 0.42178770949720673,
            "num_orange_tokens": 167,
            "orange_fraction": 0.4664804469273743,
            "green_z_score": 0.8414871114040992,
            "orange_z_score": 104.32323221912701,
            "green_p_value": 0.2000375512191903,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 323,
            "num_green_tokens": 172,
            "green_fraction": 0.5325077399380805,
            "num_orange_tokens": 131,
            "orange_fraction": 0.4055727554179567,
            "green_z_score": 4.861126099539523,
            "orange_z_score": 85.95719762429442,
            "green_p_value": 5.835993496206512e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION",
        "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper proposes the first incremental approach for probabilistic DNN certification, called Incremental Randomized Smoothing (IRS). It leverages the certification guarantees obtained from the smoothed model to certify a smoothed approximated model with very few samples. The proposed approach significantly reduces the computational cost of certification while maintaining strong robustness guarantees. The authors demonstrate the effectiveness of IRS on state-of-the-art classification models and show that it speeds up certification up to 4.1x over the standard non-incremental RS baseline.\n\n$$Strengths: \n1. The paper presents a novel approach for incremental probabilistic DNN certification, which is a significant improvement over existing methods.\n2. The authors provide a detailed analysis of the strengths and limitations of their approach and discuss its potential applications.\n3. The paper presents experimental results that demonstrate the effectiveness of IRS on state-of-the-art classification models.\n\n$$Weaknesses: \n1. The paper does not provide a comprehensive review of existing methods for probabilistic DNN certification, which might make it difficult for readers to understand the significance of the proposed approach.\n2. Some of the explanations and proofs in the paper are not clear and require additional effort to understand.\n3. The paper does not provide a detailed analysis of the trade-offs between the proposed approach and other methods, which might make it difficult for readers to understand the implications of the results.\n\n$$Suggestions for Improvement: \n1. The authors should provide a comprehensive review of existing methods for probabilistic DNN certification to make the paper more accessible to readers.\n2. The authors should provide clear and concise explanations and proofs to make the paper more understandable.\n3. The authors should provide a detailed analysis of the trade-offs between the proposed approach and other methods to make the paper more comprehensive.\n\n$$Recommendation: \nBased on the paper's strengths and weaknesses, I recommend that it be accepted for publication in a reputable conference or journal in the field of machine learning. However, the authors should address the weaknesses and suggestions for improvement to make the paper more comprehensive and accessible to readers.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents the first approach for incremental randomized smoothing certification, IRS, for probabilistic DNN certification. IRS leverages the certification guarantees obtained from the smoothed model to certify a smoothed approximated model with very few samples. This approach significantly reduces the computational cost of certification while maintaining strong robustness guarantees. The authors experimentally demonstrate the effectiveness of IRS, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of an approximate model from scratch.\n\nStrengths:\n1. IRS leverages the certification guarantees obtained from the smoothed model, significantly reducing the computational cost of certification.\n2. The approach maintains strong robustness guarantees, making it suitable for real-world applications.\n3. The authors experimentally demonstrate the effectiveness of IRS, showing its ability to speed up certification by up to 4.1x over the baseline approach.\n\nWeaknesses:\n1. The approach requires storing additional information, including the top predicted class index, its lower confidence bound, and the seeds for Gaussian corruptions, which may require some memory overhead.\n2. The value of the smoothing parameter \u03c3 affects the efficiency of IRS, with larger values leading to better results but potentially resulting in a loss of accuracy in the model.\n3. The approach may not be suitable for all types of approximations, as it relies on the similarity between the approximated and original models.\n\nSuggestions for Improvement:\n1. Investigate ways to minimize the memory overhead required by the approach, such as storing only the most critical information or using compression techniques.\n2. Investigate ways to optimize the choice of the smoothing parameter \u03c3, such as using techniques like cross-validation or grid search.\n3. Investigate ways to improve the robustness of the approach, such as using more robust approximations or modifying the certification process to handle more types of approximations.\n\nRecommendation:\nThe approach presents a significant improvement in the efficiency of probabilistic DNN certification and has the potential to be widely applicable. However, further investigation is required to address the limitations and challenges associated with the approach. With some modifications, IRS has the potential to be a widely adopted and effective method for probabilistic DNN certification.",
        "gr_score_with": 0.4923547400611621,
        "gr_score_without": 0.3892857142857143,
        "or_score_with": 0.45871559633027525,
        "or_score_without": 0.45714285714285713,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 109,
            "green_fraction": 0.3892857142857143,
            "num_orange_tokens": 128,
            "orange_fraction": 0.45714285714285713,
            "green_z_score": -0.3659625273556999,
            "orange_z_score": 94.1057690275528,
            "green_p_value": 0.6428034811828368,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 161,
            "green_fraction": 0.4923547400611621,
            "num_orange_tokens": 150,
            "orange_fraction": 0.45871559633027525,
            "green_z_score": 3.409003489442267,
            "orange_z_score": 102.05259011262204,
            "green_p_value": 0.00032600322853280683,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TILLATION: A KERNEL METHOD PERSPECTIVE",
        "abstract": "Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks. 1",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$\\boxed{Paper Summary:}$ \nThe paper proposes a novel theoretical framework for backdoor attacks on dataset distillation, which is a technique to compress large datasets into small synthetic datasets. The authors introduce two new theory-driven trigger pattern generation methods, simple-trigger and relax-trigger, and show that these methods can effectively evade existing backdoor detection and cleansing methods. The paper also presents experimental results that demonstrate the effectiveness of the proposed methods.\n\n$\\boxed{Strengths:}$ \n1. The paper provides a novel theoretical framework for backdoor attacks on dataset distillation, which is a significant contribution to the field of deep learning security.\n2. The authors introduce two new theory-driven trigger pattern generation methods, simple-trigger and relax-trigger, which are effective in evading existing backdoor detection and cleansing methods.\n3. The paper presents experimental results that demonstrate the effectiveness of the proposed methods.\n\n$\\boxed{Weaknesses:}$ \n1. The paper does not provide a clear and comprehensive review of existing backdoor detection and cleansing methods, which may limit the applicability of the proposed methods.\n2. The authors assume that the kernel matrix k(XS, XS ) is invertible, which may not always be the case in practice.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed methods, which may be a concern for large-scale datasets.\n\n$\\boxed{Suggestions for Improvement:}$ \n1. The authors should provide a more comprehensive review of existing backdoor detection and cleansing methods to demonstrate the novelty and effectiveness of the proposed methods.\n2. The authors should consider the case where the kernel matrix k(XS, XS ) is not invertible and provide a robust solution.\n3. The authors should analyze the computational complexity of the proposed methods and provide a detailed analysis of the time and space requirements.\n\n$\\boxed{Recommendation:}$ \nBased on the strengths and weaknesses of the paper, I recommend that the authors revise the paper to address the weaknesses and improve the overall quality of the paper. The authors should also consider the feedback from the reviewers and the broader community to further improve the paper.",
        "peer_review_with_watermark": "$$$${Paper Summary:\nThe paper proposes two novel theory-induced backdoor attacks, simple-trigger and relax-trigger, based on the kernel inducing points (KIP) method for dataset distillation. The backdoor effect is characterized by three key components: conflict loss, projection loss, and generalization gap. The proposed attacks are designed to exploit the weaknesses of existing backdoor detection and cleansing methods.\n\nStrengths:\n* The paper provides a novel theoretical framework for backdoor attacks and dataset distillation.\n* The proposed attacks, simple-trigger and relax-trigger, demonstrate strong backdoor resiliency against eight existing backdoor defense methods.\n* The paper provides a comprehensive analysis of the conflict loss, projection loss, and generalization gap, which are key components of the backdoor effect.\n\nWeaknesses:\n* The paper relies heavily on mathematical derivations and proofs, which may make it difficult for non-experts to understand.\n* The proposed attacks, while novel, may not be directly applicable to all scenarios and datasets.\n* The paper could benefit from more experimental results and comparisons with existing backdoor detection and cleansing methods.\n\nSuggestions for Improvement:\n* Provide more experimental results and comparisons with existing backdoor detection and cleansing methods.\n* Offer more detailed explanations and interpretations of the mathematical derivations and proofs.\n* Investigate the applicability of the proposed attacks to different scenarios and datasets.\n* Consider exploring alternative theoretical frameworks and backdoor detection and cleansing methods for comparison and improvement.\n\nRecommendation:\n* The paper presents novel and innovative research in the field of backdoor attacks and dataset distillation.\n* The proposed attacks, simple-trigger and relax-trigger, demonstrate strong backdoor resiliency against eight existing backdoor defense methods.\n* The paper provides a comprehensive analysis of the conflict loss, projection loss, and generalization gap, which are key components of the backdoor effect.\n* The research has the potential to inspire new and more effective backdoor detection and cleansing methods, and to provide valuable insights into the risks and challenges of dataset distillation-based backdoor attacks.}$$$${\n\nThe manuscript has been well-reorganized and clearly presented, and the writing is generally concise and clear. The use of mathematical notation and proofs is well-explained, and the derivations are generally accurate. The use of figures and visualizations would enhance the presentation of the results and make the manuscript more accessible to a wider audience.\n\nThe manuscript provides a comprehensive analysis of the conflict loss, projection loss, and generalization gap, which are key components of the backdoor effect. The proposed attacks, simple-trigger and relax-trigger, demonstrate strong backdoor resiliency against eight existing backdoor defense methods, and the manuscript provides a detailed analysis of their strengths and weaknesses.\n\nHowever, the manuscript could benefit from more experimental results and comparisons with existing backdoor detection and cleansing methods. The proposed attacks, while novel, may not be directly applicable to all scenarios and datasets, and the manuscript could benefit from a more detailed discussion of their limitations and potential applications.\n\nOverall, the manuscript provides a significant contribution to the field of backdoor attacks and dataset distillation, and the proposed attacks, simple-trigger and relax-trigger, demonstrate strong backdoor resiliency against eight existing backdoor defense methods. The manuscript would benefit from further experimental results and comparisons with existing backdoor detection and cleansing methods, as well as a more detailed discussion of their limitations and potential applications.",
        "gr_score_with": 0.5870206489675516,
        "gr_score_without": 0.42,
        "or_score_with": 0.34513274336283184,
        "or_score_without": 0.444,
        "output_without": {
            "num_tokens_scored": 250,
            "num_green_tokens": 105,
            "green_fraction": 0.42,
            "num_orange_tokens": 111,
            "orange_fraction": 0.444,
            "green_z_score": 0.6454972243679028,
            "orange_z_score": 88.9428934168258,
            "green_p_value": 0.2593025082143628,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 199,
            "green_fraction": 0.5870206489675516,
            "num_orange_tokens": 117,
            "orange_fraction": 0.34513274336283184,
            "green_z_score": 7.028842112012113,
            "orange_z_score": 80.1878579325105,
            "green_p_value": 1.0412723411789704e-12,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FINE-TUNING LANGUAGE MODELS FOR FACTUALITY REDUCES HALLUCINATION",
        "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as \u2018hallucinations.\u2019 These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model\u2019s confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel approach to fine-tuning language models for factuality, specifically targeting long-form text generation. The authors leverage two recent innovations in NLP: (1) reference-based automated fact-checking methods and (2) reference-free truthfulness evaluations using a model's own confidence. They introduce a preference-based reinforcement learning (RL) framework, Direct Preference Optimization (DPO), to optimize language models for factuality without human labeling. The authors evaluate their approach on two benchmark datasets and demonstrate significant improvements in factuality compared to conventional RLHF and decoding strategies.\n\nStrengths:\n\n1. The authors provide a clear and comprehensive overview of the problem of hallucinations in language models and the challenges of fine-tuning models for factuality.\n2. The proposed reference-free truthfulness estimator using a model's own confidence is a novel and promising approach.\n3. The authors provide a detailed evaluation of their approach on two benchmark datasets and demonstrate significant improvements in factuality.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of NLP and reinforcement learning, which may limit its accessibility to non-experts.\n2. The evaluation of the reference-free estimator using a model's own confidence is limited to a single dataset and may not generalize to other scenarios.\n3. The paper does not provide a detailed analysis of the limitations and potential biases of the proposed approach.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the limitations and potential biases of the proposed approach.\n2. Include a more comprehensive evaluation of the reference-free estimator using a model's own confidence on multiple datasets.\n3. Consider exploring alternative approaches to constructing factuality preferences, such as using self-correction.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to fine-tuning language models for factuality. While there are some limitations and potential biases in the proposed approach, the authors provide a clear and comprehensive overview of the problem and demonstrate significant improvements in factuality. With further evaluation and analysis, this approach has the potential to be a valuable contribution to the field of NLP.\n\n$$$",
        "peer_review_with_watermark": "$$\n$$\n\n\nPaper Summary:\nThe paper proposes a method to fine-tune language models to generate more factual content without human annotation. The approach uses preference-based reinforcement learning to optimize the model for factuality, leveraging recent advances in estimating truthfulness without human intervention. The authors evaluate their method on two benchmark datasets for generating biographies and answering medical questions, and find that it outperforms conventional reinforcement learning from human feedback (RLHF) and decoding strategies that aim to increase factuality.\n\nStrengths:\n1. The authors address a critical issue in natural language generation, namely the problem of hallucinations or factual errors.\n2. The proposed approach is practical and scalable, as it does not require human annotation.\n3. The authors leverage recent advances in estimating truthfulness without human intervention, which is a significant improvement over existing methods.\n4. The experimental results show that the proposed approach is effective in reducing factual errors and improving factuality.\n\nWeaknesses:\n1. The approach relies on the quality of the preference pairs generated by the reference-based or reference-free truthfulness estimator, which may not always accurately capture the factual accuracy of the model responses.\n2. The authors do not provide a detailed analysis of the limitations of their approach, such as the potential impact of the reference corpus on the performance of the model.\n3. The experimental results are limited to two benchmark datasets, and it is unclear how well the approach would generalize to other tasks and domains.\n4. The approach does not address the issue of overfitting to the reference corpus, which may lead to biased model responses.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of their approach and potential pitfalls.\n2. The experimental results should be extended to other tasks and domains to demonstrate the generalizability of the approach.\n3. The authors should investigate the impact of the reference corpus on the performance of the model and provide a more detailed analysis of the potential overfitting.\n4. The approach should be evaluated on a more diverse range of datasets to demonstrate its effectiveness in reducing factual errors and improving factuality.\n\nRecommendation:\nThe proposed approach is a significant contribution to the field of natural language generation, and it has the potential to improve the accuracy and reliability of language models. However, the approach has some limitations and potential pitfalls that should be addressed in future work. The authors should provide a more detailed analysis of the limitations of their approach and investigate the impact of the reference corpus on the performance of the model. Additionally, the experimental results should be extended to other tasks and domains to demonstrate the generalizability of the approach.",
        "gr_score_with": 0.4881656804733728,
        "gr_score_without": 0.38028169014084506,
        "or_score_with": 0.46745562130177515,
        "or_score_without": 0.5035211267605634,
        "output_without": {
            "num_tokens_scored": 284,
            "num_green_tokens": 108,
            "green_fraction": 0.38028169014084506,
            "num_orange_tokens": 143,
            "orange_fraction": 0.5035211267605634,
            "green_z_score": -0.6783022163561139,
            "orange_z_score": 96.83096465138044,
            "green_p_value": 0.7512099531513833,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 338,
            "num_green_tokens": 165,
            "green_fraction": 0.4881656804733728,
            "num_orange_tokens": 158,
            "orange_fraction": 0.46745562130177515,
            "green_z_score": 3.308661158048238,
            "orange_z_score": 97.95545286277832,
            "green_p_value": 0.0004687160786416017,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "METHOD WITH OPTIMAL ADAPTATION TO INEXACTNESS",
        "abstract": "We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, which occurs typically in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key setting. We further introduce a tensor generalization for stochastic higher-order derivatives. When the oracles are non-stochastic, the proposed tensor algorithm matches the global convergence of Nesterov Accelerated Tensor method. Both algorithms allow for approximate solutions of their auxiliary subproblems with verifiable conditions on the accuracy of the solution.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\n\nThe paper proposes a novel accelerated stochastic second-order method for convex and strongly convex optimization. The authors establish a lower bound for stochastic second-order optimization and prove that their algorithm achieves optimal convergence in both gradient and Hessian inexactness. Additionally, they introduce a tensor generalization of second-order methods for stochastic high-order derivatives.\n\n$$\\text{Strengths:}$$\n\n* The authors provide a thorough analysis of the strengths and limitations of existing second-order methods for stochastic optimization.\n* The proposed algorithm is well-structured and easy to follow.\n* The authors provide a clear and concise summary of the main results and contributions.\n\n$$\\text{Weaknesses:}$$\n\n* The paper assumes a high level of background knowledge in optimization and stochastic methods, which may make it challenging for non-experts to follow.\n* The authors could provide more detailed explanations of the assumptions and limitations of the proposed algorithm.\n* The paper could benefit from additional numerical experiments to demonstrate the effectiveness of the proposed algorithm.\n\n$$\\text{Suggestions for Improvement:}$$\n\n* Provide more detailed explanations of the assumptions and limitations of the proposed algorithm.\n* Include additional numerical experiments to demonstrate the effectiveness of the proposed algorithm.\n* Consider providing a more comprehensive review of the existing literature on stochastic second-order optimization.\n\n$$\\text{Recommendation:}$$\n\nBased on the paper's content and quality, I recommend that it be accepted for publication in a top-tier conference or journal in the field of optimization and machine learning. However, the authors should be aware of the potential for additional peer review and revision to ensure that the paper meets the high standards of the field.",
        "peer_review_with_watermark": "$$$$\n\nPaper Summary:\n\nThe paper presents a new accelerated stochastic second-order method for convex optimization, which is robust to both gradient and Hessian inexactness. The authors establish theoretical lower bounds and prove that their algorithm achieves optimal convergence in both gradient and Hessian inexactness. The paper also introduces a tensor generalization for stochastic higher-order derivatives. The authors demonstrate the effectiveness of their method on several experimental scenarios, including logistic regression problems.\n\nStrengths:\n\n1. The paper presents a new and efficient stochastic second-order method for convex optimization, which is robust to inexact derivatives.\n2. The authors establish theoretical lower bounds for the convergence of their method, which provides a solid foundation for their results.\n3. The paper introduces a tensor generalization of second-order methods for stochastic higher-order derivatives, which is an extension of existing work in the field.\n4. The authors provide experimental results that demonstrate the effectiveness of their method on several real-world applications, including logistic regression problems.\n\nWeaknesses:\n\n1. The paper does not provide a detailed comparison with existing methods, which makes it difficult to evaluate the relative performance of the proposed method.\n2. The authors do not discuss the potential drawbacks of their method, such as increased computational complexity or memory requirements.\n3. The paper lacks a comprehensive discussion of the implications of inexact derivatives on the convergence of the method.\n4. The authors do not provide explicit bounds on the number of iterations required for convergence, which makes it difficult to evaluate the practical performance of the method.\n\nSuggestions for Improvement:\n\n1. The authors should provide a detailed comparison with existing methods, including first-order, second-order, and higher-order methods, to evaluate the relative performance of their proposed method.\n2. The authors should discuss the potential drawbacks of their method, such as increased computational complexity or memory requirements, to provide a more comprehensive evaluation of its practical performance.\n3. The authors should provide explicit bounds on the number of iterations required for convergence, which would allow for a more detailed evaluation of the method's practical performance.\n4. The authors should discuss the implications of inexact derivatives on the convergence of their method, which would provide a more comprehensive understanding of its limitations and potential drawbacks.\n\nRecommendation:\n\nBased on the paper's contributions and limitations, I recommend that the authors revise and resubmit their paper for further evaluation. Specifically, I recommend that they:\n\n1. Provide a detailed comparison with existing methods to evaluate the relative performance of their proposed method.\n2. Discuss the potential drawbacks of their method, such as increased computational complexity or memory requirements.\n3. Provide explicit bounds on the number of iterations required for convergence.\n4. Discuss the implications of inexact derivatives on the convergence of their method.\n\nBy addressing these limitations, the authors can provide a more comprehensive evaluation of their proposed method and contribute to the ongoing research in stochastic second-order optimization.",
        "gr_score_with": 0.5429553264604811,
        "gr_score_without": 0.42448979591836733,
        "or_score_with": 0.3951890034364261,
        "or_score_without": 0.4204081632653061,
        "output_without": {
            "num_tokens_scored": 245,
            "num_green_tokens": 104,
            "green_fraction": 0.42448979591836733,
            "num_orange_tokens": 103,
            "orange_fraction": 0.4204081632653061,
            "green_z_score": 0.7824607964359516,
            "orange_z_score": 81.35700514825892,
            "green_p_value": 0.21697190814893358,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 291,
            "num_green_tokens": 158,
            "green_fraction": 0.5429553264604811,
            "num_orange_tokens": 115,
            "orange_fraction": 0.3951890034364261,
            "green_z_score": 4.977843233082983,
            "orange_z_score": 83.26537632387054,
            "green_p_value": 3.214834891534447e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "GENERATIVE MODELING WITH PHASE STOCHASTIC BRIDGES",
        "abstract": "We introduce a novel generative modeling framework grounded in phase space dynamics, taking inspiration from the principles underlying Critically damped Langevin Dynamics and Bridge Matching. Leveraging insights from Stochastic Optimal Control, we construct a more favorable path measure in the phase space that is highly advantageous for efficient sampling. A distinctive feature of our approach is the early-stage data prediction capability within the context of propagating generative Ordinary Differential Equations or Stochastic Differential Equations. This early prediction, enabled by the model\u2019s unique structural characteristics, sets the stage for more efficient data generation, leveraging additional velocity information along the trajectory. This innovation has spurred the exploration of a novel avenue for mitigating sampling complexity by quickly converging to realistic data samples. Our model yields comparable results in image generation and notably outperforms baseline methods, particularly when faced with a limited Number of Function Evaluations. Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential in the realm of generative modeling.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper introduces a novel generative modeling framework called Acceleration Generative Modeling (AGM) grounded in phase space dynamics, inspired by the principles of Critically damped Langevin Dynamics and Bridge Matching. AGM leverages insights from Stochastic Optimal Control to construct a more favorable path measure in the phase space, enabling efficient sampling. The framework consists of a generative Ordinary Differential Equation (ODE) or Stochastic Differential Equation (SDE) that can be used for image generation. AGM achieves comparable results in image generation and outperforms baseline methods, particularly when faced with limited Number of Function Evaluations (NFE).\n\n$$\\text{Strengths:}$$\n1. The paper introduces a novel generative modeling framework that leverages insights from Stochastic Optimal Control to construct a more favorable path measure in the phase space.\n2. The framework achieves comparable results in image generation and outperforms baseline methods, particularly when faced with limited NFE.\n3. The paper demonstrates the effectiveness of the sampling-hop technique, which significantly reduces the time required to converge to accurate predictions of realistic data points.\n\n$$\\text{Weaknesses:}$$\n1. The paper's performance lags behind state-of-the-art methods in scenarios with sufficient NFE.\n2. The paper's experimental results are limited to toy and image datasets, and further experiments are needed to demonstrate the framework's effectiveness on other datasets.\n3. The paper does not provide a detailed analysis of the framework's limitations and potential avenues for improvement.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Enhance the training quality through the adoption of techniques proposed in Karras et al. (2022), including data augmentation, fine-tuned noise scheduling, and network preconditioning.\n2. Conduct further experiments to demonstrate the framework's effectiveness on other datasets and to analyze its limitations and potential avenues for improvement.\n3. Provide a more detailed analysis of the framework's performance on scenarios with sufficient NFE and explore ways to improve its performance in such scenarios.\n\n$$\\text{Recommendation:}$$\nBased on the paper's strengths and weaknesses, I recommend that the authors continue to develop and refine the AGM framework. The paper's introduction of a novel generative modeling framework and the demonstration of its effectiveness in image generation are significant contributions to the field. However, to fully realize the framework's potential, the authors should address the limitations and potential avenues for improvement identified in this review.",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper introduces a novel generative modeling framework based on phase space dynamics, leveraging insights from Stochastic Optimal Control (SOC) theory. The framework, termed Acceleration Generative Modeling (AGM), enables efficient sampling by constructing a favorable path measure in the phase space. AGM outperforms baseline methods, particularly when faced with limited Number of Function Evaluations (NFE). The authors demonstrate competitive results with diffusion models equipped with efficient sampling techniques.\n\nStrengths:\n\n1.  AGM framework is grounded in phase space dynamics, taking inspiration from Critically damped Langevin Dynamics and Bridge Matching.\n2.  AGM incorporates additional velocity information, leading to a more favorable path measure and reduced sampling complexity.\n3.  The sampling-hop technique enables efficient sampling by leveraging additional velocity information, reducing the required time integration.\n\nWeaknesses:\n\n1.  AGM's performance lags behind state-of-the-art methods in scenarios with sufficient NFE.\n2.  The paper could benefit from a more comprehensive evaluation of AGM's performance on various datasets and tasks.\n\nSuggestions for Improvement:\n\n1.  Enhance training quality through techniques proposed in Karras et al. (2022), such as data augmentation, fine-tuned noise scheduling, and network preconditioning.\n2.  Conduct a more comprehensive evaluation of AGM's performance on various datasets and tasks to better understand its limitations and potential improvements.\n\nRecommendation:\n\nThe AGM framework presented in this paper demonstrates promising outcomes for fast sampling in unconditional generative tasks. However, further improvements are needed to enhance AGM's performance, particularly in scenarios with sufficient NFE. The authors should continue to explore and address these limitations to further develop and validate their framework.",
        "gr_score_with": 0.4397163120567376,
        "gr_score_without": 0.32590529247910865,
        "or_score_with": 0.5283687943262412,
        "or_score_without": 0.584958217270195,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 117,
            "green_fraction": 0.32590529247910865,
            "num_orange_tokens": 210,
            "orange_fraction": 0.584958217270195,
            "green_z_score": -2.8656872503640014,
            "orange_z_score": 129.36714116254694,
            "green_p_value": 0.9979194744725493,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 282,
            "num_green_tokens": 124,
            "green_fraction": 0.4397163120567376,
            "num_orange_tokens": 149,
            "orange_fraction": 0.5283687943262412,
            "green_z_score": 1.361406587313681,
            "orange_z_score": 103.42668211069667,
            "green_p_value": 0.08669261777301318,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "EFFECTIVE DATA AUGMENTATION WITH DIFFUSION MODELS",
        "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains. DA-Fusion Semantically Augment Augmented Images For Training Models Figure 1: Real images (left) are semantically modified using a publicly available off-the-shelf Stable Diffusion checkpoint. Resulting synthetic images (right) are used for training downstream classification models.",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper proposes a novel data augmentation strategy using diffusion models, called DA-Fusion. The method adapts a pre-trained diffusion model to generate high-quality augmentations for new and fine-grain concepts, improving few-shot classification accuracy. The authors evaluate their approach on several standard datasets and a real-world weed recognition task, achieving significant gains in accuracy compared to standard data augmentation methods.\n\nStrengths:\n1. The paper introduces a novel and flexible data augmentation strategy that can be used for various image classification tasks.\n2. The authors demonstrate the effectiveness of their approach on several datasets, including a real-world weed recognition task.\n3. The method is modular and can be easily integrated with existing data augmentation pipelines.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the hyperparameter tuning process, which may impact the performance of the method.\n2. The authors do not provide a clear explanation of how the pseudo-prompts are optimized, which may affect the quality of the generated augmentations.\n3. The paper does not discuss the potential risks of using generative models for data augmentation, such as the potential for generating harmful or biased content.\n\nSuggestions for Improvement:\n1. Provide a detailed analysis of the hyperparameter tuning process to ensure that the method is robust to different parameter settings.\n2. Offer more insight into the optimization process of the pseudo-prompts to improve the quality of the generated augmentations.\n3. Discuss the potential risks of using generative models for data augmentation and propose strategies to mitigate these risks.\n\nRecommendation:\nThe paper presents a novel and effective data augmentation strategy using diffusion models, which has the potential to improve few-shot classification accuracy. However, further research is needed to address the potential risks associated with using generative models for data augmentation. With some modifications to the optimization process and hyperparameter tuning, the method has the potential to become a valuable tool in the field of computer vision. \n\nNote: The review is based on the provided text and may not be exhaustive, but it should give a general idea of the strengths and weaknesses of the paper.",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\nThe paper proposes a novel data augmentation strategy using text-to-image diffusion models, known as DA-Fusion, to improve few-shot image classification tasks. The method adapts a pre-trained diffusion model to semantically modify images and generates high-quality augmentations regardless of image content. The authors evaluate their approach on six standard computer vision datasets and a real-world weed recognition task, demonstrating improvements in accuracy compared to standard data augmentation methods.\n\nStrengths:\n\n1. Innovative approach to data augmentation: DA-Fusion leverages text-to-image diffusion models to generate semantically modified images, offering a novel solution to the problem of limited data.\n2. Robust to data balance: The method is insensitive to the balance of real and synthetic data, simplifying hyper-parameter tuning.\n3. Generalization to new concepts: DA-Fusion improves few-shot classification accuracy on novel concepts, demonstrating its potential for real-world applications.\n4. Easy to implement: The authors provide open-source code and detailed experimental settings, making it easy for others to reproduce their results.\n\nWeaknesses:\n\n1. Limited evaluation datasets: The paper primarily focuses on standard computer vision datasets, with limited evaluation on real-world datasets like the leafy spurge dataset.\n2. Dependence on pre-trained models: DA-Fusion relies on pre-trained text-to-image diffusion models, which may not generalize well to new datasets or tasks.\n3. Potential leakage of internet data: The authors discuss the risk of leakage of internet data, but their method does not explicitly address this issue.\n4. Limited exploration of hyper-parameters: The authors primarily focus on the hyper-parameter t0, but do not thoroughly explore other hyper-parameters that could affect the performance of DA-Fusion.\n\nSuggestions for Improvement:\n\n1. Expand evaluation to real-world datasets: Evaluate DA-Fusion on real-world datasets like the leafy spurge dataset to demonstrate its effectiveness in practical applications.\n2. Develop a robust evaluation framework: Develop a robust evaluation framework to assess the performance of DA-Fusion on different datasets, hyper-parameters, and models.\n3. Address internet data leakage: Implement measures to prevent or detect internet data leakage, such as concept erasure or data augmentation with diverse text prompts.\n4. Thoroughly explore hyper-parameters: Conduct a thorough exploration of hyper-parameters to understand their impact on DA-Fusion's performance and to develop a more robust method.\n\nRecommendation:\nThe authors have proposed a novel and innovative approach to data augmentation, DA-Fusion, which demonstrates improvements in few-shot image classification accuracy. However, the method has limitations, and further exploration is needed to address these concerns. To improve the effectiveness and robustness of DA-Fusion, the authors should expand their evaluation to real-world datasets, develop a robust evaluation framework, address internet data leakage, and thoroughly explore hyper-parameters.\n\n$$",
        "gr_score_with": 0.5847255369928401,
        "gr_score_without": 0.3973509933774834,
        "or_score_with": 0.3556085918854415,
        "or_score_without": 0.5033112582781457,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 120,
            "green_fraction": 0.3973509933774834,
            "num_orange_tokens": 152,
            "orange_fraction": 0.5033112582781457,
            "green_z_score": -0.09396819714698924,
            "orange_z_score": 88.44459292620975,
            "green_p_value": 0.5374327900394689,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 419,
            "num_green_tokens": 245,
            "green_fraction": 0.5847255369928401,
            "num_orange_tokens": 149,
            "orange_fraction": 0.3556085918854415,
            "green_z_score": 7.718418599699739,
            "orange_z_score": 73.0170963710459,
            "green_p_value": 5.8890920819285725e-15,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY",
        "abstract": "Adversarial patch attacks, where a small patch is placed in the scene to fool neural networks, have been studied for numerous applications. Focusing on image classification, we consider the setting of a black-box transfer attack where an attacker does not know the target model. Instead of forcing corrupted image representations to cross the nearest decision boundaries or converge to a particular point, we propose a distribution-oriented approach. We rely on optimal transport to push the feature distribution of attacked images towards an already modeled distribution. We show that this new distribution-oriented approach leads to better transferable patches. Through digital experiments conducted on ImageNet-1K, we provide evidence that our new patches are the only ones that can simultaneously influence multiple Transformer models and Convolutional Neural Networks. Physical world experiments demonstrate that our patch can affect systems in deployment without explicit knowledge.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary:\nThe paper proposes a distribution-oriented approach based on optimal transport for designing adversarial patches (APAs) that can transfer to unseen models. The authors focus on image classification and consider the setting of a black-box transfer attack where the attacker does not know the target model. They introduce a new framework that relies on optimal transport to push the feature distribution of attacked images towards an already modeled distribution. The approach leads to better transferable patches and demonstrates its effectiveness on a large ensemble of network architectures, including classical CNNs and recent CNNs, as well as Vision Transformers.\n\nStrengths:\n1. The paper proposes a novel approach that addresses the limitations of previous APA methods, which focus on maximizing the probability of a target class or pushing corrupted image representations to cross the nearest decision boundary.\n2. The use of optimal transport allows for a more robust and flexible optimization of the patch attack, reducing overfitting to the source architecture and strengthening transferability to Convolutional Neural Networks and Transformer architectures.\n3. The authors provide extensive experiments on ImageNet-1K, demonstrating the effectiveness of their approach on a large ensemble of network architectures.\n4. The hybrid and physical experiments show that the patch can disturb real-world classifiers without any knowledge of the system.\n\nWeaknesses:\n1. The paper relies on a relatively small number of experiments, which may not be representative of the full range of possible scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity of their approach, which may be a concern for large-scale applications.\n3. The paper does not discuss potential limitations or challenges of the approach, such as the potential for overfitting to the target model or the impact of adversarial training on the effectiveness of the patch attack.\n\nSuggestions for Improvement:\n1. Conduct additional experiments to demonstrate the effectiveness of the approach on a wider range of scenarios and network architectures.\n2. Provide a detailed analysis of the computational complexity of the approach and discuss potential optimization techniques to improve efficiency.\n3. Discuss potential limitations and challenges of the approach, such as overfitting to the target model or the impact of adversarial training on the effectiveness of the patch attack.\n\nRecommendation:\nThe paper proposes a novel and effective approach to designing APAs that can transfer to unseen models. While there are some limitations and challenges to the approach, the authors provide a solid foundation for future research in this area. With additional experiments and analysis, the approach has the potential to be widely adopted in various applications.\n\n$$\\begin{aligned}\n$$\n$$\\begin{aligned}\n$$\n$$\\begin{aligned}\n$$\n$$\\begin{aligned}\n$$\n$$\\begin{aligned}\n$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces a novel approach for designing adversarial patches (APA) that leverage large-scale attack transferability, focusing on image classification. The proposed method uses optimal transport to push the feature distribution of attacked images towards an already modeled distribution, resulting in better transferable patches. The approach is evaluated on a large ensemble of network architectures, including classical CNNs, recent CNNs, and Vision Transformers, demonstrating its effectiveness in fooling multiple models and physical systems without explicit knowledge of the target architecture or weights.\n\nStrengths:\n1. Novel approach: The use of optimal transport for designing APA is a novel and effective approach, reducing patch overfitting to the source architecture and strengthening transferability.\n2. Comprehensive evaluation: The paper evaluates the approach on a large ensemble of network architectures, demonstrating its effectiveness in fooling multiple models.\n3. Physical experiments: The paper presents physical experiments demonstrating the criticality of the attack even for robustified networks and the potentially harmful behavior of the patch in the real-world.\n4. Ablation studies: The paper presents ablation studies investigating the impact of the choice of targeted layers, power p, and number of slices K, providing insights into the optimization process.\n\nWeaknesses:\n1. Computational complexity: The optimal transport approach used in the paper can be computationally expensive, especially for large datasets.\n2. Dependence on hyperparameters: The performance of the approach depends on the choice of hyperparameters, such as the number of slices K and the power p.\n3. Limited evaluation on defense mechanisms: While the paper presents physical experiments, the evaluation of the approach on defense mechanisms, such as Local Gradients Smoothing, is limited.\n4. No discussion on adversarial examples: The paper does not discuss the relationship between APA and adversarial examples, which is a critical area of research in adversarial attacks.\n\nSuggestions for Improvement:\n1. Improve computational efficiency: Develop more efficient algorithms for computing optimal transport distances, reducing the computational complexity of the approach.\n2. Provide more extensive evaluation on defense mechanisms: Include a more thorough evaluation of the approach on various defense mechanisms, such as Local Gradients Smoothing, to assess its robustness.\n3. Discuss the relationship with adversarial examples: Investigate the connection between APA and adversarial examples, potentially leading to a better understanding of the underlying mechanisms.\n4. Provide more insights into the optimization process: Include more in-depth analysis of the optimization process, particularly the role of hyperparameters, to improve the approach's robustness and performance.\n\nRecommendation:\nThe paper presents a novel approach for designing APA that leverages optimal transport, demonstrating its effectiveness in fooling multiple models and physical systems. While there are some limitations, the approach has the potential to improve the robustness and transferability of APA. To strengthen the approach, the authors should address the computational complexity, dependence on hyperparameters, limited evaluation on defense mechanisms, and lack of discussion on adversarial examples.",
        "gr_score_with": 0.46172248803827753,
        "gr_score_without": 0.3726541554959786,
        "or_score_with": 0.45454545454545453,
        "or_score_without": 0.517426273458445,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 139,
            "green_fraction": 0.3726541554959786,
            "num_orange_tokens": 193,
            "orange_fraction": 0.517426273458445,
            "green_z_score": -1.078053055939695,
            "orange_z_score": 121.51151000991447,
            "green_p_value": 0.8594949602052937,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 418,
            "num_green_tokens": 193,
            "green_fraction": 0.46172248803827753,
            "num_orange_tokens": 190,
            "orange_fraction": 0.45454545454545453,
            "green_z_score": 2.5758818808545594,
            "orange_z_score": 112.7973386916638,
            "green_p_value": 0.0049992397971498365,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "TENSOR TRUST: INTERPRETABLE PROMPT INJECTION ATTACKS",
        "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \u201cdefenses\u201d against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release data and code at [removed].",
        "peer_review_without_watermark": "$$$Paper Summary:\nThe paper presents a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based defenses against prompt injection, all created by players of an online game called Tensor Trust. The dataset is the largest of its kind for instruction-following Large Language Models (LLMs). The authors use the dataset to create two benchmarks for prompt injection robustness: one for prompt extraction and one for prompt hijacking. They also analyze the attack strategies used by players and find that some strategies generalize to deployed LLM-based applications.\n\nStrengths:\n1. The dataset is the largest of its kind for prompt injection attacks and defenses.\n2. The authors use a human-generated attack dataset, which provides more realistic and interpretable attack scenarios.\n3. The paper introduces two benchmarks for prompt injection robustness, which can be used to evaluate the robustness of LLMs.\n4. The authors analyze the attack strategies used by players and provide insights into the vulnerabilities of LLMs.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the strengths and limitations of the proposed benchmarks.\n2. The authors assume that the dataset is representative of the attack strategies used by players, but this may not be the case.\n3. The paper does not provide a comprehensive comparison of the proposed benchmarks with existing benchmarks for LLM robustness.\n4. The authors do not discuss the potential applications and limitations of the proposed benchmarks in real-world scenarios.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the strengths and limitations of the proposed benchmarks.\n2. The authors should discuss the potential applications and limitations of the proposed benchmarks in real-world scenarios.\n3. The authors should provide a more comprehensive comparison of the proposed benchmarks with existing benchmarks for LLM robustness.\n4. The authors should consider adding more attack strategies to the dataset to make it more representative of real-world scenarios.\n\nRecommendation:\nThe paper provides a valuable contribution to the field of LLM robustness, and the proposed dataset and benchmarks can be used to evaluate the robustness of LLMs. However, the paper needs further improvement to provide a more comprehensive analysis of the strengths and limitations of the proposed benchmarks and their potential applications and limitations in real-world scenarios.",
        "peer_review_with_watermark": "$$\n\n$$ Peer Review $$\n\n\n$$ Paper Summary: $$ \nThe paper introduces a new dataset, called Tensor Trust, which contains over 126,000 prompt injection attacks and 46,000 prompt-based defenses against prompt injection, all created by players of an online game called Tensor Trust. The attacks in the dataset have easily interpretable structure, which sheds light on the weaknesses of Large Language Models (LLMs). The authors use the dataset to create two benchmarks for prompt injection robustness, which show that many models are vulnerable to attack strategies in the dataset.\n\n\n$$ Strengths: $$ \n1. The paper introduces a new and large dataset of prompt injection attacks, which is a significant contribution to the field of LLM security.\n2. The use of a game-like setting to collect data allows for a diverse range of attack strategies to be captured, which can be more representative of real-world scenarios.\n3. The two benchmarks introduced in the paper provide a clear and quantitative evaluation of prompt injection robustness in LLMs.\n\n\n$$ Weaknesses: $$ \n1. The dataset is limited to a specific online game, which may not be representative of other scenarios or applications.\n2. The attacks in the dataset may be more easily crafted due to the game-like setting, which could make them less effective in more realistic scenarios.\n3. The paper could have provided more context on the specific LLMs used in the study, which could help to explain the results.\n\n\n$$ Suggestions for Improvement: $$ \n1. The dataset could be expanded to include more diverse scenarios and applications.\n2. The attacks in the dataset could be more representative of real-world scenarios, which could make them more effective in practical applications.\n3. The paper could have included more context on the specific LLMs used in the study, which could help to explain the results.\n\n\n$$ Recommendation: $$ \nThe paper introduces a significant contribution to the field of LLM security, with a new and large dataset of prompt injection attacks and two benchmarks for prompt injection robustness. The results show that many models are vulnerable to attack strategies in the dataset, which highlights the need for more robust prompt injection mitigations. The findings of this study are useful for future work on evaluating prompt injection mitigations, red-teaming LLM-integrated applications, and understanding multi-step attack strategies to build better defenses.",
        "gr_score_with": 0.5083612040133779,
        "gr_score_without": 0.3667953667953668,
        "or_score_with": 0.4414715719063545,
        "or_score_without": 0.525096525096525,
        "output_without": {
            "num_tokens_scored": 259,
            "num_green_tokens": 95,
            "green_fraction": 0.3667953667953668,
            "num_orange_tokens": 136,
            "orange_fraction": 0.525096525096525,
            "green_z_score": -1.0907945222784594,
            "orange_z_score": 98.06495364526842,
            "green_p_value": 0.862318346337387,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 152,
            "green_fraction": 0.5083612040133779,
            "num_orange_tokens": 132,
            "orange_fraction": 0.4414715719063545,
            "green_z_score": 3.8247565336640728,
            "orange_z_score": 88.34986879420185,
            "green_p_value": 6.545069068768172e-05,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "FAITHFUL RULE EXTRACTION FOR DIFFERENTIABLE RULE LEARNING MODELS",
        "abstract": "There is increasing interest in methods for extracting interpretable rules from ML models trained to solve a wide range of tasks over knowledge graphs (KGs), such as KG completion, node classification, question answering and recommendation. Many such approaches, however, lack formal guarantees establishing the precise relationship between the model and the extracted rules, and this lack of assurance becomes especially problematic when the extracted rules are applied in safetycritical contexts or to ensure compliance with legal requirements. Recent research has examined whether the rules derived from the influential NEURAL-LP model exhibit soundness (or completeness), which means that the results obtained by applying the model to any dataset always contain (or are contained in) the results obtained by applying the rules to the same dataset. In this paper, we extend this analysis to the context of DRUM, an approach that has demonstrated superior practical performance. After observing that the rules currently extracted from a DRUM model can be unsound and/or incomplete, we propose a novel algorithm where the output rules, expressed in an extension of Datalog, ensure both soundness and completeness. This algorithm, however, can be inefficient in practice and hence we propose additional constraints to DRUM models facilitating rule extraction, albeit at the expense of reduced expressive power.",
        "peer_review_without_watermark": "$$\\boxed{\\text{The paper presents a novel algorithm for extracting faithful Datalog rules from DRUM models, which has been demonstrated to be effective on a range of benchmark datasets. The authors propose two approaches to address the computational challenge of extracting faithful rules: one that extracts a subset of rules for a fixed dataset and another that limits the expressivity of the model. The proposed algorithms are shown to be sound, complete, and faithful, and their time complexity is analyzed. The authors also provide empirical results demonstrating the effectiveness of the proposed approach on various benchmark datasets. The work is of high quality, and the authors have provided a thorough analysis of the challenges and limitations of the approach. However, there are some minor issues with the presentation and clarity of the paper.}}$$",
        "peer_review_with_watermark": "$$\\boxed{\\text{Summary: The paper presents a novel approach to extract faithful Datalog rules from Deep Learning Models (DLMs) trained on knowledge graphs (KGs), ensuring soundness and completeness. The authors extend the concept of soundness and completeness to the context of DRUM, an approach that has demonstrated superior practical performance. The proposed algorithm extracts faithful programs with inequalities and disjunctions, enabling the necessary counting operations. The authors demonstrate the practical feasibility of the rule extraction algorithm and show that the extracted programs can faithfully represent the underlying models. The authors also propose two approaches for addressing the challenge of extracting faithful programs from DRUM models: one by limiting model expressivity and another by extracting a subset of sound rules that explain all model predictions. Experimental results show that the proposed approach outperforms state-of-the-art baselines and demonstrate the practical viability of faithful rule extraction for DLMs.}}$$",
        "gr_score_with": 0.45180722891566266,
        "gr_score_without": 0.32857142857142857,
        "or_score_with": 0.5301204819277109,
        "or_score_without": 0.6071428571428571,
        "output_without": {
            "num_tokens_scored": 140,
            "num_green_tokens": 46,
            "green_fraction": 0.32857142857142857,
            "num_orange_tokens": 85,
            "orange_fraction": 0.6071428571428571,
            "green_z_score": -1.7251638983558855,
            "orange_z_score": 72.75888820855936,
            "green_p_value": 0.9577510301380352,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 166,
            "num_green_tokens": 75,
            "green_fraction": 0.45180722891566266,
            "num_orange_tokens": 88,
            "orange_fraction": 0.5301204819277109,
            "green_z_score": 1.3625071386276266,
            "orange_z_score": 69.01631507400357,
            "green_p_value": 0.08651894641098135,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
        "abstract": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes Symphony, a novel autoregressive generative model for 3D molecular geometries that uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The model is evaluated on the QM9 dataset and shows promising results on molecular generation and completion, outperforming existing autoregressive models and approaching the performance of diffusion models.\n\nStrengths:\n- The paper introduces a novel approach to 3D molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections.\n- The model is evaluated on the QM9 dataset and shows promising results on molecular generation and completion.\n- The paper discusses the challenges of 3D molecular generation and proposes a new approach to address these challenges.\n\nWeaknesses:\n- The model is computationally expensive due to the use of tensor products to create higher degree E(3)-equivariant features.\n- The discretization of the radial components is too coarse, which affects the accuracy of the bond length distributions.\n- The model is not able to generalize well to unseen fragments, as the valid completion rate decreases as training progresses.\n\nSuggestions for Improvement:\n- Explore normalizing flows to smoothly model the radial distribution without any discretization.\n- Place entire local environment motifs at once to speed up generation.\n- Investigate the data efficiency of the model and explore ways to reduce the number of training epochs required.\n\nRecommendation:\n- The paper proposes a novel approach to 3D molecular generation and evaluation, and the results show promising results on molecular generation and completion. However, the model has some limitations, such as computational expense and poor generalization to unseen fragments. Further exploration and improvement of the model are needed to make it a competitive method for 3D molecular generation.\n\nAuthor's Response to Reviewer Comments:\n- The reviewer's comments on the computational expense and discretization of the radial components are valid and should be addressed in future work.\n- The reviewer's suggestion to explore normalizing flows and place entire local environment motifs at once is also a good idea and should be investigated further.\n- The reviewer's comment on the data efficiency of the model is also valid and should be explored further.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe authors propose Symphony, a novel autoregressive generative model for 3D molecular geometries that utilizes higher-degree E(3)-equivariant features and spherical harmonic projections. Symphony builds molecules sequentially by predicting and sampling atom types and locations of new atoms based on conditional probability distributions informed by previously placed atoms. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics, including molecular generation and completion, validity, and generalization capabilities.\n\nStrengths:\n1. Novel use of higher-degree E(3)-equivariant features and spherical harmonic projections for molecular geometry generation.\n2. Symphony's ability to accurately model the 3D geometry of molecules, capturing both local and global symmetries.\n3. The authors' use of a comprehensive evaluation protocol, including metrics such as validity, generalization capabilities, and molecular completion rates.\n\nWeaknesses:\n1. The authors' model requires significant computational resources, particularly for training, which can be a bottleneck for larger molecules.\n2. The discretization of radial components may lead to coarser bond length distributions, affecting the model's accuracy when using lookup tables for bond order assignment.\n3. The authors' model may not generalize well to unseen fragments, particularly for larger molecules, due to its reliance on conditional probability distributions informed by previously placed atoms.\n\nSuggestions for Improvement:\n1. Explore data efficiency improvements, such as using normalizing flows to model the radial distribution without discretization, or placing entire local environment motifs at once to speed up generation.\n2. Investigate the use of more advanced evaluation metrics, such as those incorporating bond order accuracy, to better assess the model's performance.\n3. Develop more sophisticated methods for handling the challenges of molecular completion, such as using more advanced sampling schemes or incorporating additional contextual information.\n\nRecommendation:\nBased on the authors' promising results, Symphony appears to be a promising approach for autoregressively generating 3D molecular geometries. However, further improvements are needed to address the model's computational costs, discretization limitations, and generalization capabilities. With continued development and evaluation, Symphony has the potential to become a leading method for molecular geometry generation.",
        "gr_score_with": 0.4797687861271676,
        "gr_score_without": 0.3184713375796178,
        "or_score_with": 0.4595375722543353,
        "or_score_without": 0.5477707006369427,
        "output_without": {
            "num_tokens_scored": 314,
            "num_green_tokens": 100,
            "green_fraction": 0.3184713375796178,
            "num_orange_tokens": 172,
            "orange_fraction": 0.5477707006369427,
            "green_z_score": -2.948964336538186,
            "orange_z_score": 114.79180436523527,
            "green_p_value": 0.99840579616391,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 166,
            "green_fraction": 0.4797687861271676,
            "num_orange_tokens": 159,
            "orange_fraction": 0.4595375722543353,
            "green_z_score": 3.028763841012414,
            "orange_z_score": 100.83769307266698,
            "green_p_value": 0.0012277827679907196,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "MAXIMUM ENTROPY MODEL CORRECTION",
        "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model\u2019s next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna\u2019s convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes a novel approach to alleviate model errors in reinforcement learning (RL) by developing a cost-efficient framework called MaxEnt model correction. This framework adopts maximum entropy density estimation to reduce model errors given a small number of queries to the true dynamics. The authors provide a thorough theoretical analysis that shows the framework can significantly accelerate the convergence rate of policy evaluation and control algorithms, and ensure convergence to the true value functions despite model errors if said errors are sufficiently small. They also develop a sample-based variant, MoCoDyna, which extends the Dyna framework.\n\nStrengths:\n\n* The paper provides a novel and cost-efficient approach to alleviate model errors in RL.\n* The theoretical analysis is thorough and provides a clear understanding of the framework's performance.\n* The authors provide a sample-based variant, MoCoDyna, which extends the Dyna framework.\n\nWeaknesses:\n\n* The paper assumes that the model errors are sufficiently small, which may not be the case in all RL scenarios.\n* The framework's performance may be affected by the choice of basis functions and the number of queries to the true dynamics.\n* The paper does not provide a detailed analysis of the framework's performance in deep RL applications.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the framework's performance in deep RL applications.\n* They should also investigate the effect of different basis functions and query strategies on the framework's performance.\n* The paper could benefit from a more comprehensive comparison with existing RL algorithms.\n\nRecommendation:\n\n* The paper is well-written and provides a clear and concise overview of the MaxEnt model correction framework.\n* The authors' approach has the potential to alleviate model errors in RL, and their theoretical analysis provides a solid foundation for further research.\n* With some additional analysis and comparison with existing algorithms, the paper could be even more effective in demonstrating the framework's potential. \n\n$$$",
        "peer_review_with_watermark": "$$$\\\\\n\\\\ Paper Summary: \n\\\\ The paper presents a framework for alleviating model errors in model-based reinforcement learning by adopting maximum entropy density estimation. The authors propose the MaxEnt model correction (MoCo) algorithm, which corrects the approximate model by minimizing the KL-divergence between the approximate and true dynamics. The authors also introduce a sample-based variant, MoCoDyna, which extends the Dyna framework. The paper provides a thorough theoretical analysis and numerical experiments to demonstrate the effectiveness of the MoCo framework.\n\n\\\\ Strengths:\n\\\\ The paper provides a clear and well-organized presentation of the MoCo framework, including its theoretical analysis and numerical experiments. The authors demonstrate the effectiveness of the MoCo framework in alleviating model errors and accelerating convergence. The paper also highlights the importance of model error in model-based reinforcement learning and provides a thorough analysis of its impact.\n\n\\\\ Weaknesses:\n\\\\ The paper assumes that the approximate model is learned using a Maximum Likelihood Estimate (MLE), which may not be the case in practice. The authors also do not provide a clear discussion of the choice of basis functions and their implications on the convergence rate. The paper also relies on some technical details that may be hard to understand without prior knowledge of maximum entropy density estimation.\n\n\\\\ Suggestions for Improvement:\n\\\\ The authors should provide a clear discussion of the choice of basis functions and their implications on the convergence rate. They should also provide a more detailed analysis of the choice of hyperparameters, such as \u03b2, and its impact on the convergence rate. The authors should also provide a more thorough discussion of the implications of the MoCo framework on the convergence of policy evaluation and control algorithms.\n\n\\\\ Recommendation:\n\\\\ The paper presents a well-organized and clear presentation of the MoCo framework, which demonstrates its effectiveness in alleviating model errors and accelerating convergence. The authors provide a thorough theoretical analysis and numerical experiments, which highlights the importance of model error in model-based reinforcement learning. The paper provides a clear and well-organized presentation of the MoCo framework, which makes it a valuable contribution to the field of reinforcement learning. Therefore, I recommend that the paper be published as is, with some minor revisions to address the suggestions for improvement. \n\n\\\\ $$$\\\\",
        "gr_score_with": 0.5422535211267606,
        "gr_score_without": 0.3805970149253731,
        "or_score_with": 0.40492957746478875,
        "or_score_without": 0.48134328358208955,
        "output_without": {
            "num_tokens_scored": 268,
            "num_green_tokens": 102,
            "green_fraction": 0.3805970149253731,
            "num_orange_tokens": 129,
            "orange_fraction": 0.48134328358208955,
            "green_z_score": -0.6483810684699618,
            "orange_z_score": 97.431634982361,
            "green_p_value": 0.7416307440802362,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 284,
            "num_green_tokens": 154,
            "green_fraction": 0.5422535211267606,
            "num_orange_tokens": 115,
            "orange_fraction": 0.40492957746478875,
            "green_z_score": 4.893465989426242,
            "orange_z_score": 84.1606311478892,
            "green_p_value": 4.953772120900986e-07,
            "orange_p_value": 0.0
        }
    },
    {
        "title": "A PRECISE CHARACTERIZATION OF SGD STABILITY USING LOSS SURFACE GEOMETRY",
        "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$Paper Summary: \nThe paper investigates the relationship between linear stability and sharpness in Stochastic Gradient Descent (SGD). The authors introduce a novel coherence measure of the loss Hessian that encapsulates the geometric properties of the loss function relevant to the linear stability of SGD. They provide a precise characterization of how the SGD hyperparameters and properties of the loss function affect its implicit regularization of model sharpness.\n\n$$\\text{Strengths}$$\n\n*   The paper provides a novel coherence measure of the loss Hessian that captures the geometric properties of the loss function relevant to the linear stability of SGD.\n*   The authors provide a precise characterization of how the SGD hyperparameters and properties of the loss function affect its implicit regularization of model sharpness.\n*   The paper includes a series of experiments that support the theoretical results and provide insight into the behavior of SGD in various scenarios.\n\n$$\\text{Weaknesses}$$\n\n*   The paper does not provide a clear explanation of the coherence measure and its relevance to the linear stability of SGD.\n*   The authors assume that the loss function is additively decomposable, but do not provide a clear justification for this assumption.\n*   The paper does not provide a clear comparison to prior work in the field.\n\n$$\\text{Suggestions for Improvement}$$\n\n*   The authors should provide a clear explanation of the coherence measure and its relevance to the linear stability of SGD.\n*   The authors should justify the assumption that the loss function is additively decomposable.\n*   The authors should provide a clear comparison to prior work in the field.\n\n$$\\text{Recommendation}$$\n\n*   The paper is well-written and provides a clear and concise overview of the problem and the proposed solution.\n*   The authors provide a series of experiments that support the theoretical results and provide insight into the behavior of SGD in various scenarios.\n*   However, the paper could benefit from a clearer explanation of the coherence measure and its relevance to the linear stability of SGD, as well as a more detailed justification of the assumption that the loss function is additively decomposable.\n\n$$\\text{Overall}$$\n\n*   The paper provides a novel contribution to the field of machine learning, specifically in the area of stochastic optimization.\n*   The authors provide a clear and concise overview of the problem and the proposed solution.\n*   However, the paper could benefit from a clearer explanation of the coherence measure and its relevance to the linear stability of SGD, as well as a more detailed justification of the assumption that the loss function is additively decomposable.",
        "peer_review_with_watermark": "$$$$\n\nPaper Title: A Precise Characterization of SGD Stability using Loss Surface Geometry\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors introduce a novel coherence measure, \u03c3, that captures relevant geometric properties of the loss function that are relevant to linear stability of SGD. The paper presents two main contributions: a sufficient condition for linear instability at a minimum and a nearly optimal condition for divergence. The authors also present a series of experiments to validate the theoretical results.\n\nStrengths:\n\n1. The paper provides a precise characterization of SGD stability, which is a long-standing open problem in the field.\n2. The introduction of the coherence measure, \u03c3, is a novel contribution that captures relevant geometric properties of the loss function.\n3. The paper presents a sufficient condition for linear instability at a minimum, which is a useful tool for understanding the behavior of SGD.\n4. The experiments provided in the paper validate the theoretical results and demonstrate the importance of considering the geometry of the loss surface in understanding the behavior of SGD.\n\nWeaknesses:\n\n1. The paper relies on several technical assumptions, such as the use of additively decomposable loss functions and the assumption that the Hessian of the loss function is positive semi-definite.\n2. The paper does not provide a clear comparison with other methods for characterizing SGD stability.\n3. The experimental results, while supportive, do not provide a comprehensive understanding of the relationship between the coherence measure, \u03c3, and SGD stability.\n4. The paper does not discuss potential applications of the coherence measure, \u03c3, in other areas of machine learning.\n\nSuggestions for Improvement:\n\n1. The paper could benefit from a more thorough discussion of the assumptions made and their implications for the results.\n2. The paper could include a more detailed comparison with other methods for characterizing SGD stability.\n3. The experimental results could be more comprehensive, including a more thorough analysis of the relationship between the coherence measure, \u03c3, and SGD stability.\n4. The paper could discuss potential applications of the coherence measure, \u03c3, in other areas of machine learning, such as neural network optimization and regularization.\n\nRecommendation:\n\nOverall, the paper presents a precise characterization of SGD stability using loss surface geometry, which is a significant contribution to the field. However, the paper could benefit from a more thorough discussion of the assumptions made and their implications for the results, as well as a more comprehensive comparison with other methods for characterizing SGD stability. The experimental results, while supportive, do not provide a comprehensive understanding of the relationship between the coherence measure, \u03c3, and SGD stability. I recommend that the authors revise the paper to address these concerns and provide a more thorough and comprehensive understanding of the relationship between the coherence measure, \u03c3, and SGD stability.\n\n$$$$\n\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors introduce a novel coherence measure, \u03c3, that captures relevant geometric properties of the loss function that are relevant to linear stability of SGD. The paper presents two main contributions: a sufficient condition for linear instability at a minimum and a nearly optimal condition for divergence. The authors also present a series of experiments to validate the theoretical results.\n\n$$$$\n\nStrengths:\n\n1. The paper provides a precise characterization of SGD stability, which is a long-standing open problem in the field.\n2. The introduction of the coherence measure, \u03c3, is a novel contribution that captures relevant geometric properties of the loss function.\n3. The paper presents a sufficient condition for linear instability at a minimum, which is a useful tool for understanding the behavior of SGD.\n4. The experiments provided in the paper validate the theoretical results and demonstrate the importance of considering the geometry of the loss surface in understanding the behavior of SGD.\n\n$$$$\n\nWeaknesses:\n\n1. The paper relies on several technical assumptions, such as the use of additively decomposable loss functions and the assumption that the Hessian of the loss function is positive semi-definite.\n2. The paper does not provide a clear comparison with other methods for characterizing SGD stability.\n3. The experimental results, while supportive, do not provide a comprehensive understanding of the relationship between the coherence measure, \u03c3, and SGD stability.\n4. The paper does not discuss potential applications of the coherence measure, \u03c3, in other areas of machine learning.\n\n$$$$\n\nSuggestions for Improvement:\n\n1. The paper could benefit from a more thorough discussion of the assumptions made and their implications for the results.\n2. The paper could include a more detailed comparison with other methods for characterizing SGD stability.\n3. The experimental results could be more comprehensive, including a more thorough analysis of the relationship between the coherence measure, \u03c3, and SGD stability.\n4. The paper could discuss potential applications of the coherence measure, \u03c3, in other areas of machine learning, such as neural network optimization and regularization.\n\n$$$$\n\nRecommendation:\n\nOverall, the paper presents a precise characterization of SGD stability using loss surface geometry, which is a significant contribution to the field. However, the paper could benefit from a more thorough discussion of the assumptions made and their implications for the results, as well as a more comprehensive comparison with other methods for characterizing SGD stability. The experimental results, while supportive, do not provide a comprehensive understanding of the relationship between the coherence measure, \u03c3, and SGD stability. I recommend that the authors revise the paper to address these concerns and provide a more thorough and comprehensive understanding of the relationship between the coherence measure, \u03c3, and SGD stability.",
        "gr_score_with": 0.45794392523364486,
        "gr_score_without": 0.3920704845814978,
        "or_score_with": 0.46105919003115264,
        "or_score_without": 0.47577092511013214,
        "output_without": {
            "num_tokens_scored": 227,
            "num_green_tokens": 89,
            "green_fraction": 0.3920704845814978,
            "num_orange_tokens": 108,
            "orange_fraction": 0.47577092511013214,
            "green_z_score": -0.2438675165715337,
            "orange_z_score": 80.8284285775546,
            "green_p_value": 0.5963332861191635,
            "orange_p_value": 0.0
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 147,
            "green_fraction": 0.45794392523364486,
            "num_orange_tokens": 148,
            "orange_fraction": 0.46105919003115264,
            "green_z_score": 2.119116373776455,
            "orange_z_score": 93.09692148997209,
            "green_p_value": 0.01704031664091958,
            "orange_p_value": 0.0
        }
    }
]