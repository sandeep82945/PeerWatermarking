[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. The CMA is designed to reason over a dataset and its associated data-generating process, and it is evaluated on a number of benchmarks and a real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). The experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery.\n\nStrengths:\n\n* The paper presents a novel and innovative approach to causal discovery that combines the strengths of LLMs and DSCMs.\n* The CMA is evaluated on a range of benchmarks and a real-world task, demonstrating its effectiveness in different scenarios.\n* The paper provides a clear and well-structured explanation of the CMA framework and its components.\n* The authors provide a detailed analysis of the results and insights gained from the experiments.\n\nWeaknesses:\n\n* The paper assumes that the causal graph is known a priori, which may not be the case in real-world scenarios.\n* The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. While the authors extend the framework to include chain graph elements, human intervention is necessary to allow appropriate Deep Chain Graph modelling.\n* The paper does not provide a clear explanation of how the LLMs are used to produce hypotheses and how they are integrated with the DSCMs.\n* The paper assumes that the metadata is accurate and complete, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of how the LLMs are used to produce hypotheses and how they are integrated with the DSCMs.\n* The authors should investigate techniques to enable fully automated chain graph modelling and extend deep SCMs to represent more flexible, non-Markovian causal graphs.\n* The authors should provide a more detailed analysis of the limitations of the CMA and how they can be addressed in future work.\n* The authors should provide a more detailed explanation of the results and insights gained from the experiments, including a more detailed analysis of the causal relationships between variables.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to causal discovery that combines the strengths of LLMs and DSCMs. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the CMA framework and its components, and the experimental results demonstrate its effectiveness in different scenarios. I recommend that the paper be accepted for publication, with the suggestion that the authors address the areas for improvement mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for the task of causal discovery. The CMA aims to synergize metadata-based reasoning capabilities of LLMs with data-driven modelling of DSCMs. The authors evaluate the CMA's performance on synthetic experiments and real-world tasks, including Alzheimer's Disease (AD) phenotyping.\n\nStrengths:\n1. The CMA offers a unified framework for causal discovery that combines LLMs with deep probabilistic graphical models, enabling the efficient exploration of causal graph space.\n2. The proposed RAG pattern enables the retrieval of domain-specific context to encourage or enforce specific relationships according to expertise.\n3. The CMA outperforms previous purely data-driven or metadata-driven approaches to causal discovery in various benchmarks.\n\nWeaknesses:\n1. The CMA relies on DSCMs to fit data, which assumes a Markovian DAG, which might not always be sufficient for complex causal structures.\n2. The proposed RAG pattern requires careful adaptation to specific domains, which might be challenging in practice.\n\nSuggestions for Improvement:\n1. Explore techniques to enable fully automated chain graph modelling, which would allow the CMA to handle more flexible, non-Markovian causal graphs.\n2. Extend the RAG pattern to incorporate other retrieval methods, enabling the adaptation of expertise to various domains.\n\nRecommendation:\nThe proposed CMA offers a powerful framework for causal discovery, which outperforms previous methods in various benchmarks. While its reliance on DSCMs and RAG pattern adaptation are important limitations, its benefits are substantial. Therefore, I recommend this paper for publication, with careful consideration of its proposed avenues for future research.\n\nSignificance:\nThe proposed CMA addresses important challenges in causal discovery, enabling the efficient exploration of causal graph space while outperforming previous methods. Its significance lies in its potential applications in scientific discovery, which would be revolutionized by accurate identification of causal structures.\n\nClarity:\nThe paper is clearly written, providing sufficient background information and thorough exposition of the proposed methods.\n\nOriginality:\nThe proposed RAG pattern and its adaptation are important novelties, enabling expertise retrieval in various domains.\n\nRelation to Previous Work:\nThe proposed CMA builds upon previous methods, while outperforming them in various benchmarks.\n\nBroader Impact:\nThe proposed CMA offers a powerful tool for scientific discovery, which would revolutionize various scientific fields.\n\n$$$",
        "gr_score_with": 0.06299212598425197,
        "gr_score_without": 0.06741573033707865,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 24,
            "green_fraction": 0.06741573033707865,
            "z_score": -2.0493292346789627,
            "p_value": 0.9797850340908052
        },
        "output_with": {
            "num_tokens_scored": 381,
            "num_green_tokens": 24,
            "green_fraction": 0.06299212598425197,
            "z_score": -2.4078829420192323,
            "p_value": 0.9919773376021226
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a framework for learning language-informed visual concepts from images by distilling pre-trained vision-language models. The framework consists of a set of concept encoders that extract visual concept embeddings from images along various concept axes specified by language. The concept embeddings are trained to be disentangled and compositional, allowing for flexible composition of concepts into high-quality image generations. The framework is evaluated on a range of tasks, including visual concept extraction, recomposition, and extrapolation, as well as text-based visual concept editing.\n\nStrengths:\n\n1. The paper proposes a novel framework for learning language-informed visual concepts from images, which is a significant contribution to the field of computer vision and artificial intelligence.\n2. The framework is evaluated on a range of tasks, including visual concept extraction, recomposition, and extrapolation, as well as text-based visual concept editing, which demonstrates its versatility and effectiveness.\n3. The paper provides a thorough evaluation of the framework, including quantitative and qualitative results, which provides a comprehensive understanding of its strengths and limitations.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained vision-language models, which may limit its generalizability to other domains and tasks.\n2. The framework assumes that the concept axes are specified by language, which may not always be the case in real-world applications.\n3. The paper does not provide a clear explanation of how the concept embeddings are disentangled and compositional, which may make it difficult for readers to understand the framework.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the concept embeddings are disentangled and compositional, including the mathematical formulation and the role of the text anchoring loss.\n2. The authors should provide more examples of how the framework can be applied to real-world applications, such as image editing and generation.\n3. The authors should provide a more thorough evaluation of the framework, including a comparison with other state-of-the-art methods and a discussion of its limitations and potential future directions.\n\nRecommendation:\n\nI recommend this paper for publication in a top-tier conference in the field of computer vision and artificial intelligence. The paper proposes a novel and effective framework for learning language-informed visual concepts from images, which has the potential to impact a wide range of applications. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more comprehensive and impactful.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary: \nThis paper presents a language-informed approach to learning visual concepts by distilling language semantics from pre-trained vision-language models. It leverages a set of language specified concept axes to extract continuous concept embeddings using inverted language directions. These continuous embeddings can be used to extract common concept structures among various visual instances by using language specified axes rather than using specific examples based approaches common among similar approaches.\n\n\nStrengths: \nThis is one of the most comprehensive papers we read based on language based approaches used to extract continuous concept embeddings using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nOne strength is using language specified axes rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches to extract common concept structures among various visual instances.\n\n\nThis allows language based approaches to extract continuous rather than specific examples based approach.\n\n\nThis is one approach among language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar approaches.\n\n\nThis allows language based approaches using inverted language directions rather than using specific examples based approaches common among similar",
        "gr_score_with": 0.07017543859649122,
        "gr_score_without": 0.06116207951070336,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 20,
            "green_fraction": 0.06116207951070336,
            "z_score": -2.341038682612835,
            "p_value": 0.9903849127168415
        },
        "output_with": {
            "num_tokens_scored": 114,
            "num_green_tokens": 8,
            "green_fraction": 0.07017543859649122,
            "z_score": -1.0614639197925866,
            "p_value": 0.8557604392829374
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a two-stage Reinforced Rationale-Prompted Paradigm (Rapper) for natural language explanation (NLE) in visual question answering (VQA) tasks. The first stage utilizes knowledge distillation from large language models (LLMs) to infuse rationale-prompting and encourage plausible NLE. The second stage employs Reinforcement Learning from NLE Feedback (RLNF) to inject visual facts into NLE generation. The proposed method achieves state-of-the-art performance on two VL-NLE benchmarks, VQA-X and e-SNLI-VE, and demonstrates improved plausibility and faithfulness of NLE.\n\nStrengths:\n\n1. The paper presents a comprehensive approach to tackle the implausibility and hallucination problems in NLE.\n2. The proposed two-stage paradigm is well-motivated and effectively addresses the limitations of existing methods.\n3. The use of knowledge distillation and RLNF is innovative and demonstrates the effectiveness of these techniques in improving NLE.\n4. The paper provides thorough experiments and evaluation metrics, including quantitative and qualitative results, to demonstrate the superiority of Rapper.\n\nWeaknesses:\n\n1. The paper assumes that the ground-truth answer and explanation are available during training, which may not be the case in all scenarios.\n2. The filtering mechanism used in knowledge distillation may not be effective in all cases, and its impact on performance is not thoroughly evaluated.\n3. The paper does not provide a detailed analysis of the RLNF algorithm and its convergence properties.\n4. The qualitative evaluation results, although impressive, are limited to a few examples and may not be representative of the overall performance.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of different LLMs and knowledge distillation techniques on the performance of Rapper.\n2. Evaluate the robustness of Rapper to variations in the input data, such as image quality or question complexity.\n3. Provide a more detailed analysis of the RLNF algorithm, including its convergence properties and the impact of hyperparameters on performance.\n4. Conduct a more thorough qualitative evaluation, including more examples and a systematic comparison with existing methods.\n\nRecommendation:\n\nThe paper presents a well-motivated and effective approach to NLE in VQA tasks. The proposed two-stage paradigm is innovative and demonstrates the superiority of Rapper over existing methods. However, the paper assumes that the ground-truth answer and explanation are available during training, which may not be the case in all scenarios. Additionally, the filtering mechanism used in knowledge distillation may not be effective in all cases, and its impact on performance is not thoroughly evaluated. Nevertheless, the paper provides thorough experiments and evaluation metrics, and the qualitative evaluation results are impressive. Therefore, I recommend accepting the paper with minor revisions to address the mentioned weaknesses.\n\n$$$",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe authors proposed a new method, named RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm, for visual question answering (VQA) with a focus on providing accurate answers and sufficient natural language explanations (NLE) that are plausible and faithful. The proposed method exploits knowledge learned from Large Language Models (LLMs) and visual content from input images through knowledge distillation and a new technique named Reinforcement Learning from NLE Feedback (RLNF). The authors evaluated their method on two visual NLE benchmarks and demonstrated that it outperforms previous methods in NLE-related metrics, as well as in assessing the plausibility and faithfulness of NLE.\n\n**Strengths**\n\n1. The authors proposed a new method, named RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm, for visual question answering (VQA) with a focus on providing accurate answers and sufficient natural language explanations (NLE) that are plausible and faithful.\n2. The authors proposed a new technique named Reinforcement Learning from NLE Feedback (RLNF), which exploits knowledge learned from Large Language Models (LLMs) and visual content from input images through knowledge distillation and a new learning strategy named knowledge distillation.\n3. The authors evaluated their method on two visual NLE benchmarks and demonstrated that it outperforms previous methods in NLE-related metrics, as well as in assessing the plausibility and faithfulness of NLE.\n4. The authors proposed a new method named knowledge distillation, which exploits knowledge learned from Large Language Models (LLMs) through a knowledge distillation process named knowledge distillation.\n\n**Weaknesses**\n\n1. The authors proposed a new method named knowledge distillation, which exploits knowledge learned from Large Language Models (LLMs) through a knowledge distillation process named knowledge distillation. However, there is a possibility that knowledge distillation might introduce hallucination or implausibility in NLE.\n2. There is a possibility that knowledge distillation or knowledge retrieval methods might introduce hallucination or implausibility in NLE.\n3. There is a possibility that knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval methods or knowledge retrieval methods through knowledge retrieval",
        "gr_score_with": 0.09595959595959595,
        "gr_score_without": 0.08542713567839195,
        "output_without": {
            "num_tokens_scored": 398,
            "num_green_tokens": 34,
            "green_fraction": 0.08542713567839195,
            "z_score": -0.9690924337596992,
            "p_value": 0.833750463772543
        },
        "output_with": {
            "num_tokens_scored": 198,
            "num_green_tokens": 19,
            "green_fraction": 0.09595959595959595,
            "z_score": -0.1895117478716539,
            "p_value": 0.5751541257695392
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method uses a dual Lagrangian view, enabling simultaneous dynamic reconstruction and meaningful scene understanding. By factorizing the scene motion under the Eulerian and Lagrangian views, the model learns to model dynamic objects, enabling meaningful part discovery. The model uses a hybrid representation, comprising feature volumes and light-weight multilayer perceptrons, leading to efficient reconstruction. The model learns to model dynamic objects, enabling meaningful part discovery, and enables efficient reconstruction, even at high resolutions. The model shows promising results, enabling simultaneous dynamic reconstruction, meaningful part discovery, and efficient reconstruction. \n\nStrengths:\n* The model uses a dual Lagrangian view, enabling simultaneous dynamic reconstruction and meaningful scene understanding. \n* The model uses a hybrid representation, comprising feature volumes and light-weight multilayer perceptrons, leading to efficient reconstruction. \n* The model learns to model dynamic objects, enabling meaningful part discovery. \n* The model enables efficient reconstruction, even at high resolutions. \n* The model shows promising results, enabling simultaneous dynamic reconstruction, meaningful part discovery, and efficient reconstruction. \n\nWeaknesses:\n* The model uses explicit storage, which makes it challenging to model motion on very long sequences. \n* Motion modeling at a specific location can be considered as a sequence decoding task, which may be challenging. \n* The model uses post-processing, which may be expensive. \n* The model uses multiple modules, which may be expensive. \n\nSuggestions for Improvement:\n* Develop a unified long sequence encoding-decoding scheme, enabling efficient handling of long sequences. \n* Develop efficient post-processing, enabling efficient handling of multiple modules. \n* Develop efficient storage, enabling efficient handling of explicit storage. \n* Develop efficient handling of multiple modules, enabling efficient handling of multiple modules. \n\nRecommendation:\nThis paper presents a promising model, enabling simultaneous dynamic reconstruction, meaningful part discovery, and efficient reconstruction. While there are potential weaknesses, the model shows promising results, enabling simultaneous dynamic reconstruction, meaningful part discovery, and efficient reconstruction. I recommend this model, enabling simultaneous dynamic reconstruction, meaningful part discovery, and efficient reconstruction, even at high resolutions. $$",
        "gr_score_with": 0.07352941176470588,
        "gr_score_without": 0.06551724137931035,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 19,
            "green_fraction": 0.06551724137931035,
            "z_score": -1.9574007317156783,
            "p_value": 0.9748498137307506
        },
        "output_with": {
            "num_tokens_scored": 204,
            "num_green_tokens": 15,
            "green_fraction": 0.07352941176470588,
            "z_score": -1.2602520756252094,
            "p_value": 0.896210778832188
        }
    },
    {
        "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
        "abstract": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, called Diffusion Model-Based Predictor (DMBP), to enhance the robustness of offline reinforcement learning (RL) algorithms against state observation perturbations. The proposed approach utilizes conditional diffusion models to recover the actual states from perturbed observations, which is fundamentally different from existing online robust RL methods. The framework consists of a diffusion model-based predictor that estimates the current state by reversely denoising the perturbed state, and a non-Markovian loss function that minimizes the sum entropy of denoised states over the RL trajectory to mitigate error accumulation. The authors evaluate the proposed framework on D4RL Gym benchmark and demonstrate its effectiveness in enhancing the robustness of existing offline RL algorithms against different types of attacks on state observations.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to enhancing the robustness of offline RL algorithms against state observation perturbations.\n2. The proposed framework is based on conditional diffusion models, which have been widely used in image generation and other applications.\n3. The non-Markovian loss function is a novel and effective way to mitigate error accumulation during testing.\n4. The experiments on D4RL Gym benchmark demonstrate the effectiveness of the proposed framework in enhancing the robustness of existing offline RL algorithms.\n\nWeaknesses:\n\n1. The paper assumes that the perturbed state observations are Gaussian noises, which may not be realistic in many real-world scenarios.\n2. The proposed framework may not be effective against more complex types of perturbations, such as adversarial attacks.\n3. The non-Markovian loss function may be computationally expensive to compute, especially for large RL trajectories.\n4. The paper does not provide a detailed analysis of the theoretical guarantees of the proposed framework.\n\nSuggestions for Improvement:\n\n1. The authors should investigate more realistic types of perturbations, such as non-Gaussian noises or adversarial attacks, and evaluate the effectiveness of the proposed framework against these types of perturbations.\n2. The authors should provide a more detailed analysis of the computational complexity of the non-Markovian loss function and explore ways to reduce it.\n3. The authors should provide a more detailed analysis of the theoretical guarantees of the proposed framework, such as convergence guarantees and error bounds.\n4. The authors should evaluate the proposed framework on more challenging RL tasks and environments to demonstrate its effectiveness in a wider range of scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to enhancing the robustness of offline RL algorithms against state observation perturbations. The proposed framework is based on conditional diffusion models, which have been widely used in image generation and other applications. The non-Markovian loss function is a novel and effective way to mitigate error accumulation during testing. The experiments on D4RL Gym benchmark demonstrate the effectiveness of the proposed framework in enhancing the robustness of existing offline RL algorithms. However, the paper assumes that the perturbed state observations are Gaussian noises, which may not be realistic in many real-world scenarios. The authors should investigate more realistic types of perturbations and evaluate the effectiveness of the proposed framework against these types of perturbations.\n\nOverall, the paper is well-written, and the proposed framework is well-motivated and effective. However, the authors should address the weaknesses and suggestions for improvement to make the paper more comprehensive and impactful.\n\nRecommendation: Accept.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called Diffusion Model-Based Predictor (DMBP) for improving the robustness of offline reinforcement learning (RL) algorithms against state observation perturbations. The proposed framework uses a conditional diffusion model to recover the actual state from a perturbed observation, which can be particularly useful in real-world applications where sensor errors or adversarial attacks can occur. The paper's contribution can be broadly categorized into three parts: first, a new framework called DMBP that uses a conditional diffusion model to predict the actual state; second, a non-Markovian loss function that can mitigate error accumulation in the denoising process; and third, a state infilling procedure inspired by image inpainting that can deal with incomplete state observations.\n\nStrengths:\n\n1.   The paper makes a clear contribution to the field of offline reinforcement learning by tackling a major limitation of existing offline RL methods, specifically their inability to deal with state observation perturbations.\n2.   The paper's problem statement and related work discussions are well-done, providing a clear sense of the paper's contribution's context and its relation to existing work.\n3.   The paper's introduction discusses a clear problem statement and its relation to related topics such as robustification and exploration in reinforcement learning settings.\n4.   The paper's contribution can be broadly categorized into a clear problem statement, related discussions about related topics such as exploration and evaluation settings, a clear presentation about its contribution's relation to its contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution's contribution",
        "gr_score_with": 0.05042016806722689,
        "gr_score_without": 0.06111111111111111,
        "output_without": {
            "num_tokens_scored": 360,
            "num_green_tokens": 22,
            "green_fraction": 0.06111111111111111,
            "z_score": -2.4595492912420727,
            "p_value": 0.9930444203300761
        },
        "output_with": {
            "num_tokens_scored": 238,
            "num_green_tokens": 12,
            "green_fraction": 0.05042016806722689,
            "z_score": -2.5496013126385133,
            "p_value": 0.9946076914545738
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$ \nPaper Title: RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRESENTATION LEARNING FOR TERTIARY STRUCTURE-BASED RNA DESIGN\n\nPaper Summary:\nIn this paper, the authors develop a new method for designing RNA structures from the perspective of the final goal - the tertiary structure. They argue that the main contribution of this method lies in the introduction of the first formulation of the tertiary structure-based RNA design problem. With the goal of tackling the fundamental challenge in this field - the lack of knowledge about the connection between the nucleotide sequence and the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final goal - the final",
        "gr_score_with": 0.09174311926605505,
        "gr_score_without": 0.06774193548387097,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 21,
            "green_fraction": 0.06774193548387097,
            "z_score": -1.8932061141568828,
            "p_value": 0.9708347676810757
        },
        "output_with": {
            "num_tokens_scored": 109,
            "num_green_tokens": 10,
            "green_fraction": 0.09174311926605505,
            "z_score": -0.2873478855663455,
            "p_value": 0.6130770197527846
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper \"SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning\" proposes a novel formulation for neural network pruning using Entropic Wasserstein Regression (EWR). The method is designed to effectively balance gradient noise reduction and covariance information preservation, leveraging the geometric properties of the optimal transport problem. The paper demonstrates that the proposed method is more resilient to noisy gradients compared to existing formulations, showcasing improved performance on various neural networks under various sparsity levels and noisy gradient conditions.\n\nStrengths:\n\n1.  The formulation is carefully designed to effectively balance gradient noise reduction and covariance information preservation.\n2.  The incorporation of entropic regularization is valuable, allowing for scalable formulations.\n3.  Empirical results showcasing improved performance under various conditions.\n4.  Algorithm is carefully presented, showcasing both applicability and usability.\n5.  Code is released, enhancing reproducibility.\n\nWeaknesses:\n\n1.  Derivations assume that gradient is given.\n2.  Empirical Fisher is used.\n3.  IHT is applied.\n4.  Only forward IHT is applied.\n5.  Further exploration is required.\n6.  Only two forward IHT iterations is applied.\n7.  Further exploration is required.\n8.  Only two forward IHT iterations is applied.\n9.  Further exploration is required.\n10. Further exploration is required.\n11. Further exploration is required.\n12. Further exploration is required.\n13. Further exploration is required.\n14. Further exploration is required.\n15. Further exploration is required.\n16. Further exploration is required.\n17. Further exploration is required.\n18. Further exploration is required.\n19. Further exploration is required.\n20. Further exploration is required.\n21. Further exploration is required.\n22. Further exploration is required.\n23. Further exploration is required.\n24. Further exploration is required.\n25. Further exploration is required.\n26. Further exploration is required.\n27. Further exploration is required.\n28. Further exploration is required.\n29. Further exploration is required.\n30. Further exploration is required.\n31. Further exploration is required.\n32. Further exploration is required.\n33. Further exploration is required.\n34. Further exploration is required.\n35. Further exploration is required.\n36. Further exploration is required.\n37. Further exploration is required.\n38. Further exploration is required.\n39. Further exploration is required.\n40. Further exploration is required.\n41. Further exploration is required.\n42. Further exploration is required.\n43. Further exploration is required.\n44. Further exploration is required.\n45. Further exploration is required.\n46. Further exploration is required.\n47. Further exploration is required.\n48. Further exploration is required.\n49. Further exploration is required.\n50. Further exploration is required.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS.\nS",
        "gr_score_with": 0.09219858156028368,
        "gr_score_without": 0.07865168539325842,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 28,
            "green_fraction": 0.07865168539325842,
            "z_score": -1.3426639813413894,
            "p_value": 0.910309598652168
        },
        "output_with": {
            "num_tokens_scored": 282,
            "num_green_tokens": 26,
            "green_fraction": 0.09219858156028368,
            "z_score": -0.43669364506197056,
            "p_value": 0.6688332306966217
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n### Paper Summary\n\nThe paper \"DRM: MASTERING VISUAL REINFORCEMENT LEARN-\" proposes a novel method for training visual reinforcement learning (RL) agents that tackles complex continuous control tasks using high-dimensional pixels. The authors identify that existing visual RL methods often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce Dormant ratio Minimization (DrM), a simple yet effective method that minimizes the dormant ratio of an agent's policy network to balance exploration and exploitation. DrM is evaluated on three visual continuous control benchmarks, including DeepMind Control Suite, MetaWorld, and Adroit, and outperforms existing baselines in terms of sample efficiency and asymptotic performance.\n\n### Strengths\n\n1.  The paper identifies a crucial issue in existing visual RL methods that is often overlooked, i.e., the agents becoming motorically inactive during early training.\n2.  The authors propose a simple yet effective method, DrM, that minimizes the dormant ratio of an agent's policy network to balance exploration and exploitation.\n3.  The paper provides detailed experimental results that demonstrate the effectiveness of DrM on three visual continuous control benchmarks.\n4.  The authors conduct a detailed analysis and comparison of the dormant ratio changes during training process of DrM and existing baselines.\n\n### Weaknesses\n\n1.  The paper does not provide a detailed explanation of the theoretical background of the dormant ratio and its correlation with the diversity and significance of an agent's action.\n2.  The authors do not provide a detailed explanation of how the dormant ratio is calculated and how it is used to guide the training process.\n3.  The paper mentions that existing baselines sometimes experience \"broken seeds,\" but it is not clear what this means or how it is avoided in DrM.\n4.  The authors do not provide a detailed explanation of the \"exploitation hyperparameter\" and how it is calculated.\n\n### Suggestions for Improvement\n\n1.  Add a detailed explanation of the theoretical background of the dormant ratio and its correlation with the diversity and significance of an agent's action.\n2.  Add a detailed explanation of how the dormant ratio is calculated and how it is used to guide the training process.\n3.  Add a detailed explanation of \"broken seeds\" and how it is avoided in DrM.\n4.  Add a detailed explanation of the \"exploitation hyperparameter\" and how it is calculated.\n5.  Add more detailed experimental results that demonstrate the effectiveness of DrM on more visual continuous control benchmarks.\n\n### Recommendation\n\nThe paper is well-written and provides detailed experimental results that demonstrate the effectiveness of DrM on three visual continuous control benchmarks. However, the paper is lacking in some theoretical explanation and detailed explanation of certain parameters. If the authors can address these issues, the paper will be highly recommendable for publication in top-tier conferences or journals.\n\n### Rating\n\n7/10\n\n### Review Status\n\nPublic Review\n\n### Review Status Date\n\nApril 10, 2023",
        "gr_score_with": 0.08743169398907104,
        "gr_score_without": 0.08310991957104558,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 31,
            "green_fraction": 0.08310991957104558,
            "z_score": -1.087338783464846,
            "p_value": 0.8615564427480635
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 32,
            "green_fraction": 0.08743169398907104,
            "z_score": -0.8014861726836191,
            "p_value": 0.7885748770942843
        }
    },
    {
        "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
        "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents a novel method for dynamically reducing the number of trainable parameters in deep transformer networks using Reinforcement Learning (RL). The method, called dynamic layer tying, involves training a primary network T and a driver network Q simultaneously. The driver network Q learns to select layers to tie together, reducing the number of trainable parameters, and serves as an effective regularization technique. Experimental evaluations demonstrate that the model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters, resulting in up to one order of magnitude less memory consumption during training.\n\nStrengths:\n1. The paper presents a novel and innovative approach to reducing the number of trainable parameters in deep transformer networks using RL.\n2. The method is demonstrated to be effective in reducing the number of trainable parameters while maintaining or improving the perplexity score.\n3. The paper provides a comprehensive evaluation of the method on multiple datasets and architectures, including GPT-2 and BERT.\n4. The ablation study provides valuable insights into the importance of weight tying and the role of layer 0 in the training process.\n\nWeaknesses:\n1. The paper assumes that the primary network T and the driver network Q are trained simultaneously, which may not be feasible in all scenarios.\n2. The method requires a large number of training steps, which may be computationally expensive.\n3. The paper does not provide a clear explanation of how the driver network Q learns to select layers to tie together.\n4. The paper assumes that the weights of the tied layers are identical, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of how the driver network Q learns to select layers to tie together.\n2. Investigate the possibility of training the primary network T and the driver network Q separately, rather than simultaneously.\n3. Provide more information on the computational cost of the method and how it can be optimized for large-scale applications.\n4. Investigate the possibility of applying the dynamic layer tying method to other types of neural networks, such as convolutional neural networks (CNNs).\n\nRecommendation:\nThe paper presents a novel and innovative approach to reducing the number of trainable parameters in deep transformer networks using RL. The method is demonstrated to be effective in reducing the number of trainable parameters while maintaining or improving the perplexity score. However, the paper assumes that the primary network T and the driver network Q are trained simultaneously, which may not be feasible in all scenarios. Therefore, the paper requires further investigation and optimization to make it more practical and scalable for large-scale applications.\n\nOverall, the paper is well-written, and the authors provide a comprehensive evaluation of the method on multiple datasets and architectures. However, the paper requires further improvement to address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents a novel method for dynamically reducing the number of trainable parameters in deep transformer networks using Reinforcement Learning (RL). The method, called DLT (Dynamic Layer Tying), selects layers to be independently trained or tied to previous layers during training, thereby reducing the number of trainable parameters. The DLT method is evaluated on two transformer architectures (GPT-2 and BERT) and four widely used datasets for language modeling tasks. The results show that DLT reduces the number of trainable parameters by up to two orders of magnitude while maintaining or improving perplexity scores.\n\nStrengths:\n1. Novel approach: DLT presents a novel approach to reducing the number of trainable parameters in transformer networks.\n2. Scalable: DLT is scalable to large models and datasets.\n3. Stable training: DLT demonstrates stable training despite drastic changes in the architecture.\n4. Competitive results: DLT achieves state-of-the-art results on various language modeling tasks.\n5. Comprehensive evaluation: DLT presents extensive results, including visualizations of the training dynamics.\n\nWeaknesses:\n1. Limited context: DLT does not explicitly compare to existing state-of-the-art models.\n2. Absence of visualizations: DLT does not provide visualizations of the learned models.\n3. Short discussion of results: DLT does not adequately discuss results, specifically where DLT does poorly.\n4. Short examination of results: DLT does not adequately examine results.\n5. Short examination of results does not adequately examine results of fine-tuned models.\n6. Short examination of results does adequately examine results of models that do poorly.\n7. Short examination of results does adequately examine results of models that do poorly.\nSuggestion of improvement of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of fine-tuned models.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.\nSuggestion of examination of results of models that do poorly.",
        "gr_score_with": 0.07755102040816327,
        "gr_score_without": 0.06358381502890173,
        "output_without": {
            "num_tokens_scored": 346,
            "num_green_tokens": 22,
            "green_fraction": 0.06358381502890173,
            "z_score": -2.2579339883959757,
            "p_value": 0.9880251123857346
        },
        "output_with": {
            "num_tokens_scored": 245,
            "num_green_tokens": 19,
            "green_fraction": 0.07755102040816327,
            "z_score": -1.1712737024998898,
            "p_value": 0.8792556108340479
        }
    },
    {
        "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
        "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to neural subset selection, focusing on incorporating background information from the superset into the subset from a symmetric perspective. The authors introduce the concept of invariant sufficient representation, which enables the construction of neural networks that learn the relationship between the subset and superset while being invariant to permutations. The proposed method, called INSET, is a probabilistic-driven approach that achieves state-of-the-art performance in various tasks, including product recommendation, set anomaly detection, and compound selection.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the limitations of existing subset encoding methods and proposes a novel approach to address these limitations.\n2. The concept of invariant sufficient representation is well-defined and provides a clear understanding of the problem.\n3. The proposed method, INSET, is a simple yet effective approach that achieves state-of-the-art performance in various tasks.\n4. The paper provides comprehensive empirical evaluations, including comparisons with existing methods and analysis of the performance of INSET in different scenarios.\n\nWeaknesses:\n\n1. The paper assumes that the superset is a nested set of subsets, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of INSET, which may be a concern for large-scale datasets.\n3. The paper assumes that the subset and superset are represented as sets of elements, which may not be the case in all scenarios (e.g., when dealing with graphs or other structured data).\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational cost of INSET and discuss potential optimization techniques to reduce the computational overhead.\n2. Investigate the applicability of INSET to scenarios where the superset is not a nested set of subsets.\n3. Explore the use of INSET in more general scenarios, such as graph-based tasks or other structured data.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to neural subset selection, and the proposed method, INSET, achieves state-of-the-art performance in various tasks. The paper is well-written and provides a clear understanding of the problem and the proposed solution. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to neural subset selection, addressing the limitation of conventional methods that focus on the subset itself rather than the interaction between the subset and the superset. The authors introduce the concept of permutation invariance and the idea of modeling the conditional distribution P (Y |S, V ), which adheres to the property 3.1. They then establish the connection between the property 3.1 and the definition of the invariant sufficient representation. Based on the theoretical analysis, the authors propose the information aggregation module, known as INSET, which outperforms the conventional methods in the empirical evaluations.\n\nStrengths:\n\nThe paper proposes a novel approach to the problem of neural subset selection, addressing the limitation of conventional methods. The introduction of the property 3.1 and the definition of the invariant sufficient representation are the essential components of the methodology. The connection between the property 3.1 and the definition of the invariant sufficient representation provides the mathematical foundation for the methodology. The introduction of the information aggregation module, known as INSET, is the practical realization of the methodology, outperforming the conventional methods in the empirical evaluations.\n\nWeaknesses:\n\nThe mathematical foundation of the methodology may be unfamiliar to the readers who are not familiar with the mathematical concepts used in the paper. Some technical issues may arise when the property 3.1 is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology.\n\nSuggestions for Improvement:\n\nThe mathematical foundation of the methodology may be unfamiliar to the readers who are not familiar with the mathematical concepts used in the paper. Some technical issues may arise when the property 3.1 is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. To mitigate the issues, the authors may provide the mathematical foundation of the methodology using the mathematical symbols used throughout the paper. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology.\n\nRecommendation:\n\nThe methodology proposed in the paper is the essential components used throughout the methodology. Some technical issues may arise when the property 3.1 is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology. Some technical issues may also arise when the information aggregation module, known as INSET, is used in the practical realization of the methodology.\n\nThe methodology may be used throughout the methodology when the property 3.1 is used in the practical realization of the methodology when the information aggregation module known as the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when the information the methodology when",
        "gr_score_with": 0.08333333333333333,
        "gr_score_without": 0.09508196721311475,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 29,
            "green_fraction": 0.09508196721311475,
            "z_score": -0.2862991671569341,
            "p_value": 0.6126755020225912
        },
        "output_with": {
            "num_tokens_scored": 216,
            "num_green_tokens": 18,
            "green_fraction": 0.08333333333333333,
            "z_score": -0.8164965809277264,
            "p_value": 0.7928919108787376
        }
    },
    {
        "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
        "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel batch normalization method for Spiking Neural Networks (SNNs) called Temporal Accumulated Batch Normalization (TAB). The method addresses the Temporal Covariate Shift (TCS) issue in SNNs by aligning with the neuron dynamics, specifically the accumulated membrane potential. The TAB method utilizes temporal accumulated batch statistics over an expanding window to normalize data, which effectively captures historical temporal dependencies similar to the accumulation process of the membrane potential in the LIF neuron model. The authors demonstrate the effectiveness of TAB by comparing it with other state-of-the-art methods on various datasets, including CIFAR-10, CIFAR-100, and ImageNet.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the challenges in directly training SNNs, particularly the TCS issue.\n2. The proposed TAB method is well-motivated and aligns with the neuron dynamics, which is a significant improvement over existing methods.\n3. The authors provide a theoretical connection between the TAB method and the neural dynamics, which is a valuable contribution to the field.\n4. The experimental results demonstrate the effectiveness of TAB on various datasets, including large-scale static and neuromorphic datasets.\n\nWeaknesses:\n\n1. The paper assumes a specific neuron model (LIF) and may not generalize to other neuron models.\n2. The authors do not provide a detailed explanation of the learning rules for computing the gradients in the TAB method.\n3. The paper does not discuss the computational complexity of the TAB method and its potential impact on large-scale SNNs.\n4. The authors do not provide a thorough comparison with other state-of-the-art methods on ImageNet.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the learning rules for computing the gradients in the TAB method.\n2. Discuss the computational complexity of the TAB method and its potential impact on large-scale SNNs.\n3. Provide a thorough comparison with other state-of-the-art methods on ImageNet.\n4. Investigate the generalizability of the TAB method to other neuron models.\n\nRecommendation:\n\nThe paper proposes a novel and effective batch normalization method for SNNs, which addresses the TCS issue by aligning with the neuron dynamics. The experimental results demonstrate the effectiveness of TAB on various datasets. However, the paper assumes a specific neuron model and does not provide a detailed explanation of the learning rules for computing the gradients. With some modifications and additional experiments, the paper has the potential to be a significant contribution to the field of SNNs.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel SNN batch normalization method called Temporal Accumulated Batch Normalization (TAB), which addresses the Temporal Covariate Shift (TCS) issue in SNNs by utilizing temporal accumulated statistics for data normalization. TAB aligns with neuron dynamics, specifically the accumulated membrane potential, establishing a natural connection between neuron dynamics and batch normalization in SNNs. Theoretical insights show that TAB alleviates the temporal covariate shift problem by utilizing the temporal accumulated input across all previous time-steps, which is linked to the discrete LIF model with close proximity of discrete neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with continuous neurons dynamic with",
        "gr_score_with": 0.05217391304347826,
        "gr_score_without": 0.06134969325153374,
        "output_without": {
            "num_tokens_scored": 326,
            "num_green_tokens": 20,
            "green_fraction": 0.06134969325153374,
            "z_score": -2.3261648576111873,
            "p_value": 0.9899951211815896
        },
        "output_with": {
            "num_tokens_scored": 115,
            "num_green_tokens": 6,
            "green_fraction": 0.05217391304347826,
            "z_score": -1.7095921484405752,
            "p_value": 0.9563293407223862
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, they propose S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling.\n\nStrengths:\n\n* The paper proposes a unified sampling framework (USF) that systematically studies the optional strategies for solver searching, which is a novel contribution to the field of diffusion probabilistic models.\n* The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, which is a significant insight.\n* The proposed framework allows free choices of solver strategy at each step, which provides a high degree of flexibility.\n* The authors propose a predictor-based search method (S3) that automatically optimizes the solver schedule, which is a useful tool for practitioners.\n\nWeaknesses:\n\n* The paper assumes that the reader has a good understanding of diffusion probabilistic models and their applications, which may not be the case for all readers.\n* The authors do not provide a detailed comparison with other state-of-the-art methods, which makes it difficult to evaluate the effectiveness of their proposed framework.\n* The paper does not provide a clear explanation of how the predictor-based search method (S3) works, which may make it difficult for readers to understand and replicate the results.\n* The authors do not provide a detailed analysis of the computational cost of their proposed framework, which may be a concern for practitioners who need to implement the framework in practice.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with other state-of-the-art methods to evaluate the effectiveness of their proposed framework.\n* The paper should include a clear explanation of how the predictor-based search method (S3) works, which will make it easier for readers to understand and replicate the results.\n* The authors should provide a detailed analysis of the computational cost of their proposed framework, which will help practitioners to evaluate the feasibility of implementing the framework in practice.\n* The paper should include more experimental results to demonstrate the effectiveness of their proposed framework, particularly on different types of datasets.\n\nRecommendation:\n\nThe paper proposes a novel unified sampling framework (USF) for diffusion probabilistic models, which provides a high degree of flexibility and allows for the automatic optimization of the solver schedule using a predictor-based search method. While the paper has some weaknesses, it provides a significant contribution to the field of diffusion probabilistic models, and the proposed framework has the potential to be useful for practitioners. Therefore, I recommend that the paper be accepted for publication.\n\nRating: 7/10\n\nReviewer's Expertise: The reviewer has expertise in the field of machine learning and deep learning, with a focus on diffusion probabilistic models and their applications.",
        "peer_review_with_watermark": "$$$\n**Paper Summary:**\n\nThe paper proposes a unified sampling framework (USF) to accelerate the sampling process of Diffusion Probabilistic Models (DPMs). The authors suggest that taking different solving strategies at different timesteps may help to further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a predictor-based search method, S3, to search for well-performing solver schedules under a certain number of function evaluation (NFE) budget.\n\n**Strengths:**\n\n* The paper provides a unified framework to study the optional strategies for solver search in Diffusion Probabilistic Models (DPMs).\n* The authors suggest that taking different solving strategies at different timesteps can improve the sample quality.\n* The predictor-based search method, S3, is proposed to search for well-performing solver schedules under a certain NFE budget.\n* Experiments show that the proposed method can boost the sample quality under a very tight budget by a large margin.\n\n**Weaknesses:**\n\n* The search space is extremely large, which makes it challenging to search for well-performing solver schedules.\n* The predictor-based search method, S3, requires a large number of pre-evaluation to train the predictor, which can be time-consuming.\n* The performance under a very tight budget may be sensitive to the hyperparameters of the predictor.\n\n**Suggestions for Improvement:**\n\n* Exploring more efficient search methods to prune the search space and reduce the search overhead.\n* Investigating the use of pre-trained models to reduce the pre-evaluation time.\n* Analyzing the performance under a very tight budget to find more robust hyperparameters for the predictor.\n\n**Recommendation:**\n\nOverall, the paper makes a significant contribution to the field of Diffusion Probabilistic Models (DPMs). The unified sampling framework (USF) and predictor-based search method (S3) show promising results in improving the sample quality under a very tight budget. However, to make the search space more manageable, to prune pre-evaluation time, to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyperparameters to pre-evaluation to find hyper",
        "gr_score_with": 0.05653710247349823,
        "gr_score_without": 0.05,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 19,
            "green_fraction": 0.05,
            "z_score": -3.2489314482696545,
            "p_value": 0.9994208030460927
        },
        "output_with": {
            "num_tokens_scored": 283,
            "num_green_tokens": 16,
            "green_fraction": 0.05653710247349823,
            "z_score": -2.4371970229388324,
            "p_value": 0.9925991939095096
        }
    }
]