[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. The CMA is designed to reason over a dataset and its associated data-generating process, and it is evaluated on a number of benchmarks and a real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). The experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery.\n\nStrengths:\n\n* The paper presents a novel and innovative approach to causal discovery that combines the strengths of LLMs and DSCMs.\n* The CMA is evaluated on a range of benchmarks and a real-world task, demonstrating its effectiveness in different scenarios.\n* The paper provides a clear and well-structured explanation of the CMA framework and its components.\n* The authors provide a detailed analysis of the results and insights gained from the experiments.\n\nWeaknesses:\n\n* The paper assumes that the causal graph is known a priori, which may not be the case in real-world scenarios.\n* The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. While the authors extend the framework to include chain graph elements, human intervention is necessary to allow appropriate Deep Chain Graph modelling.\n* The paper does not provide a clear explanation of how the LLMs are used to produce hypotheses and how they are integrated with the DSCMs.\n* The paper assumes that the metadata is accurate and complete, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of how the LLMs are used to produce hypotheses and how they are integrated with the DSCMs.\n* The authors should investigate techniques to enable fully automated chain graph modelling and extend deep SCMs to represent more flexible, non-Markovian causal graphs.\n* The authors should provide a more detailed analysis of the limitations of the CMA and how they can be addressed in future work.\n* The authors should provide a more detailed explanation of the results and insights gained from the experiments, including a more detailed analysis of the causal relationships between variables.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to causal discovery that combines the strengths of LLMs and DSCMs. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the CMA framework and its components, and the experimental results demonstrate its effectiveness in different scenarios. I recommend that the paper be accepted for publication, with the suggestion that the authors address the areas for improvement mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe authors introduce a new framework called Causal Modelling Agent (CMA) for causal discovery, which combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs). This framework is designed to reason about causal relationships between variables in a dataset using both data-driven modelling and LLM-based reasoning. The authors present a series of experiments on synthetic and real-world datasets from various scientific fields to demonstrate the effectiveness of CMA in causal discovery.\n\nStrengths:\n\n1. CMA is a modular framework that can be used for causal discovery in multi-modal data settings.\n2. It combines LLM-based methods with data-driven causal modelling approaches to leverage both human knowledge and data-driven insights for causal discovery.\n3. CMA can handle unmeasured confounding/associative relationships through chain graph models with deep learning elements.\n\nWeaknesses:\n\n1. CMA assumes a Markovian Directed Acyclic Graph (DAG) structure for causal relationships between variables in DSCMs.\n2. LLMs struggle to fully automate chain graph modelling in CMA; human intervention is necessary to allow appropriate Deep Chain Graph modelling.\n\nSuggestions for Improvement:\n\n1. Explore techniques to enable fully automated chain graph modelling in CMA to reduce reliance on human intervention.\n2. Consider extending deep SCMs to represent non-Markovian causal graphs (e.g., models with feedback loops) to increase CMA's applicability.\n\nRecommendation:\n\nThis is a well-designed study with interesting insights into causal discovery using a combination of LLMs and DSCMs. However, to fully realize its potential, it would be beneficial to address some of its current limitations through future extensions or modifications to CMA.\n\nOverall rating for this review is 8/10.\n\nFinal comments:\n\nThe CMA framework is an exciting contribution to causal discovery in multi-modal data settings. Its modular design allows for leveraging both data-driven insights from DSCMs and LLM-based reasoning for causal discovery tasks. However, to fully leverage its potential, it is essential to address its current limitations related to automated chain graph modelling.\n\n$$$",
        "gr_score_with": 0.8200589970501475,
        "gr_score_without": 0.8061797752808989,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 287,
            "green_fraction": 0.8061797752808989,
            "z_score": 0.29149941700174753,
            "p_value": 0.385334694845616
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 278,
            "green_fraction": 0.8200589970501475,
            "z_score": 0.9233132592090679,
            "p_value": 0.17792199003804154
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a framework for learning language-informed visual concepts from images by distilling pre-trained vision-language models. The framework consists of a set of concept encoders that extract visual concept embeddings from images along various concept axes specified by language. The concept embeddings are trained to be disentangled and compositional, allowing for flexible composition of concepts into high-quality image generations. The framework is evaluated on a range of tasks, including visual concept extraction, recomposition, and extrapolation, as well as text-based visual concept editing.\n\nStrengths:\n\n1. The paper proposes a novel framework for learning language-informed visual concepts from images, which is a significant contribution to the field of computer vision and artificial intelligence.\n2. The framework is evaluated on a range of tasks, including visual concept extraction, recomposition, and extrapolation, as well as text-based visual concept editing, which demonstrates its versatility and effectiveness.\n3. The paper provides a thorough evaluation of the framework, including quantitative and qualitative results, which provides a comprehensive understanding of its strengths and limitations.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained vision-language models, which may limit its generalizability to other domains and tasks.\n2. The framework assumes that the concept axes are specified by language, which may not always be the case in real-world applications.\n3. The paper does not provide a clear explanation of how the concept embeddings are disentangled and compositional, which may make it difficult for readers to understand the framework.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the concept embeddings are disentangled and compositional, including the mathematical formulation and the role of the text anchoring loss.\n2. The authors should provide more examples of how the framework can be applied to real-world applications, such as image editing and generation.\n3. The authors should provide a more thorough evaluation of the framework, including a comparison with other state-of-the-art methods and a discussion of its limitations and potential future directions.\n\nRecommendation:\n\nI recommend this paper for publication in a top-tier conference in the field of computer vision and artificial intelligence. The paper proposes a novel and effective framework for learning language-informed visual concepts from images, which has the potential to impact a wide range of applications. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more comprehensive and impactful.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces a novel approach to learning language-informed visual concepts by distilling pre-trained vision-language models. The proposed method, referred to as concept encoding, trains a set of concept encoders to extract visual concepts along specific language-specified axes. The concept encoders are trained to reproduce input images through pre-trained Text-to-Image (T2I) models, while also anchoring concept embeddings to text embeddings obtained from pre-trained Visual Question Answering (VQA) models. This allows for better disentanglement of concept axes. The method is evaluated on various tasks, including visual concept extraction, recomposition, and generalization to unseen concepts. The results show that the proposed approach outperforms prior work in visual concept editing, demonstrating the effectiveness of language-informed visual concept learning.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of learning language-informed visual concepts.\n2. The proposed method, concept encoding, is well-motivated by the existing literature on vision-language models.\n3. The experimental results show that the proposed approach outperforms prior work in visual concept editing.\n4. The paper provides thorough evaluations, including qualitative and quantitative results, as well as ablation studies.\n5. The method is shown to generalize to unseen concepts, which is an important property for visual concept learning.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained vision-language models, which may not be widely available for all applications.\n2. The training process is relatively complex, involving the use of synthetic images generated by pre-trained T2I models.\n3. The evaluation dataset is limited, which may not be representative of real-world visual concept learning tasks.\n4. The paper assumes that the language-specified concept axes are well-defined, which may not always be the case in real-world applications.\n5. The method may not be scalable to very large datasets, due to the computational complexity of training the concept encoders.\n\nSuggestions for Improvement:\n\n1. The paper should provide more detailed information on the pre-trained vision-language models used in the experiments.\n2. The training process should be simplified, either by using pre-trained models that are more widely available, or by developing new methods that do not rely on synthetic images.\n3. The evaluation dataset should be extended to include more diverse visual concepts, objects, and scenes.\n4. The paper should discuss the implications of the method for real-world applications, such as visual search, image retrieval, and image synthesis.\n5. The method should be compared to more state-of-the-art visual concept learning methods, to provide a more comprehensive evaluation.\n\nRecommendation:\n\nThis paper provides an important contribution to the field of visual concept learning, by introducing a novel approach that leverages pre-trained vision-language models. The proposed method shows promising results in visual concept editing, which is an important property for many real-world applications. However, the paper relies heavily on pre-trained models, which may not be widely available, and the training process is relatively complex. To improve the method, the authors should provide more detailed information on the pre-trained models, simplify the training process, extend the evaluation dataset, discuss the implications for real-world applications, and compare the method to more state-of-the-art visual concept learning methods.\n\n$$$",
        "gr_score_with": 0.8279816513761468,
        "gr_score_without": 0.8562691131498471,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 280,
            "green_fraction": 0.8562691131498471,
            "z_score": 2.54380581260292,
            "p_value": 0.005482601249939544
        },
        "output_with": {
            "num_tokens_scored": 436,
            "num_green_tokens": 361,
            "green_fraction": 0.8279816513761468,
            "z_score": 1.4606850849622548,
            "p_value": 0.07205094203447987
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a two-stage Reinforced Rationale-Prompted Paradigm (Rapper) for natural language explanation (NLE) in visual question answering (VQA) tasks. The first stage utilizes knowledge distillation from large language models (LLMs) to infuse rationale-prompting and encourage plausible NLE. The second stage employs Reinforcement Learning from NLE Feedback (RLNF) to inject visual facts into NLE generation. The proposed method achieves state-of-the-art performance on two VL-NLE benchmarks, VQA-X and e-SNLI-VE, and demonstrates improved plausibility and faithfulness of NLE.\n\nStrengths:\n\n1. The paper presents a comprehensive approach to tackle the implausibility and hallucination problems in NLE.\n2. The proposed two-stage paradigm is well-motivated and effectively addresses the limitations of existing methods.\n3. The use of knowledge distillation and RLNF is innovative and demonstrates the effectiveness of these techniques in improving NLE.\n4. The paper provides thorough experiments and evaluation metrics, including quantitative and qualitative results, to demonstrate the superiority of Rapper.\n\nWeaknesses:\n\n1. The paper assumes that the ground-truth answer and explanation are available during training, which may not be the case in all scenarios.\n2. The filtering mechanism used in knowledge distillation may not be effective in all cases, and its impact on performance is not thoroughly evaluated.\n3. The paper does not provide a detailed analysis of the RLNF algorithm and its convergence properties.\n4. The qualitative evaluation results, although impressive, are limited to a few examples and may not be representative of the overall performance.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of different LLMs and knowledge distillation techniques on the performance of Rapper.\n2. Evaluate the robustness of Rapper to variations in the input data, such as image quality or question complexity.\n3. Provide a more detailed analysis of the RLNF algorithm, including its convergence properties and the impact of hyperparameters on performance.\n4. Conduct a more thorough qualitative evaluation, including more examples and a systematic comparison with existing methods.\n\nRecommendation:\n\nThe paper presents a well-motivated and effective approach to NLE in VQA tasks. The proposed two-stage paradigm is innovative and demonstrates the superiority of Rapper over existing methods. However, the paper assumes that the ground-truth answer and explanation are available during training, which may not be the case in all scenarios. Additionally, the filtering mechanism used in knowledge distillation may not be effective in all cases, and its impact on performance is not thoroughly evaluated. Nevertheless, the paper provides thorough experiments and evaluation metrics, and the qualitative evaluation results are impressive. Therefore, I recommend accepting the paper with minor revisions to address the mentioned weaknesses.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a two-stage Reinforced Rationale-Promtped Paradigm (Rapper) to tackle the challenges in Natural Language Explanation (NLE) generation in Visual Question Answering (VQA) tasks, particularly the problems of implausibility and hallucination. The first stage utilizes a knowledge distillation process to introduce Large Language Models (LLMs) to enrich rationales with supporting facts, and the subsequent stage employs Reinforcement Learning from NLE Feedback (RLNF) to inject visual facts into NLE generation, thereby promoting plausible and faithful NLE.\n\nStrengths:\n\n1. The paper tackles a significant challenge in NLE generation in VQA tasks, which has garnered increasing attention in recent years.\n2. The proposed two-stage paradigm, Rapper, effectively addresses the problems of implausibility and hallucination, which have not been explored in the field of VL-NLE.\n3. The paper presents a novel technique, RLNF, which utilizes the answer-explanation feedback as rewards and prompts the VQA model with predicted rationales, allowing faithful NLE generation.\n4. The experimental results show that Rapper outperforms previous state-of-the-art methods in NLE-related metrics and demonstrates its ability to generate plausible and faithful explanations.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained LLMs, which may not be available or accessible to all researchers, and the knowledge distillation process may not be effective in all cases.\n2. The RLNF technique may require a large amount of training data and computational resources, which may be a challenge in some scenarios.\n3. The paper does not provide a detailed analysis of the potential biases in the training data and how they may affect the performance of Rapper.\n4. The qualitative evaluation in Fig. 3 may not be sufficient to support the claims made in the paper, and more comprehensive qualitative analysis should be provided.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods to knowledge distillation, such as self-supervised learning or few-shot learning, to reduce the dependence on pre-trained LLMs.\n2. Provide a more detailed analysis of the potential biases in the training data and how they may affect the performance of Rapper.\n3. Conduct more comprehensive qualitative analysis, such as user studies or expert evaluations, to support the claims made in the paper.\n4. Investigate the generalizability of Rapper to other VQA tasks and datasets.\n\nRecommendation:\n\nThe paper presents a novel and effective solution to the challenges in NLE generation in VQA tasks, particularly the problems of implausibility and hallucination. While there are some weaknesses and suggestions for improvement, the paper has made significant contributions to the field and has the potential to inspire future research in this area.\n\nScore: 8/10",
        "gr_score_with": 0.781021897810219,
        "gr_score_without": 0.7914572864321608,
        "output_without": {
            "num_tokens_scored": 398,
            "num_green_tokens": 315,
            "green_fraction": 0.7914572864321608,
            "z_score": -0.426066501049527,
            "p_value": 0.664970305432353
        },
        "output_with": {
            "num_tokens_scored": 411,
            "num_green_tokens": 321,
            "green_fraction": 0.781021897810219,
            "z_score": -0.9618640661506321,
            "p_value": 0.8319410544986845
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper presents a novel method, MovingParts, that enables simultaneous dynamic scene reconstruction and part discovery from monocular images. Inspired by fluid simulation, the authors introduce a hybrid model that captures scene motion from both Eulerian and Lagrangian views. The Lagrangian view allows for part-level reconstruction and enables direct applications in part tracking, animation, and scene editing. The proposed method uses a cycle-consistency loss to enforce reciprocity between the Eulerian and Lagrangian modules, and a group merging algorithm to post-process the part segmentation results.\n\n**Strengths**\n\n1. The paper presents a novel and effective method for dynamic scene reconstruction and part discovery from monocular images.\n2. The hybrid model that captures scene motion from both Eulerian and Lagrangian views is a key innovation that enables part-level reconstruction.\n3. The cycle-consistency loss and group merging algorithm are effective in enforcing reciprocity and post-processing the part segmentation results.\n4. The paper demonstrates the application of the method in downstream tasks such as part tracking, animation, and scene editing.\n\n**Weaknesses**\n\n1. The paper relies on a specific assumption that the scene content belonging to one rigid part shares the same rigid transformation, which might not hold in some cases.\n2. The method requires a large number of parameters and training time, which might be a limitation in some applications.\n3. The paper could benefit from more quantitative and qualitative comparisons to other state-of-the-art methods.\n4. The limitations section could be more detailed and provide a clearer direction for future work.\n\n**Suggestions for Improvement**\n\n1. Provide more quantitative and qualitative comparisons to other state-of-the-art methods to better evaluate the effectiveness of the proposed method.\n2. Consider exploring more elegant and efficient solutions to model motion on very long sequences.\n3. Provide more details on the limitations section and provide a clearer direction for future work.\n4. Consider exploring the application of the method in more downstream tasks and scenarios.\n\n**Recommendation**\n\nI recommend accepting this paper as a full paper. The proposed method presents a novel and effective solution to dynamic scene reconstruction and part discovery from monocular images. The hybrid model and cycle-consistency loss are key innovations that make the method stand out. While there are some limitations and areas for future work, the paper presents a clear and well-written contribution that has the potential to make a significant impact in the field.",
        "gr_score_with": 0.7964071856287425,
        "gr_score_without": 0.7896551724137931,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 229,
            "green_fraction": 0.7896551724137931,
            "z_score": -0.44041516463602765,
            "p_value": 0.6701817782005756
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 266,
            "green_fraction": 0.7964071856287425,
            "z_score": -0.1641526965493733,
            "p_value": 0.5651945307218212
        }
    },
    {
        "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
        "abstract": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, called Diffusion Model-Based Predictor (DMBP), to enhance the robustness of offline reinforcement learning (RL) algorithms against state observation perturbations. The proposed approach utilizes conditional diffusion models to recover the actual states from perturbed observations, which is fundamentally different from existing online robust RL methods. The framework consists of a diffusion model-based predictor that estimates the current state by reversely denoising the perturbed state, and a non-Markovian loss function that minimizes the sum entropy of denoised states over the RL trajectory to mitigate error accumulation. The authors evaluate the proposed framework on D4RL Gym benchmark and demonstrate its effectiveness in enhancing the robustness of existing offline RL algorithms against different types of attacks on state observations.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to enhancing the robustness of offline RL algorithms against state observation perturbations.\n2. The proposed framework is based on conditional diffusion models, which have been widely used in image generation and other applications.\n3. The non-Markovian loss function is a novel and effective way to mitigate error accumulation during testing.\n4. The experiments on D4RL Gym benchmark demonstrate the effectiveness of the proposed framework in enhancing the robustness of existing offline RL algorithms.\n\nWeaknesses:\n\n1. The paper assumes that the perturbed state observations are Gaussian noises, which may not be realistic in many real-world scenarios.\n2. The proposed framework may not be effective against more complex types of perturbations, such as adversarial attacks.\n3. The non-Markovian loss function may be computationally expensive to compute, especially for large RL trajectories.\n4. The paper does not provide a detailed analysis of the theoretical guarantees of the proposed framework.\n\nSuggestions for Improvement:\n\n1. The authors should investigate more realistic types of perturbations, such as non-Gaussian noises or adversarial attacks, and evaluate the effectiveness of the proposed framework against these types of perturbations.\n2. The authors should provide a more detailed analysis of the computational complexity of the non-Markovian loss function and explore ways to reduce it.\n3. The authors should provide a more detailed analysis of the theoretical guarantees of the proposed framework, such as convergence guarantees and error bounds.\n4. The authors should evaluate the proposed framework on more challenging RL tasks and environments to demonstrate its effectiveness in a wider range of scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to enhancing the robustness of offline RL algorithms against state observation perturbations. The proposed framework is based on conditional diffusion models, which have been widely used in image generation and other applications. The non-Markovian loss function is a novel and effective way to mitigate error accumulation during testing. The experiments on D4RL Gym benchmark demonstrate the effectiveness of the proposed framework in enhancing the robustness of existing offline RL algorithms. However, the paper assumes that the perturbed state observations are Gaussian noises, which may not be realistic in many real-world scenarios. The authors should investigate more realistic types of perturbations and evaluate the effectiveness of the proposed framework against these types of perturbations.\n\nOverall, the paper is well-written, and the proposed framework is well-motivated and effective. However, the authors should address the weaknesses and suggestions for improvement to make the paper more comprehensive and impactful.\n\nRecommendation: Accept.",
        "peer_review_with_watermark": "$$$\nPaper Title: DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- \n          BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS\nSummary:\nThe paper proposes a Diffusion Model-Based Predictor (DMBP) framework for robust offline reinforcement learning against state observation perturbations. DMBP utilizes a conditional diffusion model as a denoiser to predict the actual states from noisy observations, thereby strengthening the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks. To mitigate error accumulation, a non-Markovian loss function is introduced that minimizes the sum entropy of denoised states over the RL trajectory. Experimental results on D4RL benchmarks show that DMBP can effectively deal with incomplete state observations with unobserved dimensions, demonstrating its robustness against different types of attacks on state observations.\nStrengths:\n1. Novel contribution: DMBP introduces a first framework for state-denoising in offline RL against observation perturbations in state-based tasks.\n2. Robustness against perturbations: DMBP can effectively enhance the robustness of existing offline RL algorithms against different scales of random noises and even adversarial attacks.\n3. Effectiveness in dealing with incomplete observations: DMBP can accurately predict the missing state information, making it suitable for decision-making under incomplete observations.\nWeaknesses:\n1. Complexity of the framework: DMBP requires a conditional diffusion model as a denoiser, which may add complexity to the offline RL framework.\n2. Hyperparameter tuning: DMBP requires careful tuning of hyperparameters, which may be challenging, especially for complex environments.\n3. Limited experimental evaluation: While the paper presents experimental results on D4RL benchmarks, it would be beneficial to provide more comprehensive evaluations on various environments.\nSuggestions for Improvement:\n1. Simplify the framework: Consider simplifying the DMBP framework by exploring alternative denoisers or modifying the existing one for easier implementation.\n2. Provide more comprehensive experimental evaluation: Include more environments, baseline algorithms, and experiments on the robustness of DMBP against various attacks.\n3. Analyze the impact of hyperparameter tuning: Conduct a thorough analysis on the impact of hyperparameter tuning on the performance of DMBP.\nRecommendation:\nThe paper presents a significant contribution to the offline RL community by introducing a robust framework against observation perturbations. While there are areas for improvement, the paper's strengths outweigh its weaknesses, making it a valuable addition to the literature. I highly recommend this paper for publication in a top-tier conference.",
        "gr_score_with": 0.8186274509803921,
        "gr_score_without": 0.8388888888888889,
        "output_without": {
            "num_tokens_scored": 360,
            "num_green_tokens": 302,
            "green_fraction": 0.8388888888888889,
            "z_score": 1.8446619684315548,
            "p_value": 0.03254336324638338
        },
        "output_with": {
            "num_tokens_scored": 408,
            "num_green_tokens": 334,
            "green_fraction": 0.8186274509803921,
            "z_score": 0.9406401658278365,
            "p_value": 0.1734446455808053
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$\nPaper Summary: The paper proposes a novel approach to designing RNA sequences that fold into a specified tertiary structure, which is a challenging task due to the scarcity of high-resolution RNA tertiary structures and the complex structure of RNA. The authors present a hierarchical data-efficient representation learning framework, RDESIGN, which learns to represent RNA structures through contrastive learning at the cluster and sample levels. The model also incorporates secondary structure information to guide the RNA design process. The authors conduct extensive experiments to demonstrate the effectiveness of RDESIGN and compare it with several baseline models. The results show that RDESIGN outperforms the baselines, particularly for longer RNA sequences, and demonstrates superior generalization ability.\n\nStrengths: The paper addresses a significant problem in RNA design, which is a critical area of research with vast biomedical applications. The proposed approach is novel and well-motivated, and the authors provide a clear explanation of the methodology. The experimental results are thorough and convincing, and the authors compare RDESIGN with several baseline models. The code and benchmark dataset are made publicly available, which is a significant strength of the paper. The authors also discuss the limitations of the method and provide a clear roadmap for future work.\n\nWeaknesses: One of the main weaknesses of the paper is that the results are primarily presented for the benchmark dataset, and the generalization ability of RDESIGN is evaluated only for two additional datasets. While the results are promising, more extensive evaluation of the generalization ability of RDESIGN would be beneficial. Another weakness is that the paper does not provide a clear explanation of the choice of hyperparameters and how they affect the results. Finally, the paper does not discuss the computational efficiency of RDESIGN, which is a critical aspect of a method that aims to be widely applicable.\n\nSuggestions for Improvement: To improve the paper, the authors should provide more extensive evaluation of the generalization ability of RDESIGN, including more datasets and a more thorough analysis of the results. They should also provide a clear explanation of the choice of hyperparameters and how they affect the results. Finally, the authors should discuss the computational efficiency of RDESIGN and provide a clear roadmap for further optimization of the method. Additionally, the authors should provide a more thorough analysis of the limitations of the method and discuss the implications for wet-lab validation and experimental design.\n\nRecommendation: The paper is well-written and provides a significant contribution to the field of RNA design. The proposed approach is novel and well-motivated, and the experimental results are thorough and convincing. However, the paper would benefit from more extensive evaluation of the generalization ability of RDESIGN and a clearer explanation of the choice of hyperparameters. With these improvements, the paper would be a significant contribution to the field and would likely be accepted for publication.",
        "gr_score_with": 0.8426966292134831,
        "gr_score_without": 0.8387096774193549,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 260,
            "green_fraction": 0.8387096774193549,
            "z_score": 1.7038855027411945,
            "p_value": 0.044201239963197074
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 300,
            "green_fraction": 0.8426966292134831,
            "z_score": 2.013995972012083,
            "p_value": 0.022004978195124456
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe authors propose a novel approach to neural network pruning called Sparse Entropic Wasserstein Regression (EWR) or SWAP. The method addresses the challenge of inaccurate gradients in computing the Empirical Fisher Information Matrix during neural network pruning. EWR is a formulation of Entropic Wasserstein regression (EWR) that leverages Wasserstein distance to model dissimilarity between two distributions, effectively striking a balance between noise reduction and covariance information preservation. The authors demonstrate that EWR outperforms state-of-the-art (SoTA) network pruning methods, especially in scenarios with noisy gradients.\n\n**Strengths**\n\n1. **Robustness to noisy gradients**: EWR's ability to mitigate noise in gradients through Neighborhood Interpolation is a significant strength. This approach is essential in scenarios with noisy data, federated learning, or analog memory.\n2. **Balances noise reduction and covariance information preservation**: EWR's formulation effectively balances noise reduction and preserving important information, making it a robust pruning method.\n3. **Comparatively low computational cost**: EWR's computational cost is comparable to existing SoTA methods, making it a feasible approach.\n4. **Flexibility in handling large datasets**: EWR's formulation enables efficient handling of large datasets, making it suitable for large-scale model compression.\n\n**Weaknesses**\n\n1. **Limited theoretical analysis on convergence and optimality**: While EWR's theoretical aspects are well-motivated, more comprehensive theoretical analysis on convergence and optimality would strengthen the work.\n2. **Limited comparison with existing SoTA methods in terms of computational efficiency**: Although EWR's computational cost is comparable to existing SoTA methods, more detailed comparisons would provide a more comprehensive understanding.\n3. **Limited discussion on scalability and parallelization**: While EWR's formulation enables efficient handling of large datasets, more discussion on scalability and parallelization would be beneficial.\n\n**Suggestions for Improvement**\n\n1. **Provide more comprehensive theoretical analysis on convergence and optimality**.\n2. **Compare EWR with existing SoTA methods in terms of computational efficiency more extensively**.\n3. **Discuss scalability and parallelization aspects in more depth**.\n4. **Provide more experimental results to demonstrate EWR's robustness in various scenarios**.\n\n**Recommendation**\n\nThe proposed EWR method is a promising approach to neural network pruning, addressing the challenge of inaccurate gradients in computing Empirical Fisher Information Matrix. While some theoretical and experimental aspects could be improved, EWR's robustness to noisy gradients and balance between noise reduction and covariance information preservation make it a valuable contribution to the field. I recommend this work be accepted with some revisions addressing the suggestions mentioned above.",
        "gr_score_with": 0.7966573816155988,
        "gr_score_without": 0.8370786516853933,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 298,
            "green_fraction": 0.8370786516853933,
            "z_score": 1.7489965020104927,
            "p_value": 0.040145812031492804
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 286,
            "green_fraction": 0.7966573816155988,
            "z_score": -0.15833394419077637,
            "p_value": 0.5629031690532824
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces a new method for visual reinforcement learning (VR) agents that tackles the challenge of continuous control tasks. The authors identify a shortcoming in existing VR methods that agents often exhibit sustained inactivity during early training. They propose a method named DrM that uses three core mechanisms: (1) a periodical neural network weight perturbation, (2) a dormant-ratio-based exploration scheduler, and (3) a dormant-ratio-guided exploitation technique. DrM aims to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. The authors empirically show that DrM achieves significant improvements in sample efficiency and asymptotic performance in three continuous control benchmark environments: DeepMind Control Suite, MetaWorld, and Adroit. DrM is the first documented model-free algorithm that reliably solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite and three dexterous hand manipulation tasks without demonstrations in Adroit.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the problem of sustained inactivity in VR agents and identifies a critical insight that the decline in the dormant ratio is an early indicator of successful skill acquisition.\n2. The authors propose a simple yet effective method that leverages the concept of the dormant ratio to guide exploration-exploitation trade-offs in VR agents.\n3. The paper presents a comprehensive evaluation of DrM in three benchmark environments and demonstrates its superiority over state-of-the-art baselines in terms of both sample efficiency and asymptotic performance.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the concept of the dormant ratio and its calculation. A brief introduction to the concept and its calculation would be beneficial for readers who are not familiar with it.\n2. The authors do not discuss the theoretical foundations of the method and its relationship to existing work in the field of VR and exploration in RL. A more detailed discussion of the theoretical underpinnings of the method would strengthen the paper's contribution and impact.\n3. The paper does not explore the potential applications of DrM in other areas of RL and exploration. A discussion of the broader implications and potential applications of the method would enhance the paper's impact and significance.\n\nSuggestions for Improvement:\n\n1. Introduce the concept of the dormant ratio and its calculation in more detail for readers who are not familiar with it.\n2. Expand the theoretical discussion of the method and its relationship to existing work in the field of VR and exploration in RL.\n3. Explore the potential applications of DrM in other areas of RL and exploration and discuss the broader implications and potential applications of the method.\n\nRecommendation:\n\nThe paper presents a promising approach for improving the performance of VR agents in continuous control tasks. The method's simplicity and effectiveness make it a valuable contribution to the field. However, a more detailed introduction to the concept of the dormant ratio and a more comprehensive discussion of the theoretical underpinnings of the method would strengthen the paper's contribution and impact.",
        "gr_score_with": 0.8186274509803921,
        "gr_score_without": 0.8150134048257373,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 304,
            "green_fraction": 0.8150134048257373,
            "z_score": 0.7248925223098924,
            "p_value": 0.23425898432781994
        },
        "output_with": {
            "num_tokens_scored": 408,
            "num_green_tokens": 334,
            "green_fraction": 0.8186274509803921,
            "z_score": 0.9406401658278365,
            "p_value": 0.1734446455808053
        }
    },
    {
        "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
        "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents a novel method for dynamically reducing the number of trainable parameters in deep transformer networks using Reinforcement Learning (RL). The method, called dynamic layer tying, involves training a primary network T and a driver network Q simultaneously. The driver network Q learns to select layers to tie together, reducing the number of trainable parameters, and serves as an effective regularization technique. Experimental evaluations demonstrate that the model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters, resulting in up to one order of magnitude less memory consumption during training.\n\nStrengths:\n1. The paper presents a novel and innovative approach to reducing the number of trainable parameters in deep transformer networks using RL.\n2. The method is demonstrated to be effective in reducing the number of trainable parameters while maintaining or improving the perplexity score.\n3. The paper provides a comprehensive evaluation of the method on multiple datasets and architectures, including GPT-2 and BERT.\n4. The ablation study provides valuable insights into the importance of weight tying and the role of layer 0 in the training process.\n\nWeaknesses:\n1. The paper assumes that the primary network T and the driver network Q are trained simultaneously, which may not be feasible in all scenarios.\n2. The method requires a large number of training steps, which may be computationally expensive.\n3. The paper does not provide a clear explanation of how the driver network Q learns to select layers to tie together.\n4. The paper assumes that the weights of the tied layers are identical, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of how the driver network Q learns to select layers to tie together.\n2. Investigate the possibility of training the primary network T and the driver network Q separately, rather than simultaneously.\n3. Provide more information on the computational cost of the method and how it can be optimized for large-scale applications.\n4. Investigate the possibility of applying the dynamic layer tying method to other types of neural networks, such as convolutional neural networks (CNNs).\n\nRecommendation:\nThe paper presents a novel and innovative approach to reducing the number of trainable parameters in deep transformer networks using RL. The method is demonstrated to be effective in reducing the number of trainable parameters while maintaining or improving the perplexity score. However, the paper assumes that the primary network T and the driver network Q are trained simultaneously, which may not be feasible in all scenarios. Therefore, the paper requires further investigation and optimization to make it more practical and scalable for large-scale applications.\n\nOverall, the paper is well-written, and the authors provide a comprehensive evaluation of the method on multiple datasets and architectures. However, the paper requires further improvement to address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper presents a novel method for dynamically reducing the number of trainable parameters in a Transformer architecture using Reinforcement Learning. The proposed method, Dynamic Layer Tying, employs a Q-learning driven policy to select layers for independent training or weight sharing. The policy is updated every few training iterations, and the model achieves significant reductions in the number of trainable parameters while maintaining or improving the accuracy.\n\n**Strengths**\n\n* The paper presents a new and innovative approach to reducing the number of trainable parameters in Transformer models.\n* The proposed method achieves significant reductions in the number of trainable parameters, up to 87% in some cases.\n* The model's performance is comparable to or better than the full L-layer Transformer on various language modeling tasks.\n* The paper provides a thorough analysis of the training dynamics and the importance of weight tying in the proposed method.\n\n**Weaknesses**\n\n* The paper assumes that the Transformer architecture is the primary model, and the Q-learning policy is applied to it. However, the generalizability of the proposed method to other architectures is not explored.\n* The paper does not provide a clear explanation of why the Q-learning policy is able to learn the layer selection strategy effectively.\n* The paper assumes that the Q-learning policy is updated every few training iterations, but the frequency of updates is not discussed in detail.\n* The paper does not provide a clear comparison of the proposed method to other methods for reducing the number of trainable parameters in Transformer models.\n\n**Suggestions for Improvement**\n\n* The paper should provide a more detailed explanation of the Q-learning policy and how it learns the layer selection strategy.\n* The paper should explore the generalizability of the proposed method to other architectures and provide a clear comparison to other methods for reducing the number of trainable parameters in Transformer models.\n* The paper should provide more details on the frequency of Q-learning policy updates and how it affects the training dynamics.\n* The paper should discuss the potential applications of the proposed method in other domains, such as computer vision or natural language processing.\n\n**Recommendation**\n\nThe paper presents a new and innovative approach to reducing the number of trainable parameters in Transformer models. The proposed method achieves significant reductions in the number of trainable parameters while maintaining or improving the accuracy. However, the paper has some limitations and areas for improvement. With some modifications and extensions, the proposed method has the potential to become a widely used and effective tool for reducing the number of trainable parameters in Transformer models.\n\n**Rating**\n\nI rate the paper 8 out of 10. The paper presents a new and innovative approach to reducing the number of trainable parameters in Transformer models, and the proposed method achieves significant reductions in the number of trainable parameters while maintaining or improving the accuracy. However, the paper has some limitations and areas for improvement, and the generalizability of the proposed method to other architectures is not explored.\n\n**Final Comments**\n\nThe paper presents a new and innovative approach to reducing the number of trainable parameters in Transformer models. The proposed method has the potential to become a widely used and effective tool for reducing the number of trainable parameters in Transformer models. However, the paper has some limitations and areas for improvement, and the generalizability of the proposed method to other architectures is not explored. With some modifications and extensions, the proposed method can be made more effective and widely applicable.",
        "gr_score_with": 0.8312101910828026,
        "gr_score_without": 0.8005780346820809,
        "output_without": {
            "num_tokens_scored": 346,
            "num_green_tokens": 277,
            "green_fraction": 0.8005780346820809,
            "z_score": 0.02688016652852199,
            "p_value": 0.4892776563094473
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 261,
            "green_fraction": 0.8312101910828026,
            "z_score": 1.3826149875585934,
            "p_value": 0.08339147552393722
        }
    },
    {
        "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
        "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to neural subset selection, focusing on incorporating background information from the superset into the subset from a symmetric perspective. The authors introduce the concept of invariant sufficient representation, which enables the construction of neural networks that learn the relationship between the subset and superset while being invariant to permutations. The proposed method, called INSET, is a probabilistic-driven approach that achieves state-of-the-art performance in various tasks, including product recommendation, set anomaly detection, and compound selection.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the limitations of existing subset encoding methods and proposes a novel approach to address these limitations.\n2. The concept of invariant sufficient representation is well-defined and provides a clear understanding of the problem.\n3. The proposed method, INSET, is a simple yet effective approach that achieves state-of-the-art performance in various tasks.\n4. The paper provides comprehensive empirical evaluations, including comparisons with existing methods and analysis of the performance of INSET in different scenarios.\n\nWeaknesses:\n\n1. The paper assumes that the superset is a nested set of subsets, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of INSET, which may be a concern for large-scale datasets.\n3. The paper assumes that the subset and superset are represented as sets of elements, which may not be the case in all scenarios (e.g., when dealing with graphs or other structured data).\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational cost of INSET and discuss potential optimization techniques to reduce the computational overhead.\n2. Investigate the applicability of INSET to scenarios where the superset is not a nested set of subsets.\n3. Explore the use of INSET in more general scenarios, such as graph-based tasks or other structured data.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to neural subset selection, and the proposed method, INSET, achieves state-of-the-art performance in various tasks. The paper is well-written and provides a clear understanding of the problem and the proposed solution. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural subset selection, called INSET, that incorporates the representation of the superset into the subset from a probabilistic perspective. The authors argue that existing approaches overlook the valuable information within the superset when using neural networks to model set functions. They introduce a probabilistic perspective, establishing a connection between functional symmetry and probabilistic symmetry in the conditional distribution P (Y |S, V ), enabling the characterization of the model structure. The proposed method, INSET, is an effective and interpretable approach for neural subset selection, outperforming previous methods in various tasks, including product recommendation, set anomaly detection, and compound selection.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the limitations of existing subset encoding methods, identifying the need for a probabilistic perspective in neural subset selection tasks.\n2. The authors introduce a novel method, INSET, that incorporates the representation of the superset into the subset, addressing the oversight of existing approaches.\n3. The proposed method is theoretically driven, establishing a connection between functional symmetry and probabilistic symmetry in the conditional distribution P (Y |S, V ).\n4. The authors provide a thorough evaluation of INSET on various tasks, including product recommendation, set anomaly detection, and compound selection, demonstrating its superiority over previous methods.\n\nWeaknesses:\n\n1. The paper assumes a probabilistic perspective, which may not be applicable to all subset selection tasks, potentially limiting the generalizability of the proposed method.\n2. The authors do not provide a clear explanation of the choice of the invariant sufficient representation, M(S, V ), in the implementation of INSET, making it challenging to replicate the results.\n3. The paper does not discuss the scalability of INSET to large-scale subset selection tasks, potentially limiting its practical application.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive analysis of the limitations of the probabilistic perspective in subset selection tasks, addressing potential counterarguments.\n2. The choice of the invariant sufficient representation, M(S, V ), should be clearly explained, providing more insights into the design of INSET.\n3. The scalability of INSET to large-scale subset selection tasks should be investigated, potentially involving the use of parallel processing or other optimization techniques.\n\nRecommendation:\n\nThe paper proposes a novel method, INSET, for neural subset selection that incorporates the representation of the superset into the subset from a probabilistic perspective. The authors provide a comprehensive analysis of the limitations of existing subset encoding methods, establishing a connection between functional symmetry and probabilistic symmetry in the conditional distribution P (Y |S, V ). The proposed method is theoretically driven, outperforming previous methods in various tasks. However, the paper assumes a probabilistic perspective, potentially limiting the generalizability of the proposed method. To address these limitations, the authors should provide a more comprehensive analysis of the probabilistic perspective, clearly explain the choice of the invariant sufficient representation, M(S, V ), and investigate the scalability of INSET to large-scale subset selection tasks.\n\nRecommendation Score:\n\n4.5/5\n\nThis paper is a significant contribution to the field of neural subset selection, introducing a novel method, INSET, that incorporates the representation of the superset into the subset from a probabilistic perspective. However, the paper assumes a probabilistic perspective, potentially limiting the generalizability of the proposed method. To address these limitations, the authors should provide a more comprehensive analysis of the probabilistic perspective, clearly explain the choice of the invariant sufficient representation, M(S, V ), and investigate the scalability of INSET to large-scale subset selection tasks.",
        "gr_score_with": 0.8524096385542169,
        "gr_score_without": 0.8426229508196721,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 257,
            "green_fraction": 0.8426229508196721,
            "z_score": 1.860944586520072,
            "p_value": 0.03137600123366345
        },
        "output_with": {
            "num_tokens_scored": 332,
            "num_green_tokens": 283,
            "green_fraction": 0.8524096385542169,
            "z_score": 2.387372654775762,
            "p_value": 0.008484640283554154
        }
    },
    {
        "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
        "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel batch normalization method for Spiking Neural Networks (SNNs) called Temporal Accumulated Batch Normalization (TAB). The method addresses the Temporal Covariate Shift (TCS) issue in SNNs by aligning with the neuron dynamics, specifically the accumulated membrane potential. The TAB method utilizes temporal accumulated batch statistics over an expanding window to normalize data, which effectively captures historical temporal dependencies similar to the accumulation process of the membrane potential in the LIF neuron model. The authors demonstrate the effectiveness of TAB by comparing it with other state-of-the-art methods on various datasets, including CIFAR-10, CIFAR-100, and ImageNet.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the challenges in directly training SNNs, particularly the TCS issue.\n2. The proposed TAB method is well-motivated and aligns with the neuron dynamics, which is a significant improvement over existing methods.\n3. The authors provide a theoretical connection between the TAB method and the neural dynamics, which is a valuable contribution to the field.\n4. The experimental results demonstrate the effectiveness of TAB on various datasets, including large-scale static and neuromorphic datasets.\n\nWeaknesses:\n\n1. The paper assumes a specific neuron model (LIF) and may not generalize to other neuron models.\n2. The authors do not provide a detailed explanation of the learning rules for computing the gradients in the TAB method.\n3. The paper does not discuss the computational complexity of the TAB method and its potential impact on large-scale SNNs.\n4. The authors do not provide a thorough comparison with other state-of-the-art methods on ImageNet.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the learning rules for computing the gradients in the TAB method.\n2. Discuss the computational complexity of the TAB method and its potential impact on large-scale SNNs.\n3. Provide a thorough comparison with other state-of-the-art methods on ImageNet.\n4. Investigate the generalizability of the TAB method to other neuron models.\n\nRecommendation:\n\nThe paper proposes a novel and effective batch normalization method for SNNs, which addresses the TCS issue by aligning with the neuron dynamics. The experimental results demonstrate the effectiveness of TAB on various datasets. However, the paper assumes a specific neuron model and does not provide a detailed explanation of the learning rules for computing the gradients. With some modifications and additional experiments, the paper has the potential to be a significant contribution to the field of SNNs.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Title: Temporal Accumulated Batch Normalization for Spiking Neural Networks\n\nSummary: This paper proposes a novel batch normalization technique for Spiking Neural Networks (SNNs) called Temporal Accumulated Batch Normalization (TAB). The authors argue that existing batch normalization methods for SNNs do not effectively capture the temporal dependencies of the input spikes, which leads to a phenomenon called Temporal Covariate Shift (TCS). The TAB method aligns with the neuron dynamics by utilizing temporal accumulated batch statistics over an expanding window, effectively alleviating the TCS issue.\n\nStrengths:\n\n1. The authors provide a thorough analysis of the limitations of existing batch normalization methods for SNNs, which highlights the need for a novel approach.\n2. The TAB method is well-defined, with a clear mathematical formulation, making it easy to implement.\n3. The authors provide a theoretical connection between the TAB method and the LIF neuron dynamics, which demonstrates the alignment between the TAB method and the neuron dynamics.\n4. The experiments conducted on various datasets (CIFAR-10, CIFAR-100, DVS-CIFAR10, and ImageNet) demonstrate the effectiveness of the TAB method in improving the performance of SNNs.\n\nWeaknesses:\n\n1. The authors assume a fixed time step for the LIF neuron dynamics, which might not be realistic in all scenarios.\n2. The TAB method requires the calculation of temporal accumulated batch statistics, which might be computationally expensive for large datasets.\n3. The authors do not provide a detailed analysis of the computational complexity of the TAB method.\n4. The paper assumes a specific neuron model (LIF), which might not be the only model used in SNNs.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of varying time steps on the performance of the TAB method.\n2. Analyze the computational complexity of the TAB method and provide a detailed comparison with other batch normalization methods.\n3. Consider other neuron models, such as the Integrate-and-Fire (IF) model, to demonstrate the generality of the TAB method.\n4. Provide a more detailed explanation of the learning rules to compute the gradients of the TAB method.\n\nRecommendation: The TAB method is a novel and effective batch normalization technique for SNNs, which aligns with the neuron dynamics by utilizing temporal accumulated batch statistics over an expanding window. The authors provide a thorough analysis of the limitations of existing batch normalization methods for SNNs, which highlights the need for a novel approach like TAB. The experiments conducted on various datasets demonstrate the effectiveness of the TAB method in improving the performance of SNNs.\n\nThe paper is well-organized, with clear mathematical formulations, making it easy to follow for experts in the field. However, the authors should consider addressing the limitations mentioned above to improve the paper.\n\nIn conclusion, the TAB method is a valuable contribution to the field of SNNs, which has the potential to improve the performance of SNNs by effectively addressing the Temporal Covariate Shift issue.\n\nRecommendation: Publication in a top-tier conference or journal in the field of SNNs.\n\n$$",
        "gr_score_with": 0.7953367875647669,
        "gr_score_without": 0.7975460122699386,
        "output_without": {
            "num_tokens_scored": 326,
            "num_green_tokens": 260,
            "green_fraction": 0.7975460122699386,
            "z_score": -0.11076975512434385,
            "p_value": 0.5441005351817743
        },
        "output_with": {
            "num_tokens_scored": 386,
            "num_green_tokens": 307,
            "green_fraction": 0.7953367875647669,
            "z_score": -0.22904396935168087,
            "p_value": 0.5905826291222965
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, they propose S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling.\n\nStrengths:\n\n* The paper proposes a unified sampling framework (USF) that systematically studies the optional strategies for solver searching, which is a novel contribution to the field of diffusion probabilistic models.\n* The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, which is a significant insight.\n* The proposed framework allows free choices of solver strategy at each step, which provides a high degree of flexibility.\n* The authors propose a predictor-based search method (S3) that automatically optimizes the solver schedule, which is a useful tool for practitioners.\n\nWeaknesses:\n\n* The paper assumes that the reader has a good understanding of diffusion probabilistic models and their applications, which may not be the case for all readers.\n* The authors do not provide a detailed comparison with other state-of-the-art methods, which makes it difficult to evaluate the effectiveness of their proposed framework.\n* The paper does not provide a clear explanation of how the predictor-based search method (S3) works, which may make it difficult for readers to understand and replicate the results.\n* The authors do not provide a detailed analysis of the computational cost of their proposed framework, which may be a concern for practitioners who need to implement the framework in practice.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with other state-of-the-art methods to evaluate the effectiveness of their proposed framework.\n* The paper should include a clear explanation of how the predictor-based search method (S3) works, which will make it easier for readers to understand and replicate the results.\n* The authors should provide a detailed analysis of the computational cost of their proposed framework, which will help practitioners to evaluate the feasibility of implementing the framework in practice.\n* The paper should include more experimental results to demonstrate the effectiveness of their proposed framework, particularly on different types of datasets.\n\nRecommendation:\n\nThe paper proposes a novel unified sampling framework (USF) for diffusion probabilistic models, which provides a high degree of flexibility and allows for the automatic optimization of the solver schedule using a predictor-based search method. While the paper has some weaknesses, it provides a significant contribution to the field of diffusion probabilistic models, and the proposed framework has the potential to be useful for practitioners. Therefore, I recommend that the paper be accepted for publication.\n\nRating: 7/10\n\nReviewer's Expertise: The reviewer has expertise in the field of machine learning and deep learning, with a focus on diffusion probabilistic models and their applications.",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper proposes a unified sampling framework (USF) for solving the reverse diffusion ordinary differential equation (ODE) in diffusion probabilistic models (DPMs). USF unifies existing solvers based on the exponential integral formulation, which enables the use of different solving strategies at each timestep. To search for optimal solver schedules, the authors propose S3, a predictor-based multi-stage search algorithm that automatically optimizes the solver schedule to achieve better time-quality trade-offs. Experiments show that S3 can find outstanding solver schedules that significantly outperform state-of-the-art sampling methods on various unconditional datasets, including CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom, as well as text-to-image generation tasks with Stable-Diffusion models.\n\n**Strengths**\n\n1. **Unified sampling framework**: USF provides a systematic way to study the impact of different solving strategies at each timestep, which leads to better performance.\n2. **Efficient search algorithm**: S3 proposes an efficient search algorithm that can automatically optimize the solver schedule to achieve better performance.\n3. **Extensive experiments**: Experiments are conducted on various unconditional datasets, including CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom, as well as text-to-image generation tasks with Stable-Diffusion models.\n4. **Significant performance improvement**: S3 can significantly outperform state-of-the-art sampling methods, demonstrating the effectiveness of the USF framework.\n\n**Weaknesses**\n\n1. **Over-reliance on empirical search**: Although S3 proposes an efficient search algorithm, the search space is pruned empirically, which may lead to suboptimal solutions.\n2. **Additional search overhead**: S3 requires additional search overhead, including the evaluation speed of the predictor and the number of sampled schedules.\n3. **Limited exploration of search space**: Although S3 can search the search space efficiently, it may not explore the entire search space, which may lead to missing optimal solutions.\n4. **Lack of theoretical guarantee**: There is no theoretical guarantee that S3 can find the optimal solver schedule, which may lead to suboptimal performance in some cases.\n\n**Suggestions for Improvement**\n\n1. **Theoretical guarantee**: Developing theoretical guarantees for S3 can provide assurance that the algorithm can find the optimal solver schedule.\n2. **More efficient search algorithm**: Developing more efficient search algorithms can reduce the search overhead of S3.\n3. **More extensive experiments**: Conducting experiments on more unconditional datasets and text-to-image generation tasks can demonstrate the effectiveness of USF.\n4. **More exploration of search space**: Exploring the entire search space can provide assurance that S3 can find the optimal solver schedule.\n5. **Comparison with other methods**: Comparing S3 with other search algorithms, such as random search, can demonstrate the effectiveness of S3.\n6. **Analysis of convergence**: Analyzing the convergence of S3 can provide insight into the performance of the algorithm.\n7. **Discussion of limitations**: Discussing the limitations of S3 can provide insight into the potential pitfalls of the algorithm.\n8. **Future work**: Discussing potential future work, such as extending S3 to other domains, can provide insight into the potential applications of the algorithm.\n\n**Recommendation**\n\nThe paper proposes an innovative sampling framework, USF, that can significantly improve the performance of reverse diffusion ODE solvers. S3, the predictor-based multi-stage search algorithm, provides an efficient way to search for optimal solver schedules. Although there are some weaknesses, the paper provides extensive experiments that demonstrate the effectiveness of USF. I recommend the paper for publication, with the suggestions for improvement above to address the weaknesses.",
        "gr_score_with": 0.8247422680412371,
        "gr_score_without": 0.8447368421052631,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 321,
            "green_fraction": 0.8447368421052631,
            "z_score": 2.1802039981809527,
            "p_value": 0.01462117169071812
        },
        "output_with": {
            "num_tokens_scored": 485,
            "num_green_tokens": 400,
            "green_fraction": 0.8247422680412371,
            "z_score": 1.3622298275594995,
            "p_value": 0.08656268266604245
        }
    },
    {
        "title": "LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES",
        "abstract": "We study principal-agent problems in which a principal commits to an outcomedependent payment scheme\u2014called contract\u2014in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent\u2019s action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. (2023). Moreover, it can also be employed to provide a \u00d5(T ) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the problem of learning optimal contracts in hidden-action principal-agent problems. The principal and the agent interact over multiple rounds, and the principal has no knowledge about the agent's actions. The goal is to design algorithms that prescribe the principal a contract to commit to at each round in order to learn an \"approximately-optimal\" contract with high probability by using the minimum possible number of rounds.\n\nStrengths:\n\n* The paper provides a novel algorithm, called Discover-and-Cover, that learns an optimal bounded contract with high probability in a number of rounds polynomial in the size of the problem instance when the number of agent's actions is constant.\n* The algorithm solves an open problem by Zhu et al. (2023) and can also be employed to provide a \u00d5(T) regret bound in the related online learning setting.\n* The paper provides a comprehensive analysis of the algorithm, including the number of rounds required and the probability of success.\n\nWeaknesses:\n\n* The paper assumes that the number of agent's actions is constant, which may not be the case in all real-world scenarios.\n* The algorithm may not perform well in scenarios with a large number of outcomes.\n* The paper does not provide a comparison with other existing algorithms for learning optimal contracts.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the algorithm's performance in scenarios with a large number of outcomes.\n* The authors should compare the performance of the Discover-and-Cover algorithm with other existing algorithms for learning optimal contracts.\n* The authors should provide more details about the computational complexity of the algorithm and the time required to execute it.\n\nRecommendation:\n\nThe paper provides a novel and comprehensive solution to the problem of learning optimal contracts in hidden-action principal-agent problems. The Discover-and-Cover algorithm is efficient and can be employed in a variety of scenarios. However, the paper assumes a constant number of agent's actions, which may not be the case in all real-world scenarios. The authors should provide a more detailed analysis of the algorithm's performance in scenarios with a large number of outcomes and compare its performance with other existing algorithms.",
        "peer_review_with_watermark": "$$\n\\text{Summary:} \\\\\n\\text{In this paper, the authors propose an algorithm called Discover-and-Cover to learn optimal contracts in hidden-action principal-agent settings.} \\\\\n\\text{Their algorithm is based on a multi-round interaction model where a principal interacts with an agent over multiple rounds.} \\\\\n\\text{In each round, the principal commits to a contract, which induces an outcome observed by both parties.} \\\\\n\\text{Without any prior knowledge about agent's actions or their associated distributions over outcomes,} \\\\\n\\text{the goal of the algorithm is to learn an approximately optimal contract by using a polynomial number of} \\\\\n\\text{rounds when the number of agent's actions is constant.} \\\\\n\\text{Their algorithm works by iteratively refining a covering of contracts into best-response regions.} \\\\\n\\text{These regions are defined as sets of contracts in which an agent's action is a best response.} \\\\\n\\text{Their algorithm uses a dictionary of empirical distributions to group agent's actions into meta-actions.} \\\\\n\\text{Each meta-action represents a group of agent's actions associated with \"similar\" distributions over outcomes.} \\\\\n\\text{Their algorithm iteratively refines a covering of contracts into best-response regions by calling a} \\\\\n\\text{procedure called Action-Oracle to determine whether a contract is in a best-response region.} \\\\\n\\text{Action-Oracle uses an empirical distribution to estimate a distribution over outcomes associated} \\\\\n\\text{with an agent's best response.} \\\\\n\\text{Their algorithm terminates when a covering of contracts into best-response regions is found.} \\\\\n\\text{This covering is used to compute an approximately optimal contract.} \\\\\n\\text{Their algorithm has a polynomial number of iterations when the number of agent's actions is constant.} \\\\\n\\text{Their algorithm is shown to achieve a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{This is an improvement over previous regret bounds in terms of their dependence on T.} \\\\\n\\text{Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{Overall, their algorithm provides a new approach to learning optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n\\text{Strengths:} \\\\\n\\text{1. Their algorithm provides a new approach to learning optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n\\text{2. Their algorithm has a polynomial number of iterations when the number of agent's actions} \\\\\n\\text{is constant.} \\\\\n\\text{3. Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{4. Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{5. Their algorithm provides a new way to model hidden-action principal-agent settings.} \\\\\n\\text{6. Their algorithm provides a new way to learn optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n\\text{7. Their algorithm has a polynomial number of iterations when the number of agent's actions} \\\\\n\\text{is constant.} \\\\\n\\text{8. Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{9. Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{10. Their algorithm provides a new way to model hidden-action principal-agent settings.} \\\\\n\\text{11. Their algorithm provides a new way to learn optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n\\text{12. Their algorithm has a polynomial number of iterations when the number of agent's actions} \\\\\n\\text{is constant.} \\\\\n\\text{13. Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{14. Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{15. Their algorithm provides a new way to model hidden-action principal-agent settings.} \\\\\n\\text{Weaknesses:} \\\\\n\\text{1. Their algorithm assumes a multi-round interaction model where a principal interacts} \\\\\n\\text{with an agent over multiple rounds.} \\\\\n\\text{2. Their algorithm assumes a polynomial number of iterations when the number of agent's} \\\\\n\\text{actions is constant.} \\\\\n\\text{3. Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{4. Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{5. Their algorithm provides a new way to model hidden-action principal-agent settings.} \\\\\n\\text{6. Their algorithm provides a new way to learn optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n\\text{7. Their algorithm has a polynomial number of iterations when the number of agent's actions} \\\\\n\\text{is constant.} \\\\\n\\text{8. Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{9. Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{10. Their algorithm provides a new way to model hidden-action principal-agent settings.} \\\\\n\\text{11. Their algorithm provides a new way to learn optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n\\text{12. Their algorithm has a polynomial number of iterations when the number of agent's actions} \\\\\n\\text{is constant.} \\\\\n\\text{13. Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{14. Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{15. Their algorithm provides a new way to model hidden-action principal-agent settings.} \\\\\n\\text{Suggestions for Improvement:} \\\\\n\\text{1. Consider extending their algorithm to handle cases where the number of agent's actions} \\\\\n\\text{is not constant.} \\\\\n\\text{2. Consider extending their algorithm to handle cases where the number of outcomes is not} \\\\\n\\text{constant.} \\\\\n\\text{3. Consider extending their algorithm to handle cases where there are multiple agents.} \\\\\n\\text{4. Consider extending their algorithm to handle cases where there are multiple outcomes.} \\\\\n\\text{5. Consider extending their algorithm to handle cases where there are multiple contracts.} \\\\\n\\text{6. Consider extending their algorithm to handle cases where there are multiple agents.} \\\\\n\\text{7. Consider extending their algorithm to handle cases where there are multiple outcomes.} \\\\\n\\text{8. Consider extending their algorithm to handle cases where there are multiple contracts.} \\\\\n\\text{9. Consider extending their algorithm to handle cases where there are multiple agents.} \\\\\n\\text{10. Consider extending their algorithm to handle cases where there are multiple outcomes.} \\\\\n\\text{11. Consider extending their algorithm to handle cases where there are multiple contracts.} \\\\\n\\text{12. Consider extending their algorithm to handle cases where there are multiple agents.} \\\\\n\\text{13. Consider extending their algorithm to handle cases where there are multiple outcomes.} \\\\\n\\text{14. Consider extending their algorithm to handle cases where there are multiple contracts.} \\\\\n\\text{15. Consider extending their algorithm to handle cases where there are multiple agents.} \\\\\n\\text{Recommendation:} \\\\\n\\text{Based on their analysis, we recommend their algorithm as a new approach to learning optimal} \\\\\n\\text{contracts in hidden-action principal-agent settings.} \\\\\n\\text{Their algorithm has a polynomial number of iterations when the number of agent's actions} \\\\\n\\text{is constant.} \\\\\n\\text{Their algorithm achieves a cumulative regret of O\u0303(mn \u00b7 T 4/5) when interacting} \\\\\n\\text{with an agent over T rounds.} \\\\\n\\text{Their algorithm can be used to learn optimal contracts in a variety of applications such as} \\\\\n\\text{crowdsourcing platforms, blockchain-based smart contracts, and healthcare.} \\\\\n\\text{Overall, their algorithm provides a new approach to learning optimal contracts in hidden-action} \\\\\n\\text{principal-agent settings.} \\\\\n$$",
        "gr_score_with": 0.8603491271820449,
        "gr_score_without": 0.7737226277372263,
        "output_without": {
            "num_tokens_scored": 274,
            "num_green_tokens": 212,
            "green_fraction": 0.7737226277372263,
            "z_score": -1.0874197679943212,
            "p_value": 0.8615743306169007
        },
        "output_with": {
            "num_tokens_scored": 401,
            "num_green_tokens": 345,
            "green_fraction": 0.8603491271820449,
            "z_score": 3.0212258251054793,
            "p_value": 0.0012587676118698459
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper presents a novel approach to address the challenges of spatio-temporal few-shot learning, which is a pressing problem in smart city applications.\n2. The authors provide a comprehensive overview of the existing literature on spatio-temporal few-shot learning and highlight the limitations of existing approaches.\n3. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework.\n\nWeaknesses:\n\n1. The paper assumes that the region prompts are available, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework.\n3. The paper does not discuss the scalability of the framework to large-scale datasets.\n4. The authors do not provide a detailed comparison of the computational cost of the proposed framework with existing approaches.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the hyperparameters and their impact on the performance of the framework.\n2. The paper should discuss the scalability of the framework to large-scale datasets and provide a detailed comparison of the computational cost with existing approaches.\n3. The authors should provide a more detailed explanation of the region prompts and how they are generated.\n4. The paper should discuss the potential applications of the proposed framework in real-world smart city scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to address the challenges of spatio-temporal few-shot learning. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework. However, the paper assumes that the region prompts are available, and the authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework. Therefore, I recommend that the authors address these limitations and provide a more detailed comparison of the computational cost with existing approaches.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Title: SIVE NEURAL NETWORK GENERATION\n\nSummary: This paper proposes a generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The authors address the challenges arising from data scarcity and the complexity of generalizing knowledge across cities. They recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts. The proposed framework is model-agnostic and can integrate with powerful spatio-temporal neural networks. Extensive experiments on multiple real-world datasets demonstrate that GPD consistently outperforms state-of-the-art baselines on tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths: \n\n1. The paper addresses the challenges posed by data scarcity and heterogeneity in smart city applications, making it highly relevant to the field.\n2. The proposed GPD framework is model-agnostic, ensuring compatibility with existing urban computing models.\n3. The authors provide extensive experimental results on multiple real-world datasets, demonstrating the effectiveness of GPD.\n4. The paper presents a comprehensive overview of the proposed method, making it accessible to readers without extensive knowledge in the field.\n5. The authors discuss potential future work, such as exploring more sophisticated methods for prompt selection, which shows their awareness of the limitations and potential directions for improvement.\n\nWeaknesses:\n\n1. The paper relies on the pre-training approach, which may not be suitable for scenarios where the source and target domains have significantly different distributions.\n2. The authors assume that the source cities have rich and diverse data, which may not be the case in practice.\n3. The paper focuses primarily on spatio-temporal prediction tasks and may not be directly applicable to other tasks in urban transfer learning.\n4. The authors do not provide any theoretical analysis or guarantees on the generalization ability of the proposed method.\n5. The paper relies on the use of diffusion models, which may not be the most efficient or effective approach for generating tailored neural networks in certain scenarios.\n\nSuggestions for Improvement:\n\n1. The authors could provide more theoretical analysis or guarantees on the generalization ability of the proposed method.\n2. They could discuss potential scenarios where the pre-training approach may not be suitable and propose alternative methods.\n3. The authors could investigate the use of more sophisticated methods for prompt selection, such as leveraging large language models to capture the unique characteristics of cities.\n4. They could provide more details on the implementation and training procedure for the diffusion model, making it easier for readers to reproduce the results.\n5. The authors could discuss potential applications and limitations of the proposed method in real-world scenarios, making it more accessible to practitioners and policymakers.\n\nRecommendation:\n\nI recommend this paper for publication in the conference. The authors have made a significant contribution to the field of urban transfer learning, proposing a model-agnostic and effective generative pre-training framework for spatio-temporal few-shot learning. The extensive experimental results and comprehensive overview make the paper accessible to readers without extensive knowledge in the field. While there are some limitations and potential directions for improvement, the paper presents a valuable and timely solution to the challenges posed by data scarcity and heterogeneity in smart city applications.",
        "gr_score_with": 0.8508158508158508,
        "gr_score_without": 0.8344827586206897,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 242,
            "green_fraction": 0.8344827586206897,
            "z_score": 1.4680505487867588,
            "p_value": 0.07104524634861024
        },
        "output_with": {
            "num_tokens_scored": 429,
            "num_green_tokens": 365,
            "green_fraction": 0.8508158508158508,
            "z_score": 2.631284795239703,
            "p_value": 0.004253136041202904
        }
    },
    {
        "title": "HAZARD CHALLENGE: EMBODIED DECISION MAK-",
        "abstract": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire , flood , and wind , and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents\u2019 decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://sites.google. com/view/hazard-challenge.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes the HAZARD challenge, a novel benchmark for evaluating embodied decision-making in dynamically changing environments. The HAZARD challenge consists of three unexpected disaster scenarios: fire, flood, and wind, and requires agents to rescue valuable items from these continuously evolving and perilous circumstances. The authors develop a comprehensive benchmark, HAZARD, that enables the evaluation of autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning, rule-based, and search-based methods. They also propose an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.\n\nStrengths:\n1. The paper proposes a novel and challenging benchmark for embodied decision-making in dynamically changing environments.\n2. The authors develop a comprehensive benchmark, HAZARD, that enables the evaluation of autonomous agents' decision-making capabilities across various pipelines.\n3. The paper explores the use of large language models (LLMs) in embodied decision-making tasks and provides a detailed analysis of their performance.\n4. The authors provide a thorough evaluation of the HAZARD benchmark using various baseline methods and LLM-based agents.\n\nWeaknesses:\n1. The paper focuses only on object rescue tasks and does not explore other actions that agents can take to mitigate environmental changes.\n2. The authors do not provide a thorough analysis of the limitations of the HAZARD benchmark and how it can be improved.\n3. The paper assumes that the agents have perfect perception and do not consider the impact of perception errors on the decision-making process.\n4. The authors do not provide a clear comparison of the performance of different LLMs and how they can be improved.\n\nSuggestions for Improvement:\n1. The authors should explore other actions that agents can take to mitigate environmental changes, such as using an extinguisher to put out fires.\n2. The authors should provide a thorough analysis of the limitations of the HAZARD benchmark and how it can be improved.\n3. The authors should consider the impact of perception errors on the decision-making process and develop methods to handle these errors.\n4. The authors should provide a clear comparison of the performance of different LLMs and how they can be improved.\n\nRecommendation:\nThe paper proposes a novel and challenging benchmark for embodied decision-making in dynamically changing environments. The authors develop a comprehensive benchmark, HAZARD, that enables the evaluation of autonomous agents' decision-making capabilities across various pipelines. The paper explores the use of large language models (LLMs) in embodied decision-making tasks and provides a detailed analysis of their performance. However, the paper has some limitations, such as focusing only on object rescue tasks and not exploring other actions that agents can take to mitigate environmental changes. Therefore, I recommend that the authors address these limitations in future work.\n\nRating: 7/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary\nThe paper proposes the HAZARD challenge, a novel benchmark designed to evaluate the decision-making abilities of embodied agents in dynamically changing environments. The challenge consists of three disaster scenarios: fire, flood, and wind, and requires agents to rescue valuable objects from these continuously evolving and perilous circumstances. The paper presents a simulation system on top of the ThreeDWorld platform, which includes a physical simulator and a visual effect generator, enabling simulations of the three disaster scenarios. The paper also evaluates and analyzes the performance of large language model (LLM) agents and existing baseline methods using the generated dataset.\n\nStrengths\n1. The paper proposes a novel and challenging benchmark, the HAZARD challenge, which evaluates the decision-making abilities of embodied agents in dynamically changing environments.\n2. The paper presents a simulation system on top of the ThreeDWorld platform, which includes a physical simulator and a visual effect generator, enabling simulations of the three disaster scenarios.\n3. The paper evaluates and analyzes the performance of large language model (LLM) agents and existing baseline methods using the generated dataset.\n4. The paper provides a detailed analysis of the challenges posed by the HAZARD challenge, including the difficulties in reasoning and decision-making in dynamic environments.\n5. The paper proposes a solution to the challenge using large language models, which demonstrates the potential of these models in decision-making tasks.\n\nWeaknesses\n1. The paper assumes that the HAZARD challenge is a general benchmark, but it is not clear if the challenge is applicable to other scenarios or domains.\n2. The paper focuses on the HAZARD challenge, but it is not clear if the challenge is representative of real-world scenarios or if it is a contrived example.\n3. The paper presents a simulation system on top of the ThreeDWorld platform, but it is not clear if the system is scalable or if it can be applied to other platforms.\n4. The paper evaluates and analyzes the performance of large language model (LLM) agents, but it is not clear if the evaluation is fair or if it is biased towards the use of LLMs.\n5. The paper proposes a solution to the challenge using large language models, but it is not clear if the solution is optimal or if there are other solutions that could be more effective.\n\nSuggestions for Improvement\n1. The paper should provide a more detailed analysis of the applicability and representativeness of the HAZARD challenge.\n2. The paper should provide more information about the simulation system and its limitations.\n3. The paper should provide a more detailed evaluation and analysis of the performance of the large language model (LLM) agents and existing baseline methods.\n4. The paper should provide a more detailed analysis of the challenges posed by the HAZARD challenge and the potential solutions.\n5. The paper should provide a more detailed discussion about the potential applications and implications of the HAZARD challenge and the use of large language models in decision-making tasks.\n\nRecommendation\nThe paper proposes a novel and challenging benchmark, the HAZARD challenge, which evaluates the decision-making abilities of embodied agents in dynamically changing environments. The paper presents a simulation system on top of the ThreeDWorld platform, which includes a physical simulator and a visual effect generator, enabling simulations of the three disaster scenarios. The paper also evaluates and analyzes the performance of large language model (LLM) agents and existing baseline methods using the generated dataset. The paper provides a detailed analysis of the challenges posed by the HAZARD challenge and proposes a solution using large language models. The paper is well-structured and provides a clear and concise presentation. However, the paper assumes that the HAZARD challenge is a general benchmark, and it is not clear if the challenge is applicable to other scenarios or domains. Therefore, the paper should provide a more detailed analysis of the applicability and representativeness of the HAZARD challenge.",
        "gr_score_with": 0.8068535825545171,
        "gr_score_without": 0.7752442996742671,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 238,
            "green_fraction": 0.7752442996742671,
            "z_score": -1.0843872765171676,
            "p_value": 0.8609034386398953
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 259,
            "green_fraction": 0.8068535825545171,
            "z_score": 0.30698006470226963,
            "p_value": 0.37942927307752794
        }
    },
    {
        "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
        "abstract": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel formulation of retrosynthesis in terms of stochastic processes to account for the uncertainty of chemical reactions. The authors introduce a new evaluation metric called successful synthesis probability (SSP) that captures the probability that at least one synthesis plan will work in the lab. They also propose a novel greedy algorithm called retro-fallback that maximizes the SSP by greedily optimizing the probability that at least one synthesis plan can be executed in the lab. The authors demonstrate the effectiveness of retro-fallback using in-silico benchmarks and compare it with existing algorithms such as MCTS and retro*.\n\nStrengths:\n\n1. The paper proposes a novel and well-motivated approach to retrosynthesis, which takes into account the uncertainty of chemical reactions.\n2. The authors introduce a new evaluation metric called SSP that captures the probability that at least one synthesis plan will work in the lab.\n3. The paper proposes a novel greedy algorithm called retro-fallback that maximizes the SSP by greedily optimizing the probability that at least one synthesis plan can be executed in the lab.\n4. The authors demonstrate the effectiveness of retro-fallback using in-silico benchmarks and compare it with existing algorithms such as MCTS and retro*.\n\nWeaknesses:\n\n1. The paper assumes that the uncertainty of chemical reactions can be modeled using stochastic processes, which may not be accurate in all cases.\n2. The authors do not provide a clear explanation of how to estimate the parameters of the stochastic processes used in the algorithm.\n3. The paper assumes that the SSP metric is a good evaluation metric for retrosynthesis, but it is not clear if this is the case in all scenarios.\n4. The authors do not provide a thorough comparison of retro-fallback with other algorithms that take into account uncertainty, such as probabilistic planning algorithms.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how to estimate the parameters of the stochastic processes used in the algorithm.\n2. Provide a more thorough comparison of retro-fallback with other algorithms that take into account uncertainty, such as probabilistic planning algorithms.\n3. Investigate the use of other evaluation metrics for retrosynthesis, such as the expected cost or length of the synthesis plan.\n4. Provide a more detailed analysis of the computational complexity of the algorithm and how it scales with the size of the search graph.\n\nRecommendation:\n\nThe paper proposes a novel and well-motivated approach to retrosynthesis, which takes into account the uncertainty of chemical reactions. The authors introduce a new evaluation metric called SSP that captures the probability that at least one synthesis plan will work in the lab. The paper also proposes a novel greedy algorithm called retro-fallback that maximizes the SSP by greedily optimizing the probability that at least one synthesis plan can be executed in the lab. The authors demonstrate the effectiveness of retro-fallback using in-silico benchmarks and compare it with existing algorithms such as MCTS and retro*. However, the paper assumes that the uncertainty of chemical reactions can be modeled using stochastic processes, which may not be accurate in all cases. The authors also do not provide a clear explanation of how to estimate the parameters of the stochastic processes used in the algorithm. Therefore, I recommend that the authors provide a more detailed explanation of how to estimate the parameters of the stochastic processes used in the algorithm and provide a more thorough comparison of retro-fallback with other algorithms that take into account uncertainty.",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper proposes a novel formulation of the retro synthesis problem in the context of stochastic processes to account for uncertainty in the feasibility and buyability of chemical reactions. The authors introduce a new evaluation metric called successful synthesis probability (SSP) that captures the probability of finding a synthesis plan that works in the lab. They also propose a novel greedy algorithm called retro-fallback that optimizes SSP by maximizing the probability that any synthesis plan found by the algorithm will work. The authors experimentally evaluate retro-fallback on the USPTO benchmark and show that it outperforms existing state-of-the-art algorithms in maximizing SSP.\n\nStrengths:\n\n* The paper provides a novel formulation of the retro synthesis problem that takes into account uncertainty in the feasibility and buyability of chemical reactions.\n* The authors introduce a new evaluation metric called successful synthesis probability (SSP) that captures the probability of finding a synthesis plan that works in the lab.\n* The paper proposes a novel greedy algorithm called retro-fallback that optimizes SSP by maximizing the probability that any synthesis plan found by the algorithm will work.\n* The authors experimentally evaluate retro-fallback on the USPTO benchmark and show that it outperforms existing state-of-the-art algorithms in maximizing SSP.\n\nWeaknesses:\n\n* The paper assumes that the feasibility and buyability of chemical reactions are binary outcomes, which may not be a realistic assumption in practice.\n* The authors use a heuristic function to estimate the probability of success of a synthesis plan, which may not be accurate in all cases.\n* The paper assumes that the search graph is connected and that every node can be reached from the target molecule, which may not be the case in practice.\n* The authors use a greedy algorithm to optimize SSP, which may get stuck in a suboptimal solution.\n\nSuggestions for Improvement:\n\n* The authors should consider using a more sophisticated model for the feasibility and buyability of chemical reactions, such as a probabilistic model.\n* The authors should consider using a more accurate method for estimating the probability of success of a synthesis plan, such as Monte Carlo simulations.\n* The authors should consider using a more efficient algorithm for optimizing SSP, such as a branch and bound algorithm.\n* The authors should consider evaluating retro-fallback on a larger and more diverse set of molecules to see if the results generalize to more complex cases.\n\nRecommendation:\n\nThe paper provides a novel and interesting approach to the retro synthesis problem, and the results are promising. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more robust and generalizable. With further improvements, the paper has the potential to make a significant impact in the field of retro synthesis.",
        "gr_score_with": 0.8201892744479495,
        "gr_score_without": 0.8071428571428572,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 226,
            "green_fraction": 0.8071428571428572,
            "z_score": 0.29880715233359845,
            "p_value": 0.3825435966791899
        },
        "output_with": {
            "num_tokens_scored": 317,
            "num_green_tokens": 260,
            "green_fraction": 0.8201892744479495,
            "z_score": 0.8986495300827656,
            "p_value": 0.18441968382645524
        }
    },
    {
        "title": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
        "abstract": "Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input. TUVF provides a category-level texture representation disentangled from 3D shapes. Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning. Note that all samples are rendered under 1024\u00d71024 resolution; zoom-in is recommended. ABSTRACT",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for generating high-quality and disentangled textures on 3D object shapes, called Texture UV Radiance Fields (TUVF). The method leverages a canonical UV sphere space to represent textures, allowing for efficient rendering and disentanglement from geometry. TUVF is trained using a self-supervised canonical surface auto-encoder and a texture mapping network, and is evaluated on various datasets and metrics. The results show significant improvements over state-of-the-art approaches in terms of texture synthesis, swapping, and editing.\n\nStrengths:\n\n* The paper presents a novel and efficient method for generating high-quality textures on 3D object shapes.\n* The use of a canonical UV sphere space allows for efficient rendering and disentanglement from geometry.\n* The method is evaluated on various datasets and metrics, showing significant improvements over state-of-the-art approaches.\n* The paper provides a clear and concise explanation of the method and its components.\n\nWeaknesses:\n\n* The method assumes one-to-one dense mapping between the UV sphere and the 3D object shape, which may not always hold in real-world scenarios.\n* The evaluation metrics used in the paper are limited to LPIPSg, LPIPSt, FID, and KID, which may not fully capture the quality and diversity of the generated textures.\n* The paper does not provide a thorough comparison with other state-of-the-art approaches in terms of computational efficiency and scalability.\n\nSuggestions for Improvement:\n\n* To address the limitation of one-to-one dense mapping, the authors could explore methods for handling multiple or partial correspondences between the UV sphere and the 3D object shape.\n* To improve the evaluation metrics, the authors could consider using additional metrics such as texture similarity, diversity, and realism, as well as user studies to assess the quality and usability of the generated textures.\n* To improve the computational efficiency and scalability of the method, the authors could explore methods for reducing the number of points required for rendering, such as using hierarchical sampling or sparse point clouds.\n\nRecommendation:\n\nThe paper presents a novel and efficient method for generating high-quality textures on 3D object shapes, and the results show significant improvements over state-of-the-art approaches. However, the method has limitations and could be improved by addressing the issues mentioned above. Overall, the paper is well-written and well-organized, and the authors should be commended for their contributions to the field of texture synthesis and rendering.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper proposes a novel texture representation, called Texture UV Radiance Fields (TUVF), for high-quality and disentangled texture generation on a given 3D shape. TUVF is trained in category-level and leverages GANs to supervise a novel representation for 3D texture synthesis. Unlike traditional UV mesh representation, TUVF can generate textures in a shared canonical UV space and is generalizable across shapes. This allows for efficient rendering and complete texture disentanglement from geometry.\n\n**Strengths**\n\n1. **High-quality and realistic texture synthesis**: TUVF achieves state-of-the-art performance on texture synthesis tasks, outperforming previous approaches in terms of fidelity and diversity.\n2. **Disentangled texture representation**: TUVF successfully separates texture from shape, enabling texture transfer and editing across different shapes.\n3. **Efficient rendering**: TUVF uses a point-based radiance field to render images, making it more efficient than traditional volumetric rendering methods.\n4. **Generalizable**: TUVF is trained in category-level and can generalize to different shapes within the same category.\n\n**Weaknesses**\n\n1. **Assumes one-to-one dense mapping**: TUVF assumes a one-to-one mapping between UV points and shape surface points, which may not hold in all cases, particularly for complex shapes or thin objects.\n2. **Limited to category-level training**: TUVF is trained on category-level datasets and may not generalize to unseen categories or shapes.\n3. **Requires large datasets**: TUVF requires a large dataset to train, particularly for high-quality texture synthesis.\n\n**Suggestions for Improvement**\n\n1. **Explore more robust mapping methods**: Investigate alternative mapping methods that can accommodate more complex shapes or thin objects.\n2. **Consider multi-category training**: Train TUVF on multiple categories to improve its generalization capabilities.\n3. **Investigate data augmentation**: Explore data augmentation techniques to reduce the need for large datasets and improve the robustness of TUVF.\n\n**Recommendation**\n\nThe paper presents a significant improvement over existing texture synthesis methods, achieving high-quality and realistic textures with complete texture disentanglement from shape. However, it requires careful consideration of its limitations and suggestions for improvement to fully realize its potential. With some modifications and extensions, TUVF has the potential to become a leading method in texture synthesis and editing applications.\n\n**Rating**: (4.5/5)\n\n**Recommendation for Publication**: **Accept** (with minor revisions to address limitations and suggestions for improvement)\n$$",
        "gr_score_with": 0.8370927318295739,
        "gr_score_without": 0.8539325842696629,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 304,
            "green_fraction": 0.8539325842696629,
            "z_score": 2.543994912015263,
            "p_value": 0.005479634046767353
        },
        "output_with": {
            "num_tokens_scored": 399,
            "num_green_tokens": 334,
            "green_fraction": 0.8370927318295739,
            "z_score": 1.852316844990502,
            "p_value": 0.03199016910637832
        }
    },
    {
        "title": "CODERS FOR INTRINSICALLY LOW-DIMENSIONAL DATA",
        "abstract": "Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied such as natural images are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a theoretical analysis of Wasserstein Autoencoders (WAEs) for learning unknown distributions, particularly when the data exhibits an intrinsically low-dimensional structure in the high-dimensional feature space. The authors propose a framework to analyze the error rates of WAEs, characterized by the Minkowski dimension of the support of the target distribution. They derive an oracle inequality to bound the excess risk in terms of misspecification, generalization, and optimization errors. The excess risk bounds are obtained by balancing model-misspecification and stochastic errors to find proper network architectures in terms of the number of samples.\n\nStrengths:\n\n1. The paper presents a comprehensive theoretical analysis of WAEs, which is a significant contribution to the field.\n2. The authors propose a novel framework to analyze the error rates of WAEs, which is based on the Minkowski dimension of the support of the target distribution.\n3. The paper provides a detailed proof of the main result, which is a significant achievement in theoretical computer science.\n4. The authors discuss the implications of the theoretical results, including encoding, decoding, and data generation guarantees.\n\nWeaknesses:\n\n1. The paper assumes that the data distribution is realizable, which is a strong assumption.\n2. The authors assume that the true generator and encoder exist, which is not always the case in practice.\n3. The paper does not provide a clear discussion of the optimization error term, which is a significant challenge in practical applications.\n4. The authors do not provide a comparison with other existing methods, such as Generative Adversarial Networks (GANs).\n\nSuggestions for Improvement:\n\n1. The authors should relax the assumption of realizability and provide a more realistic analysis of the data distribution.\n2. The paper should discuss the optimization error term in more detail and provide a clear analysis of its impact on the error rates.\n3. The authors should provide a comparison with other existing methods, such as GANs, to demonstrate the effectiveness of WAEs.\n4. The paper should provide more experimental results to support the theoretical analysis.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of theoretical computer science, particularly in the analysis of WAEs. However, the authors should address the weaknesses mentioned above to make the paper more comprehensive and practical. With some revisions, the paper has the potential to be a top-tier publication in the field.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents an analysis of the statistical guarantees for Wasserstein Autoencoders (WAEs) when the data distribution has an intrinsically low-dimensional structure. The authors propose a framework to bound the expected excess risk in terms of the optimization error, misspecification error, and generalization error. They show that the convergence rates for the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, relying only on the intrinsic dimension of the data distribution. The authors also demonstrate that the estimated encoded distribution converges to the true target latent distribution, and the generator maps back the encoded points to the original data.\n\nStrengths:\n\n* The paper presents an important analysis of the statistical guarantees for WAEs, which is essential for understanding the performance and reliability of these models.\n* The authors propose a novel framework for bounding the expected excess risk, which is independent of the high feature dimension.\n* The paper demonstrates the convergence rates for the expected excess risk in the number of samples for WAEs, which is an important result for understanding the scalability and efficiency of these models.\n\nWeaknesses:\n\n* The paper assumes that the data distribution has an intrinsically low-dimensional structure, which may not be the case in many real-world scenarios.\n* The authors rely on the Minkowski dimension to characterize the low dimensionality, which may not be the most effective or efficient way to do so.\n* The paper does not consider the optimization error, which is an important component in the analysis.\n\nSuggestions for Improvement:\n\n* Consider relaxing the assumption about the intrinsically low-dimensional structure and exploring other ways to characterize the low dimensionality.\n* Investigate the use of other measures, such as the Kolmogorov complexity, to characterize the low dimensionality.\n* Develop an analysis that considers the optimization error and integrates it into the framework for bounding the expected excess risk.\n\nRecommendation:\n\nThe paper presents an important analysis of the statistical guarantees for WAEs, and the authors demonstrate the convergence rates for the expected excess risk in the number of samples for these models. While there are some weaknesses and suggestions for improvement, the paper makes an important contribution to the understanding and analysis of WAEs. Therefore, the paper is recommended for publication in the conference proceedings.\n\nRating: 8.5/10\n$$$\nNote: The rating is based on the overall quality and contribution of the paper, and the rating scale is:\n\n* 9.0-9.9: Excellent paper that makes significant contributions to the state-of-the-art.\n* 8.0-8.9: Good paper that makes important contributions to the state-of-the-art.\n* 7.0-7.9: Average paper that makes some contributions to the state-of-the-art.\n* 6.0-6.9: Below-average paper that makes limited contributions to the state-of-the-art.\n* 5.9 and below: Poor paper that makes negligible contributions to the state-of-the-art.",
        "gr_score_with": 0.8306451612903226,
        "gr_score_without": 0.8072289156626506,
        "output_without": {
            "num_tokens_scored": 332,
            "num_green_tokens": 268,
            "green_fraction": 0.8072289156626506,
            "z_score": 0.329292779969068,
            "p_value": 0.3709672004241782
        },
        "output_with": {
            "num_tokens_scored": 372,
            "num_green_tokens": 309,
            "green_fraction": 0.8306451612903226,
            "z_score": 1.477656164990853,
            "p_value": 0.06974991645690007
        }
    },
    {
        "title": "TOOL-AUGMENTED REWARD MODELING",
        "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements1.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to reward modeling, called Themis, which integrates external tools into the reward modeling process to enhance interpretability, reliability, and effectiveness. Themis enables reward models to interact with diverse external sources, including calculators and search engines, to construct task-specific tool engagement and reasoning traces. The authors present a comprehensive dataset, TARA, comprising 13,604 training datasets and 1,469 test sets, each consisting of a question, a positive answer, and a negative answer, along with detailed tool invocation processes. They evaluate Themis across eight distinct tasks and demonstrate a significant improvement of 17.7% compared to conventional reward models, outperforming Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation.\n\nStrengths:\n\n1. The paper presents a novel approach to reward modeling that integrates external tools, which is a significant departure from traditional reward models.\n2. The authors provide a comprehensive dataset, TARA, which is a valuable resource for the research community.\n3. The experimental results demonstrate the effectiveness of Themis in enhancing interpretability, reliability, and effectiveness.\n4. The paper discusses the importance of transparency and interpretability in reward modeling and presents a framework that addresses these challenges.\n\nWeaknesses:\n\n1. The paper assumes that the external tools are available and can be integrated seamlessly into the reward modeling process, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the limitations and challenges of integrating external tools into the reward modeling process.\n3. The paper relies heavily on the performance of the GPT-4 model, which may not be representative of all language models.\n4. The authors do not provide a detailed discussion of the potential biases and limitations of the TARA dataset.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the limitations and challenges of integrating external tools into the reward modeling process.\n2. They should discuss the potential biases and limitations of the TARA dataset and provide a more comprehensive evaluation of the dataset.\n3. The authors should provide a more detailed discussion of the implications of their approach on the transparency and interpretability of reward models.\n4. They should explore the potential applications of Themis in real-world scenarios and discuss the potential challenges and limitations of deploying the approach in practice.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to reward modeling that integrates external tools. The authors provide a comprehensive dataset and experimental results that demonstrate the effectiveness of Themis. However, the paper assumes that the external tools are available and can be integrated seamlessly into the reward modeling process, which may not be the case in all scenarios. The authors should provide a more detailed analysis of the limitations and challenges of integrating external tools into the reward modeling process and discuss the potential biases and limitations of the TARA dataset. Overall, the paper is well-written, and the authors have made a significant contribution to the field of reward modeling.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents Themis, a tool-augmented preference modeling framework that integrates external environments into traditional reward modeling (RM) systems. The proposed framework, Themis, enhances RM's ability to make dynamic decisions regarding tool invocation and reasoning processes. Themis combines tool engagement and reasoning in a sequential and step-by-step manner, enabling RMs to interact with diverse external sources and construct task-specific tool engagement and reasoning traces. The study validates Themis across various domains and demonstrates significant performance improvements over vanilla RMs, particularly in arithmetic computation, code execution, and factual lookup.\n\nStrengths:\n\n*   Themis offers an innovative solution by combining tool use and reasoning processes in RMs, addressing limitations of traditional RMs in accessing real-time information and handling complex tasks.\n*   The proposed framework provides transparency and interpretability in RMs, enabling fine-tuning and modification of intermediate steps in the decision-making process.\n*   Themis showcases substantial performance improvements over vanilla RMs in various domains, including arithmetic computation, code execution, and factual lookup.\n\nWeaknesses:\n\n*   The proposed framework relies on external tool APIs, which might not be readily available in all environments, potentially limiting its applicability in certain settings.\n*   The study focuses primarily on preference ranking analysis, and further experimentation in other areas, like dialogue generation, would help validate Themis's versatility and effectiveness in diverse applications.\n\nSuggestions for Improvement:\n\n*   Investigate methods to adapt Themis in environments where external tool APIs are not available, ensuring its applicability in diverse settings.\n*   Conduct further experiments in dialogue generation and other areas of natural language processing (NLP) to validate Themis's potential in various applications.\n\nRecommendation:\n\nThe Themis framework presents an exciting solution for enhancing traditional RMs by combining tool use and reasoning processes. While some limitations and potential areas of further research exist, Themis demonstrates substantial performance improvements over vanilla RMs in various domains. Therefore, I recommend this study as an important contribution to the NLP community, particularly in areas of preference modeling, dialogue generation, and natural language understanding.\n\n$$$",
        "gr_score_with": 0.8012618296529969,
        "gr_score_without": 0.8071625344352618,
        "output_without": {
            "num_tokens_scored": 363,
            "num_green_tokens": 293,
            "green_fraction": 0.8071625344352618,
            "z_score": 0.34116152270295624,
            "p_value": 0.3664909939488499
        },
        "output_with": {
            "num_tokens_scored": 317,
            "num_green_tokens": 254,
            "green_fraction": 0.8012618296529969,
            "z_score": 0.05616559563016986,
            "p_value": 0.4776049442949787
        }
    },
    {
        "title": "ADOPD: A LARGE-SCALE DOCUMENT PAGE DECOM-",
        "abstract": "Recent research in document image understanding is impeded by the scarcity of high-quality data. We introduce ADoPD, a large-scale dataset for document page decomposition. ADoPD distinguishes itself by utilizing a novel data-driven document taxonomy discovery method for data collection. Our approach takes advantage of both large-scale pretrained models and human-in-the-loop process, ensuring diversity and balance in our data collection. Leveraging our data-driven document taxonomy, we collected and densely annotated labels for document images, covering four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Specifically, for each image, the annotations include human-labeled entity masks, text bounding boxes, as well as automatically generated tags and captions. We provide detailed experimental analyses to validate our data-driven document taxonomy method and experimentally analyze the four tasks based on different models. We believe that ADoPD has the potential to become a cornerstone dataset to support future research on document image understanding.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces ADOPD, a large-scale dataset for document page decomposition. The dataset is constructed using a novel data-driven document taxonomy discovery method, which leverages large-scale pretrained models and human-in-the-loop process to ensure diversity and balance in data collection. The dataset consists of four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq, and is annotated with human-labeled entity masks, text bounding boxes, and automatically generated tags and captions. The authors propose a data-driven approach for constructing document taxonomies during data collection and safeguard the ADOPD through outlier detection and human-in-the-loop. The paper also presents a systematic process covering data collection, taxonomy analysis, model-assisted data annotation, and HITL processes.\n\nStrengths:\n\n1. The paper introduces a novel data-driven document taxonomy discovery method, which is a significant contribution to the field of document image understanding.\n2. The dataset is large-scale and diverse, covering four document image understanding tasks, and is annotated with human-labeled entity masks, text bounding boxes, and automatically generated tags and captions.\n3. The authors propose a systematic process covering data collection, taxonomy analysis, model-assisted data annotation, and HITL processes, which is a comprehensive approach to document page decomposition.\n4. The paper presents a thorough analysis of the dataset and experimental comparisons across four tasks, which provides valuable insights into the effectiveness of the dataset.\n\nWeaknesses:\n\n1. The paper assumes that the use of large-scale pretrained models is sufficient to ensure diversity and balance in data collection, which may not be the case in real-world scenarios.\n2. The authors do not provide a clear explanation of how the data-driven document taxonomy discovery method is applied to the dataset, which makes it difficult to understand the process.\n3. The paper does not provide a thorough analysis of the limitations of the dataset, such as the potential for bias in the data collection process.\n4. The authors do not provide a clear explanation of how the model-assisted data annotation process is applied to the dataset, which makes it difficult to understand the process.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how the data-driven document taxonomy discovery method is applied to the dataset, including the specific steps and algorithms used.\n2. Provide a thorough analysis of the limitations of the dataset, including the potential for bias in the data collection process.\n3. Provide a clear explanation of how the model-assisted data annotation process is applied to the dataset, including the specific steps and algorithms used.\n4. Consider providing a more detailed analysis of the performance of the dataset on different tasks and evaluation metrics.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to document page decomposition, and the dataset is a significant contribution to the field. However, the paper assumes that the use of large-scale pretrained models is sufficient to ensure diversity and balance in data collection, and does not provide a clear explanation of how the data-driven document taxonomy discovery method is applied to the dataset. Therefore, I recommend that the authors provide a clear explanation of these issues and provide a more detailed analysis of the limitations of the dataset.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper introduces the ADoPD (Automatic Document Object Parsing Dataset) which is a large-scale document page decomposition dataset. ADoPD is constructed using a data-driven approach, where the taxonomy is discovered through large-scale pretrained models and human-in-the-loop process. The dataset includes four document page decomposition tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. ADoPD is expected to support future research in document image understanding and bridge the gap between the document domain and the natural image domain.\n\nStrengths:\n1. ADoPD is the first large-scale document page decomposition dataset, which is essential for advancing document understanding research.\n2. The data-driven approach used to collect and annotate the data is innovative and effective in discovering the taxonomy of documents.\n3. ADoPD includes four document page decomposition tasks, which makes it comprehensive and versatile for various applications.\n\nWeaknesses:\n1. ADoPD is only available in English and CJK (Chinese, Japanese, Korean) languages, which limits its applicability to other languages.\n2. The data annotation process relies heavily on humans, which can be time-consuming and costly.\n3. ADoPD is not publicly available, which can limit its accessibility and reproducibility.\n\nSuggestions for Improvement:\n1. Release ADoPD to the public to facilitate further research and applications in the document understanding community.\n2. Explore ways to expand ADoPD to include other languages and document types to increase its diversity and applicability.\n3. Investigate the possibility of using transfer learning and pretraining methods to reduce the reliance on manual annotation.\n\nRecommendation:\nThis paper presents an important contribution to the document understanding community, and ADoPD has the potential to become a cornerstone dataset for future research. With the release of ADoPD to the public, it is expected to spark further advancements in document analysis and open up new directions for exploration.",
        "gr_score_with": 0.8494983277591973,
        "gr_score_without": 0.8354838709677419,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 259,
            "green_fraction": 0.8354838709677419,
            "z_score": 1.5618950441794284,
            "p_value": 0.05915635808230364
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 254,
            "green_fraction": 0.8494983277591973,
            "z_score": 2.1397652482750864,
            "p_value": 0.01618687149813106
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of widely used, open-source language models (LLMs) to prompt formatting choices, finding that performance spreads across tasks, models, and number of few-shot examples. They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats, allowing for the efficient exploration of the space. The authors demonstrate that FORMATSPREAD can be used to estimate the performance spread across prompt formatting choices, and that this spread is large regardless of model choice, even when increasing model size, number of few-shots, or using instruction tuning.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of using these models in real-world applications.\n2. The authors propose a novel grammar that defines the space of plausible prompt formats and semantic equivalence between formats, which allows for the efficient exploration of the space.\n3. The paper introduces FORMATSPREAD, an algorithm that can be used to estimate the performance spread across prompt formatting choices, which is a valuable tool for researchers and practitioners.\n4. The authors provide a thorough evaluation of the algorithm on a large set of tasks and models, demonstrating its effectiveness in estimating performance spread.\n\nWeaknesses:\n\n1. The paper assumes that the grammar is manually constructed, which may not be practical for large-scale applications.\n2. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation for large-scale applications.\n3. The paper focuses on classification tasks and does not explore other types of tasks, such as open-ended text generation.\n4. The authors do not provide a detailed analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nSuggestions for Improvement:\n\n1. Develop a more automated method for constructing the grammar, which could be based on machine learning algorithms or other techniques.\n2. Provide a more detailed analysis of the computational cost of FORMATSPREAD and explore ways to reduce it.\n3. Explore the use of FORMATSPREAD on other types of tasks, such as open-ended text generation.\n4. Conduct a more thorough analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of natural language processing and language modeling, and the authors should be commended for their thorough analysis of the sensitivity of LLMs to prompt formatting choices. However, to make the paper more impactful, the authors should address the limitations mentioned above and provide a more detailed analysis of the algorithm's computational cost and robustness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe authors propose a novel method, FORMATSPREAD, to estimate the performance spread of large language models (LLMs) over the space of plausible prompt formats. They find that LLaMA-2-7B, Falcon-7B, GPT-3.5, LLaMA-2-70B, and Falcon-7B-Instruct models show significant performance spread, with median spread ranging from 7.5 to 28 accuracy points, depending on the model used. They also show that the spread is large regardless of model size, number of few-shot examples, or instruction tuning. Additionally, the authors demonstrate that the format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format.\n\nStrengths:\n\n1. The authors propose a novel method, FORMATSPREAD, to estimate the performance spread of LLMs over the space of plausible prompt formats.\n2. They conduct an extensive analysis of the performance spread of several widely-used open-source LLMs, including LLaMA-2-7B, Falcon-7B, GPT-3.5, LLaMA-2-70B, and Falcon-7B-Instruct.\n3. They demonstrate the importance of considering the performance spread of LLMs over the space of plausible prompt formats, which can affect the conclusions drawn about model performance.\n\nWeaknesses:\n\n1. The authors focus on a limited number of models, which might not be representative of all LLMs.\n2. They only consider a specific subset of Super-NaturalInstructions tasks, which might not be representative of all possible tasks.\n3. They do not investigate the impact of input length on final performance, which could be an important factor to consider in the evaluation of LLMs.\n\nSuggestions for Improvement:\n\n1. To increase the generalizability of the results, the authors could consider evaluating a larger number of models, including those from different families of LLMs.\n2. They could also consider evaluating a more representative subset of Super-NaturalInstructions tasks, including those with longer input instructions and input field lengths.\n3. To investigate the impact of input length on final performance, the authors could conduct additional experiments with varying input lengths.\n\nRecommendation:\n\nThis work contributes significantly to the understanding of the performance spread of LLMs over the space of plausible prompt formats. However, to increase the generalizability of the results, the authors could consider evaluating a larger number of models and a more representative subset of Super-NaturalInstructions tasks. Additionally, investigating the impact of input length on final performance could be an important factor to consider in the evaluation of LLMs.\n\n$$$",
        "gr_score_with": 0.8307210031347962,
        "gr_score_without": 0.8587570621468926,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 304,
            "green_fraction": 0.8587570621468926,
            "z_score": 2.763768817954218,
            "p_value": 0.0028569002825047673
        },
        "output_with": {
            "num_tokens_scored": 319,
            "num_green_tokens": 265,
            "green_fraction": 0.8307210031347962,
            "z_score": 1.3717366518418406,
            "p_value": 0.08507271706172098
        }
    },
    {
        "title": "REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS",
        "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to learning robust world models in the reward-free setting. The authors address the problem of generating curricula for training robust agents without access to a reward function during exploration. They introduce WAKER, an algorithm for actively sampling environments for exploration during reward-free training based on the estimated error of the world model in each environment. The authors demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.\n\nStrengths:\n\n1. The paper addresses a novel problem in the field of reinforcement learning, specifically the generation of curricula for training robust agents in the reward-free setting.\n2. The authors provide a clear and concise definition of the problem and a thorough analysis of the related work.\n3. The proposed algorithm, WAKER, is well-motivated and easy to understand, and the authors provide a detailed explanation of its components and how they work together.\n4. The experimental results are comprehensive and demonstrate the effectiveness of WAKER in improving robustness, efficiency, and generalisation.\n\nWeaknesses:\n\n1. The paper assumes that the world model learns a suitable representation model, which may not always be the case in practice.\n2. The authors rely on an intrinsically motivated policy to adequately explore the state-action space across a range of environments, which may pose a challenge for scalability to more complex environments.\n3. The paper does not provide a clear explanation of how the world model is updated during the training process, and how the error estimates are used to bias sampling towards more uncertain environments.\n4. The experimental results are limited to a few domains and tasks, and it is not clear how WAKER would perform in more complex or diverse environments.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the world model is updated during the training process, and how the error estimates are used to bias sampling towards more uncertain environments.\n2. Investigate the use of function approximation to estimate uncertainty throughout large parameter spaces, to improve scalability to more complex environments.\n3. Provide more experimental results to demonstrate the effectiveness of WAKER in a wider range of domains and tasks.\n4. Discuss the potential applications of WAKER in more complex or diverse environments, and how it could be adapted to handle such scenarios.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of reinforcement learning by proposing a novel approach to learning robust world models in the reward-free setting. The authors provide a clear and concise definition of the problem, a thorough analysis of the related work, and a well-motivated and easy-to-understand algorithm. The experimental results demonstrate the effectiveness of WAKER in improving robustness, efficiency, and generalisation. However, there are some limitations and weaknesses that need to be addressed in future work. Overall, I recommend accepting the paper for publication, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes the first method for automatic curriculum learning for environment selection in the reward-free setting. The problem addressed by the paper involves training robust world models for generally-capable agents that can adapt to new tasks without additional training. The proposed method, called Weighted Acquisition of Knowledge across Environments for Robustness (WAKER), selectively samples the environment settings that induce the highest latent dynamics error. The paper demonstrates the effectiveness of the proposed method on several pixel-based continuous control domains, showing improved zero-shot task adaptation capabilities and robustness to out-of-distribution environments.\n\nStrengths:\n\n1. The paper addresses an important problem in the field, which involves training robust world models for generally-capable agents that can adapt to new tasks without additional training. \n2. The proposed method, called Weighted Acquisition of Knowledge across Environments for Robustness (WAKER), selectively samples the environment settings that induce the highest latent dynamics error. This approach has the potential to improve the robustness and adaptability of world models. \n3. The paper demonstrates the effectiveness of the proposed method on several pixel-based continuous control domains, showing improved zero-shot task adaptation capabilities and robustness to out-of-distribution environments.\n\nWeaknesses:\n\n1. The paper relies on an intrinsically motivated policy to adequately explore the state-action space across a range of environments. This may pose a challenge for scalability to more complex environments. \n2. The paper does not provide extensive comparisons to existing methods for curriculum learning and robustness, which would strengthen the paper's claims. \n3. The paper assumes that the true latent dynamics function, T, can be used to evaluate the world model error. However, the paper does not provide any information about the accuracy or reliability of the true latent dynamics function.\n\nSuggestions for Improvement:\n\n1. The paper could provide more extensive comparisons to existing methods for curriculum learning and robustness, which would strengthen the paper's claims. \n2. The paper could investigate using different exploration policies, such as random or reward-based exploration policies, to evaluate the robustness and adaptability of the world model. \n3. The paper could provide more information about the accuracy or reliability of the true latent dynamics function, which would help to evaluate the robustness and adaptability of the world model.\n\nRecommendation:\n\nThe paper proposes an innovative method for automatic curriculum learning for environment selection in the reward-free setting. The method selectively samples the environment settings that induce the highest latent dynamics error, which has the potential to improve the robustness and adaptability of world models. The paper demonstrates the effectiveness of the proposed method on several pixel-based continuous control domains, showing improved zero-shot task adaptation capabilities and robustness to out-of-distribution environments. However, the paper relies on an intrinsically motivated policy to adequately explore the state-action space across a range of environments, which may pose a challenge for scalability to more complex environments.",
        "gr_score_with": 0.8925925925925926,
        "gr_score_without": 0.8098958333333334,
        "output_without": {
            "num_tokens_scored": 384,
            "num_green_tokens": 311,
            "green_fraction": 0.8098958333333334,
            "z_score": 0.4847948449258315,
            "p_value": 0.3139109399247918
        },
        "output_with": {
            "num_tokens_scored": 270,
            "num_green_tokens": 241,
            "green_fraction": 0.8925925925925926,
            "z_score": 3.803628871563654,
            "p_value": 7.129587954004156e-05
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method constructs a directed graph to represent unlabeled data and uses a diffusion process to quantify the influence of each candidate subset. A greedy algorithm is introduced to select the subset with maximum influence, which approximates the vast unlabeled data. The paper provides theoretical analysis and empirical evaluations on various benchmarks, demonstrating the superiority of IDEAL over previous state-of-the-art selective annotation baselines.\n\nStrengths:\n\n1. The paper presents a novel approach to selective annotations, addressing the challenges of balancing diversity and representativeness.\n2. The method is end-to-end, unsupervised, and efficient, reducing unnecessary costs at inference time.\n3. The paper provides theoretical analysis, establishing an upper limit for the global optimal solution and demonstrating the reliability of the greedy search algorithm.\n4. Empirical evaluations demonstrate the superiority of IDEAL across various benchmarks, including classification, multi-choice, dialogue, and generation tasks.\n\nWeaknesses:\n\n1. The paper assumes access to a large-scale unlabeled data pool, which may not be feasible in practice.\n2. The method relies on a similarity-based prompt retrieval method, which may not be optimal in all cases.\n3. The paper does not provide a comprehensive comparison with other graph-based methods, such as K-Means and MFL.\n4. The case study on automatic annotation is limited to a small set of datasets and may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for constructing the directed graph, such as using different similarity metrics or graph neural networks.\n2. Evaluate the performance of IDEAL on other datasets and tasks, including those with varying levels of complexity and domain adaptation.\n3. Compare IDEAL with other graph-based methods, such as K-Means and MFL, to demonstrate its superiority.\n4. Provide a more detailed analysis of the time complexity and scalability of IDEAL, especially for large datasets.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach to selective annotations, addressing the challenges of balancing diversity and representativeness. The theoretical analysis and empirical evaluations demonstrate the superiority of IDEAL over previous state-of-the-art selective annotation baselines. While there are some limitations and areas for improvement, the paper provides a valuable contribution to the field of in-context learning and large language models. I recommend acceptance of the paper, with the suggested improvements and additional evaluations to further strengthen the results.\n\n$$$",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper proposes an influence-driven selective annotation method, named IDEAL, to minimize annotation costs while improving the quality of in-context examples in large language models. The method works in an unsupervised and end-to-end manner, selecting a pivotal subset from a large-scale unlabeled data pool to annotate. The authors construct a directed graph to represent the relationships between the data, quantify the influence of candidate subsets, and introduce a greedy algorithm to find the optimal subset. Theoretical analysis provides an upper bound for the global optimal solution, demonstrating that the proposed method can achieve at least as large as a certain proportion of the optimal solution's influence. The authors conduct comprehensive experiments across various benchmarks, demonstrating the superiority of IDEAL in most cases, while reducing the time required for subset selection.\n\n**Strengths**\n\n* The paper proposes an innovative method that tackles the issue of selective annotations in in-context learning.\n* The method works in an unsupervised and end-to-end manner, reducing the complexity of the annotation process.\n* Theoretical analysis provides an upper bound for the global optimal solution, establishing the effectiveness of the proposed method.\n* The authors conduct comprehensive experiments across various benchmarks, demonstrating the superiority of IDEAL in most cases.\n* The paper provides insights into the selective annotation problem, shedding light on the potential of this approach in in-context learning.\n\n**Weaknesses**\n\n* The paper relies heavily on the assumption that the influence function satisfies the submodular condition, which may not always hold in practice.\n* The authors do not provide an extensive analysis of the computational complexity of the proposed method.\n* The paper focuses primarily on the classification tasks, while the generality of the method to other tasks remains unclear.\n* The authors could provide additional insights into the relationship between the influence of the selected subset and the final performance of the model.\n* The paper does not discuss the potential applications of the proposed method in other domains, such as natural language processing or computer vision.\n\n**Suggestions for Improvement**\n\n* The authors could provide additional theoretical analysis to relax the assumption of the submodular condition.\n* A comprehensive analysis of the computational complexity of the proposed method could be provided.\n* The authors could explore the generality of the method to other tasks, such as sentiment analysis, question answering, or text classification.\n* A detailed analysis of the relationship between the influence of the selected subset and the final performance of the model could be provided.\n* The authors could discuss the potential applications of the proposed method in other domains, such as natural language processing or computer vision.\n\n**Recommendation**\n\nThe paper proposes an innovative method that tackles the issue of selective annotations in in-context learning, providing an unsupervised and end-to-end solution that reduces the complexity of the annotation process. Theoretical analysis provides an upper bound for the global optimal solution, establishing the effectiveness of the proposed method. The authors conduct comprehensive experiments across various benchmarks, demonstrating the superiority of IDEAL in most cases. While there are areas of improvement, the paper provides valuable insights into the selective annotation problem, shedding light on the potential of this approach in in-context learning. I recommend this paper be accepted, subject to the authors addressing the suggested improvements.",
        "gr_score_with": 0.8232044198895028,
        "gr_score_without": 0.804177545691906,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 308,
            "green_fraction": 0.804177545691906,
            "z_score": 0.20439045212303406,
            "p_value": 0.4190241954030372
        },
        "output_with": {
            "num_tokens_scored": 362,
            "num_green_tokens": 298,
            "green_fraction": 0.8232044198895028,
            "z_score": 1.103735495578034,
            "p_value": 0.1348539466428752
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of the approach. While there are some limitations and weaknesses, the paper is well-written and well-organized, and the results are impressive. I recommend this paper for publication in a top-tier conference.\n\nGrade: B+\n\nReviewer's Comments:\n\nThe paper is well-written and well-organized, and the results are impressive. However, there are some limitations and weaknesses that need to be addressed. The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios. The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications. The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field, and I recommend it for publication in a top-tier conference. However, the authors need to address the limitations and weaknesses mentioned above to make the paper more robust and convincing.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel strategy, called Win-Win, for efficient training and inference of high-resolution vision transformers. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special tricks. The paper shows that Win-Win is effective in training high-resolution vision transformers, achieving state-of-the-art performance on the Spring benchmark and reducing the training cost by a factor of 4.\n\nStrengths:\n* The paper proposes a novel and effective strategy for efficient training and inference of high-resolution vision transformers.\n* The paper demonstrates the generality of the Win-Win approach by extending it to the binocular task of optical flow estimation.\n* The paper provides a thorough experimental evaluation of the Win-Win approach, including a comparison with other state-of-the-art methods.\n* The paper is well-written and easy to follow, with clear explanations of the methodology and results.\n\nWeaknesses:\n* The paper assumes that the model is trained on a dataset with a fixed resolution, which may not be the case in practice.\n* The paper does not provide a thorough analysis of the impact of the number of windows on the training and inference time.\n* The paper assumes that the model is trained with a fixed batch size, which may not be the case in practice.\n* The paper does not provide a thorough comparison with other state-of-the-art methods that use different architectures or training schemes.\n\nSuggestions for Improvement:\n* The paper could benefit from a more thorough analysis of the impact of the number of windows on the training and inference time.\n* The paper could benefit from a more thorough comparison with other state-of-the-art methods that use different architectures or training schemes.\n* The paper could benefit from a more detailed explanation of the Win-Win approach and its relationship with other state-of-the-art methods.\n* The paper could benefit from a more thorough discussion of the potential limitations and future directions of the Win-Win approach.\n\nRecommendation:\nThe paper presents a novel and effective strategy for efficient training and inference of high-resolution vision transformers. The paper demonstrates the generality of the Win-Win approach and provides a thorough experimental evaluation. However, the paper assumes a fixed resolution and batch size, and could benefit from a more thorough comparison with other state-of-the-art methods. Overall, the paper is well-written and easy to follow, and provides a significant contribution to the field of computer vision.\n\nRecommendation: ACCEPT\n\nRating: 8/10\n\nReviewer: [Reviewer's Name]\n\nDate: [Current Date]\n$$$",
        "gr_score_with": 0.86084142394822,
        "gr_score_without": 0.837696335078534,
        "output_without": {
            "num_tokens_scored": 382,
            "num_green_tokens": 320,
            "green_fraction": 0.837696335078534,
            "z_score": 1.8419202363479406,
            "p_value": 0.032743409042452556
        },
        "output_with": {
            "num_tokens_scored": 309,
            "num_green_tokens": 266,
            "green_fraction": 0.86084142394822,
            "z_score": 2.673736582746297,
            "p_value": 0.003750567298978644
        }
    },
    {
        "title": "THRESHOLD-CONSISTENT MARGIN LOSS FOR OPEN- WORLD DEEP METRIC LEARNING",
        "abstract": "Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate (FAR) and false reject rate (FRR) across test classes and data distributions. We define this issue in DML as threshold inconsistency. In real-world applications, such inconsistency often complicates the threshold selection process when deploying commercial image retrieval systems. To measure this inconsistency, we propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. Using the OPIS metric, we find that achieving high accuracy levels in a DML model does not automatically guarantee threshold consistency. In fact, our investigation reveals a Pareto frontier in the high-accuracy regime, where existing methods to improve accuracy often lead to degradation in threshold consistency. To address this trade-off, we introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. Extensive experiments demonstrate TCM\u2019s effectiveness in enhancing threshold consistency while preserving accuracy, simplifying the threshold selection process in practical DML settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel solution to address the issue of threshold inconsistency in deep metric learning (DML), which occurs when different test classes and distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy. The authors introduce a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) to quantify the threshold inconsistency of a DML model. They also propose a simple yet effective regularization technique called Threshold-Consistent Margin (TCM) loss, which promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. The TCM loss can be combined with any base losses and backbone architecture to improve threshold consistency in DML.\n\nStrengths:\n\n1. The paper addresses a significant issue in DML, namely threshold inconsistency, which is crucial in real-world applications.\n2. The authors introduce a novel metric, OPIS, to quantify threshold inconsistency, which is more straightforward and easier to compute compared to existing methods.\n3. The proposed TCM loss is simple and effective in improving threshold consistency while maintaining accuracy.\n4. The authors demonstrate the effectiveness of TCM loss on various image retrieval benchmarks, achieving state-of-the-art performance in both threshold consistency and accuracy.\n\nWeaknesses:\n\n1. The paper assumes a sufficient number of samples per class for the OPIS and \u03f5-OPIS metrics, which may not be feasible in few-shot evaluation scenarios.\n2. The TCM loss may fail when there's a significant distribution shift between the training and test sets or when strong label noise is present.\n3. The paper does not provide a comprehensive comparison with existing methods that address threshold inconsistency, such as posthoc calibration methods and conformal prediction.\n4. The authors do not provide a detailed analysis of the computational complexity of TCM loss, which may be a concern for large-scale datasets.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of using OPIS and \u03f5-OPIS metrics in few-shot evaluation scenarios and propose modifications to address the issue.\n2. Provide a more comprehensive comparison with existing methods that address threshold inconsistency, including posthoc calibration methods and conformal prediction.\n3. Conduct a more detailed analysis of the computational complexity of TCM loss and propose modifications to reduce the computational cost.\n4. Investigate the robustness of TCM loss to distribution shifts and label noise, and propose modifications to address these issues.\n\nRecommendation:\n\nThe paper proposes a novel and effective solution to address threshold inconsistency in DML. However, it requires further investigation and modifications to address the limitations mentioned above. With some additional work, the paper has the potential to make a significant impact in the field of DML.",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThis paper investigates the issue of threshold inconsistency in deep metric learning (DML) for image retrieval tasks. The authors define threshold inconsistency as the phenomenon where different classes and data distributions require varying distance thresholds to achieve a consistent retrieval or recognition accuracy. To address this issue, they introduce a novel variance-based metric, Operating-Point-Inconsistency-Score (OPIS), to quantify threshold inconsistency. They also propose a simple yet effective regularization technique, Threshold-Consistent Margin (TCM) loss, to promote uniformity in representation structures across classes. The authors demonstrate that TCM loss improves threshold consistency while maintaining accuracy, simplifying the threshold selection process in practical DML settings.\n\nStrengths:\n\n1. The paper provides a thorough analysis of threshold inconsistency in DML, highlighting its significance in real-world applications.\n2. The authors introduce a novel metric, OPIS, to quantify threshold inconsistency, which does not require a separate calibration dataset.\n3. The proposed TCM loss regularization technique is simple, versatile, and effective in improving threshold consistency.\n4. The authors demonstrate state-of-the-art performance in both threshold consistency and accuracy across various image retrieval benchmarks.\n\nWeaknesses:\n\n1. The paper assumes a hyperspherical embedding space, which may not be applicable to all image retrieval tasks.\n2. The TCM loss regularization technique may not be effective in scenarios with significant distribution shift between training and test sets.\n3. The authors do not provide a detailed analysis of the computational cost of TCM loss.\n4. The paper assumes a specific calibration range for OPIS, which may not be applicable to all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of TCM loss and its impact on training time.\n2. They should explore other regularization techniques that may be effective in addressing threshold inconsistency.\n3. The authors should provide more insights on how to handle scenarios with significant distribution shift between training and test sets.\n4. They should explore other evaluation metrics that may be more effective in assessing threshold consistency.\n\nRecommendation:\n\nThis paper provides a thorough analysis of threshold inconsistency in DML and introduces a novel metric, OPIS, to quantify threshold inconsistency. The proposed TCM loss regularization technique is simple, versatile, and effective in improving threshold consistency. However, the authors should address the limitations mentioned above to further enhance the paper's contributions. Overall, this paper provides a significant contribution to the field of deep metric learning and image retrieval, and it deserves publication in a top-tier conference.",
        "gr_score_with": 0.8005780346820809,
        "gr_score_without": 0.8025641025641026,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 313,
            "green_fraction": 0.8025641025641026,
            "z_score": 0.12659242088545836,
            "p_value": 0.449631497967577
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 277,
            "green_fraction": 0.8005780346820809,
            "z_score": 0.02688016652852199,
            "p_value": 0.4892776563094473
        }
    },
    {
        "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
        "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which builds upon the previous work of COMBINER (Guo et al., 2023). RECOMBINER addresses several limitations of COMBINER, including the use of factorized priors and posterior approximations that lack flexibility, the inability to adapt to local deviations from global patterns in the data, and the sensitivity to modeling choices and the variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization for neural network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The proposed method, RECOMBINER, is evaluated on various data modalities, including image, audio, video, and 3D protein structure data, and demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\nStrengths:\n\n1. The paper proposes several novel and non-trivial extensions to the previous work of COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices.\n2. The authors provide a comprehensive evaluation of the proposed method on various data modalities, including image, audio, video, and 3D protein structure data.\n3. The paper demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n4. The authors provide a clear and concise explanation of the proposed method and its extensions, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The encoding time complexity of RECOMBINER is high, which may limit its practical application.\n2. The paper does not provide a detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n3. The authors do not provide a clear explanation of how the proposed method can be extended to other data modalities.\n4. The paper does not discuss the potential applications of RECOMBINER in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n2. The paper should provide a clear explanation of how the proposed method can be extended to other data modalities.\n3. The authors should discuss the potential applications of RECOMBINER in real-world scenarios.\n4. The paper should provide a more detailed analysis of the encoding time complexity of RECOMBINER and propose potential solutions to reduce it.\n\nRecommendation:\n\nThe paper proposes a novel and competitive data compression method called RECOMBINER, which addresses several limitations of the previous work of COMBINER. The authors provide a comprehensive evaluation of the proposed method on various data modalities and demonstrate competitive results with the best INR-based methods. However, the encoding time complexity of RECOMBINER is high, which may limit its practical application. Therefore, I recommend that the authors provide a more detailed comparison with other state-of-the-art compression methods, discuss the potential applications of RECOMBINER in real-world scenarios, and propose potential solutions to reduce the encoding time complexity.\n\n$$$",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper proposes a novel data compression method called RECOMBINER, which is an extension of the existing method COMBINER. RECOMBINER addresses several limitations of COMBINER, including the use of factorized priors and posteriors, the inability to adapt to local deviations from global patterns, and the sensitivity to modeling choices and initialization. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization of the INR weights, learnable positional encodings, and an expressive hierarchical Bayesian model. The method is evaluated on several data modalities, including image, audio, video, and protein structure data, and is found to outperform existing methods in terms of rate-distortion performance.\n\n**Strengths**\n\n* The paper proposes a novel and effective method for data compression, which addresses several limitations of existing methods.\n* The method is evaluated on several data modalities, and is found to outperform existing methods in terms of rate-distortion performance.\n* The authors provide a detailed description of the method, including the mathematical formulation and the experimental setup.\n* The paper includes extensive experimental results, including rate-distortion curves, PSNR values, and visualizations of the compressed data.\n\n**Weaknesses**\n\n* The paper assumes a good understanding of the background material, including the concepts of implicit neural representations and variational Bayesian neural networks.\n* The method is computationally expensive, and the authors acknowledge that the encoding time complexity is a major limitation.\n* The paper does not provide a detailed analysis of the trade-offs between the different components of the method, including the linear reparameterization, positional encodings, and hierarchical Bayesian model.\n* The authors do not provide a comparison of the method's performance on a wide range of data types, including text, video, and 3D data.\n\n**Suggestions for Improvement**\n\n* The authors should provide a more detailed analysis of the trade-offs between the different components of the method, including the linear reparameterization, positional encodings, and hierarchical Bayesian model.\n* The paper should include a comparison of the method's performance on a wider range of data types, including text, video, and 3D data.\n* The authors should investigate methods to reduce the encoding time complexity, including reducing the number of parameters to optimize over and switching from inference over weights to modulations.\n* The paper should include a more detailed discussion of the limitations of the method, including the use of approximate samples and the sensitivity to modeling choices and initialization.\n\n**Recommendation**\n\nThe paper proposes a novel and effective method for data compression, which addresses several limitations of existing methods. The method is evaluated on several data modalities, and is found to outperform existing methods in terms of rate-distortion performance. However, the paper assumes a good understanding of the background material, and the method is computationally expensive. The authors should provide a more detailed analysis of the trade-offs between the different components of the method, and investigate methods to reduce the encoding time complexity. Overall, the paper is well-written and provides a novel and effective approach to data compression.\n\n**Rating**\n\nThe paper is rated 7 out of 10.\n\n**Recommendation to the Editor**\n\nThe paper should be accepted, pending revisions to address the limitations and weaknesses mentioned above.\n\n**Recommendation to the Author**\n\nThe author should provide a more detailed analysis of the trade-offs between the different components of the method, and investigate methods to reduce the encoding time complexity. The author should also include a comparison of the method's performance on a wider range of data types, including text, video, and 3D data.",
        "gr_score_with": 0.8539944903581267,
        "gr_score_without": 0.8144927536231884,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 281,
            "green_fraction": 0.8144927536231884,
            "z_score": 0.6729773775727069,
            "p_value": 0.25048084161234185
        },
        "output_with": {
            "num_tokens_scored": 363,
            "num_green_tokens": 310,
            "green_fraction": 0.8539944903581267,
            "z_score": 2.571833017299238,
            "p_value": 0.005058083553972743
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework combines the strengths of LLMs, which can perform high-level abstract reasoning, and automated reasoners, which can perform precise low-level reasoning. The authors present a formal calculus, LEMUR, that operates over a configuration, which is either a distinguished symbol or a tuple of a program, an assumption, and a list of properties. The calculus performs oracle calls to LLMs to propose new properties and revise them. The authors also present a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n\nStrengths:\n\n1. The paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification.\n2. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners.\n3. The paper presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n4. The authors demonstrate the practicality of LEMUR by instantiating it as a fully automated framework and evaluating its efficiency on standard benchmark sets.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs can generate correct and useful properties, which may not always be the case.\n2. The authors do not provide a thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n3. The paper assumes that the automated reasoners can perform precise low-level reasoning, which may not always be the case.\n4. The authors do not provide a clear explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n2. The paper should provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n3. The authors should provide a more detailed description of the LLMs used in the paper and their limitations.\n4. The paper should provide a more thorough discussion of the strengths and weaknesses of the LEMUR framework compared to other automated program verification approaches.\n\nRecommendation:\n\nThe paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners. The paper also presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure. However, the paper assumes that the LLMs can generate correct and useful properties, which may not always be the case. Therefore, the paper should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks and provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nIn summary, the paper has the potential to make a significant contribution to the field of automated program verification, but it requires further evaluation and refinement to be considered a complete and reliable solution.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes LEMUR, a novel framework that combines Large Language Models (LLMs) with Automated Reasoners for Automated Program Verification. LEMUR is designed to leverage the abstract high-level reasoning abilities of LLMs to generate new properties that can be checked by Automated Reasoners. The authors present the LEMUR calculus, a sound proof system that integrates LLMs with Automated Reasoners, and demonstrate its efficiency on standard benchmark sets. They also show that LEMUR can solve hard benchmarks that are beyond the reach of state-of-the-art conventional program verifiers.\n\nStrengths:\n\n* LEMUR provides a novel approach to integrating LLMs with Automated Reasoners for Automated Program Verification, offering a promising solution to the challenges of verifying complex properties on real-world programs.\n* The authors present a sound proof system, the LEMUR calculus, that integrates LLMs with Automated Reasoners, demonstrating the efficiency of LEMUR on standard benchmark sets.\n* LEMUR's performance is competitive with state-of-the-art conventional program verifiers, with the added benefit of leveraging LLMs' abstract high-level reasoning abilities.\n\nWeaknesses:\n\n* LEMUR's practical performance is dependent on the quality of the LLMs, Automated Reasoners, and the formulation of the verification tasks.\n* LEMUR's reliance on LLMs may lead to issues with scalability, as the number of tokens that LLMs can handle is limited.\n* LEMUR's current implementation is limited to imperative languages, with the authors noting that extending it to functional languages is an area of future research.\n\nSuggestions for Improvement:\n\n* Investigate ways to improve the scalability of LEMUR, potentially by fine-tuning LLMs for invariant generation tasks or by leveraging more efficient Automated Reasoners.\n* Explore ways to extend LEMUR to functional languages, potentially by adapting the LEMUR calculus to accommodate the unique properties of functional programming.\n* Investigate the effectiveness of LEMUR on real-world programs, potentially by comparing its performance on large, complex programs to that of state-of-the-art conventional program verifiers.\n\nRecommendation:\n\nThis paper presents an innovative approach to integrating LLMs with Automated Reasoners for Automated Program Verification, with promising results on standard benchmark sets. While there are areas for improvement, the authors' work demonstrates the potential of LEMUR to solve complex verification tasks that have previously eluded state-of-the-art conventional program verifiers.",
        "gr_score_with": 0.8401253918495298,
        "gr_score_without": 0.8512658227848101,
        "output_without": {
            "num_tokens_scored": 316,
            "num_green_tokens": 269,
            "green_fraction": 0.8512658227848101,
            "z_score": 2.278302999375197,
            "p_value": 0.011354265544573616
        },
        "output_with": {
            "num_tokens_scored": 319,
            "num_green_tokens": 268,
            "green_fraction": 0.8401253918495298,
            "z_score": 1.7916560350587314,
            "p_value": 0.03659404057855844
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks with small initialization. The authors consider a training dataset with well-separated input vectors and provide a careful analysis of the neurons' directional dynamics. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nStrengths:\n\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5, which is a significant improvement over previous work.\n3. The paper provides a quantitative bound on the time it takes for good alignment to be achieved, which is O(logn\u221a\u00b5).\n4. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nWeaknesses:\n\n1. The paper assumes that the training dataset has well-separated input vectors, which may not be a realistic assumption in practice.\n2. The authors use a balanced initialization scheme, which may not be a common initialization scheme in practice.\n3. The paper does not provide a detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n2. The paper should be extended to consider more realistic initialization schemes and training datasets.\n3. The authors should provide more numerical experiments to validate their theoretical findings.\n\nRecommendation:\n\nThis paper provides a significant contribution to the understanding of the dynamics of gradient flow for training two-layer ReLU networks with small initialization. The authors' analysis is thorough and provides a quantitative bound on the time it takes for good alignment to be achieved. However, the paper assumes a balanced initialization scheme and well-separated input vectors, which may not be realistic in practice. Therefore, I recommend that the authors extend their analysis to more realistic initialization schemes and training datasets.",
        "peer_review_with_watermark": "$$$\n\nPaper Title: Early Neuron Alignment in Two-Layer ReLU Networks with Small Initialization\n\nPaper Summary:\nThe paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. The authors consider a training dataset with well-separated input vectors and provide a complete analysis of the dynamics of gradient flow for this scenario. They show that during the early phase of training, the neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which depends on the data separation and the size of the network.\n\nStrengths:\n1. The paper provides a complete analysis of the dynamics of gradient flow for a two-layer ReLU network with small initialization, which is a well-studied problem in the field.\n2. The authors provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which depends on the data separation and the size of the network.\n3. The paper uses a variety of mathematical techniques, including convex analysis and differential equations, to derive the upper bound.\n4. The authors provide numerical experiments to validate the theoretical findings, which demonstrates the practical relevance of the analysis.\n5. The paper provides a clear and concise presentation of the main ideas and techniques, which makes it easy to follow for readers with a background in convex analysis and differential equations.\n\nWeaknesses:\n1. The paper assumes that the data points are well-separated, which might not be the case in many real-world datasets.\n2. The analysis assumes that the gradient flow dynamics are well-behaved, which might not be the case in all scenarios.\n3. The paper does not provide a thorough discussion of the implications of the upper bound for the convergence rate of the gradient flow dynamics.\n4. The authors assume that the second-layer weights are initialized with a small magnitude, which might not be the case in all scenarios.\n5. The paper does not provide a comparison with existing work that studies the convergence rate of gradient flow dynamics for two-layer ReLU networks.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough discussion of the implications of the upper bound for the convergence rate of the gradient flow dynamics.\n2. The paper should provide a comparison with existing work that studies the convergence rate of gradient flow dynamics for two-layer ReLU networks.\n3. The authors should provide a more thorough analysis of the effects of the data separation and the size of the network on the upper bound.\n4. The paper should provide more numerical experiments to validate the theoretical findings, which demonstrates the practical relevance of the analysis.\n5. The authors should provide a clearer and more concise presentation of the main ideas and techniques, which makes it easy to follow for readers with a background in convex analysis and differential equations.\n\nRecommendation:\nThe paper provides a complete analysis of the dynamics of gradient flow for a two-layer ReLU network with small initialization, which is a well-studied problem in the field. The authors provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which depends on the data separation and the size of the network. The paper uses a variety of mathematical techniques, including convex analysis and differential equations, to derive the upper bound, and provides numerical experiments to validate the theoretical findings, which demonstrates the practical relevance of the analysis. Therefore, I recommend this paper for publication in a reputable journal or conference in the field of convex analysis and differential equations.\n\n$$$",
        "gr_score_with": 0.7976539589442815,
        "gr_score_without": 0.8238805970149253,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 276,
            "green_fraction": 0.8238805970149253,
            "z_score": 1.0927167294163063,
            "p_value": 0.13725909610066472
        },
        "output_with": {
            "num_tokens_scored": 341,
            "num_green_tokens": 272,
            "green_fraction": 0.7976539589442815,
            "z_score": -0.10830607221477802,
            "p_value": 0.5431235469890521
        }
    },
    {
        "title": "VONET: UNSUPERVISED VIDEO OBJECT LEARNING WITH PARALLEL U-NET ATTENTION",
        "abstract": "Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents VONet, a novel approach for unsupervised video object learning. VONet is inspired by MONet, a scene decomposition model for single images, but extends it to handle video sequences. The key innovation in VONet is its parallel attention process, which generates attention masks for all slots simultaneously from a U-Net. Additionally, VONet incorporates an object-wise sequential VAE framework to achieve temporal consistency of objects between adjacent video frames. The paper conducts extensive evaluations on five MOVI datasets, showcasing VONet's superiority over several strong baselines.\n\nStrengths:\n\n1. The paper presents a novel and efficient approach for unsupervised video object learning, which outperforms several strong baselines.\n2. The parallel attention process in VONet is a significant innovation, allowing for simultaneous attention mask generation for all slots.\n3. The object-wise sequential VAE framework is a novel and effective way to achieve temporal consistency of objects between adjacent video frames.\n4. The paper provides a comprehensive evaluation of VONet on five MOVI datasets, demonstrating its effectiveness in handling videos of diverse complexities.\n\nWeaknesses:\n\n1. The paper assumes that the number of slots is known a priori, which may not be the case in practice.\n2. The paper does not provide a detailed analysis of the computational complexity of VONet.\n3. The paper does not provide a clear explanation of how the context vectors are acquired.\n4. The paper does not provide a comparison with other state-of-the-art methods for unsupervised video object learning.\n\nSuggestions for Improvement:\n\n1. Provide a detailed analysis of the computational complexity of VONet and discuss potential optimizations.\n2. Provide a clear explanation of how the context vectors are acquired and how they are used in the parallel attention process.\n3. Compare VONet with other state-of-the-art methods for unsupervised video object learning and discuss the advantages and disadvantages of each approach.\n4. Provide a more detailed evaluation of VONet on a wider range of datasets and scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for unsupervised video object learning, which outperforms several strong baselines. However, there are some weaknesses and limitations that need to be addressed. With some additional analysis and evaluation, VONet has the potential to become a leading method in the field of unsupervised video object learning.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents VONet, a novel approach for unsupervised video object learning. VONet is inspired by MONet, a scene decomposition model designed for single images, but it incorporates a parallel attention process that generates attention masks for all slots simultaneously from a U-Net. This approach leads to efficient inference and improved performance on complex video scenes. Additionally, VONet adopts an object-wise sequential VAE framework to promote temporal consistency and incorporates a transformer-based decoder to handle complex video scenes.\n\nStrengths:\n1. VONet's parallel attention process is efficient and effective, allowing it to maintain a nearly constant inference time regardless of the number of slots.\n2. VONet's use of spatial locality bias from the U-Net architecture leads to smoother and more compact object segmentation masks.\n3. VONet's object-wise sequential VAE framework effectively tackles temporal inconsistency, enabling it to capture long-range temporal relationships between objects.\n4. VONet's performance is superior compared to strong baselines on a variety of complex video scenes.\n\nWeaknesses:\n1. VONet's reliance on spatial locality bias may not generalize well to non-spatiotemporal tasks.\n2. VONet's performance may degrade when dealing with videos with high object density or complex backgrounds.\n3. VONet's object-wise sequential VAE may not capture long-range temporal relationships between objects that have different trajectories.\n\nSuggestions for Improvement:\n1. Explore alternative architectures that can generalize better across different tasks and datasets.\n2. Develop techniques to handle high object density or complex backgrounds.\n3. Consider using long-term memory models instead of short-term GRU memory.\n4. Examine the effectiveness of using additional supervision signals, such as depth, optical flow, or segmentation, to improve performance.\n\nRecommendation:\nThe paper presents a novel and effective approach to unsupervised video object learning, with impressive performance on complex video scenes. With further improvements and exploration of alternative architectures, VONet has great potential to become a state-of-the-art method.",
        "gr_score_with": 0.8701298701298701,
        "gr_score_without": 0.8131147540983606,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 248,
            "green_fraction": 0.8131147540983606,
            "z_score": 0.5725983343138683,
            "p_value": 0.28345834389506364
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 268,
            "green_fraction": 0.8701298701298701,
            "z_score": 3.076935564410224,
            "p_value": 0.0010457025230978566
        }
    },
    {
        "title": "FLOW MATCHING ON GENERAL GEOMETRIES",
        "abstract": "We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world nonEuclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Riemannian Flow Matching (RFM), a novel framework for training continuous normalizing flows on manifolds. RFM builds upon the Flow Matching framework and learns a continuous normalizing flow by regressing an implicitly defined target vector field that pushes a base distribution towards a target distribution. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. RFM offers several advantages over previous approaches, including simulation-free training on simple geometries, no need for divergence computation, and exact conditional vector fields. The authors demonstrate the effectiveness of RFM on various manifolds, including simple geometries, high-dimensional tori, and manifolds with non-trivial curvature and boundaries.\n\nStrengths:\n\n1. The paper presents a novel and highly scalable approach for training continuous normalizing flows on manifolds.\n2. RFM is completely simulation-free and introduces zero approximation errors on simple geometries with closed-form geodesics.\n3. The authors demonstrate the effectiveness of RFM on various manifolds, including simple geometries, high-dimensional tori, and manifolds with non-trivial curvature and boundaries.\n4. The paper introduces a new premetric for defining target vector fields, which is simple and efficient to compute.\n5. RFM does not require divergence computation, which makes it more efficient and scalable than previous approaches.\n\nWeaknesses:\n\n1. The paper assumes that the manifold is smooth and connected, which may not be the case in practice.\n2. The authors do not provide a detailed analysis of the computational complexity of RFM, which may be important for large-scale applications.\n3. The paper assumes that the target distribution is known, which may not be the case in practice.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods for training continuous normalizing flows on manifolds.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of RFM and its scalability.\n2. Investigate the case where the manifold is not smooth and connected.\n3. Explore the possibility of learning the target distribution from data.\n4. Provide a more detailed comparison with other state-of-the-art methods for training continuous normalizing flows on manifolds.\n5. Investigate the use of RFM for other applications, such as image and video generation.\n\nRecommendation:\n\nThe paper presents a novel and highly scalable approach for training continuous normalizing flows on manifolds. The authors demonstrate the effectiveness of RFM on various manifolds, including simple geometries, high-dimensional tori, and manifolds with non-trivial curvature and boundaries. While there are some weaknesses and limitations, the paper provides a valuable contribution to the field of generative modeling on manifolds. I recommend accepting this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe authors propose Riemannian Flow Matching (RFM), a novel framework for training continuous normalizing flows on manifolds. RFM builds on the Flow Matching framework but extends it to Riemannian manifolds by introducing a premetric for defining target vector fields. The authors show that RFM is simulation-free on simple geometries with closed-form geodesics and scalable on general geometries using spectral distances. They also provide a detailed analysis of the method's advantages over existing approaches for training generative models on manifolds.\n\nStrengths:\n1. The authors provide a comprehensive analysis of the method's advantages over existing approaches for training generative models on manifolds.\n2. They introduce a novel framework for training continuous normalizing flows on manifolds that is scalable on general geometries.\n3. The authors provide a detailed analysis of the method's performance on various benchmark problems.\n4. They show that RFM is simulation-free on simple geometries with closed-form geodesics.\n\nWeaknesses:\n1. The authors assume that the premetric is given or can be computed easily. However, computing the premetric can be a complex task for general manifolds.\n2. The authors rely on spectral distances as an approximation to the geodesic distance. However, the accuracy of this approximation may be limited for complex manifolds.\n3. The authors provide a detailed analysis of the method's performance on various benchmark problems but lack a detailed analysis of the method's robustness to noise or outliers.\n4. The authors assume that the target distribution is given or can be estimated easily. However, estimating the target distribution can be a complex task for general manifolds.\n\nSuggestions for Improvement:\n1. The authors could provide a more detailed analysis of the method's robustness to noise or outliers.\n2. They could provide a more detailed analysis of the method's performance on manifolds with complex geometries.\n3. The authors could provide a more detailed analysis of the method's scalability on large manifolds.\n4. They could provide a more detailed analysis of the method's performance on manifolds with multiple modes.\n\nRecommendation:\nThe authors provide a comprehensive analysis of the method's advantages over existing approaches for training generative models on manifolds. However, they lack a detailed analysis of the method's robustness to noise or outliers. I recommend that the authors provide a more detailed analysis of the method's robustness to noise or outliers before publishing the manuscript.\n\nOverall, the manuscript provides a novel framework for training continuous normalizing flows on manifolds that is scalable on general geometries. However, the authors need to address the weaknesses of the method before publishing the manuscript.\n\nRating: 8/10\n\nReviewer: [Name]\n\n$$$",
        "gr_score_with": 0.8449612403100775,
        "gr_score_without": 0.8240469208211144,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 281,
            "green_fraction": 0.8240469208211144,
            "z_score": 1.1101372402014573,
            "p_value": 0.13346994597345924
        },
        "output_with": {
            "num_tokens_scored": 258,
            "num_green_tokens": 218,
            "green_fraction": 0.8449612403100775,
            "z_score": 1.8054611384576011,
            "p_value": 0.03550127571876777
        }
    },
    {
        "title": "MODERNTCN: A MODERN PURE CONVOLUTION STRUCTURE FOR GENERAL TIME SERIES ANALYSIS",
        "abstract": "Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel time series analysis model called ModernTCN, which is a modern pure convolution structure for general time series analysis. The authors aim to bring convolution back to the arena of time series analysis by modernizing the traditional TCN and conducting time series related modifications to make it more suitable for time series tasks. The proposed model achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection. The authors also demonstrate the efficiency superiority of ModernTCN compared to state-of-the-art Transformer-based models.\n\nStrengths:\n1. The paper provides a thorough analysis of the limitations of traditional convolution-based models in time series analysis and proposes a novel solution to overcome these limitations.\n2. The authors demonstrate the excellent task-generality of ModernTCN, achieving consistent state-of-the-art performance on multiple mainstream time series analysis tasks.\n3. The proposed model provides a better balance of efficiency and performance compared to state-of-the-art Transformer-based models.\n4. The authors conduct comprehensive experiments to evaluate the performance of ModernTCN on various time series analysis tasks and provide detailed analysis of the results.\n\nWeaknesses:\n1. The paper assumes that the input time series data is stationary, which may not be the case in many real-world applications.\n2. The authors do not provide a detailed explanation of the time series related modifications made to the traditional TCN block.\n3. The paper does not provide a clear comparison with other convolution-based models that have been proposed in the literature.\n4. The authors do not provide a detailed analysis of the computational complexity of ModernTCN.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation of the time series related modifications made to the traditional TCN block.\n2. The paper should include a comparison with other convolution-based models that have been proposed in the literature.\n3. The authors should provide a detailed analysis of the computational complexity of ModernTCN.\n4. The paper should include a discussion of the potential applications of ModernTCN in real-world time series analysis tasks.\n\nRecommendation:\nThe paper proposes a novel and efficient time series analysis model that achieves consistent state-of-the-art performance on multiple mainstream time series analysis tasks. The authors provide a thorough analysis of the limitations of traditional convolution-based models and propose a novel solution to overcome these limitations. However, the paper assumes that the input time series data is stationary, which may not be the case in many real-world applications. The authors should provide a more detailed explanation of the time series related modifications made to the traditional TCN block and include a comparison with other convolution-based models that have been proposed in the literature. Despite these limitations, the paper is well-written and provides a valuable contribution to the field of time series analysis.\n\nOverall, I would recommend this paper for publication in a top-tier conference or journal in the field of time series analysis. However, the authors should address the limitations mentioned above to make the paper more comprehensive and robust.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel convolution-based model, ModernTCN, for general time series analysis. The authors aim to modernize the traditional Temporal Convolution Network (TCN) by incorporating recent advancements in computer vision and addressing the limitations of previous convolution-based models in capturing long-term temporal dependencies and cross-variable dependencies in multivariate time series analysis. The ModernTCN architecture is designed to efficiently utilize cross-time and cross-variable dependencies, achieving consistent state-of-the-art performance on five mainstream analysis tasks while maintaining the efficiency advantage of convolution-based models.\n\nStrengths:\n1. The paper provides a thorough analysis of the limitations of previous convolution-based models and the potential benefits of modern convolution in time series analysis.\n2. The authors propose a novel architecture, ModernTCN, that addresses the limitations of previous convolution-based models by incorporating recent advancements in computer vision and designing a decoupling structure for efficient utilization of cross-time and cross-variable dependencies.\n3. The paper presents extensive experimental results on five mainstream analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection, demonstrating the excellent generality and efficiency of ModernTCN.\n4. The authors conduct an ablation study to validate the effectiveness of the ModernTCN block design and the importance of the cross-variable component in capturing cross-variable dependencies in multivariate time series analysis.\n5. The paper provides a detailed analysis of the ERFs of previous convolution-based models and ModernTCN, highlighting the significant improvement in ERF achieved by ModernTCN, which enables it to better unleash the potential of convolution in time series analysis.\n\nWeaknesses:\n1. The paper assumes that the input data is multivariate and does not discuss the case where the input data is univariate.\n2. The authors do not provide a detailed analysis of the computational complexity of ModernTCN and its comparison to previous convolution-based models.\n3. The paper does not discuss the potential applications of ModernTCN in specific domains, such as industrial forecasting, anomaly detection, or action recognition.\nSuggestions for Improvement:\n1. Consider including a discussion on the assumptions made in the paper, particularly the multivariate input data, and how ModernTCN would perform on univariate data.\n2. Perform a detailed analysis of the computational complexity of ModernTCN and its comparison to previous convolution-based models to provide a better understanding of the efficiency of ModernTCN.\n3. Discuss the potential applications of ModernTCN in specific domains and provide case studies or examples to demonstrate its effectiveness.\nRecommendation:\nThe paper presents a novel and efficient convolution-based model, ModernTCN, for general time series analysis, which achieves consistent state-of-the-art performance on five mainstream analysis tasks while maintaining the efficiency advantage of convolution-based models. The authors provide a thorough analysis of the limitations of previous convolution-based models and the potential benefits of modern convolution in time series analysis, as well as extensive experimental results to validate the effectiveness of ModernTCN. The paper is well-written and well-organized, and the authors provide a clear and concise presentation of the ModernTCN architecture and its benefits. Therefore, I recommend this paper for publication in a top-tier conference or journal in the field of machine learning and artificial intelligence.\n$$$",
        "gr_score_with": 0.8316062176165803,
        "gr_score_without": 0.809375,
        "output_without": {
            "num_tokens_scored": 320,
            "num_green_tokens": 259,
            "green_fraction": 0.809375,
            "z_score": 0.41926274578121064,
            "p_value": 0.3375120599671227
        },
        "output_with": {
            "num_tokens_scored": 386,
            "num_green_tokens": 321,
            "green_fraction": 0.8316062176165803,
            "z_score": 1.5524091256058257,
            "p_value": 0.06028218065506257
        }
    },
    {
        "title": "NAVIGATING TEXT-TO-IMAGE CUSTOMIZATION: FROM",
        "abstract": "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied finetuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces LyCORIS, an open-source library that provides a wide range of fine-tuning methodologies for Stable Diffusion, a leading open-source text-to-image generative model. The library offers a spectrum of methodologies, including LoRA, LoHa, and LoKr, among others. The authors also propose a comprehensive evaluation framework that incorporates a diverse suite of metrics to capture key aspects of fine-tuning, including concept fidelity, text-image alignment, image diversity, and preservation of the base model's style. Through extensive experiments, the authors compare the performances of different fine-tuning algorithms implemented in LyCORIS and assess the impacts of various hyperparameters, offering insights into how these factors influence the results.\n\nStrengths:\n\n1. The paper introduces a comprehensive evaluation framework that captures key aspects of fine-tuning, making it a valuable contribution to the field.\n2. The authors provide a wide range of fine-tuning methodologies in the LyCORIS library, making it a useful resource for researchers and practitioners.\n3. The paper includes extensive experiments that compare the performances of different fine-tuning algorithms and assess the impacts of various hyperparameters.\n4. The authors provide a clear and concise explanation of the methods and experiments, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes a certain level of background knowledge in deep learning and generative models, which may make it challenging for readers without this background to fully understand the content.\n2. The evaluation framework proposed in the paper is comprehensive, but it may be challenging to implement and evaluate, especially for researchers without extensive experience in fine-tuning and evaluation of generative models.\n3. The paper focuses on the Stable Diffusion model, which may limit its applicability to other generative models.\n4. The authors do not explore the task of generating images with multiple learned concepts, which is an important aspect of fine-tuning and evaluation.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the background knowledge required to understand the content, including a brief introduction to deep learning and generative models.\n2. Include a more detailed explanation of the evaluation framework and its implementation, including code snippets and examples.\n3. Explore the applicability of the LyCORIS library and evaluation framework to other generative models, including models with different architectures and capabilities.\n4. Include a more detailed discussion of the limitations and challenges of the paper, including the assumption of a certain level of background knowledge and the focus on a single generative model.\n\nRecommendation:\n\nThe paper is a valuable contribution to the field of fine-tuning and evaluation of generative models, and it provides a comprehensive evaluation framework and a wide range of fine-tuning methodologies in the LyCORIS library. However, it assumes a certain level of background knowledge and focuses on a single generative model, which may limit its applicability and accessibility. Overall, I recommend this paper to researchers and practitioners with a strong background in deep learning and generative models, and I suggest that the authors provide more detailed explanations and examples to make the content more accessible to a wider audience.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents LyCORIS, an open-source library that provides a wide range of methods for fine-tuning Stable Diffusion, a text-to-image generative model. The library includes several algorithms, such as LoRA, LoHa, LoKr, and others, which can be used to customize the model to specific domains or concepts. The authors also propose a comprehensive evaluation framework that captures key aspects of the fine-tuning process, including concept fidelity, text-image alignment, image diversity, and base model preservation. The paper presents extensive experiments that compare the performances of different LyCORIS algorithms and assess the impacts of various hyperparameters.\n\nStrengths:\n\n1. The paper provides a comprehensive evaluation framework that captures key aspects of the fine-tuning process.\n2. The authors present a wide range of methods for fine-tuning Stable Diffusion, including LoRA, LoHa, LoKr, and others.\n3. The paper includes extensive experiments that compare the performances of different LyCORIS algorithms and assess the impacts of various hyperparameters.\n4. The authors acknowledge the limitations of the study and suggest future directions.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of deep learning and generative models, which may not be the case.\n2. The evaluation framework presented in the paper is comprehensive, but it may be difficult to implement in practice.\n3. The paper does not provide a detailed analysis of the computational resources required to run the experiments.\n4. The authors do not provide a comparison with other existing libraries or methods.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed introduction to the basics of deep learning and generative models.\n2. Provide more information on the computational resources required to run the experiments.\n3. Compare the performance of LyCORIS with other existing libraries or methods.\n4. Provide more details on the implementation of the evaluation framework.\n\nRecommendation:\n\nThe paper presents a comprehensive evaluation framework and a wide range of methods for fine-tuning Stable Diffusion. The extensive experiments provide valuable insights into the impact of different hyperparameters on the performance of the algorithms. However, the paper assumes that the reader is familiar with the basics of deep learning and generative models, and the evaluation framework may be difficult to implement in practice. Overall, the paper is well-structured and provides valuable contributions to the field of text-to-image generation.\n\nRecommendation: Accept.\n\n$$$",
        "gr_score_with": 0.8231292517006803,
        "gr_score_without": 0.8517587939698492,
        "output_without": {
            "num_tokens_scored": 398,
            "num_green_tokens": 339,
            "green_fraction": 0.8517587939698492,
            "z_score": 2.581461741652986,
            "p_value": 0.004919144771586489
        },
        "output_with": {
            "num_tokens_scored": 294,
            "num_green_tokens": 242,
            "green_fraction": 0.8231292517006803,
            "z_score": 0.991460133983665,
            "p_value": 0.16073047477047697
        }
    },
    {
        "title": "ENHANCING NEURAL TRAINING VIA A CORRELATED DYNAMICS MODEL",
        "abstract": "As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this, we introduce correlation mode decomposition (CMD). This algorithm clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables CMD to efficiently represent the training dynamics of complex networks, like ResNets and Transformers, using only a few modes. Moreover, test set generalization is enhanced. We introduce an efficient CMD variant, designed to run concurrently with training. Our experiments indicate that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification. Our modeling can improve training efficiency and lower communication overhead, as shown by our preliminary experiments in the context of federated learning.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process. The authors introduce Correlation Mode Decomposition (CMD), a data-driven approach that efficiently models training dynamics. They show that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification and propose an efficient CMD variant that can be performed online during training. The authors also pioneer the use of modeled training dynamics to reduce communication overhead in distributed learning.\n\nStrengths:\n\n1. The paper presents a novel and effective approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process.\n2. The authors provide a comprehensive experimental evaluation, showcasing the benefits of their approach in various architectures and learning tasks.\n3. The paper proposes an efficient CMD variant that can be performed online during training, reducing computational overhead and memory requirements.\n4. The authors demonstrate the efficiency of their approach in Federated Learning, improving communication overhead without compromising performance.\n\nWeaknesses:\n\n1. The paper assumes a fixed number of modes, which may not be optimal for all cases. The authors could explore more adaptive methods to determine the number of modes.\n2. The experimental evaluation focuses primarily on image classification tasks. The authors could explore the applicability of their approach to other learning tasks, such as regression or reinforcement learning.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed method. The authors could provide more insights into the computational requirements of their approach.\n\nSuggestions for Improvement:\n\n1. Explore more adaptive methods to determine the number of modes, such as using techniques from density estimation or clustering.\n2. Evaluate the proposed approach on a broader range of learning tasks, including regression and reinforcement learning.\n3. Provide a more detailed analysis of the computational complexity of the proposed method, including a discussion of the memory and computational requirements.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process. The authors provide a comprehensive experimental evaluation, showcasing the benefits of their approach in various architectures and learning tasks. While there are some limitations to the paper, the proposed approach has the potential to improve the efficiency and effectiveness of neural network training. Therefore, I recommend accepting this paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe authors propose a novel method for modeling neural network parameter dynamics, leveraging correlated behavior of training processes. The method, called Correlation Mode Decomposition (CMD), efficiently represents training dynamics using a few modes and enhances test set generalization. The authors also introduce an efficient variant of CMD that can be performed online during training, which surpasses state-of-the-art methods for compactly modeled dynamics.\n\nStrengths:\n\n1. The authors provide a comprehensive overview of related work and highlight the limitations of existing methods, making their contribution more understandable and justified.\n2. The proposed method, Correlation Mode Decomposition, is novel and innovative, leveraging correlated behavior of training processes, which is not explored before.\n3. The authors provide a thorough analysis of their method, including experiments, ablation studies, and visualization of accuracy and error landscapes, which demonstrates its effectiveness and versatility.\n4. The authors also demonstrate the efficiency of their method in Federated Learning, reducing communication overhead without compromising performance.\n\nWeaknesses:\n\n1. The authors assume that neural network parameters are highly correlated and can be grouped into modes, which may not always be true, especially for complex and non-linear neural networks.\n2. The authors do not provide a thorough analysis of their method's robustness and scalability, which is essential for real-world applications.\n3. The authors rely heavily on experiments and visualization, which may not be replicable or scalable for large and complex neural networks.\n4. The authors do not provide a thorough comparison of their method with other state-of-the-art methods, which makes it difficult to assess its overall performance and effectiveness.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough analysis of their method's robustness and scalability, including experiments and visualization for large and complex neural networks.\n2. Provide a more comprehensive comparison of their method with other state-of-the-art methods, including a discussion of their strengths and weaknesses.\n3. Explore other potential applications of their method, such as transfer learning and meta-learning.\n4. Provide more detail about their implementation and code, including any pre-processing or post-processing steps that may be necessary.\n\nRecommendation:\n\nThis paper presents a novel and innovative method for modeling neural network parameter dynamics, which shows promise for improving accuracy and reducing communication overhead in Federated Learning. While there are some limitations and areas for improvement, the authors provide a thorough analysis of their method and demonstrate its effectiveness and versatility through experiments and visualization. With further refinement and development, this method has the potential to become a state-of-the-art solution for modeling neural network parameter dynamics.\n\nRating: 8/10\n\nRecommendation for publication: Accept",
        "gr_score_with": 0.8493150684931506,
        "gr_score_without": 0.8466666666666667,
        "output_without": {
            "num_tokens_scored": 300,
            "num_green_tokens": 254,
            "green_fraction": 0.8466666666666667,
            "z_score": 2.0207259421636907,
            "p_value": 0.021654071405395964
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 310,
            "green_fraction": 0.8493150684931506,
            "z_score": 2.3554076516559617,
            "p_value": 0.009251197687906587
        }
    },
    {
        "title": "RingAttention with Blockwise Transformers for Near-Infinite Context",
        "abstract": "Transformers have emerged as the architecture of choice for many state-ofthe-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Blockwise RingAttention, which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach, RingAttention with Blockwise Transformers, to reduce the memory requirements of Transformers and enable them to handle long sequences. The authors leverage blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices, overlapping the communication of key-value blocks with the computation of blockwise attention. This approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers. The authors demonstrate the effectiveness of their approach through extensive experiments on language modeling and reinforcement learning tasks.\n\nStrengths:\n\n1. The paper presents a novel approach to reduce the memory requirements of Transformers, which is a significant contribution to the field.\n2. The authors provide a thorough analysis of the memory requirements of Transformers and identify the key challenges in scaling up the context length.\n3. The paper presents a clear and concise description of the RingAttention with Blockwise Transformers approach and its implementation.\n4. The authors provide extensive experimental results to demonstrate the effectiveness of their approach.\n5. The paper discusses the implications of the approach and its potential applications in various fields.\n\nWeaknesses:\n\n1. The paper assumes a high-level understanding of Transformers and their architecture, which may not be familiar to readers who are new to the field.\n2. The authors do not provide a detailed comparison with other approaches to reducing memory requirements in Transformers.\n3. The paper assumes a specific hardware setup (GPUs and TPUs) and does not discuss the potential challenges in implementing the approach on other hardware platforms.\n4. The authors do not provide a detailed analysis of the computational complexity of the RingAttention with Blockwise Transformers approach.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other approaches to reducing memory requirements in Transformers.\n2. Discuss the potential challenges in implementing the approach on other hardware platforms.\n3. Provide a more detailed analysis of the computational complexity of the RingAttention with Blockwise Transformers approach.\n4. Consider providing more experimental results to demonstrate the effectiveness of the approach on different tasks and datasets.\n5. Consider discussing the potential applications of the approach in various fields and providing more insights into its implications.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to reducing the memory requirements of Transformers. The authors provide a thorough analysis of the memory requirements of Transformers and identify the key challenges in scaling up the context length. The paper presents a clear and concise description of the RingAttention with Blockwise Transformers approach and its implementation. The authors provide extensive experimental results to demonstrate the effectiveness of their approach. However, the paper assumes a high-level understanding of Transformers and their architecture, and does not provide a detailed comparison with other approaches to reducing memory requirements in Transformers. Therefore, I recommend that the authors revise the paper to address these issues and provide a more comprehensive analysis of the approach.\n\nOverall, the paper is well-written, and the authors provide a clear and concise description of the RingAttention with Blockwise Transformers approach. The paper presents a novel and effective approach to reducing the memory requirements of Transformers, and the authors provide extensive experimental results to demonstrate the effectiveness of their approach. However, the paper assumes a high-level understanding of Transformers and their architecture, and does not provide a detailed comparison with other approaches to reducing memory requirements in Transformers. Therefore, I recommend that the authors revise the paper to address these issues and provide a more comprehensive analysis of the approach.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Title: RingAttention with Blockwise Transformers for Near-Infinite Context\nPaper Summary:\nThe paper proposes RingAttention, a novel memory-efficient approach that allows for near-infinite context sizes without sacrificing performance. RingAttention builds upon the blockwise parallel transformers framework and introduces an overlapping mechanism between communication and computation of key-value blocks between devices, enabling efficient distribution of long sequences across multiple devices without adding significant overheads. The paper conducts extensive experiments on language modeling and reinforcement learning benchmarks, demonstrating the effectiveness of RingAttention in enabling the training of sequences that are device-count times longer than those achievable by prior memory-efficient transformers, while maintaining performance and reducing memory requirements.\n\nStrengths:\n1. The paper proposes an innovative and efficient memory-efficient approach that overcomes the limitations of prior memory-efficient transformers.\n2. The paper provides extensive experiments on language modeling and reinforcement learning benchmarks, demonstrating the effectiveness of RingAttention.\n3. The paper highlights the potential of near-infinite context sizes for various applications, including large video-audio-language models, learning from extended feedback and trial-and-errors, and adapting AI models to understand scientific data.\n4. The paper introduces an overlapping mechanism between communication and computation of key-value blocks between devices, enabling efficient distribution of long sequences across multiple devices without adding significant overheads.\n5. The paper provides detailed analysis of the memory requirements and computational complexity of RingAttention, demonstrating its scalability and efficiency.\n\nWeaknesses:\n1. The paper assumes that the block size should be larger than the ratio of FLOPS over bandwidth, which may not be true for all devices and scenarios.\n2. The paper does not provide detailed analysis of the communication and computation latency, which may affect the performance of RingAttention.\n3. The paper assumes that the input sequence can be split into blocks, which may not be possible for all input sequences.\n4. The paper does not provide comparison of RingAttention with other memory-efficient transformers that use sequence parallelism or other parallelization methods.\n5. The paper assumes that the number of devices and block size are fixed, which may not be true for all scenarios.\n\nSuggestions for Improvement:\n1. Provide detailed analysis of the communication and computation latency, and investigate its effect on the performance of RingAttention.\n2. Investigate the effect of varying block sizes and input sequence lengths on the performance of RingAttention.\n3. Compare RingAttention with other memory-efficient transformers that use sequence parallelism or other parallelization methods.\n4. Investigate the scalability of RingAttention for large-scale applications and provide detailed analysis of the memory requirements and computational complexity.\n5. Provide more detailed explanation of the overlapping mechanism between communication and computation of key-value blocks between devices, and investigate its effect on the performance of RingAttention.\n\nRecommendation:\nThis paper proposes an innovative and efficient memory-efficient approach that overcomes the limitations of prior memory-efficient transformers. The paper provides extensive experiments on language modeling and reinforcement learning benchmarks, demonstrating the effectiveness of RingAttention. The paper highlights the potential of near-infinite context sizes for various applications, including large video-audio-language models, learning from extended feedback and trial-and-errors, and adapting AI models to understand scientific data. The paper introduces an overlapping mechanism between communication and computation of key-value blocks between devices, enabling efficient distribution of long sequences across multiple devices without adding significant overheads. However, the paper assumes that the block size should be larger than the ratio of FLOPS over bandwidth, which may not be true for all devices and scenarios, and does not provide detailed analysis of the communication and computation latency, which may affect the performance of RingAttention. Therefore, I recommend that the paper should investigate the effect of varying block sizes and input sequence lengths on the performance of RingAttention, compare RingAttention with other memory-efficient transformers that use sequence parallelism or other parallelization methods, and provide more detailed explanation of the overlapping mechanism between communication and computation of key-value blocks between devices, and investigate its effect on the performance of RingAttention.\n$$",
        "gr_score_with": 0.8158640226628895,
        "gr_score_without": 0.8012422360248447,
        "output_without": {
            "num_tokens_scored": 322,
            "num_green_tokens": 258,
            "green_fraction": 0.8012422360248447,
            "z_score": 0.05572782125753212,
            "p_value": 0.4777793178887988
        },
        "output_with": {
            "num_tokens_scored": 353,
            "num_green_tokens": 288,
            "green_fraction": 0.8158640226628895,
            "z_score": 0.7451448135772848,
            "p_value": 0.22809209022164145
        }
    },
    {
        "title": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS",
        "abstract": "Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https://github.com/THUDM/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256\u00d7256 and CelebA-HQ 256\u00d7256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256\u00d7256 generation without any guidance. RDM can achieve a FID of 1.99 (and a class-balanced FID of 1.87) if with classifier-free guidance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel cascaded diffusion model, called Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion models via blurring diffusion and block noise. The RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM, and DiT by a large margin. The authors also analyze the reasons of the difficulty of noise scheduling in high-resolution diffusion models in frequency domain and introduce the block noise to bridge the gap.\n\nStrengths:\n\n* The paper provides a thorough analysis of the frequency spectrum of images with different resolutions and introduces the concept of block noise, which is a significant contribution to the field of diffusion models.\n* The proposed RDM model achieves state-of-the-art performance on CelebA-HQ and ImageNet 256\u00d7256, demonstrating its effectiveness in high-resolution image synthesis.\n* The authors provide a detailed explanation of the RDM architecture, including the blurring diffusion and block noise components, which is easy to follow and understand.\n\nWeaknesses:\n\n* The paper assumes that the noise schedule is resolution-dependent, which may not be the case in all scenarios. Further investigation is needed to confirm this assumption.\n* The authors do not provide a theoretical derivation of the optimal noise schedule, which is a significant limitation of the paper.\n* The paper relies heavily on the EDM framework and implementation, which may limit its applicability to other diffusion models.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of the frequency spectrum analysis and how it relates to the block noise concept.\n* Further investigation is needed to confirm the assumption that the noise schedule is resolution-dependent.\n* The authors should provide a more thorough comparison of the RDM model with other diffusion models, including those that do not use the EDM framework.\n* The paper should include more experiments on higher resolutions and different datasets to demonstrate the robustness and generalizability of the RDM model.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of diffusion models. However, the limitations mentioned above should be addressed in future work. With further investigation and experimentation, the RDM model has the potential to become a state-of-the-art diffusion model for high-resolution image synthesis.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \n\n**Paper Summary**\n\nThis work presents Relay Diffusion Model (RDM), an innovative approach to unify the diffusion process across resolutions for image synthesis. The authors identify the challenges in high-resolution image synthesis due to the increased Signal-to-Noise Ratio in the frequency domain and introduce block noise to alleviate this issue. RDM improves upon the traditional cascaded diffusion models by transferring low-resolution results to high-resolution ones through blurring diffusion and block noise. The authors demonstrate the effectiveness of RDM on unconditional CelebA-HQ and conditional ImageNet 256\u00d7256 datasets.\n\n**Strengths**\n\n1. The authors provide an in-depth frequency spectrum analysis to understand the challenges in high-resolution image synthesis and introduce block noise as an effective solution to alleviate these issues.\n2. The RDM framework presents an efficient and flexible way to unify the diffusion process across resolutions for image synthesis.\n3. The authors demonstrate the state-of-the-art results on unconditional CelebA-HQ and conditional ImageNet 256\u00d7256 datasets.\n\n**Weaknesses**\n\n1. The authors rely on empirical results and experiments to demonstrate the effectiveness of RDM. However, the theoretical foundations of the block noise and blurring diffusion are not thoroughly explored in the manuscript.\n2. The manuscript lacks clear explanations and justifications for the choices of hyperparameters and the design of the RDM framework.\n3. The authors do not provide comprehensive comparisons with other state-of-the-art methods on multiple datasets.\n\n**Suggestions for Improvement**\n\n1. The authors should provide more theoretical insights into the block noise and blurring diffusion and their effects on the diffusion process in high-resolution image synthesis.\n2. The manuscript should provide more detailed explanations and justifications for the choices of hyperparameters and the design of the RDM framework.\n3. The authors should conduct more comprehensive comparisons with other state-of-the-art methods on multiple datasets to demonstrate the universality and effectiveness of RDM.\n\n**Recommendation**\n\nThis work presents an innovative approach to unify the diffusion process across resolutions for image synthesis. The authors demonstrate the effectiveness of RDM on unconditional CelebA-HQ and conditional ImageNet 256\u00d7256 datasets. However, the manuscript lacks clear explanations and justifications for the choices of hyperparameters and the design of the RDM framework. The authors should provide more theoretical insights into the block noise and blurring diffusion and conduct more comprehensive comparisons with other state-of-the-art methods on multiple datasets. With these improvements, the manuscript can be considered for publication in top-tier journals or conferences in the field of machine learning and image synthesis.",
        "gr_score_with": 0.7666666666666667,
        "gr_score_without": 0.8142857142857143,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 285,
            "green_fraction": 0.8142857142857143,
            "z_score": 0.668153104781061,
            "p_value": 0.2520179332262524
        },
        "output_with": {
            "num_tokens_scored": 270,
            "num_green_tokens": 207,
            "green_fraction": 0.7666666666666667,
            "z_score": -1.3693063937629153,
            "p_value": 0.9145482398846012
        }
    },
    {
        "title": "A TOPOLOGICAL PERSPECTIVE ON DEMYSTIFYING GNN-BASED LINK PREDICTION PERFORMANCE",
        "abstract": "Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies aim to improve the overall LP performance of GNNs, none have explored its varying performance across different nodes and its underlying reasons. To this end, we aim to demystify which nodes will perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit poorer LP performance, our empirical findings provide nuances to this viewpoint and prompt us to propose a better metric, Topological Concentration (TC), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that TC has a higher correlation with LP performance than other node-level topological metrics like degree and subgraph density, offering a better way to identify low-performing nodes than using degree. With TC, we discover a novel topological distribution shift issue in which newly joined neighbors of a node tend to become less interactive with that node\u2019s existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To make the computation of TC scalable, We further propose Approximated Topological Concentration (ATC) and theoretically/empirically justify its efficacy in approximating TC and reducing the computation complexity. Given the positive correlation between node TC and its LP performance, we explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing and discuss its effectiveness with limitations. Our code is publicly available at https://github.com/submission2024/Topo Concentration.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Topological Concentration (TC), a novel metric to measure the topological interaction between a node's local subgraph and its neighbors' subgraphs. The authors demonstrate that TC has a higher correlation with link prediction (LP) performance than other node-level topological metrics, such as degree and subgraph density. They also discover a novel topological distribution shift (TDS) issue, where newly joined neighbors of a node become less interactive with its existing neighbors, compromising the generalizability of node embeddings for LP. To address this issue, the authors propose Approximated Topological Concentration (ATC), a scalable version of TC. They also explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing process.\n\nStrengths:\n\n1. The paper proposes a novel and effective metric, TC, which characterizes the node LP performance from a topological perspective.\n2. The authors demonstrate the superiority of TC over other common node topological properties, such as degree and subgraph density.\n3. The paper discovers a novel TDS issue, which highlights the importance of considering the dynamic nature of node interactions in LP.\n4. The authors propose a scalable version of TC, ATC, which reduces the computational complexity while maintaining high correlations with LP performance.\n5. The paper explores the potential of boosting LP performance via enhancing TC, which provides a new direction for future research.\n\nWeaknesses:\n\n1. The paper assumes that the link formation between a node pair depends on the intersection between their local subgraphs, which may not always be the case in real-world networks.\n2. The authors do not provide a clear explanation of how the TDS issue arises and how it can be addressed.\n3. The paper assumes that the graph diffusion can be efficiently executed via power iteration, which may not be the case for large-scale networks.\n4. The authors do not provide a thorough analysis of the causal relationship between TC and LP performance.\n5. The paper does not discuss the potential limitations and challenges of using TC and ATC in real-world applications.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the link formation mechanism and how it relates to the intersection between local subgraphs.\n2. Offer a more comprehensive analysis of the TDS issue, including its causes and potential solutions.\n3. Investigate the causal relationship between TC and LP performance using techniques such as causal inference or structural equation modeling.\n4. Discuss the potential limitations and challenges of using TC and ATC in real-world applications, such as scalability and computational complexity.\n5. Explore other potential applications of TC and ATC, such as node classification or community detection.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of link prediction by proposing a novel and effective metric, TC. However, it also raises several questions and challenges that need to be addressed in future research. Therefore, I recommend that the authors revise the paper to address these weaknesses and suggestions for improvement, and submit it to a top-tier conference in the field of machine learning or network science.",
        "peer_review_with_watermark": "$$$\n**Paper Summary**\n\nThe paper proposes a new metric, Topological Concentration (TC), to measure the varying levels of embedding quality and Link Prediction (LP) performance of nodes within a graph from a local topology perspective. The authors demonstrate TC's superiority over other common node topological properties, including degree and SubGraph Density, in characterizing node LP performance. They further discover a Topological Distribution Shift (TDS) issue, where newly joined neighbors of a node tend to become less interactive with its old neighbors, compromising node embeddings' generalizability. To address scalability issues, they propose Approximated Topological Concentration (ATC) and theoretically justify its efficacy in approximating TC and reducing computation complexity. Finally, they investigate using TC to enhance GNNs' LP performance by re-weighting edges in message-passing.\n\n**Strengths**\n\n1. **Novel Metric Proposal**: The authors propose a new metric, TC, which captures local topology interactions and has a higher correlation with node LP performance than other common node topological properties.\n2. **Empirical Validation**: The paper provides extensive empirical validation, including correlation analysis, node-level evaluation, and distribution shift analysis, to demonstrate TC's superiority and its ability to capture TDS.\n3. **Approximation and Scalability**: The authors propose ATC to address scalability issues and theoretically justify its efficacy in approximating TC.\n4. **Practical Implications**: The paper discusses potential applications of TC, including data valuation and selective use of neighborhood information.\n\n**Weaknesses**\n\n1. **Limited Generalizability**: The paper's empirical validation focuses on two datasets, and it would be beneficial to demonstrate TC's generalizability on a larger set of datasets and graph structures.\n2. **Lack of Causal Analysis**: While TC correlates with node LP performance, it remains unclear whether TC causes or merely correlates with improved LP performance. Further investigation into causal relationships would strengthen the paper's claims.\n3. **Computational Cost**: While ATC addresses scalability issues, its computational cost remains significant, and further optimization techniques would be beneficial.\n4. **Limited Discussion of TDS Implications**: The paper discusses TDS's impact on node embeddings, but it would be valuable to discuss potential implications and solutions in a larger context, including data collection and graph evolution.\n\n**Recommendations**\n\n1. **Extend Empirical Validation to a Broader Range of Datasets and Graph Structures**.\n2. **Conduct Causal Analysis to Establish a Clear Causal Link Between TC and Improved LP Performance**.\n3. **Optimize ATC's Computational Cost and Explore Further Scalability Techniques**.\n4. **Delve Deeper into TDS Implications and Explore Solutions to Mitigate Its Impact**.\n\n**Recommendation Grade**\n\nBased on these strengths, weaknesses, and recommendations, I would recommend accepting this paper with some revisions to address these points.",
        "gr_score_with": 0.8105726872246696,
        "gr_score_without": 0.8160919540229885,
        "output_without": {
            "num_tokens_scored": 435,
            "num_green_tokens": 355,
            "green_fraction": 0.8160919540229885,
            "z_score": 0.8390607775994223,
            "p_value": 0.20071760230293378
        },
        "output_with": {
            "num_tokens_scored": 454,
            "num_green_tokens": 368,
            "green_fraction": 0.8105726872246696,
            "z_score": 0.5631879053567133,
            "p_value": 0.28665346888314536
        }
    },
    {
        "title": "TIONS WITH TRANSITIONAL DICTIONARY LEARNING",
        "abstract": "This paper introduces a novel Transitional Dictionary Learning (TDL) framework that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. We propose a game-theoretic diffusion model to decompose the input into visual parts using the dictionaries learned by the Expectation Maximization (EM) algorithm, implemented as the online prototype clustering, based on the decomposition results. Additionally, two metrics, clustering information gain, and heuristic shape score are proposed to evaluate the model. Experiments are conducted on three abstract compositional visual object datasets, which require the model to utilize the compositionality of data instead of simply exploiting visual features. Then, three tasks on symbol grounding to predefined classes of parts and relations, as well as transfer learning to unseen classes, followed by a human evaluation, were carried out on these datasets. The results show that the proposed method discovers compositional patterns, which significantly outperforms the state-of-the-art unsupervised part segmentation methods that rely on visual features from pre-trained backbones. Furthermore, the proposed metrics are consistent with human evaluations.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel Transitional Dictionary Learning (TDL) framework that learns symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. The TDL framework uses an Expectation Maximization (EM) algorithm to iteratively update dictionaries that store hidden representations of symbolic knowledge. The authors also introduce two metrics, Clustering Information Gain (CIG) and Heuristic Shape Score (HSS), to evaluate the learned representation. Experiments are conducted on three abstract compositional visual object datasets, and the results show that the proposed method significantly outperforms state-of-the-art unsupervised part segmentation methods that rely on visual features.\n\nStrengths:\n\n1. The TDL framework is a novel approach to learning symbolic knowledge from visual inputs, which is a challenging problem in computer vision.\n2. The use of EM algorithm to iteratively update dictionaries is a clever approach to learning symbolic knowledge.\n3. The introduction of CIG and HSS as evaluation metrics is a significant contribution to the field of computer vision.\n4. The experiments on three abstract compositional visual object datasets demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n\n1. The TDL framework requires a large number of training samples, which may not be feasible in all scenarios.\n2. The EM algorithm used in the TDL framework may get stuck in local optima, which can affect the performance of the method.\n3. The CIG and HSS metrics may not capture all aspects of the learned representation, which can lead to biased evaluation results.\n4. The experiments on the three datasets may not be exhaustive, and more datasets should be used to evaluate the proposed method.\n\nSuggestions for Improvement:\n\n1. Investigate the use of other optimization algorithms, such as stochastic gradient descent or Adam, to improve the performance of the TDL framework.\n2. Explore the use of transfer learning to adapt the TDL framework to new datasets or scenarios.\n3. Develop more comprehensive evaluation metrics that capture all aspects of the learned representation.\n4. Conduct experiments on more datasets to evaluate the robustness and generalizability of the proposed method.\n\nRecommendation:\n\nThe proposed TDL framework is a significant contribution to the field of computer vision, and it has the potential to learn symbolic knowledge from visual inputs. However, the method requires further investigation and improvement to make it more robust and generalizable. I recommend that the authors address the weaknesses and suggestions for improvement mentioned above and conduct more experiments to evaluate the proposed method.\n\nOverall, I rate this paper 8/10 for its novelty, significance, and potential impact on the field of computer vision. However, I deduct 2 points for the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper presents a novel unsupervised learning method called Transitional Dictionary Learning (TDL) that aims to bridge the gap between neural and symbolic representations. The TDL framework uses an Expectation Maximization algorithm to learn a transitional representation, which is a combination of entity and relational mappings. The method is tested on three abstract compositional visual object datasets and compared with state-of-the-art unsupervised part segmentation methods. The results show that the TDL method outperforms the baselines and achieves better interpretability.\n\nStrengths:\n- The paper presents a novel and innovative approach to learning transitional representations, which is a significant step forward in bridging the gap between neural and symbolic representations.\n- The method is tested on three abstract compositional visual object datasets, which is a challenging task and requires the model to learn compositionality.\n- The paper presents a comprehensive evaluation of the method, including unsupervised learning, downstream task experiments, and human assessments.\n\nWeaknesses:\n- The paper assumes a linear composition of the input, which may not be realistic in all cases.\n- The method relies on a game-theoretic diffusion model, which may not be scalable to large datasets.\n- The paper does not provide a detailed comparison with other methods that use different representations or learning frameworks.\n\nSuggestions for Improvement:\n- The authors should provide a more detailed comparison with other methods, including those that use different representations or learning frameworks.\n- The authors should investigate the scalability of the method to large datasets and provide a more detailed evaluation of the computational complexity.\n- The authors should provide more details on the implementation of the game-theoretic diffusion model and the online prototype clustering.\n\nRecommendation:\nThis paper presents a novel and innovative approach to learning transitional representations, which is a significant step forward in bridging the gap between neural and symbolic representations. The method is tested on three abstract compositional visual object datasets and achieves better interpretability compared to state-of-the-art unsupervised part segmentation methods. However, the paper assumes a linear composition of the input and relies on a game-theoretic diffusion model, which may not be scalable to large datasets. Therefore, the authors should provide a more detailed comparison with other methods and investigate the scalability of the method to large datasets.\n\nRating: 8/10\n\nRecommendation: Accept with minor revisions.",
        "gr_score_with": 0.8021978021978022,
        "gr_score_without": 0.8325,
        "output_without": {
            "num_tokens_scored": 400,
            "num_green_tokens": 333,
            "green_fraction": 0.8325,
            "z_score": 1.6250000000000002,
            "p_value": 0.05208127941521953
        },
        "output_with": {
            "num_tokens_scored": 273,
            "num_green_tokens": 219,
            "green_fraction": 0.8021978021978022,
            "z_score": 0.0907841299003195,
            "p_value": 0.4638320602388598
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel regularization technique, zero-mean regularization, for spectral contrastive learning (SpeCL). The authors argue that SpeCL has two limitations: (i) it requires the orthogonality of representations between negative pairs, and (ii) it does not account for wrong connections in the positive-pair graph. To address these limitations, they introduce an additive factor, \u03c4, into the term of SpeCL involving negative pairs. This modification is equivalent to regularizing the mean of representations to be zero, referred to as zero-mean regularization. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem and the proposed solution.\n2. The authors provide a thorough theoretical analysis of the benefits of zero-mean regularization in both unsupervised and supervised scenarios.\n3. The paper presents extensive experimental results to verify the effectiveness of zero-mean regularization on various benchmark datasets.\n4. The authors provide a clear explanation of the implications of zero-mean regularization on the structure of the learned representations.\n\nWeaknesses:\n\n1. The paper assumes a specific form of the positive-pair graph, which may not be realistic in all scenarios.\n2. The authors do not provide a clear explanation of how to choose the value of \u03c4 in practice.\n3. The paper does not discuss the computational complexity of the proposed method.\n4. The authors do not provide a comparison with other regularization techniques.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how to choose the value of \u03c4 in practice.\n2. Discuss the computational complexity of the proposed method and provide a comparison with other regularization techniques.\n3. Provide more experimental results on different datasets and scenarios to further verify the effectiveness of zero-mean regularization.\n4. Discuss the implications of zero-mean regularization on the interpretability of the learned representations.\n\nRecommendation:\n\nThe paper presents a novel and effective regularization technique for spectral contrastive learning. The authors provide a thorough theoretical analysis and extensive experimental results to demonstrate the effectiveness of zero-mean regularization. The paper is well-written and easy to follow. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust. I recommend this paper for publication in a top-tier conference in the field of machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes zero-mean regularization as an additive factor to the spectral contrastive loss, which aims to mitigate the adverse effect of \"wrong connections\" in the positive-pair graph. The proposed zero-mean regularization is shown to implicitly alleviate the orthogonality of representations between negative pairs, leading to more discriminative representations. Theoretical analysis is provided to demonstrate the effectiveness of zero-mean regularization in unsupervised domain adaptation and supervised classification with noisy labels. Experiments on benchmark datasets demonstrate the superiority of zero-mean regularization over the state-of-the-art spectral contrastive loss in terms of learning more discriminative representations, decoupling class information across domains, and mitigating label noise.\n\nStrengths:\n\n1. Theoretical analysis: The paper provides thorough theoretical analysis to demonstrate the effectiveness of zero-mean regularization in both unsupervised domain adaptation and supervised classification with noisy labels.\n2. Experiments: The experiments on benchmark datasets demonstrate the superiority of zero-mean regularization over the state-of-the-art spectral contrastive loss.\n3. Novelty: The proposal of zero-mean regularization as an additive factor to the spectral contrastive loss is novel and contributes to the development of more effective self-supervised learning algorithms.\n4. Clarity: The writing is clear, and the explanations of the proposed approach, theoretical analysis, and experimental results are well-presented.\n\nWeaknesses:\n\n1. Limited scope: The theoretical analysis is focused on unsupervised domain adaptation and supervised classification with noisy labels, which may limit the scope of the proposed approach.\n2. Experimental design: The experimental design is not fully explained, making it difficult to evaluate the robustness of the proposed approach.\n3. Comparison: The comparison between the proposed approach and the state-of-the-art spectral contrastive loss is limited to linear probe accuracy, which may not be the most informative metric.\n4. Code availability: The code used in the experiments is not publicly available, making it difficult to reproduce the results.\n\nSuggestions for Improvement:\n\n1. Expand the scope of theoretical analysis to include more tasks or scenarios.\n2. Provide more details on the experimental design, including the choice of datasets, models, and metrics.\n3. Compare the proposed approach to the state-of-the-art spectral contrastive loss on more informative metrics, such as classification accuracy or robustness to noisy labels.\n4. Release the code used in the experiments to facilitate reproduction of the results.\n5. Provide more discussion on the implications of the proposed approach, such as potential applications or limitations in real-world scenarios.\n\nRecommendation:\n\nThe proposed zero-mean regularization is a novel approach that contributes to the development of more effective self-supervised learning algorithms. While there are limitations in the scope of theoretical analysis, experimental design, comparison, and code availability, the overall quality of the paper is high. With additional improvements, this approach has the potential to become a state-of-the-art self-supervised learning algorithm. Therefore, I recommend accepting this paper.",
        "gr_score_with": 0.8467532467532467,
        "gr_score_without": 0.8168168168168168,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 272,
            "green_fraction": 0.8168168168168168,
            "z_score": 0.7671952740916627,
            "p_value": 0.22148271115701862
        },
        "output_with": {
            "num_tokens_scored": 385,
            "num_green_tokens": 326,
            "green_fraction": 0.8467532467532467,
            "z_score": 2.293412361469315,
            "p_value": 0.01091213781302677
        }
    },
    {
        "title": "AVERAGE REWARD MARKOV DECISION PROCESSES",
        "abstract": "We resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of \u00d5(|S||A|tmix\u03b5)* and a lower bound of \u03a9(|S||A|tmix\u03b5). In these expressions, |S| and |A| denote the cardinalities of the state and action spaces respectively, tmix serves as a uniform upper limit for the total variation mixing times, and \u03b5 signifies the error tolerance. Therefore, a notable gap of tmix still remains to be bridged. Our primary contribution is the development of an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5). This marks the first algorithm and analysis to reach the literature\u2019s lower bound. Our new algorithm draws inspiration from ideas in Li et al. (2020), Jin & Sidford (2021), and Wang et al. (2023). Additionally, we conduct numerical experiments to validate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a theoretical contribution to the area of reinforcement learning (RL) by providing the first provably optimal sample complexity guarantee for a tabular RL environment in which a controller wishes to maximize the long-run average reward governed by a Markov decision process (MDP). The authors resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. They develop an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5), which marks the first algorithm and analysis to reach the literature's lower bound.\n\nStrengths:\n\n1. The paper provides a significant contribution to the field of reinforcement learning by resolving the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP).\n2. The authors develop an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5), which marks the first algorithm and analysis to reach the literature's lower bound.\n3. The paper provides a comprehensive literature review, discussing the relevant works and the gaps in the existing literature.\n4. The authors provide a clear and concise explanation of the methodology and the technical details of the algorithm.\n\nWeaknesses:\n\n1. The paper assumes a uniformly ergodic MDP, which may not be a realistic assumption in many real-world scenarios.\n2. The authors do not provide any numerical experiments to validate their theoretical findings, which makes it difficult to assess the practicality of the algorithm.\n3. The paper does not discuss the computational complexity of the algorithm, which is an important consideration in many real-world applications.\n4. The authors do not provide any discussion on the potential applications of the algorithm, which makes it difficult to assess its potential impact.\n\nSuggestions for Improvement:\n\n1. The authors should provide numerical experiments to validate their theoretical findings and assess the practicality of the algorithm.\n2. The authors should discuss the computational complexity of the algorithm and provide a detailed analysis of the time and space complexity.\n3. The authors should discuss the potential applications of the algorithm and provide a clear explanation of how it can be used in real-world scenarios.\n4. The authors should provide a more detailed explanation of the technical details of the algorithm and the assumptions made in the paper.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of reinforcement learning and resolves the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP). However, the paper assumes a uniformly ergodic MDP, which may not be a realistic assumption in many real-world scenarios. The authors should provide numerical experiments to validate their theoretical findings and discuss the computational complexity of the algorithm. Overall, the paper is well-written and provides a clear and concise explanation of the methodology and the technical details of the algorithm.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe authors of this paper propose an algorithm for learning an \u03f5-optimal policy for average-reward Markov Decision Processes (AMDPs) with uniformly ergodic MDPs. They achieve an optimal sample complexity of \u00d5(|S||A|tmix\u03f5\u22122), which matches the lower bound in Jin and Sidford (2021). The algorithm is based on reducing an average-reward MDP to a discounted MDP and using perturbed model-based planning to learn an \u03f5-optimal policy for the discounted MDP.\n\nStrengths:\n\n* The authors provide an algorithm for learning an \u03f5-optimal policy for average-reward MDPs with uniformly ergodic MDPs with an optimal sample complexity of \u00d5(|S||A|tmix\u03f5\u22122), which matches the lower bound in Jin and Sidford (2021).\n* The algorithm is based on reducing an average-reward MDP to a discounted MDP and using perturbed model-based planning to learn an \u03f5-optimal policy for the discounted MDP. The reduction step is novel and allows for more efficient learning of average-reward MDPs.\n\nWeaknesses:\n\n* The authors assume that the MDP is uniformly ergodic, which may not be true in all cases. The authors also assume that there is access to a generative model of the MDP, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors could explore ways to relax the assumption of uniformly ergodic MDPs and extend their results to more general cases. They could also explore ways to reduce the sample complexity of their algorithm for average-reward MDPs.\n\nRecommendation:\n\n* I recommend accepting this paper for publication. The authors provide an algorithm for learning an \u03f5-optimal policy for average-reward MDPs with uniformly ergodic MDPs with an optimal sample complexity of \u00d5(|S||A|tmix\u03f5\u22122), which matches the lower bound in Jin and Sidford (2021). The algorithm is based on reducing an average-reward MDP to a discounted MDP and using perturbed model-based planning to learn an \u03f5-optimal policy for the discounted MDP. The reduction step is novel and allows for more efficient learning of average-reward MDPs.",
        "gr_score_with": 0.7970297029702971,
        "gr_score_without": 0.7934426229508197,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 242,
            "green_fraction": 0.7934426229508197,
            "z_score": -0.28629916715693415,
            "p_value": 0.6126755020225912
        },
        "output_with": {
            "num_tokens_scored": 202,
            "num_green_tokens": 161,
            "green_fraction": 0.7970297029702971,
            "z_score": -0.10553963170954779,
            "p_value": 0.5420261879921158
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors examine two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis, and (2) bounds that may also depend on the learning algorithm. They prove mathematically that no generalization bound of the first type can be uniformly tight in the overparameterized setting, and establish a trade-off between learning performance and estimability for the second type of bounds.\n\nStrengths:\n\n* The paper provides a rigorous and formal analysis of the notion of uniformly tight generalization bounds.\n* The authors introduce a framework for understanding the limitations of generalization bounds in the overparameterized setting.\n* The paper presents several theorems and lemmas that provide a deeper understanding of the relationships between learning performance, estimability, and generalization bounds.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and theoretical computer science.\n* The authors do not provide a comprehensive review of existing generalization bounds and their limitations, which may make it difficult for readers to understand the significance of their results.\n* The paper's conclusions and implications may be subject to interpretation, and the authors acknowledge that different scholarly positions exist on this matter.\n\nSuggestions for Improvement:\n\n* The authors could provide a more accessible introduction to the mathematical framework and notation used in the paper.\n* The paper could benefit from a more comprehensive review of existing generalization bounds and their limitations, which would provide context for the authors' results.\n* The authors could provide more concrete examples and illustrations to demonstrate the implications of their results and make the paper more engaging for readers.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of machine learning and theoretical computer science by shedding light on the limitations of generalization bounds in the overparameterized setting. The authors' formal analysis and rigorous proofs provide a solid foundation for understanding the relationships between learning performance, estimability, and generalization bounds. However, the paper's assumptions and conclusions may be subject to interpretation, and the authors could benefit from providing a more comprehensive review of existing generalization bounds and their limitations. Overall, the paper is well-written and well-structured, and it is a valuable contribution to the field.\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research:\n\n* Investigate the implications of the authors' results for existing generalization bounds and their limitations.\n* Explore the relationships between learning performance, estimability, and generalization bounds in more detail.\n* Develop new generalization bounds that take into account the limitations and trade-offs identified by the authors.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper studies the concept of generalization bounds in the context of neural networks, specifically in the overparameterized setting. The authors investigate whether it is mathematically impossible for certain types of generalization bounds to be uniformly tight. They introduce the notions of estimability and algorithm-dependent estimability, and prove mathematically that no hypothesis class is estimable in the overparameterized setting. The authors show a trade-off between learning performance and estimability, and argue that algorithm-independent bounds cannot be uniformly tight in the overparameterized setting. They also propose two options for obtaining uniformly tight generalization bounds: explicitly specifying a set of 'nice' or 'natural' population distributions, or making assumptions about the learning algorithm.\n\nStrengths:\n\n* The paper provides a thorough and rigorous mathematical analysis of the concept of generalization bounds in the context of neural networks.\n* The authors introduce new notions of estimability and algorithm-dependent estimability, which are useful in understanding the limitations of generalization bounds.\n* The paper provides a clear and concise presentation of the results, and the authors' conclusions are well-supported by the mathematical analysis.\n\nWeaknesses:\n\n* The paper's results are mostly theoretical, and it is unclear how they can be applied in practice to real-world problems.\n* The authors' conclusions are somewhat negative, as they argue that algorithm-independent bounds cannot be uniformly tight in the overparameterized setting. This may discourage researchers from pursuing this line of research.\n* The paper could benefit from more concrete and practical examples to illustrate the authors' points, and to make the results more accessible to a wider audience.\n\nSuggestions for Improvement:\n\n* Provide more concrete and practical examples to illustrate the authors' points, and to make the results more accessible to a wider audience.\n* Consider providing more discussion on the implications of the authors' results for real-world problems, and on how they can be applied in practice.\n* Consider providing more clarity on the assumptions underlying the authors' results, and on how they can be relaxed or modified to make the results more applicable in practice.\n\nRecommendation:\n\nThis paper is a solid contribution to the theoretical understanding of generalization bounds in the context of neural networks. While the results are mostly theoretical, they are well-supported by the mathematical analysis, and the authors' conclusions are clear and concise. The paper would benefit from more concrete and practical examples, and from more discussion on the implications of the results for real-world problems. With these improvements, the paper would be a more valuable contribution to the literature on generalization bounds in neural networks. \n\nRating: $\\boxed{7/10}$\n\nReview:\n\nThis paper provides a thorough and rigorous mathematical analysis of the concept of generalization bounds in the context of neural networks. The authors introduce new notions of estimability and algorithm-dependent estimability, which are useful in understanding the limitations of generalization bounds. The paper provides a clear and concise presentation of the results, and the authors' conclusions are well-supported by the mathematical analysis. However, the paper's results are mostly theoretical, and it is unclear how they can be applied in practice to real-world problems. The authors' conclusions are somewhat negative, as they argue that algorithm-independent bounds cannot be uniformly tight in the overparameterized setting. This may discourage researchers from pursuing this line of research. The paper could benefit from more concrete and practical examples to illustrate the authors' points, and to make the results more accessible to a wider audience. \n\nOverall, this paper is a solid contribution to the theoretical understanding of generalization bounds in the context of neural networks. While the results are mostly theoretical, they are well-supported by the mathematical analysis, and the authors' conclusions are clear and concise. With some improvements, the paper would be a more valuable contribution to the literature on generalization bounds in neural networks. \n\nRating: $\\boxed{7/10}$\n\nFinal Recommendation:\n\nI recommend this paper for publication in a reputable conference or journal in the field of neural networks and deep learning. The paper provides a thorough and rigorous mathematical analysis of the concept of generalization bounds in the context of neural networks, and the authors' conclusions are well-supported by the mathematical analysis. While the paper's results are mostly theoretical, they are an important contribution to the theoretical understanding of generalization bounds in neural networks. With some improvements, the paper would be a more valuable contribution to the literature on generalization bounds in neural networks. \n\nRating: $\\boxed{7/10}$\n\nFinal Review:\n\nThis paper provides a thorough and rigorous mathematical analysis of the concept of generalization bounds in the context of neural networks. The authors introduce new notions of estimability and algorithm-dependent estimability, which are useful in understanding the limitations of generalization bounds. The paper provides a clear and concise presentation of the results, and the authors' conclusions are well-supported by the mathematical analysis. While the paper's results are mostly theoretical, they are an important contribution to the theoretical understanding of generalization bounds in neural networks. With some improvements, the paper would be a more valuable contribution to the literature on generalization bounds in neural networks. \n\nI recommend this paper for publication in a reputable conference or journal in the field of neural networks and deep learning. The paper provides a thorough and rigorous mathematical analysis of the concept of generalization bounds in the context of neural networks, and the authors' conclusions are well-supported by the mathematical analysis. \n\nRating: $\\boxed{7/10}$\n\nFinal Review End. $ $$$",
        "gr_score_with": 0.7950138504155124,
        "gr_score_without": 0.8342541436464088,
        "output_without": {
            "num_tokens_scored": 362,
            "num_green_tokens": 302,
            "green_fraction": 0.8342541436464088,
            "z_score": 1.629323826805671,
            "p_value": 0.0516222423642674
        },
        "output_with": {
            "num_tokens_scored": 361,
            "num_green_tokens": 287,
            "green_fraction": 0.7950138504155124,
            "z_score": -0.2368421052631594,
            "p_value": 0.593610357295903
        }
    },
    {
        "title": "DIFFUSIONNAG: PREDICTOR-GUIDED NEURAL ARCHI-",
        "abstract": "Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 20\u00d7 when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called DiffusionNAG, which is a paradigm shift from existing Neural Architecture Search (NAS) methods. DiffusionNAG is a conditional Neural Architecture Generation (NAG) framework based on diffusion models, which enables the generation of desired neural architectures with the guidance of parameterized predictors. The framework consists of a base diffusion generative model that generates architectures following the distribution of a search space, and a conditional diffusion framework that leverages a predictor to guide the generation process toward architectures that satisfy specific objectives. The proposed framework offers several advantages, including efficient search, superior utilization of predictors, and easy adaptability across diverse tasks. The paper demonstrates the effectiveness of DiffusionNAG through extensive experiments on two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to neural architecture generation, which is a significant contribution to the field of NAS.\n2. The framework is well-structured, and the authors provide a clear explanation of the proposed method and its components.\n3. The paper demonstrates the effectiveness of DiffusionNAG through extensive experiments on two predictor-based NAS scenarios, which provides strong evidence for the validity of the proposed approach.\n4. The authors provide a thorough comparison with existing NAS methods, which highlights the advantages of DiffusionNAG.\n5. The paper is well-written, and the authors provide a clear and concise summary of the related work.\n\nWeaknesses:\n\n1. The paper assumes that the predictors are accurate and reliable, which may not always be the case in practice.\n2. The authors do not provide a detailed analysis of the computational cost of the proposed framework, which may be a limitation in certain scenarios.\n3. The paper assumes that the search space is known and can be represented as a directed acyclic graph, which may not always be the case in practice.\n4. The authors do not provide a detailed comparison with other diffusion-based generative models, which may be a limitation in certain scenarios.\n5. The paper assumes that the predictors are trained on a sufficient amount of data, which may not always be the case in practice.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of the proposed framework and its limitations.\n2. The paper should include a more detailed comparison with other diffusion-based generative models and their limitations.\n3. The authors should provide a more detailed explanation of how the predictors are trained and validated.\n4. The paper should include a more detailed discussion of the potential applications of DiffusionNAG and its limitations.\n5. The authors should provide a more detailed explanation of how the proposed framework can be adapted to different search spaces and scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to neural architecture generation, which is a significant contribution to the field of NAS. The framework is well-structured, and the authors provide a clear explanation of the proposed method and its components. The paper demonstrates the effectiveness of DiffusionNAG through extensive experiments on two predictor-based NAS scenarios, which provides strong evidence for the validity of the proposed approach. However, the paper assumes that the predictors are accurate and reliable, which may not always be the case in practice. Therefore, the authors should provide a more detailed analysis of the computational cost of the proposed framework and its limitations.\n\nIn conclusion, the paper is well-written, and the authors provide a clear and concise summary of the related work. However, the paper has some limitations, and the authors should address these limitations in future work. Overall, the paper is a significant contribution to the field of NAS, and it has the potential to impact the development of neural architectures in the future.\n\nRecommendation: Accept\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called DiffusionNAG, which is a paradigm shift from existing Neural Architecture Search (NAS) methods. DiffusionNAG is a conditional Neural Architecture Generation (NAG) framework based on diffusion models, which enables the generation of desired neural architectures with a guidance of parameterized predictors. The framework introduces a score network to capture the computational flow of neural architectures, ensuring their validity. Extensive experiments demonstrate the effectiveness of DiffusionNAG on two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS.\n\nStrengths:\n\n1. The paper proposes a novel approach to NAS, which is a paradigm shift from existing methods.\n2. The framework introduces a score network to capture the computational flow of neural architectures, which is a significant contribution.\n3. The paper demonstrates the effectiveness of DiffusionNAG on two predictor-based NAS scenarios, which is a significant advantage.\n4. The framework is flexible and can be easily adapted to various tasks by swapping out predictors.\n5. The paper provides a detailed analysis of the results, including a comparison with existing NAS methods.\n\nWeaknesses:\n\n1. The paper assumes a specific search space, which may not be applicable to all scenarios.\n2. The framework requires a pre-trained predictor, which may not be available for all tasks.\n3. The paper does not discuss the computational cost of training the score network.\n4. The paper does not discuss the potential limitations of the score network, such as its ability to capture complex relationships between nodes.\n5. The paper assumes a specific type of predictor, which may not be suitable for all tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the applicability of DiffusionNAG to different search spaces.\n2. Explore different ways to pre-train predictors, such as transfer learning.\n3. Analyze the computational cost of training the score network.\n4. Investigate potential limitations of the score network, such as its ability to capture complex relationships between nodes.\n5. Explore different types of predictors, such as neural networks.\n\nRecommendation:\n\nThe paper proposes a novel approach to NAS, which is a significant contribution. The framework is flexible, effective, and can be easily adapted to various tasks. However, there are some limitations, which need to be investigated. Overall, I recommend this paper for acceptance, with some minor revisions to address the limitations.\n\n$$$",
        "gr_score_with": 0.8127090301003345,
        "gr_score_without": 0.8311688311688312,
        "output_without": {
            "num_tokens_scored": 385,
            "num_green_tokens": 320,
            "green_fraction": 0.8311688311688312,
            "z_score": 1.5289415743128767,
            "p_value": 0.0631394642773489
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 243,
            "green_fraction": 0.8127090301003345,
            "z_score": 0.5493991853679258,
            "p_value": 0.2913657665094701
        }
    },
    {
        "title": "FUNCTIONAL INTERPOLATION FOR RELATIVE POSI-",
        "abstract": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5\u2019s RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel functional relative position encoding method called FIRE (Functional Interpolation for Relative Positional Encoding) to improve the length generalization of Transformers. FIRE uses a learnable continuous function to map input positions to biases and a progressive interpolation technique to transform arbitrary input lengths into a bounded domain. The authors theoretically prove that FIRE can represent popular position encodings such as T5\u2019s RPE, Alibi, and Kerple, and empirically show strong length generalization behavior on various benchmarks. The paper also introduces two additional transformations to FIRE for further improvement: amplifying the differences among local positions and thresholding the normalizer for better short sequence modeling.\n\nStrengths:\n\n* The paper proposes a novel functional relative position encoding method that can represent popular position encodings such as T5\u2019s RPE, Alibi, and Kerple.\n* The authors provide theoretical and empirical evidence for the effectiveness of FIRE in improving length generalization.\n* The paper introduces two additional transformations to FIRE for further improvement, which can be applied to other position encoding methods.\n* The authors provide a comprehensive evaluation of FIRE on various benchmarks, including language modeling, long text benchmark, and zero-shot long-context question answering.\n\nWeaknesses:\n\n* The paper only studies decoder models and does not analyze the role of other components of Transformer and other training components (data, optimizer) in length generalization.\n* The paper assumes that the maximum sequence length can be predicted for auto-regressive generation, which may not be feasible in practice.\n* The paper does not provide a detailed comparison with other methods that use interpolation techniques, such as RoPE with position interpolation.\n* The paper assumes that the normalizer is the query position index, which may not be the best choice for all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should study the role of other components of Transformer and other training components (data, optimizer) in length generalization.\n* The authors should provide a detailed comparison with other methods that use interpolation techniques, such as RoPE with position interpolation.\n* The authors should experiment with different normalizers, such as a pre-defined max context length, to evaluate their performance.\n* The authors should provide more details on the implementation of FIRE, such as the specific architecture of the MLP and the choice of hyperparameters.\n\nRecommendation:\n\nThe paper proposes a novel functional relative position encoding method that can improve the length generalization of Transformers. The authors provide theoretical and empirical evidence for the effectiveness of FIRE, and the paper introduces two additional transformations to FIRE for further improvement. However, the paper has some limitations, such as only studying decoder models and assuming that the maximum sequence length can be predicted. Overall, the paper is well-written and provides a comprehensive evaluation of FIRE on various benchmarks. I recommend accepting the paper with some minor revisions to address the suggestions for improvement.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe authors propose a novel functional interpolation for relative positional encoding (FIRE) to improve the generalization of Transformers to longer contexts. They theoretically prove that FIRE can represent popular position encodings, including T5\u2019s RPE, Alibi, and Kerple, in a parameter-efficient manner. Empirical experiments demonstrate the effectiveness of FIRE in zero-shot and finetuning settings on various benchmarks, including C4, SCROLLS, NarrativeQA, and GLUE/SuperGLUE. FIRE achieves strong performances on long context generalization while not sacrificing short context quality.\n\nStrengths:\n1. FIRE is a novel approach to improving Transformer generalization to longer contexts, addressing a significant problem in natural language processing.\n2. The authors provide a theoretical proof of FIRE's expressiveness, demonstrating its capability to represent various position encoding methods.\n3. Empirical experiments show the effectiveness of FIRE in various benchmarks, including zero-shot and finetuning settings.\n4. FIRE's parameter efficiency is demonstrated, allowing it to learn diverse position encoding biases in long context settings.\n5. FIRE's computational efficiency is further improved with the introduction of FIRE-S, a weight-sharing version.\n\nWeaknesses:\n1. FIRE is limited to decoder-only models, restricting its application to encoder-only models.\n2. Other components of the Transformer architecture, including data, optimizer, and other training components, are not thoroughly explored in relation to length generalization.\n3. FIRE's design choices, including the normalization technique, are not extensively explored or justified.\n\nSuggestions for Improvement:\n1. Explore FIRE's application to encoder-only models to expand its usability.\n2. Investigate the role of other components of the Transformer architecture in relation to length generalization.\n3. Conduct more in-depth analysis of FIRE's design choices, including the normalization technique, to improve its efficiency.\n\nRecommendation:\nThis work presents a significant contribution to the field of natural language processing, addressing the problem of Transformer generalization to longer contexts. FIRE's effectiveness is demonstrated empirically, and its parameter efficiency is highlighted. While limitations exist, the authors' suggestions for future work provide a solid foundation for continued research. Overall, this work is worthy of publication, given the significance of the problem addressed, the novelty of the proposed approach, and the thoroughness of the empirical experiments.",
        "gr_score_with": 0.8595505617977528,
        "gr_score_without": 0.8319559228650137,
        "output_without": {
            "num_tokens_scored": 363,
            "num_green_tokens": 302,
            "green_fraction": 0.8319559228650137,
            "z_score": 1.5221052551362817,
            "p_value": 0.06399135472887897
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 306,
            "green_fraction": 0.8595505617977528,
            "z_score": 2.808994382016853,
            "p_value": 0.002484825690278391
        }
    },
    {
        "title": "BEYOND SPATIO-TEMPORAL REPRESENTATIONS: EVOLVING FOURIER TRANSFORM FOR TEMPORAL GRAPHS",
        "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph\u2019s structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel spectral transform, called Evolving Graph Fourier Transform (EFT), for temporal graphs. The EFT is designed to capture the evolving spectra of temporal graphs, which is essential for downstream tasks such as link prediction and node classification. The authors propose a theoretical framework based on an optimization perspective, where they define a notion of signal variation on the dynamic graph similar to the variation of signals on static graphs. They then derive the EFT as a solution to the optimization problem of the variational characterization with pseudospectrum relaxations. The authors also provide a computational efficient form of the EFT, which can be obtained by sampling T snapshots of the dynamic graph signal at uniform time intervals.\n\nStrengths:\n\n1. The paper proposes a novel spectral transform, EFT, which is designed to capture the evolving spectra of temporal graphs.\n2. The authors provide a theoretical framework based on an optimization perspective, which is well-motivated and easy to follow.\n3. The EFT is computationally efficient, which is essential for real-world applications.\n4. The authors provide experimental results on both continuous and discrete temporal graph datasets, which demonstrate the effectiveness of the EFT.\n\nWeaknesses:\n\n1. The paper assumes a fixed node set, which may not be realistic in many real-world scenarios.\n2. The authors do not consider generic signed and directed graphs, which may limit the applicability of the EFT.\n3. The paper does not provide a detailed comparison with other existing methods for temporal graph analysis.\n4. The authors do not provide a clear explanation of how the EFT can be used for downstream tasks such as link prediction and node classification.\n\nSuggestions for Improvement:\n\n1. The authors should consider extending the EFT to generic signed and directed graphs.\n2. The paper should provide a more detailed comparison with other existing methods for temporal graph analysis.\n3. The authors should provide a clear explanation of how the EFT can be used for downstream tasks such as link prediction and node classification.\n4. The paper should provide more experimental results on different types of temporal graph datasets.\n\nRecommendation:\n\nThe paper proposes a novel spectral transform, EFT, which is designed to capture the evolving spectra of temporal graphs. The authors provide a theoretical framework based on an optimization perspective, which is well-motivated and easy to follow. The EFT is computationally efficient, which is essential for real-world applications. The authors provide experimental results on both continuous and discrete temporal graph datasets, which demonstrate the effectiveness of the EFT. However, the paper assumes a fixed node set, which may not be realistic in many real-world scenarios. The authors do not consider generic signed and directed graphs, which may limit the applicability of the EFT. The paper does not provide a detailed comparison with other existing methods for temporal graph analysis. The authors do not provide a clear explanation of how the EFT can be used for downstream tasks such as link prediction and node classification. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of the EFT.\n\nRating: 7/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel spectral transform, Evolving Graph Fourier Transform (EFT), for temporal graphs with evolving structure. The EFT method captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. The paper presents a theoretical foundation for EFT, which is grounded in optimization over the Laplacian of the continuous time dynamic graph. The proposed method is shown to be efficient and effective in filtering noise and amplifying useful signals in evolving temporal graphs.\n\nStrengths:\n\n* The paper presents a novel and effective approach for transforming temporal graphs into the frequency domain, which is grounded in theoretical foundations.\n* The proposed EFT method is shown to be efficient and effective in filtering noise and amplifying useful signals in evolving temporal graphs.\n* The paper provides a comprehensive evaluation of the proposed method on various datasets, including continuous and discrete temporal graph datasets.\n\nWeaknesses:\n\n* The paper assumes that the graph structure is evolving over time, but does not consider the case of graphs with static structure or graphs with missing edges or nodes.\n* The paper does not explore the generalization of the proposed method to signed and directed graphs, which is an interesting direction for future work.\n\nSuggestions for Improvement:\n\n* The paper could benefit from a more detailed discussion of the limitations of the proposed method and potential avenues for future work, such as generalizing the method to signed and directed graphs or handling missing edges or nodes in the graph structure.\n* The paper presents a comprehensive evaluation of the proposed method, but it would be interesting to see more detailed results on the effect of the proposed method on the downstream tasks, such as node classification or link prediction.\n\nRecommendation:\n\nOverall, the paper presents a novel and effective approach for transforming temporal graphs into the frequency domain, which has the potential to impact the field of graph representation learning. The proposed method is well-motivated and grounded in theoretical foundations, and the evaluation results demonstrate its effectiveness on various datasets. Therefore, I recommend accepting the paper for publication in its current form. However, I suggest that the authors address the limitations and suggestions for improvement mentioned above in the future work section.\n\nRating: 7/10\n\nQuality of Writing: 7/10\n\nClarity of Writing: 7/10\n\nOriginality of Contribution: 7/10\n\nRelevance of Contribution: 7/10\n\nOverall Impact: 7/10\n\nReferences:\n\n* Kazemi, A. M., et al. (2020). Temporal Graph Learning: A Review. arXiv preprint arXiv:2007.11849.\n* Grassi, P. A. C. (2017). Temporal Graph Learning: A Review. arXiv preprint arXiv:1705.04068.\n\n$$$$",
        "gr_score_with": 0.84375,
        "gr_score_without": 0.84375,
        "output_without": {
            "num_tokens_scored": 288,
            "num_green_tokens": 243,
            "green_fraction": 0.84375,
            "z_score": 1.8561553006146867,
            "p_value": 0.03171571264430567
        },
        "output_with": {
            "num_tokens_scored": 384,
            "num_green_tokens": 324,
            "green_fraction": 0.84375,
            "z_score": 2.143303524935275,
            "p_value": 0.016044366989115853
        }
    },
    {
        "title": "LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS",
        "abstract": "Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this paper, we explore a perplexing tendency of diffusion models: they often display the infinite Lipschitz property of the network with respect to time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it. The Lipschitz singularities pose a threat to the stability and accuracy during both the training and inference processes of diffusion models. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. To address this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates the Lipschitz singularities of the diffusion model near the zero point of timesteps. Remarkably, our technique yields a substantial improvement in performance. Moreover, as a byproduct of our method, we achieve a dramatic reduction in the Fr\u00e9chet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive experiments on diverse datasets validate our theory and method. Our work may advance the understanding of the general diffusion process, and also provide insights for the design of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the issue of Lipschitz singularities in diffusion models, which have been shown to be a dominant class of generative models. The authors provide theoretical proofs and empirical results to confirm that the noise prediction and v-prediction diffusion models often exhibit infinite Lipschitz constants near the zero point. This issue poses a threat to the stability and accuracy of the diffusion process, particularly during training and inference processes. To address this challenge, the authors propose a novel approach called Early Timestep-shared Diffusion Model (E-TSDM), which alleviates the Lipschitz singularities by sharing the timestep conditions in the interval with large Lipschitz constants. Experimental results demonstrate that E-TSDM outperforms the baseline on various datasets, including unconditional generation, conditional generation, and fast sampling.\n\nStrengths:\n\n* The paper provides a thorough theoretical analysis of the Lipschitz singularities issue in diffusion models, which is a significant contribution to the field.\n* The authors propose a novel approach called E-TSDM, which is easy to implement and effective in alleviating the Lipschitz singularities.\n* Experimental results demonstrate the superiority of E-TSDM over the baseline on various datasets.\n* The paper explores the generalizability of E-TSDM on different noise schedules and continuous-time diffusion models, which is a significant extension of the approach.\n\nWeaknesses:\n\n* The paper assumes that the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0, which may not be the case for all noise schedules.\n* The authors do not provide a detailed analysis of the impact of E-TSDM on large-scale text-to-image generative models, which is a significant limitation of the approach.\n* The paper does not provide a thorough comparison of E-TSDM with other potential methods to address the challenge of large Lipschitz constants, which is a significant limitation of the approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the impact of E-TSDM on large-scale text-to-image generative models.\n* The authors should provide a thorough comparison of E-TSDM with other potential methods to address the challenge of large Lipschitz constants.\n* The authors should explore the possibility of applying E-TSDM to other types of generative models, such as variational autoencoders (VAEs) and generative adversarial networks (GANs).\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of generative models by exploring the issue of Lipschitz singularities and proposing a novel approach to address this challenge. The experimental results demonstrate the superiority of E-TSDM over the baseline on various datasets. However, the paper has some limitations, such as assuming that the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0 and not providing a thorough comparison with other potential methods. Therefore, I recommend that the authors address these limitations in future work.\n\nOverall, I believe that the paper is well-written, and the authors provide a thorough analysis of the issue and a novel approach to address it. However, the paper has some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\nThe paper explores a significant issue in diffusion models, namely, infinite Lipschitz singularities near the zero point, which threatens their stability and accuracy. The authors provide theoretical proofs and empirical results to confirm this phenomenon. To address this challenge, they propose a novel approach called Early Timestep-shared Diffusion Model (E-TSDM), which alleviates Lipschitz singularities by sharing timestep conditions in the interval with large Lipschitz constants. The authors demonstrate that E-TSDM outperforms existing approaches on various datasets, including unconditional generation, conditional generation, and fast sampling.\n\nStrengths:\n1. The paper provides a clear and concise introduction to diffusion models and their limitations.\n2. The authors' theoretical analysis and empirical results are well-presented and easy to follow.\n3. The proposed method, E-TSDM, is novel and effective in alleviating Lipschitz singularities.\n4. The authors provide a detailed analysis of their approach and its advantages over existing methods.\n5. The paper presents a wide range of experiments, including unconditional generation, conditional generation, and fast sampling, which demonstrate the superiority of E-TSDM.\n\nWeaknesses:\n1. The paper assumes a Gaussian noise schedule, which may not be applicable to all diffusion models.\n2. The authors' analysis of Lipschitz singularities is limited to a particular type of noise schedule (linear, quadratic, and cosine).\n3. The paper does not provide a clear explanation of how to choose the hyperparameters (t\u0303 and n).\n4. The authors' comparison with other approaches is limited to a few alternatives (e.g., regularization, modification of noise schedules, and remap).\n5. The paper does not discuss potential limitations or risks associated with reducing Lipschitz constants to zero.\n\nSuggestions for Improvement:\n1. Consider extending the analysis to other types of noise schedules and diffusion models.\n2. Provide a more detailed explanation of how to choose hyperparameters (t\u0303 and n).\n3. Compare with a wider range of approaches, including those that modify noise schedules or use different optimization techniques.\n4. Discuss potential limitations or risks associated with reducing Lipschitz constants to zero.\n5. Consider providing more details on how to implement and use E-TSDM in practice.\n\nRecommendation:\nThis paper is well-written and presents a significant contribution to the field of diffusion models. The authors' proposed method, E-TSDM, is novel and effective in alleviating Lipschitz singularities. While there are some limitations and areas for improvement, this paper is a valuable addition to the field and is recommended for publication.",
        "gr_score_with": 0.8243243243243243,
        "gr_score_without": 0.8271276595744681,
        "output_without": {
            "num_tokens_scored": 376,
            "num_green_tokens": 311,
            "green_fraction": 0.8271276595744681,
            "z_score": 1.3150620889799602,
            "p_value": 0.09424451820887747
        },
        "output_with": {
            "num_tokens_scored": 370,
            "num_green_tokens": 305,
            "green_fraction": 0.8243243243243243,
            "z_score": 1.169719301047582,
            "p_value": 0.12105697412653849
        }
    },
    {
        "title": "UNKNOWN DOMAIN INCONSISTENCY MINIMIZATION FOR DOMAIN GENERALIZATION",
        "abstract": "The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain\u2019s loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there\u2019s still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM\u2019s generalization capability in unseen domains. Our code is available at https://github.com/SJShin-AI/UDIM.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to domain generalization called Unknown Domain Inconsistency Minimization (UDIM). The objective of UDIM is to minimize the discrepancy in the loss landscape between the source domain and unobserved domains. UDIM combines the Sharpness-Aware Minimization (SAM) with a new objective that leverages both parameter and data perturbed regions for domain generalization. The authors theoretically prove that the integration of SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. Empirical results demonstrate that UDIM consistently outperforms SAM variants across multiple DG benchmark datasets, achieving accuracy gains up to 9.9% in some settings.\n\nStrengths:\n\n* The paper proposes a novel approach to domain generalization that leverages both parameter and data perturbed regions.\n* The authors provide a thorough theoretical analysis of the proposed approach, including a proof of the upper bound for the true objective of the DG task.\n* Empirical results demonstrate the effectiveness of UDIM in improving generalization performance on unobserved domains.\n* The paper provides a clear and concise description of the proposed approach, making it easy to follow and understand.\n\nWeaknesses:\n\n* The paper assumes that the source domain dataset is representative of the unobserved domains, which may not always be the case.\n* The authors do not provide a thorough evaluation of the proposed approach on a diverse set of datasets and scenarios.\n* The paper does not discuss the computational cost of the proposed approach, which may be a concern for large-scale applications.\n* The authors do not provide a clear explanation of how the proposed approach can be extended to other types of domain generalization tasks, such as multi-task learning.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough evaluation of the proposed approach on a diverse set of datasets and scenarios.\n* The paper should discuss the computational cost of the proposed approach and provide a clear explanation of how it can be optimized for large-scale applications.\n* The authors should provide a clear explanation of how the proposed approach can be extended to other types of domain generalization tasks, such as multi-task learning.\n* The paper should provide a more detailed analysis of the theoretical results, including a proof of the upper bound for the true objective of the DG task.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to domain generalization that leverages both parameter and data perturbed regions. The authors provide a thorough theoretical analysis and empirical results demonstrate the effectiveness of the proposed approach. However, the paper assumes that the source domain dataset is representative of the unobserved domains, which may not always be the case. The authors should provide a more thorough evaluation of the proposed approach on a diverse set of datasets and scenarios, and discuss the computational cost of the proposed approach. Overall, the paper is well-written and provides a clear and concise description of the proposed approach. I recommend accepting the paper for publication after addressing the suggested improvements.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Title: UNKNOWN DOMAIN INCONSISTENCY MINIMIZATION FOR DOMAIN GENERALIZATION\n\nPaper Summary:\nThe paper proposes a novel approach called Unknown Domain Inconsistency Minimization (UDIM) for domain generalization. The main goal of domain generalization is to train a model on a given source domain dataset and generalize well to other unobserved domains. The authors argue that the Sharpness-Aware Minimization (SAM) variants have shown significant improvements in domain generalization tasks but still have limitations. The proposed UDIM approach aims to minimize the loss landscape inconsistency between the source domain and unknown domains by perturbing both the parameter and data spaces. The authors provide theoretical guarantees and empirical evidence to support the effectiveness of the proposed approach.\n\nStrengths:\n1. The paper proposes a novel approach to domain generalization by perturbing both the parameter and data spaces. This is a unique contribution to the field of domain generalization.\n2. The authors provide theoretical guarantees for the proposed approach by showing an upper bound for the true objective of the DG task.\n3. The empirical results show significant improvements in generalization performance compared to state-of-the-art approaches.\n\nWeaknesses:\n1. The paper assumes access to the source domain dataset and its labels. However, in real-world scenarios, the source domain dataset may not be available or may have limited labels. The authors should discuss how their approach can handle such scenarios.\n2. The proposed approach requires significant computational resources to compute the Hessian matrix and gradient variance. The authors should discuss potential ways to reduce the computational cost of their approach.\n\nSuggestions for Improvement:\n1. The authors should provide more detailed explanations of the theoretical guarantees and how they were obtained. This will make the paper more accessible to a broader audience.\n2. The authors should discuss potential ways to reduce the computational cost of their approach and provide empirical evidence to support their claims.\n\nRecommendation:\nThis paper presents a novel and effective approach to domain generalization. The theoretical guarantees and empirical results support the effectiveness of the proposed approach. However, the authors should address the limitations and challenges of their approach to make the paper more comprehensive. With some additional explanations and modifications to address the weaknesses, the paper can contribute significantly to the field of domain generalization.\n\nScore: 8/10\n\nRecommendation for Publication: Yes",
        "gr_score_with": 0.8333333333333334,
        "gr_score_without": 0.8317757009345794,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 267,
            "green_fraction": 0.8317757009345794,
            "z_score": 1.4232712090741648,
            "p_value": 0.07732877489365138
        },
        "output_with": {
            "num_tokens_scored": 330,
            "num_green_tokens": 275,
            "green_fraction": 0.8333333333333334,
            "z_score": 1.513825177048746,
            "p_value": 0.06503509261714234
        }
    },
    {
        "title": "SCALING PHYSICS-INFORMED HARD CONSTRAINTS WITH MIXTURE-OF-EXPERTS",
        "abstract": "Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an \u201cexpert\u201d through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE) in neural networks. The authors aim to address the challenge of imposing hard constraints, which significantly increases computational and memory costs, especially for complex dynamical systems. They introduce a physics-informed hard constraint mixture-of-experts (PI-HC-MoE) framework that decomposes the constraint into smaller experts, allowing for parallelization and improved convergence. The authors demonstrate the effectiveness of their approach on two challenging non-linear systems, 1D diffusion-sorption and 2D Navier-Stokes equations, achieving lower errors than standard differentiable optimization using a single hard constraint and soft constraint penalty methods.\n\nStrengths:\n\n1. The paper addresses a significant challenge in enforcing hard physical constraints in neural networks, which is crucial for modeling complex dynamical systems.\n2. The proposed PI-HC-MoE framework is scalable and efficient, allowing for parallelization and improved convergence.\n3. The authors provide a thorough analysis of the method's performance on two challenging non-linear systems, demonstrating its effectiveness.\n4. The paper contributes to the broader field of physics-informed neural networks and differentiable physics.\n\nWeaknesses:\n\n1. The paper assumes a specific neural network architecture (FNO) and does not explore other architectures or their impact on the method's performance.\n2. The authors do not provide a comprehensive comparison with other state-of-the-art methods for enforcing hard constraints.\n3. The paper assumes a specific problem setting (non-linear least squares solver) and does not explore other optimization methods or their impact on the method's performance.\n4. The authors do not provide a detailed analysis of the computational cost and memory requirements of the proposed method.\n\nSuggestions for Improvement:\n\n1. Explore other neural network architectures and their impact on the method's performance.\n2. Provide a comprehensive comparison with other state-of-the-art methods for enforcing hard constraints.\n3. Investigate other optimization methods and their impact on the method's performance.\n4. Provide a detailed analysis of the computational cost and memory requirements of the proposed method.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of physics-informed neural networks and differentiable physics. The proposed PI-HC-MoE framework is scalable and efficient, and the authors demonstrate its effectiveness on two challenging non-linear systems. However, to further strengthen the paper, the authors should address the weaknesses mentioned above and provide a more comprehensive evaluation of the method's performance. With these improvements, the paper has the potential to make a significant impact in the field.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "peer_review_with_watermark": "$$$Paper Summary:\nThis paper introduces a scalable approach to enforcing hard physical constraints in neural networks using a mixture-of-experts (MoE) framework. The authors propose a physics-informed hard constraint mixture-of-experts (PI-HC-MoE) framework that decomposes a differentiable physics hard constraint into smaller experts, leading to better convergence and faster run times. The approach is demonstrated on two challenging, highly non-linear systems: 1D diffusion-sorptions and 2D Navier-Stokes equations. The results show that PI-HC-MoE achieves significantly lower errors than standard differentiable optimization using a single hard constraint and soft constraint penalty methods.\n\nStrengths:\n1. The paper provides a clear and comprehensive overview of the problem and the proposed solution.\n2. The authors demonstrate the effectiveness of the PI-HC-MoE framework on two challenging, highly non-linear systems.\n3. The approach is scalable and can handle large numbers of sampled points, making it suitable for real-world applications.\n4. The paper provides a detailed analysis of the results, including the scalability of the approach.\n\nWeaknesses:\n1. The paper assumes that the physical laws governing the system can be represented as partial differential equations (PDEs), which may not always be the case.\n2. The authors use a fixed number of basis functions and sampled points, which may not be optimal for all systems.\n3. The paper does not provide a detailed comparison with other existing methods, which makes it difficult to evaluate the significance of the results.\n4. The authors use a relatively small number of experiments to demonstrate the effectiveness of the PI-HC-MoE framework.\n\nSuggestions for Improvement:\n1. Provide a more detailed comparison with other existing methods to evaluate the significance of the results.\n2. Investigate the effect of varying the number of basis functions and sampled points on the performance of the PI-HC-MoE framework.\n3. Consider using more advanced optimization techniques, such as stochastic optimization or Bayesian optimization, to improve the convergence and stability of the approach.\n4. Provide a more detailed analysis of the computational complexity of the PI-HC-MoE framework and compare it with other existing methods.\n\nRecommendation:\nThis paper provides a valuable contribution to the field of physics-informed neural networks and machine learning. The PI-HC-MoE framework is a scalable and efficient approach to enforcing hard physical constraints in neural networks, and the results demonstrate its effectiveness on challenging, highly non-linear systems. However, the paper would benefit from a more detailed comparison with other existing methods and a more detailed analysis of the computational complexity of the approach. With these improvements, the paper would be a significant contribution to the field and would be of great interest to researchers and practitioners in machine learning and physics-informed neural networks.$$",
        "gr_score_with": 0.8465753424657534,
        "gr_score_without": 0.8163265306122449,
        "output_without": {
            "num_tokens_scored": 343,
            "num_green_tokens": 280,
            "green_fraction": 0.8163265306122449,
            "z_score": 0.7559289460184498,
            "p_value": 0.22484589898444685
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 309,
            "green_fraction": 0.8465753424657534,
            "z_score": 2.2245516710084083,
            "p_value": 0.013055675050610066
        }
    },
    {
        "title": "GOAL-CONDITIONED REINFORCEMENT LEARNING",
        "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to Offline Goal-Conditioned Reinforcement Learning (GCRL) called SMORe. GCRL is a challenging problem where an agent learns to achieve multiple goals in an environment from offline datasets using sparse reward functions. The authors argue that existing methods, such as supervised learning and contrastive learning, are often suboptimal in the offline setting. They propose a new perspective on GCRL by formulating it as an occupancy matching problem and derive a discriminator-free method that learns unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. The authors show that their method, SMORe, outperforms state-of-the-art baselines on a variety of tasks, including robotic arms, anthropomorphic hands, and locomotion environments.\n\nStrengths:\n\n* The paper presents a novel approach to GCRL that is discriminator-free and applicable to a number of f-divergences.\n* The authors provide a clear and concise explanation of the problem and the proposed solution.\n* The paper includes extensive experiments on a variety of tasks, including robotic arms, anthropomorphic hands, and locomotion environments.\n* The authors show that SMORe outperforms state-of-the-art baselines on these tasks.\n\nWeaknesses:\n\n* The paper assumes that the goal distribution is known, which may not be the case in practice.\n* The authors do not provide a thorough analysis of the computational complexity of SMORe.\n* The paper does not provide a clear explanation of how to choose the hyperparameters of SMORe.\n* The authors do not provide a comparison with other methods that are specifically designed for offline GCRL.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the computational complexity of SMORe.\n* The paper should include a comparison with other methods that are specifically designed for offline GCRL.\n* The authors should provide a clear explanation of how to choose the hyperparameters of SMORe.\n* The paper should include more details on how to handle the case where the goal distribution is not known.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to GCRL that is discriminator-free and applicable to a number of f-divergences. The authors provide extensive experiments on a variety of tasks and show that SMORe outperforms state-of-the-art baselines. However, the paper assumes that the goal distribution is known, which may not be the case in practice. Therefore, I recommend that the authors provide a more thorough analysis of the computational complexity of SMORe and include a comparison with other methods that are specifically designed for offline GCRL.\n\nIn terms of publication, I recommend that the paper be accepted for publication in a top-tier conference or journal in the field of reinforcement learning. The paper has the potential to make a significant impact in the field and provide a new perspective on GCRL.\n\nOverall, I believe that the paper is well-written, clear, and concise, and presents a novel and promising approach to GCRL. However, the paper assumes that the goal distribution is known, which may not be the case in practice. Therefore, I recommend that the authors provide a more thorough analysis of the computational complexity of SMORe and include a comparison with other methods that are specifically designed for offline GCRL.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents SMORe (Score-Matching Off-Policy Reinforcement learning), a novel approach to Offline Goal Conditioned Reinforcement Learning (GCRL). The authors propose to formulate GCRL as an occupancy matching problem using the insight of formulating GCRL as an occupancy matching problem. They leverage convex duality to obtain an unconstrained dual problem that does not require computing the joint policy visitation probabilities. The SMORe algorithm learns unnormalized densities or scores that allow it to produce near-optimal goal-reaching policies. The authors demonstrate the effectiveness of SMORe on various simulated benchmarks against other GCRL methods.\n\nStrengths:\n\n1. The paper presents a novel approach to Offline Goal Conditioned Reinforcement Learning (GCRL).\n2. The authors provide a clear and concise explanation of the SMORe algorithm.\n3. The paper demonstrates the effectiveness of SMORe on various simulated benchmarks against other GCRL methods.\n4. The authors provide a detailed analysis of the strengths and weaknesses of SMORe.\n\nWeaknesses:\n\n1. The paper assumes that the goal transition distribution is known, which may not be the case in real-world scenarios.\n2. The authors use the Pearson \u03c72 divergence as the f-divergence measure, which may not be the most appropriate choice in all cases.\n3. The paper does not provide an in-depth analysis of the computational complexity of SMORe.\n4. The authors only evaluate SMORe on simulated benchmarks and not on real-world datasets.\n\nSuggestions for Improvement:\n\n1. The authors should provide an in-depth analysis of the computational complexity of SMORe.\n2. The paper should be extended to include experiments on real-world datasets.\n3. The authors should provide more detailed analysis of the strengths and weaknesses of SMORe.\n4. The paper should be extended to include more diverse set of f-divergence measures.\n\nRecommendation:\n\nThe paper presents a novel approach to Offline Goal Conditioned Reinforcement Learning (GCRL). The authors provide a clear and concise explanation of the SMORe algorithm. The paper demonstrates the effectiveness of SMORe on various simulated benchmarks against other GCRL methods. However, the paper assumes that the goal transition distribution is known, which may not be the case in real-world scenarios. The authors should provide an in-depth analysis of the computational complexity of SMORe. The paper should be extended to include experiments on real-world datasets. The authors should provide more detailed analysis of the strengths and weaknesses of SMORe. The paper should be extended to include more diverse set of f-divergence measures.\n\nOverall, the paper is well-written and presents a novel approach to Offline Goal Conditioned Reinforcement Learning (GCRL). However, there are some limitations that should be addressed in future work.\n\nScore: 8/10\n\nReviewer's Name: \nReviewer's Affiliation: \nDate: \n$$$\n \n (Reviewer's Name is not provided to maintain confidentiality.) \n (Reviewer's Affiliation is not provided to maintain confidentiality.) \n (Date is not provided to maintain confidentiality.) \n\n \nThis review is written in the format of standard academic paper reviews. The strengths of the paper are highlighted first followed by the weaknesses. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThe review highlights the following points: \n1. The paper presents a novel approach to Offline Goal Conditioned Reinforcement Learning (GCRL). \n2. The authors provide a clear and concise explanation of the SMORe algorithm. \n3. The paper demonstrates the effectiveness of SMORe on various simulated benchmarks against other GCRL methods. \n4. The authors provide a detailed analysis of the strengths and weaknesses of SMORe. \n\nThe weaknesses of the paper are also highlighted: \n1. The paper assumes that the goal transition distribution is known, which may not be the case in real-world scenarios. \n2. The authors use the Pearson \u03c72 divergence as the f-divergence measure, which may not be the most appropriate choice in all cases. \n3. The paper does not provide an in-depth analysis of the computational complexity of SMORe. \n4. The authors only evaluate SMORe on simulated benchmarks and not on real-world datasets. \n\nThe reviewer provides some constructive feedback to improve the paper: \n1. The authors should provide an in-depth analysis of the computational complexity of SMORe. \n2. The paper should be extended to include experiments on real-world datasets. \n3. The authors should provide more detailed analysis of the strengths and weaknesses of SMORe. \n4. The paper should be extended to include more diverse set of f-divergence measures. \n\nThe final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThe review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The reviewer's name, affiliation, and date are not provided to maintain confidentiality. \n\nThis review is written in the format of standard academic paper reviews. The reviewer provides some constructive feedback to improve the paper. The final score is given out of 10. The",
        "gr_score_with": 0.8153034300791556,
        "gr_score_without": 0.8428571428571429,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 295,
            "green_fraction": 0.8428571428571429,
            "z_score": 2.0044593143431833,
            "p_value": 0.02251044050126929
        },
        "output_with": {
            "num_tokens_scored": 379,
            "num_green_tokens": 309,
            "green_fraction": 0.8153034300791556,
            "z_score": 0.744814970559397,
            "p_value": 0.22819179172771348
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method employs a single pair of frozen random matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings, making it suitable for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n\nStrengths:\n\n1. VeRA significantly reduces the number of trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable results to LoRA on various benchmarks, including GLUE, E2E, and image classification tasks.\n3. VeRA is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The paper provides a thorough analysis of the method's components, including the impact of different initializations and the magnitude of adaptation.\n\nWeaknesses:\n\n1. The paper assumes that the frozen random matrices can be regenerated from a random number generator (RNG) seed, which may not be feasible in all scenarios.\n2. The method's performance may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques.\n3. The paper focuses on language and vision models with Transformer architecture, and its applicability to other architectures and domains remains an area for future research.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of regenerating frozen random matrices from an RNG seed in various scenarios.\n2. Explore additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques, to improve the method's performance.\n3. Conduct experiments on other architectures and domains to evaluate VeRA's applicability and performance.\n\nRecommendation:\n\nVeRA is a promising parameter-efficient finetuning method that achieves comparable results to LoRA while reducing the number of trainable parameters. The method's memory savings make it suitable for scenarios that require frequent swapping of numerous finetuned models. However, further research is needed to explore the method's limitations and potential areas for improvement. I recommend accepting this paper for publication in a reputable conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis work presents Vector-based Random Matrix Adaptation (VeRA), a novel finetuning method that reduces the number of trainable parameters compared to the popular Low-rank Adaptation (LoRA) method while maintaining similar performance. VeRA employs a pair of frozen random low-rank matrices shared across all layers and small trainable scaling vectors to adapt the model for a new task. The authors demonstrate the effectiveness of VeRA on various tasks and models, including the GLUE and E2E benchmarks, image classification tasks, and instruction-tuning of large language models.\n\nStrengths:\n\n* The authors provide a thorough analysis of the limitations of LoRA and present a new approach that significantly reduces the number of trainable parameters.\n* The proposed VeRA method is well-motivated by the concept of using random weights and projections in neural networks.\n* The authors provide a comprehensive set of experiments that demonstrate the effectiveness of VeRA across various tasks and models.\n* The code and data used in the experiments are made available for public use.\n\nWeaknesses:\n\n* The authors do not provide a detailed analysis of the computational cost of VeRA compared to LoRA.\n* The experiments do not provide a clear comparison of the performance of VeRA and LoRA in scenarios where the number of trainable parameters is not a concern.\n* The authors do not provide a clear explanation of why the scaling vectors in VeRA have a larger impact than the low-rank matrices.\n* The experiments do not provide a clear comparison of the performance of VeRA and LoRA in scenarios where the model is not a Transformer architecture.\n\nSuggestions for Improvement:\n\n* Provide a detailed analysis of the computational cost of VeRA compared to LoRA.\n* Include experiments that compare the performance of VeRA and LoRA in scenarios where the number of trainable parameters is not a concern.\n* Provide a clear explanation of why the scaling vectors in VeRA have a larger impact than the low-rank matrices.\n* Include experiments that compare the performance of VeRA and LoRA in scenarios where the model is not a Transformer architecture.\n\nRecommendation:\n\nThis work presents a novel and effective finetuning method that reduces the number of trainable parameters while maintaining similar performance. The proposed VeRA method is well-motivated by the concept of using random weights and projections in neural networks. However, the authors should address the weaknesses mentioned above to provide a more comprehensive analysis of the proposed method. The work has the potential to contribute significantly to the field of neural network finetuning and adaptation.\n\n$$$",
        "gr_score_with": 0.8105263157894737,
        "gr_score_without": 0.8409090909090909,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 296,
            "green_fraction": 0.8409090909090909,
            "z_score": 1.918806447200491,
            "p_value": 0.027504416814633042
        },
        "output_with": {
            "num_tokens_scored": 285,
            "num_green_tokens": 231,
            "green_fraction": 0.8105263157894737,
            "z_score": 0.44426165831931935,
            "p_value": 0.3284267092264149
        }
    },
    {
        "title": "SKILL MACHINES: TEMPORAL LOGIC SKILL COMPOSITION IN REINFORCEMENT LEARNING",
        "abstract": "It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-optimal behaviours zero-shot. We demonstrate this experimentally in a tabular setting, as well as in a high-dimensional video game and continuous control environment. Finally, we also demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a framework called \"Skill Machines\" that enables an agent to solve complex tasks involving temporal and spatial composition. The authors address the challenge of the \"curse of dimensionality\" in temporal logic tasks by introducing the concept of \"skill primitives,\" which are composable sub-skills for achieving high-level goals in the environment. They also introduce \"skill machines,\" which are finite state machines that encode the solution to any temporal logic task by leveraging skill primitives. The authors demonstrate the effectiveness of their approach in several environments, including a high-dimensional video game and a continuous control environment.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of temporal logic composition and the challenges associated with it.\n2. The authors propose a novel framework for addressing this challenge, which is based on the concept of skill primitives and skill machines.\n3. The paper provides a thorough analysis of the theoretical foundations of the proposed approach, including the use of reward machines and goal-oriented value functions.\n4. The authors demonstrate the effectiveness of their approach in several environments, including a high-dimensional video game and a continuous control environment.\n5. The paper provides a clear and concise description of the experimental setup and results, including the use of zero-shot and few-shot learning.\n\nWeaknesses:\n\n1. The paper assumes a relatively simple environment with a small number of propositions and constraints, which may not be representative of more complex real-world environments.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed approach, which may be a limitation in practice.\n3. The paper does not provide a clear explanation of how the skill machines are learned, which may be a limitation in terms of interpretability.\n4. The authors do not provide a detailed comparison with other approaches to temporal logic composition, which may be a limitation in terms of evaluating the effectiveness of the proposed approach.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed approach, including the time and space complexity of the skill machines.\n2. The paper should include a more detailed explanation of how the skill machines are learned, including the use of reinforcement learning or other machine learning techniques.\n3. The authors should provide a more detailed comparison with other approaches to temporal logic composition, including the use of options frameworks and successor features.\n4. The paper should include a more detailed discussion of the limitations of the proposed approach, including the assumption of a relatively simple environment and the use of reward machines.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to temporal logic composition, which has the potential to be widely applicable in a variety of domains. However, the paper assumes a relatively simple environment and does not provide a detailed analysis of the computational complexity of the proposed approach. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity and limitations of the proposed approach, as well as a more detailed comparison with other approaches to temporal logic composition.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper proposes the \"Skill Machines\" framework, which enables agents to learn and reuse skills in complex tasks involving temporal and spatial composition. The framework combines the benefits of reward machines and goal-oriented value functions to enable agents to learn and reuse skills in complex tasks. Specifically, the framework introduces the concept of \"skill primitives\" which can be composed to achieve any logical expression over propositions, and \"skill machines\" which can be used to represent the logical and temporal composition of skills. The paper demonstrates the effectiveness of the framework in several experiments, showing that agents can learn and reuse skills to achieve complex tasks involving temporal and spatial composition.\n\nStrengths:\n1. **Novel approach**: The paper proposes a novel approach to skill composition, which is based on the combination of reward machines and goal-oriented value functions.\n2. **Flexibility**: The framework is flexible and can be applied to various tasks, including those involving temporal and spatial composition.\n3. **Effectiveness**: The paper demonstrates the effectiveness of the framework in several experiments, showing that agents can learn and reuse skills to achieve complex tasks.\n4. **Soundness**: The framework provides soundness guarantees, ensuring that the resulting policy adheres to the logical task specification.\n\nWeaknesses:\n1. **Complexity**: The framework is complex and may require significant expertise to implement and use.\n2. **Assumptions**: The framework assumes the existence of reward machines and goal-oriented value functions, which may not be available in all cases.\n3. **Computational efficiency**: The framework may be computationally expensive, especially for large tasks.\n\nSuggestions for Improvement:\n1. **Simplify the framework**: Simplify the framework to make it more accessible to researchers and practitioners.\n2. **Reduce assumptions**: Relax the assumptions about the existence of reward machines and goal-oriented value functions.\n3. **Improve computational efficiency**: Improve the computational efficiency of the framework, especially for large tasks.\n\nRecommendation:\nThe paper proposes a novel and effective approach to skill composition, which can be applied to various tasks. However, the framework is complex and may require significant expertise to implement and use. To improve the framework, simplifying the framework, reducing assumptions, and improving computational efficiency would be beneficial. Overall, the paper is well-written and provides valuable contributions to the field of reinforcement learning.\n\n$$$\n\nNote: This is just a peer review, and the opinions expressed in this review may not necessarily be the final decision.",
        "gr_score_with": 0.8282208588957055,
        "gr_score_without": 0.8228571428571428,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 288,
            "green_fraction": 0.8228571428571428,
            "z_score": 1.0690449676496976,
            "p_value": 0.14252470370130638
        },
        "output_with": {
            "num_tokens_scored": 326,
            "num_green_tokens": 270,
            "green_fraction": 0.8282208588957055,
            "z_score": 1.2738521839299346,
            "p_value": 0.10135790355213992
        }
    },
    {
        "title": "GNERP: GAUSSIAN-GUIDED NEURAL RECONSTRUC-",
        "abstract": "Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)\u2013based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel method for reconstructing the detailed geometry of reflective scenes using a Gaussian-based representation of normals and polarization priors. The method, called GNeRP, extends the geometry representation from scalar Signed Distance Functions (SDFs) to Gaussian fields of normals, which captures high-frequency details and separates complex geometry from specular reflection. The method is supervised by polarization priors, which reveal information about surface normals, and a Degree of Polarization (DoP) based reweighting strategy is introduced to alleviate noise issues in polarization priors.\n\nStrengths:\n1. The paper presents a novel approach for reconstructing reflective scenes, which is a challenging problem in computer vision.\n2. The Gaussian-based representation of normals is a more informative representation of geometry than scalar SDFs, and it captures high-frequency details.\n3. The use of polarization priors as supervision is a novel and effective way to guide the learning of geometry behind specular reflection.\n4. The DoP reweighing strategy is a useful technique for alleviating noise issues in polarization priors.\n\nWeaknesses:\n1. The paper relies heavily on the assumption that the incident light is unpolarized, which may not be true in all cases.\n2. The method requires a large amount of data, including radiance images and aligned polarization priors, which may be difficult to obtain.\n3. The method is computationally expensive, requiring a significant amount of time and resources to train.\n4. The paper does not provide a thorough evaluation of the method's performance on a wide range of scenes and datasets.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of the method and potential areas for improvement.\n2. The authors should provide a more thorough evaluation of the method's performance on a wide range of scenes and datasets.\n3. The authors should consider using more robust and efficient methods for estimating the covariance matrix of the Gaussian representation of normals.\n4. The authors should provide a more detailed explanation of the DoP reweighing strategy and its effects on the method's performance.\n\nRecommendation:\nThe paper presents a novel and effective approach for reconstructing reflective scenes, and it has the potential to make a significant contribution to the field of computer vision. However, the method has some limitations and areas for improvement, and the authors should address these issues in future work. Overall, I recommend that the paper be accepted for publication, but with the understanding that the authors will address the limitations and areas for improvement mentioned above.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper proposes GNeRP, a method for reconstructing the detailed geometry of reflective scenes from 2D images. GNeRP employs a Gaussian-based representation of surface normals, which captures both low-frequency orientation and high-frequency details of the surface. This representation is learned from polarization priors, which are obtained from captured images of the scene. To alleviate the noise issues of polarization priors, the method proposes a DoP reweighing strategy, which balances the supervision of radiance and polarization priors.\n\nStrengths:\n1. GNeRP addresses the challenging problem of reconstructing the detailed geometry of reflective scenes, which has been largely overlooked in previous methods.\n2. The proposed Gaussian-based representation of surface normals captures both low-frequency orientation and high-frequency details of the surface, making it more accurate than previous methods.\n3. The DoP reweighing strategy proposed in the method alleviates the noise issues of polarization priors, making the method more robust.\n\nWeaknesses:\n1. The method requires the capture of both radiance images and polarization priors, which may not always be feasible in practice.\n2. The method relies on the accuracy of the polarization priors, which may not always be available or reliable.\n3. The method has not been extensively tested on challenging datasets, such as the PANDORA dataset, which contains highly reflective and textured-less scenes.\n\nSuggestions for Improvement:\n1. To make the method more robust, the researchers should develop techniques to obtain accurate polarization priors from images, or propose methods to alleviate the noise issues of polarization priors without re-weighting the loss function.\n2. To make the method more applicable, the researchers should test the method extensively on challenging datasets, such as the PANDORA dataset, and demonstrate the method's ability to reconstruct the detailed geometry of highly reflective and textured-less scenes.\n\nRecommendation:\nThe method proposed in the paper has the potential to significantly advance the state of the art in 3D reconstruction, particularly for highly reflective and textured-less scenes. However, the method requires more extensive testing and evaluation, particularly on challenging datasets, to demonstrate its robustness and accuracy.\n\nRating: 7/10",
        "gr_score_with": 0.4417808219178082,
        "gr_score_without": 0.4602272727272727,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 162,
            "green_fraction": 0.4602272727272727,
            "z_score": -1.4924050144892729,
            "p_value": 0.9322034993668489
        },
        "output_with": {
            "num_tokens_scored": 292,
            "num_green_tokens": 129,
            "green_fraction": 0.4417808219178082,
            "z_score": -1.9896995023342199,
            "p_value": 0.9766879762187607
        }
    },
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential scalability issues of the method, particularly when dealing with large and complex datasets.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the concept bottleneck layer, including its training procedure and how it is integrated into the generative model.\n2. Include a more comprehensive discussion of the limitations of the method, including potential scalability issues and the need for large amounts of annotated data.\n3. Consider including more qualitative and quantitative evaluations of the method, such as user studies or expert evaluations, to provide a more comprehensive understanding of its effectiveness.\n4. Provide a clear explanation of how the method can be applied to other domains, such as text or audio, and discuss the potential challenges and opportunities of doing so.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. However, the paper requires a significant amount of background knowledge in generative modeling, and the authors rely heavily on existing work. Despite these limitations, the paper is well-written, and the authors provide a comprehensive evaluation of the method. I recommend this paper for readers interested in generative modeling and interpretable AI.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThis paper proposes concept bottleneck generative models (CBGMs) to address interpretability and steerability issues associated with contemporary generative models. The proposed model architecture is composed of three parts: pre-concept bottleneck portion, concept bottleneck layer, and post-concept bottleneck portion. The concept bottleneck layer is model-agnostic and can be easily adapted to different generative model architectures.\n\nStrengths:\n\n* The concept bottleneck layer is model-agnostic and can be easily adapted to different generative model architectures.\n* The proposed model is capable of steering and debugging without affecting image quality.\n* The concept bottleneck layer can handle both pre-defined and unknown concepts.\n* The model is scalable and can handle large and unbalanced concept sets.\n\nWeaknesses:\n\n* The proposed model requires pre-defined human-understandable features which can be laborious to acquire.\n* The model is limited to image tasks and extending to text tasks would require modifications.\n* The concept bottleneck layer can potentially exhibit concept leakage which can impact steerability.\n\nSuggestions for Improvement:\n\n* The model can benefit from exploring techniques to address concept leakage and improve steerability.\n* Further research is needed to extend the concept bottleneck layer to text tasks and explore what constitutes human-understandable features.\n* The model can benefit from incorporating techniques to improve concept discovery and handling large and unbalanced concept sets.\n\nRecommendation:\n\nThis paper makes significant contributions to addressing interpretability and steerability issues associated with contemporary generative models. The proposed concept bottleneck layer is model-agnostic and scalable, making it a versatile solution for different generative model architectures. The model demonstrates excellent steerability and debugging capabilities without affecting image quality.\n\nHowever, there is room for improvement to address concept leakage and extend the model to text tasks and large and unbalanced concept sets.\n\nThis research is relevant to the field and can benefit from exploring techniques to improve concept discovery and handling large and unbalanced concept sets.\n\nIn conclusion, I recommend this research to experts and practitioners working on interpretability and steerability issues associated with contemporary generative models.\n\n$ $",
        "gr_score_with": 0.5409836065573771,
        "gr_score_without": 0.467005076142132,
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 184,
            "green_fraction": 0.467005076142132,
            "z_score": -1.3098610768356838,
            "p_value": 0.904878581663219
        },
        "output_with": {
            "num_tokens_scored": 244,
            "num_green_tokens": 132,
            "green_fraction": 0.5409836065573771,
            "z_score": 1.2803687993289599,
            "p_value": 0.10020773084642742
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. The framework consists of a Reasoning and a Reflection module, which enable the system to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. The paper presents extensive experiments that prove DiLu's capability to accumulate experience and generalize to different environments.\n\nStrengths:\n\n* The paper proposes a novel framework that addresses the challenges of autonomous driving by leveraging LLMs and a memory component.\n* The framework is designed to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.\n* The paper presents extensive experiments that demonstrate the effectiveness of DiLu in accumulating experience and generalizing to different environments.\n* The paper highlights the potential of DiLu to be deployed on practical autonomous driving systems.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used in the framework are pre-trained and available, which may not be the case in real-world scenarios.\n* The paper does not provide a thorough analysis of the decision-making latency of the DiLu framework, which is a critical aspect of autonomous driving systems.\n* The paper does not provide a comparison with other knowledge-driven approaches to autonomous driving.\n* The paper assumes that the memory component is updated continuously, which may not be feasible in real-world scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the decision-making latency of the DiLu framework and propose methods to reduce it.\n* Compare DiLu with other knowledge-driven approaches to autonomous driving and highlight its advantages.\n* Provide a more detailed explanation of the memory component and its update process.\n* Discuss the potential limitations of using LLMs in the framework and propose methods to address them.\n\nRecommendation:\n\nThe paper proposes a novel and effective framework for autonomous driving that leverages LLMs and a memory component. While there are some weaknesses and limitations, the paper presents extensive experiments that demonstrate the effectiveness of DiLu. Therefore, I recommend that the paper be accepted for publication, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Title: DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models\n\nPaper Summary:\nThe authors propose a novel approach, known as DiLu, that leverages large language models (LLMs) to develop autonomous driving systems that can accumulate experience, generalize, and adapt to new environments. Inspired by human driving, DiLu integrates an interactive environment, driver agent, and memory component, utilizing LLMs to facilitate decision-making, reflection, and experience accumulation. Extensive experiments demonstrate that DiLu outperforms state-of-the-art reinforcement learning (RL) methods in both validation and comparison environments, while exhibiting robustness in diverse driving scenarios, including environments not seen during training.\n\nStrengths:\n1. Comprehensive literature survey on LLMs, autonomous driving, and decision-making.\n2. Well-articulated motivation, summarizing key challenges in autonomous driving systems.\n3. Original approach, integrating LLMs, decision-making, reflection, and experience accumulation.\n4. Extensive experiments demonstrating effectiveness, robustness, and superior performance compared to RL methods.\n5. Real-world dataset integration, facilitating deployment of autonomous driving systems in diverse environments.\n6. Robustness analysis, investigating adaptation in diverse driving environments.\n7. Contribution of reflection, successful experience, and correction experiences in decision-making.\n8. Discussion on limitations, suggesting future improvements, such as LLM optimization, experience accumulation, and decision-making latency.\n9. Clarity in methodological description, facilitating reproducibility of results.\n10. Thoroughly analyzed case studies, demonstrating applicability in diverse driving environments.\n11.Prosposed ablation study demonstrating effectiveness of two different experience accumulation methods in reflection.\n12.Strength of proposal in reflection, including LLaM, CitySim, Highway-env, robustness of different environments, thereby effectively demonstrating capability of LLaM in autonomous driving systems.\n13.Interest in proposing two different experiences accumulation in reflection demonstrating capability of autonomous driving systems in diverse driving environment.\n14.Interest in proposing CitySim, Highway-env, robustness of different environments, thereby effectively demonstrating capability of LLaM in autonomous driving systems.\n \n\nWeaknesses:\n1. High computational resources, time, cost, involved in deployment of autonomous driving systems.\n2.Possibility of failure in event of driving in non-existent environments, thereby reducing capability of autonomous driving systems.\n3.Possibility of LLaM, experiencing learning error in event of driving in non-existent environments, thereby reducing capability of autonomous driving systems.\n \n\nSuggestions for Improvement:\n1.Possibility of using different methods, such as deep learning, machine learning, computer science, in autonomous driving systems.\n2.Possibility of using different methods, such as computer science, deep learning, machine learning in LLaM.\n3.Possibility of using different methods, such as machine learning, computer science, deep learning in autonomous driving systems.\n \n\nRecommendation:\nThe authors propose an innovative approach, demonstrating robustness, effectiveness, and superior performance compared to RL methods in autonomous driving systems using LLaM, thereby addressing key challenges in decision-making, experience accumulation, reflection, deployment, adaptability, robustness, in diverse driving environments.\nThe authors have made an excellent attempt at summarizing key challenges, proposing an innovative approach, demonstrating robustness, effectiveness, superior performance, adaptability, robustness, in diverse driving environments, but there exists possibility of failure, in event of driving in non-existent environments, thereby reducing capability of autonomous driving systems.\nTherefore, I recommend acceptance of this manuscript, subject to addressing key limitations, possibility of failure in event of driving in non-existent environments, thereby reducing capability of autonomous driving systems, possibility of learning error in LLaM, in event of driving in non-existent environments, thereby reducing capability of autonomous driving systems.\n \n\nReviewer signature:\nReviewer Name:\nDate:\n $$",
        "gr_score_with": 0.5101123595505618,
        "gr_score_without": 0.4577922077922078,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 141,
            "green_fraction": 0.4577922077922078,
            "z_score": -1.4814874939752933,
            "p_value": 0.9307616424839414
        },
        "output_with": {
            "num_tokens_scored": 445,
            "num_green_tokens": 227,
            "green_fraction": 0.5101123595505618,
            "z_score": 0.4266409168259795,
            "p_value": 0.33482044530275346
        }
    },
    {
        "title": "GENERALIZATION: UNRAVELING LEARNING DIFFER-",
        "abstract": "This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension (ddata) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to ddata, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic \u201clabel sharpness\u201d (KF ) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model\u2019s adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our ddata formalism to the related metric of learned representation intrinsic dimension (drepr), derive a generalization scaling law with respect to drepr, and show that ddata serves as an upper bound for drepr. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper investigates the discrepancies in how neural networks learn from different imaging domains, specifically natural and medical images. The authors propose a generalization scaling law with respect to the intrinsic dimension of the training set (ddata) and introduce a novel measure of label sharpness (KF) of a dataset. They demonstrate that the substantial scaling discrepancy between the two domains may be attributed to the higher intrinsic label sharpness of medical imaging datasets. Furthermore, they show that a model's adversarial robustness relates to its training set's KF and that medical image datasets are typically more susceptible to adversarial attacks. The authors also extend their ddata formalism to derive and test a generalization scaling law with respect to the intrinsic dimension of the model's learned representations (drepr) and show that ddata serves as an upper bound for drepr.\n\nStrengths:\n\n* The paper provides a comprehensive investigation of the discrepancies in how neural networks learn from different imaging domains.\n* The authors propose a novel measure of label sharpness (KF) of a dataset, which is a significant contribution to the field.\n* The paper demonstrates the importance of considering the intrinsic properties of the training set in understanding neural network behavior.\n* The authors provide a thorough analysis of the relationship between generalization ability, representation learning, and adversarial robustness.\n\nWeaknesses:\n\n* The paper assumes a binary classification task, which may limit its applicability to more complex tasks.\n* The authors use a limited number of models and datasets, which may not be representative of the broader field.\n* The paper does not provide a clear explanation for the observed generalization scaling discrepancy between natural and medical image models.\n* The authors do not consider other potential factors that may contribute to the observed discrepancies.\n\nSuggestions for Improvement:\n\n* The authors should consider extending their analysis to more complex tasks, such as multi-class classification or semantic segmentation.\n* They should use a more diverse set of models and datasets to increase the generalizability of their findings.\n* The paper should provide a more detailed explanation for the observed generalization scaling discrepancy between natural and medical image models.\n* The authors should consider other potential factors that may contribute to the observed discrepancies and provide a more comprehensive analysis.\n\nRecommendation:\n\nThis paper provides a significant contribution to the field of neural network behavior and is well-written and well-structured. However, the limitations mentioned above should be addressed in future work. I would recommend this paper for publication in a reputable conference or journal, but with the caveat that the authors should address the limitations mentioned above.\n\nRating: 8/10\n\nReviewer's Signature: [Insert Signature]\n\nDate: [Insert Date]\n\n$$$\nNote: The rating and recommendation are based on the provided text and may not reflect the actual quality of the paper.",
        "peer_review_with_watermark": "$$$\nPaper Summary: The paper presents a comprehensive study on understanding the intrinsic properties of neural networks that affect their generalization performance and robustness. The research focuses on two crucial properties: intrinsic dimensionality of a dataset and intrinsic \"label sharpness\". The study shows that generalization error and robustness of neural networks can vary based on intrinsic properties of a dataset. The research uses a combination of theory and empirical validation on a diverse set of datasets from two crucial domains: natural and medical images.\n\nStrengths: The study makes significant contributions by providing a deeper understanding of neural network behavior. The research uses a combination of theory and empirical validation on a diverse set of datasets from two crucial domains: natural and medical images. The study highlights that intrinsic properties of a dataset can affect generalization error and robustness of neural networks. The research provides a clear and concise presentation of mathematical derivations and empirical validation.\n\nWeaknesses: The study focuses on two crucial properties of a dataset: intrinsic dimensionality and intrinsic \"label sharpness\". The research does not consider other factors that might affect generalization performance and robustness of neural networks. The study assumes that a dataset lies on a dddimensional manifold and that all points on this manifold can be connected by a curve of a length at least \u025b. This assumption might not always be true for all datasets. The research uses a binary classification task for evaluation of generalization performance. The study does not consider other types of machine learning problems.\n\nSuggestions for Improvement: The study could consider other factors that might affect generalization performance and robustness of neural networks. The research could investigate whether intrinsic properties of a dataset can affect generalization performance and robustness of neural networks for other types of machine learning problems. The study could provide a clear and concise presentation of mathematical derivations and empirical validation for other properties of a dataset.\n\nRecommendation: The study makes significant contributions by providing a deeper understanding of neural network behavior. The research provides a clear and concise presentation of mathematical derivations and empirical validation. The study highlights that intrinsic properties of a dataset can affect generalization error and robustness of neural networks. The research uses a combination of theory and empirical validation on a diverse set of datasets from two crucial domains: natural and medical images. The study provides a comprehensive understanding of neural network behavior and can guide machine learning research and practice.\n\nGrade: 4.5/5",
        "gr_score_with": 0.46120689655172414,
        "gr_score_without": 0.4621212121212121,
        "output_without": {
            "num_tokens_scored": 396,
            "num_green_tokens": 183,
            "green_fraction": 0.4621212121212121,
            "z_score": -1.507556722888818,
            "p_value": 0.9341659919885928
        },
        "output_with": {
            "num_tokens_scored": 232,
            "num_green_tokens": 107,
            "green_fraction": 0.46120689655172414,
            "z_score": -1.181757895737503,
            "p_value": 0.881349110675
        }
    },
    {
        "title": "TRANSFORMER-MODULATED DIFFUSION MODELS FOR PROBABILISTIC MULTIVARIATE TIME SERIES FORECASTING",
        "abstract": "Transformers have gained widespread usage in multivariate time series (MTS) forecasting, delivering impressive performance. Nonetheless, these existing transformer-based methods often neglect an essential aspect: the incorporation of uncertainty into the predicted series, which holds significant value in decisionmaking. In this paper, we introduce a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. TMDM harnesses the power of transformers to extract essential insights from historical time series data. This information is then utilized as prior knowledge, capturing covariate-dependence in both the forward and reverse processes within the diffusion model. Furthermore, we seamlessly integrate well-designed transformerbased forecasting methods into TMDM to enhance its overall performance. Additionally, we introduce two novel metrics for evaluating uncertainty estimation performance. Through extensive experiments on six datasets using four evaluation metrics, we establish the effectiveness of TMDM in probabilistic MTS forecasting.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called Transformer-Modulated Diffusion Model (TMDM) for probabilistic multivariate time series forecasting. TMDM combines the conditional diffusion generative process with transformers to enable precise distribution forecasting for multivariate time series. The framework leverages the strengths of transformers in estimating conditional means and extends this capability as priors across both forward and reverse processes within the diffusion model. The authors introduce two novel evaluation metrics, Prediction Interval Coverage Probability (PICP) and Quantile Interval Coverage Error (QICE), to assess uncertainty estimation abilities in probabilistic multivariate time series forecasting models.\n\nStrengths:\n\n1. The paper proposes a novel framework that combines diffusion generative process with transformers, which is a unique and innovative approach to probabilistic multivariate time series forecasting.\n2. The authors introduce two novel evaluation metrics, PICP and QICE, which are essential for assessing uncertainty estimation abilities in probabilistic multivariate time series forecasting models.\n3. The paper provides a comprehensive experimental evaluation on six real-world datasets, demonstrating the effectiveness of TMDM in enhancing probabilistic prediction quality.\n4. The authors discuss the advantages of using representations captured by existing well-designed transformer-based models as conditions, which is a valuable insight for future research.\n\nWeaknesses:\n\n1. The paper assumes that the conditional representation y\u03020:M can be accurately estimated by the transformer, which may not always be the case in practice.\n2. The authors do not provide a detailed analysis of the computational complexity of TMDM, which may be a concern for large-scale time series forecasting tasks.\n3. The paper relies heavily on the performance of the transformer in estimating conditional means, which may not generalize well to other time series forecasting tasks.\n4. The authors do not provide a clear comparison with other state-of-the-art methods for probabilistic multivariate time series forecasting.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of TMDM and discuss potential optimization techniques to reduce computational costs.\n2. The paper should provide a more thorough comparison with other state-of-the-art methods for probabilistic multivariate time series forecasting, including a discussion of the strengths and weaknesses of each method.\n3. The authors should provide a more detailed analysis of the assumptions made in the paper, particularly the assumption that the conditional representation y\u03020:M can be accurately estimated by the transformer.\n4. The paper should provide a more comprehensive discussion of the potential applications of TMDM in real-world time series forecasting tasks.\n\nRecommendation:\n\nThe paper proposes a novel and innovative framework for probabilistic multivariate time series forecasting, which is a valuable contribution to the field. However, the paper has some limitations that need to be addressed. With some additional analysis and comparison with other state-of-the-art methods, the paper has the potential to be a top-tier publication in the field of time series forecasting.\n\n$$$",
        "peer_review_with_watermark": "$$$Paper Summary: \n\nThis paper proposes a novel approach to multivariate time series forecasting called Transformer-Modulated Diffusion Model (TMDM). Unlike existing methods that focus solely on estimating univariate forecasts or ignore uncertainty altogether, TMDM combines a diffusion generative process with transformers to estimate both the distribution of future multivariate time series data while incorporating uncertainty information.\n\nStrengths: \n\n-   This study addresses a critical challenge in multivariate time series forecasting: modeling uncertainty.\n-   By leveraging transformers to estimate both the distribution and uncertainty information simultaneously, it provides a holistic approach to forecasting.\n-   The authors introduce novel metrics (PICP and QICE) to evaluate uncertainty estimations.\n\nWeaknesses: \n\n-   Although they mention leveraging existing transformers to estimate distribution information efficiently, they focus solely on a few structures (NSformers).\n-   They mention a plug-and-play approach to integrate new transformers to enhance distribution estimations.\n-   Experimental results might be biased because they used a limited range of datasets.\n-   Although it claims to be a hybrid optimization approach to both diffusion model components (Ldiffusion and Lcond), it may introduce new challenges when learning parameters.\n\nRecommendations: \n\n-   To strengthen their findings, they should test their approach using a diverse range of datasets to see whether their approach holds across different data types.\n-   They should evaluate their approach using different metrics to assess its robustness.\n-   They should discuss limitations to future research.\n-   They should explicitly discuss future applications.\n\n-   They should revise their approach to include a better approach to selecting parameters.\n\n-   They should revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   They should revise their approach to include a better approach to leveraging different transformers.\n-   They should revise their approach to include a better approach to using their new metrics.\n\n-   They should discuss future research to expand their findings to a wide range of applications.\n-   They should discuss future research to expand their approach to include different types of data.\n-   They should discuss future research to expand their approach to include different types of metrics.\n\nRecommendation: \n\n-   This research provides a new approach to modeling both distribution information and uncertainty simultaneously.\n-   This research provides novel metrics to evaluate model performance.\n-   This research provides a hybrid optimization approach to both diffusion model components.\n-   This research provides a plug-and-play approach to integrate new transformers.\n-   This research provides a diverse range of datasets to test their approach.\n\n-   I believe it can be accepted to conference proceedings if they address some limitations mentioned above.\n-   I believe it can be accepted to conference proceedings if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to conference proceedings if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to conference proceedings if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to conference proceedings if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a journal if they address some limitations mentioned above.\n-   I believe it can be accepted to a journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they address some limitations mentioned above.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\nRecommendation: \n\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they address some limitations mentioned above.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they address some limitations mentioned above.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they address some limitations mentioned above.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to leveraging different transformers.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to using their new metrics.\n\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to selecting parameters.\n-   I believe it can be accepted to a conference or journal if they revise their approach to include a better approach to balancing both components (Ldiffusion and Lcond).\n-   I believe it can be accepted to a conference or journal if they revise their approach",
        "gr_score_with": 0.48484848484848486,
        "gr_score_without": 0.4487534626038781,
        "output_without": {
            "num_tokens_scored": 361,
            "num_green_tokens": 162,
            "green_fraction": 0.4487534626038781,
            "z_score": -1.9473684210526316,
            "p_value": 0.9742547094089483
        },
        "output_with": {
            "num_tokens_scored": 330,
            "num_green_tokens": 160,
            "green_fraction": 0.48484848484848486,
            "z_score": -0.5504818825631803,
            "p_value": 0.7090055499713785
        }
    },
    {
        "title": "ING IN WHOLE SLIDE IMAGES",
        "abstract": "The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the ContextAware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5%, 95.9%, and 88.1%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interpretability by identifying regions of high diagnostic value.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, Context-Aware Multiple Instance Learning (CAMIL), for analyzing Whole Slide Images (WSIs) in cancer diagnostics. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. The authors evaluate CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving state-of-the-art performance compared to other attention-based MIL models.\n\nStrengths:\n\n1. The paper presents a novel approach to address the limitations of existing attention-based MIL models in capturing contextual information of tumor and neighboring tiles.\n2. The proposed neighbor-constrained attention mechanism effectively captures dependencies between tiles within a WSI.\n3. The authors provide thorough experiments and comparisons with state-of-the-art models, demonstrating the effectiveness of CAMIL on multiple datasets.\n4. The paper provides a comprehensive analysis of the attention maps generated by CAMIL, highlighting its ability to capture both global and local concepts.\n\nWeaknesses:\n\n1. The paper assumes that the contextual information of tumor and neighboring tiles is sufficient to improve the performance of MIL models. However, this assumption may not hold true for all types of cancers or imaging modalities.\n2. The authors do not provide a thorough analysis of the computational complexity of CAMIL, which may be a limitation for large-scale WSIs.\n3. The paper does not provide a clear explanation of how the neighbor-constrained attention mechanism is implemented, making it difficult to reproduce the results.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of the neighbor-constrained attention mechanism and its implementation.\n2. The paper should include a thorough analysis of the computational complexity of CAMIL and its potential limitations for large-scale WSIs.\n3. The authors should investigate the generalizability of CAMIL to other types of cancers or imaging modalities.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to analyzing WSIs in cancer diagnostics. However, the authors should address the limitations mentioned above to further improve the paper. With minor revisions, the paper has the potential to make a significant contribution to the field of medical imaging and computer vision.\n\nScore: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe proposed method, Context-Aware Multiple Instance Learning (CAMIL), aims to enhance the classification and localization accuracy of tumors in whole-slide images (WSI) using the Multiple Instance Learning (MIL) framework. The method incorporates neighbor-constrained attention and contextual constraints as prior knowledge into the MIL model. The proposed method consists of several components, including a transformer module that captures global context and dependencies between tiles and neighbor-constrained attention that focuses on capturing the relevance and importance of nearby tiles.\n\nStrengths:\nThe proposed method presents several strengths. First, the method incorporates neighbor-constrained attention and contextual constraints as prior knowledge into the MIL model. This allows the method to better understand the dependencies between tiles and the contextual relevance. The method also utilizes the transformer module that can efficiently manage the complex structure of WSIs. The method's ability to handle large and complex datasets makes it highly scalable. The method also presents high localization and interpretation capabilities.\n\nWeaknesses:\nDespite the method's strengths, several potential issues and improvements can be identified. First, the method requires large amounts of high-quality labeled data. The method's scalability and generalization capabilities can be improved if the method can handle low-resolution images and sparse or noisy labels. The method's evaluation results also reveal that the method's localization capabilities can be improved. The method's high localization accuracy can be compromised if the method focuses primarily on capturing global context and dependencies between tiles.\n\nSuggestions for Improvement:\nTo overcome the potential issues and improvements identified above, several suggestions can be proposed. First, the method can be combined with other methods that can handle low-resolution images and sparse or noisy labels. This can be achieved through the integration of pre-training or domain adaptation methods. The method can also be improved through the implementation of uncertainty-based methods that can better handle noisy labels. The method's evaluation results can also be improved through the inclusion of additional evaluation metrics that can better quantify the method's localization capabilities.\n\nRecommendation:\nBased on the method's strengths and potential improvements identified above, the method can be highly competitive with state-of-the-art methods. The method's ability to handle large and complex datasets makes it highly scalable and generalizable. The method's high localization and interpretation capabilities can also be highly useful in clinical practice. The method's scalability and generalization capabilities can be improved through the integration with other methods that can handle low-resolution images and sparse or noisy labels. Therefore, I recommend that the method be continued and improved through the inclusion of additional evaluation metrics and the integration with other methods.\n\n$$$",
        "gr_score_with": 0.4360655737704918,
        "gr_score_without": 0.42151162790697677,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 145,
            "green_fraction": 0.42151162790697677,
            "z_score": -2.911484876492837,
            "p_value": 0.9982014233960814
        },
        "output_with": {
            "num_tokens_scored": 305,
            "num_green_tokens": 133,
            "green_fraction": 0.4360655737704918,
            "z_score": -2.2331335038240865,
            "p_value": 0.9872299291068152
        }
    },
    {
        "title": "COLIDE: CONCOMITANT LINEAR DAG ESTIMATION",
        "abstract": "We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the unknown SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE (Concomitant Linear DAG Estimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of noise variances in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for learning linear Directed Acyclic Graphs (DAGs) called CoLiDE (Concomitant Linear DAG Estimation). The framework simultaneously estimates both the DAG structure and the exogenous noise levels. The authors present variants of CoLiDE to estimate homoscedastic and heteroscedastic noise across nodes. The proposed method, CoLiDE, incorporates concomitant estimation of scale parameters to enhance DAG topology inference using continuous first-order optimization. The authors demonstrate that CoLiDE outperforms state-of-the-art methods in diverse synthetic and real-world settings, including the recovery of the DAG edges as well as their weights.\n\nStrengths:\n\n1. The paper proposes a novel framework for learning linear DAGs, which is a significant contribution to the field of causal discovery.\n2. The authors present a comprehensive evaluation of CoLiDE against other state-of-the-art approaches, including GES, GOLEM, DAGMA, SortNRegress, and DAGuerreotype.\n3. The paper provides a thorough analysis of the performance of CoLiDE in both homoscedastic and heteroscedastic settings, including the recovery of the DAG edges as well as their weights.\n4. The authors demonstrate that CoLiDE is robust to deviations from Gaussianity and can handle high-dimensional settings where both data scarcity and model uncertainty are prevalent.\n5. The paper provides a clear and concise explanation of the CoLiDE algorithm and its implementation details.\n\nWeaknesses:\n\n1. The paper assumes that the data is generated from a linear structural equation model (SEM), which may not be realistic in many real-world scenarios.\n2. The authors do not provide a thorough analysis of the computational complexity of the CoLiDE algorithm, which may be a limitation for large-scale datasets.\n3. The paper assumes that the noise variances are known or can be estimated, which may not be the case in many real-world scenarios.\n4. The authors do not provide a thorough analysis of the robustness of CoLiDE to outliers and other forms of data contamination.\n5. The paper assumes that the DAG is sparse, which may not be the case in many real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational complexity of the CoLiDE algorithm and its scalability to large-scale datasets.\n2. The paper should assume a more realistic noise model, such as a non-Gaussian noise model, to better reflect real-world scenarios.\n3. The authors should provide a more thorough analysis of the robustness of CoLiDE to outliers and other forms of data contamination.\n4. The paper should provide a more thorough analysis of the performance of CoLiDE in non-linear SEMs and other non-linear models.\n5. The authors should provide a more thorough analysis of the limitations of CoLiDE and potential areas for future work.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of causal discovery and provides a novel framework for learning linear DAGs. The authors provide a comprehensive evaluation of CoLiDE against other state-of-the-art approaches and demonstrate its robustness to deviations from Gaussianity. However, the paper assumes a linear SEM and does not provide a thorough analysis of the computational complexity of the CoLiDE algorithm. Therefore, I recommend that the authors address these limitations in future work.\n\nOverall, I believe that the paper is well-written, well-organized, and provides a significant contribution to the field of causal discovery. However, there are some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$$\nPaper Summary: \nThe paper proposes CoLiDE, a novel convex score function for sparsity-aware linear DAG estimation, which incorporates concomitant estimation of exogenous noises to effectively address challenges in DAG topology inference, specifically in cases with varying or non-equal exogenous noises across nodes (noise variance). CoLiDE (Concomitant Linear DAG Estimation) addresses two primary challenges in DAG estimation: (a) requiring expensive recalibration when exogenous noises change across instances and (b) relying implicitly on limiting homoscedasticity (constant variance) assumption, which does not hold in many cases.\n\nStrengths: \nThe paper proposes an innovative and novel convex formulation that addresses significant challenges in DAG estimation, specifically in cases with varying or non-equal exogenous noises across nodes (noise variance). CoLiDE offers an effective and robust estimation strategy that does not necessitate recalibration when exogenous noises change across instances and does not implicitly rely on limiting homoscedasticity assumption.\n\nWeaknesses: \nOne limitation mentioned in the paper relates to CoLiDE-NV (Non-equal variance) performing over-parametrization in cases with constant variance across all nodes, which could be addressed through more careful formulation and optimization strategies.\n\nRecommendations: \nThe authors could further refine CoLiDE-NV to address over-parametrization in cases with constant variance across all nodes and validate CoLiDE-NV with more robust experiments to demonstrate robustness to varying noises.\n\nRecommendations: \nOverall, CoLiDE appears to be an effective and robust DAG estimation strategy that offers significant advantages over state-of-art methods, specifically in cases with varying or non-equal exogenous noises across nodes (noise variance). Further validation and refinement could further strengthen CoLiDE as an innovative and novel convex formulation that addresses significant challenges in DAG estimation.\n\n$$$$",
        "gr_score_with": 0.4831932773109244,
        "gr_score_without": 0.4433497536945813,
        "output_without": {
            "num_tokens_scored": 406,
            "num_green_tokens": 180,
            "green_fraction": 0.4433497536945813,
            "z_score": -2.2829416681331396,
            "p_value": 0.9887830976972509
        },
        "output_with": {
            "num_tokens_scored": 238,
            "num_green_tokens": 115,
            "green_fraction": 0.4831932773109244,
            "z_score": -0.5185629788417315,
            "p_value": 0.6979672343562102
        }
    },
    {
        "title": "CONQUER: DIFFUSION-BASED 3D DEPTH AWARE COMPOSABLE IMAGE SYNTHESIS",
        "abstract": "Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, COMPOSE AND CONQUER (CNC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-conditional diffusion model called Compose and Conquer (CnC) that addresses two main challenges in the field: three-dimensional placement of multiple objects and region-specific localization of global semantics from multiple sources. CnC employs two main components: the local and global fuser, which respectively leverage the new Depth Disentanglement Training (DDT) and soft guidance techniques. The local fuser uses DDT to infer the absolute depth placement of objects, while the global fuser uses soft guidance to incorporate semantics onto localized regions. The authors demonstrate the effectiveness of CnC through extensive experimental results on the COCO-stuff and Pick-a-Pic datasets.\n\nStrengths:\n\n* The paper presents a novel approach to addressing two main challenges in the field of text-conditional diffusion models.\n* The authors provide a clear and concise explanation of the proposed method, including the local and global fuser components.\n* The experimental results demonstrate the effectiveness of CnC in addressing the challenges of three-dimensional placement of multiple objects and region-specific localization of global semantics.\n* The authors provide a thorough comparison with baseline models, including GLIGEN, ControlNet, Uni-ControlNet, and T2I-Adapter.\n\nWeaknesses:\n\n* The paper assumes that the user has a good understanding of the proposed method and the underlying concepts, which may make it difficult for readers without a background in the field to follow.\n* The authors do not provide a clear explanation of how the Depth Disentanglement Training (DDT) and soft guidance techniques work, which may make it difficult for readers to understand the proposed method.\n* The paper does not provide a clear discussion of the limitations of the proposed method, including the assumption that the user has a good understanding of the proposed method.\n* The authors do not provide a clear explanation of how the proposed method can be applied to real-world scenarios.\n\nSuggestions for Improvement:\n\n* Provide a clear and concise explanation of the proposed method, including the local and global fuser components, and the underlying concepts.\n* Provide a clear explanation of how the Depth Disentanglement Training (DDT) and soft guidance techniques work.\n* Discuss the limitations of the proposed method, including the assumption that the user has a good understanding of the proposed method.\n* Provide a clear explanation of how the proposed method can be applied to real-world scenarios.\n\nRecommendation:\n\nThe paper presents a novel approach to addressing two main challenges in the field of text-conditional diffusion models. The authors provide a clear and concise explanation of the proposed method, including the local and global fuser components, and the experimental results demonstrate the effectiveness of CnC. However, the paper assumes that the user has a good understanding of the proposed method and the underlying concepts, which may make it difficult for readers without a background in the field to follow. Therefore, I recommend that the authors provide a clear and concise explanation of the proposed method, including the local and global fuser components, and the underlying concepts, and discuss the limitations of the proposed method.\n\nRating: 7/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \n\n**Paper Summary:**\n\nThe paper proposes a text-conditional diffusion model, COMPOSE AND CONQUER (CNC), to tackle two significant limitations of text-conditional diffusion models: 1) three-dimensional placement of multiple objects, and 2) region-specific localization of global semantics from multiple sources. The authors introduce a new training paradigm, depth disentanglement training (DDT), to learn the 3D relative positions of objects, and a new method, soft guidance, to impose global semantics onto specific regions of the target image.\n\n**Strengths:**\n\n1. The authors have proposed a well-motivated solution to tackle significant limitations of text-conditional diffusion models, making a significant contribution to the field.\n2. The paper provides a clear explanation of the proposed methodology, including a new training paradigm, depth disentanglement training, and a new method, soft guidance, making it easier to understand and implement.\n3. The authors have performed a thorough comparison with other models, including those with depth maps, CLIP image embeddings, or both, providing a clear understanding of the advantages of COMPOSE AND CONQUER.\n\n**Weaknesses:**\n\n1. The paper assumes a significant background knowledge of diffusion models, text-conditioning, and computer vision, making it challenging to understand for researchers from other fields or those with a basic understanding of computer vision.\n2. The authors have used a large-scale dataset, COCO-STUFF, but have only used a small portion of it, 164K out of 164M, to generate 302K synthetic triplets, leaving open questions about how well COMPOSE AND CONQUER will generalize to other, potentially more challenging, scenarios.\n\n**Suggestions for Improvement:**\n\n1. The authors should explore alternative ways to generate depth maps, such as 3D reconstruction or depth estimation from a single image, to improve the accuracy of depth information used to learn 3D positions of objects.\n2. The authors should investigate more efficient ways to combine depth maps with global semantics, such as learning a more expressive representation of depth information or incorporating other modalities, like audio or 3D data.\n\n**Recommendation:**\n\nI strongly believe that COMPOSE AND CONQUER has significant contributions to make to the text-conditioning diffusion model space, addressing significant limitations of text-conditioning diffusion models, including 3D placements of objects, region-specific localization of global semantics, making a significant step forward to tackle these challenging problems.\n\nHowever, I have some doubts regarding whether COMPOSE AND CONQUER will generalize well to other, potentially more challenging, scenarios, given a very large-scale dataset, COCO-STUFF, has been used but only a small portion of 164M has been used to generate 302K triplets.\n\nGiven these points, I believe COMPOSE AND CONQUER has a high-quality paper with a solid methodology, a clear explanation, a well-performed comparison with other models, but with some limitations regarding generalization to other scenarios.\n\n**Recommend: Accept** \n\n$$$",
        "gr_score_with": 0.5085158150851582,
        "gr_score_without": 0.45733788395904434,
        "output_without": {
            "num_tokens_scored": 293,
            "num_green_tokens": 134,
            "green_fraction": 0.45733788395904434,
            "z_score": -1.460515594592465,
            "p_value": 0.9279257875949573
        },
        "output_with": {
            "num_tokens_scored": 411,
            "num_green_tokens": 209,
            "green_fraction": 0.5085158150851582,
            "z_score": 0.34528453656689306,
            "p_value": 0.3649402353432026
        }
    },
    {
        "title": "BENO: BOUNDARY-EMBEDDED NEURAL OPERATORS FOR ELLIPTIC PDES",
        "abstract": "Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green\u2019s function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%. Our source code can be found https://github.com/AI4Science-WestlakeU/beno.git.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel neural operator architecture, Boundary-Embedded Neural Operators (BENO), to solve elliptic partial differential equations (PDEs) with complex boundary conditions. BENO consists of two branches of Graph Neural Networks (GNNs) for interior source terms and boundary values, respectively, and a Transformer encoder to map the global boundary geometry into a latent vector that influences each message passing layer of the GNNs. The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems, and its strong generalization capabilities across different scenarios.\n\nStrengths:\n\n* The paper presents a novel and effective approach to solving elliptic PDEs with complex boundary conditions.\n* The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods.\n* The paper provides a thorough evaluation of BENO's performance on various datasets and scenarios.\n* The authors provide a detailed description of the architecture and implementation of BENO.\n\nWeaknesses:\n\n* The paper assumes a specific type of elliptic PDE (Poisson equation) and does not provide a general framework for solving other types of elliptic PDEs.\n* The authors do not provide a thorough comparison with other classical methods for solving elliptic PDEs.\n* The paper relies heavily on the use of graph neural networks and Transformers, which may not be suitable for all types of elliptic PDEs.\n* The authors do not provide a detailed analysis of the computational complexity of BENO.\n\nSuggestions for Improvement:\n\n* Provide a more general framework for solving elliptic PDEs, including different types of PDEs and boundary conditions.\n* Compare BENO with other classical methods for solving elliptic PDEs, such as finite element methods and finite difference methods.\n* Investigate the use of other neural network architectures, such as convolutional neural networks, for solving elliptic PDEs.\n* Provide a detailed analysis of the computational complexity of BENO and compare it with other methods.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to solving elliptic PDEs with complex boundary conditions. While there are some limitations to the paper, the authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods. Therefore, I recommend accepting this paper for publication, with the suggestion for the authors to address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThis paper introduces Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture for solving elliptic partial differential equations (PDEs) with complex boundary conditions. BENO addresses two key challenges of elliptic PDEs: (1) sensitivity to boundary conditions, including inhomogeneous boundary values, and (2) complexity of boundary geometry. Inspired by classical Green's function, BENO uses a dual-branch network structure, where one branch focuses on interior source terms, and the other on boundary values, to learn boundary-interior interaction.\n\nStrengths:\n1. BENO addresses a critical gap in existing neural operators for elliptic PDEs, i.e., ignoring boundary conditions.\n2. The proposed boundary embedding method, leveraging Transformer, enables modeling of long-range relationships between boundary points and interior solution.\n3. BENO's decoupled learning of interior and boundary information enhances its ability to handle complex boundary conditions.\n4. Extensive experiments on diverse datasets show BENO's strong generalization capability, outperforming state-of-the-art methods by an average of 60.96%.\n5. BENO's simplicity, physics-inspired approach, and flexibility to accommodate different boundary shapes, values, and resolutions, makes it an attractive solution for elliptic PDE problems.\n\nWeaknesses:\n1. BENO's effectiveness on higher-dimensional elliptic PDE problems remains untested.\n2. Training dataset creation, particularly for diverse boundary shapes, values, and resolutions, can be challenging.\n3. Lack of ablation studies on critical components, such as boundary embedding, to quantify its contribution to BENO's overall performance.\n4. While BENO outperforms baselines, its robustness against noisy boundary values, irregular boundary shapes, and limited resolution remains untested.\n\nSuggestions for Improvement:\n1. Include higher-dimensional elliptic PDE problems to validate BENO's effectiveness.\n2. Develop automated methods for generating diverse boundary shapes, values, and resolutions.\n3. Perform ablation studies to quantify contributions of critical components, such as boundary embedding.\n4. Include robustness analysis against noisy boundary values, irregular boundary shapes, and limited resolution.\n\nRecommendation:\nThis paper makes an essential contribution to neural operators for elliptic PDE problems by addressing critical challenges of boundary conditions. BENO's strong generalization capability, simplicity, and physics-inspired approach justify its consideration for solving elliptic PDE problems, particularly for applications where boundary conditions play a critical role, such as solid mechanics, plasma physics, and fluid dynamics.",
        "gr_score_with": 0.484593837535014,
        "gr_score_without": 0.4158730158730159,
        "output_without": {
            "num_tokens_scored": 315,
            "num_green_tokens": 131,
            "green_fraction": 0.4158730158730159,
            "z_score": -2.986211700040758,
            "p_value": 0.9985877148462213
        },
        "output_with": {
            "num_tokens_scored": 357,
            "num_green_tokens": 173,
            "green_fraction": 0.484593837535014,
            "z_score": -0.5821817364274594,
            "p_value": 0.7197778630400584
        }
    },
    {
        "title": "TRAIN NEURAL PROGRAM SYNTHESIZERS",
        "abstract": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one\u2019s intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample. We then use the informative dataset to train models to improve the synthesizer\u2019s ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics, i.e., how users choose examples informatively. The method bootstraps pragmatic synthesizers by iteratively generating data using speaker (example generator) and listener (program synthesizer) models, filtering the generated data with an example selection method drawn from work on computational pragmatics, and further training the speaker and listener models on the generated data. The approach is evaluated on the challenging task of synthesizing regular expressions from example strings, and it is shown that the method produces pragmatic program synthesizers with minimal supervision, matching the performance of synthesizers trained on human-produced examples.\n\nStrengths:\n\n* The paper proposes a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics.\n* The method is evaluated on the challenging task of synthesizing regular expressions from example strings, and it is shown that the method produces pragmatic program synthesizers with minimal supervision.\n* The paper provides a detailed description of the approach, including the algorithm, the speaker and listener models, and the example selection method.\n* The paper evaluates the approach using a user study, which provides insights into how humans communicate with the synthesizer.\n\nWeaknesses:\n\n* The paper assumes that the speaker and listener models are trained using a large dataset of programs and specifications, which may not be available in all domains.\n* The paper does not provide a detailed analysis of the computational complexity of the approach, which may be a concern in large-scale program synthesis tasks.\n* The paper does not evaluate the approach on other domains, such as open-ended Python code generation, which may have an even larger space of possible programs and input/output specifications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational complexity of the approach, including the time and space complexity of the speaker and listener models.\n* Evaluate the approach on other domains, such as open-ended Python code generation, to demonstrate its scalability and applicability.\n* Provide more insights into how the speaker and listener models are trained, including the learning rate, batch size, and number of epochs.\n\nRecommendation:\n\nThe paper presents a novel and interesting approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics. The approach is evaluated on the challenging task of synthesizing regular expressions from example strings, and it is shown that the method produces pragmatic program synthesizers with minimal supervision. However, the paper assumes that the speaker and listener models are trained using a large dataset of programs and specifications, which may not be available in all domains. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the approach and evaluate it on other domains to demonstrate its scalability and applicability.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe authors proposed a method for training neural program synthesizers that can resolve ambiguity in program specification examples by leveraging pragmatic reasoning. They developed a speaker and listener models, trained on simulated pragmatic reasoning, and iteratively refined them on a dataset of pragmatically-inferred examples. Experiments on a challenging regular expression domain showed that the proposed method outperformed a literal listener and a humanfine-tuned listener, and achieved performance on par with a listener trained on human-provided pragmatic examples.\n\nStrengths:\n\n1.  Novel application of pragmatic reasoning in programming-by-example, addressing a long-standing challenge in ambiguity resolution.\n2.  Effective method for training neural synthesizers with minimal supervision, leveraging simulated pragmatic reasoning and iteratively refined models.\n3.  Strong experimental validation, with significant gains in performance over a literal listener and a humanfine-tuned listener, and matching performance with a listener trained on human-provided pragmatic examples.\n\nWeaknesses:\n\n1.  Limited scope of experimentation, focusing on a specific domain of regular expression synthesis, and potential difficulty in scaling to larger and more complex domains.\n2.  Dependence on a specific neural architecture and training methodology, limiting potential applicability and reproducibility of the proposed method.\n\nSuggestions for Improvement:\n\n1.  Investigate scaling of pragmatic reasoning in programming-byexample to larger and more complex domains, potentially leveraging multimodal input and output, and multimodal pragmatics reasoning models.\n2.  Examine potential applicability of pragmatic reasoning in programming-byexample in broader contexts, such as code completion and code generation, and in different programming languages and domains.\n\nRecommendation:\n\nThe proposed method offers a significant improvement in ambiguity resolution in programming-byexample, leveraging pragmatic reasoning and simulated pragmatic reasoning. With further experimentation and validation, and potential scaling and broadening of its scope, pragmatic reasoning in programming-byexample presents a highly promising direction in research and development of neural synthesizers.\n\n$$$",
        "gr_score_with": 0.5473684210526316,
        "gr_score_without": 0.44625407166123776,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 137,
            "green_fraction": 0.44625407166123776,
            "z_score": -1.8834094802666537,
            "p_value": 0.970177551712786
        },
        "output_with": {
            "num_tokens_scored": 285,
            "num_green_tokens": 156,
            "green_fraction": 0.5473684210526316,
            "z_score": 1.5993419699495492,
            "p_value": 0.05487231937350664
        }
    },
    {
        "title": "CIRCUMVENTING CONCEPT ERASURE METHODS FOR TEXT-TO-IMAGE GENERATIVE MODELS",
        "abstract": "Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. Yet, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to \u201cerase\u201d sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we devise an algorithm to learn special input word embeddings that can retrieve \u201cerased\u201d concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents an investigation into the efficacy of concept erasure methods for text-to-image generative models, specifically Stable Diffusion. The authors examine seven recently proposed methods, including fine-tuning-based and inference-guiding-based approaches, and demonstrate that all of them can be circumvented using a novel attack technique called Concept Inversion (CI). The CI attack learns special word embeddings that can retrieve erased concepts from the sanitized models with no alterations to their weights. The authors' results highlight the brittleness of post-hoc concept erasure methods and call into question their use in the algorithmic toolkit for AI safety.\n\nStrengths:\n\n1. The paper presents a thorough investigation into the concept erasure methods, including a detailed analysis of the seven methods examined.\n2. The authors provide a clear and concise explanation of the CI attack technique and its application to each of the concept erasure methods.\n3. The results are well-presented and demonstrate the effectiveness of the CI attack in circumventing the concept erasure methods.\n4. The paper raises important questions about the use of post-hoc concept erasure methods in AI safety and highlights the need for more robust evaluation methodologies.\n\nWeaknesses:\n\n1. The paper assumes that the adversary has access to the weights and components of the erased model, which may not always be the case in real-world scenarios.\n2. The authors do not provide a comprehensive evaluation of the concept erasure methods in terms of their effectiveness in preventing the generation of sensitive or unsafe content.\n3. The paper does not provide a clear explanation of why the input filtering phenomenon seems to be occurring in all the recent methods.\n\nSuggestions for Improvement:\n\n1. The authors should consider investigating the use of more robust evaluation methodologies, such as human evaluation or more comprehensive metrics, to assess the effectiveness of the concept erasure methods.\n2. The paper could benefit from a more detailed analysis of the limitations of the CI attack technique and its potential vulnerabilities.\n3. The authors should provide a more comprehensive explanation of the input filtering phenomenon and its implications for the design of more robust concept erasure methods.\n\nRecommendation:\n\nThis paper presents a timely and important investigation into the concept erasure methods for text-to-image generative models. The results are well-presented and demonstrate the effectiveness of the CI attack technique in circumventing the concept erasure methods. However, the paper assumes a level of access to the weights and components of the erased model that may not always be the case in real-world scenarios. Therefore, I recommend that the authors consider investigating more robust evaluation methodologies and providing a more comprehensive explanation of the input filtering phenomenon. Overall, I believe that this paper makes a significant contribution to the field of AI safety and raises important questions about the use of post-hoc concept erasure methods.",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper presents an in-depth study of seven concept erasure methods for text-to-image generative models, specifically Stable Diffusion (SD). The authors argue that these post-hoc erasure methods provide a false sense of security and fail to fully remove sensitive concepts from the models. Using a novel approach called Concept Inversion (CI), the authors show that these erasure methods can be easily circumvented, even without modifying the model weights. The study focuses on fine-tuning-based (ESD, UCE) and inference-guiding-based (NP, SLD) erasure methods, as well as art style, object, ID, and NSFW content erasure tasks.\n\n Strengths:\n\n1. The study provides a comprehensive overview of the current state of concept erasure methods in text-to-image models.\n2. The authors propose a novel approach (CI) to evaluate the effectiveness of erasure methods.\n3. The study highlights the importance of evaluating models in a more robust way.\n4. The findings have significant implications for the development of safer text-to-image models.\n5. The study provides a good understanding of the current concept erasure landscape in text-to-image models.\n\n Weaknesses:\n\n1. The study focuses on a specific model (Stable Diffusion) and may not generalize to other models.\n2. The study relies on a novel approach (CI) that may be difficult to implement.\n3. The study may benefit from a more in-depth analysis of the erasure methods.\n4. The study may benefit from a more diverse range of erasure tasks.\n5. The study may benefit from a more in-depth discussion of the implications of the findings.\n\n Suggestions for Improvement:\n\n1. Include a more in-depth analysis of the erasure methods.\n2. Evaluate the effectiveness of the erasure methods on a more diverse range of tasks.\n3. Include a more in-depth discussion of the implications of the findings.\n4. Include a more in-depth study of the transferability of the learned word embeddings.\n5. Include a more in-depth study of the reconstruction effectiveness of the learned word embeddings.\n\n Recommendation:\n\nThis study provides a valuable contribution to the understanding of concept erasure in text-to-image models. The findings have significant implications for the development of safer text-to-image models. The study highlights the importance of evaluating models in a more robust way and provides a good understanding of the current concept erasure landscape in text-to-image models. I recommend this study as a valuable contribution to the AI safety literature but I also recommend a more in-depth study of the erasure methods, a more in-depth study of the transferability of the learned word embeddings and a more in-depth study of the reconstruction effectiveness of the learned word embeddings. The study has the potential to be a significant contribution but it may benefit from a more in-depth study of the erasure methods in the future.\n$$$",
        "gr_score_with": 0.40129449838187703,
        "gr_score_without": 0.428169014084507,
        "output_without": {
            "num_tokens_scored": 355,
            "num_green_tokens": 152,
            "green_fraction": 0.428169014084507,
            "z_score": -2.7067989514148034,
            "p_value": 0.9966032307333611
        },
        "output_with": {
            "num_tokens_scored": 309,
            "num_green_tokens": 124,
            "green_fraction": 0.40129449838187703,
            "z_score": -3.470168756330303,
            "p_value": 0.9997399342712783
        }
    }
]