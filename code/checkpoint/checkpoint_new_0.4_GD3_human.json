[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "$$$\\boxed{\\text{Paper Summary:}}$\n\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines large language models (LLMs) with deep structural causal models (DSCMs) for causal discovery. The CMA integrates metadata-based reasoning capabilities of LLMs with data-driven modelling of DSCMs to reason over causal graphs and identify causal relationships among variables. The paper evaluates the CMA's performance on synthetic experiments and causal discovery benchmarks, including the Alzheimer's Disease benchmark, and demonstrates its ability to outperform previous purely data-driven or metadata-driven approaches to causal discovery.\n\n$\\boxed{\\text{Strengths:}}$\n\n1. The CMA framework provides a unified approach to causal discovery by integrating metadata-based reasoning with data-driven modelling.\n2. The use of LLMs as a prior, critic, and post-processor enables the CMA to leverage domain knowledge and generate causal graphs that are more informative than those generated by traditional data-driven approaches.\n3. The CMA's ability to handle multi-modal data and propose insights into the causal relationships among variables is demonstrated through its application to real-world datasets, including the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset.\n4. The paper provides a detailed evaluation of the CMA's performance on various benchmarks, including synthetic experiments and real-world datasets, and demonstrates its ability to outperform previous approaches to causal discovery.\n\n$\\boxed{\\text{Weaknesses:}}$\n\n1. The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. This limitation may be addressed by extending the framework to include chain graph elements and more flexible, non-Markovian causal graphs.\n2. The CMA's ability to represent chains in a fully automated manner is limited, and human intervention is necessary to allow appropriate Deep Chain Graph modelling.\n3. The paper does not provide a detailed evaluation of the CMA's performance on datasets with few synthetic variables, which may be challenging for the CMA to handle.\n4. The paper's conclusion suggests that future work should address several important limitations, but a more detailed discussion of these limitations and potential solutions is needed.\n\n$\\boxed{\\text{Suggestions for Improvement:}}$\n\n1. Extend the CMA framework to include chain graph elements and more flexible, non-Markovian causal graphs to address the limitation of relying on DSCMs to fit data.\n2. Investigate techniques to enable fully automated chain graph modelling and more generally extend deep SCMs to represent more flexible, non-Markovian causal graphs.\n3. Provide a more detailed evaluation of the CMA's performance on datasets with few synthetic variables.\n4. Discuss the potential solutions to the limitations mentioned in the paper's conclusion.\n\n$\\boxed{\\text{Recommendation:}}$\n\nThe CMA framework is a promising approach to causal discovery that integrates metadata-based reasoning with data-driven modelling. While it has several limitations, the paper demonstrates its ability to outperform previous approaches to causal discovery and provides a detailed evaluation of its performance on various benchmarks. With further development and extension of the framework, the CMA has the potential to become a leading approach to causal discovery in various scientific fields.",
        "peer_review_with_watermark": "$$$\\begin{tabular}{|l|} \\hline \\textbf{Paper Summary:} & The paper introduces the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for causal discovery. The CMA uses LLMs to generate hypotheses, which are then used to fit DSCMs and produce causal graphs. The paper demonstrates the effectiveness of the CMA on synthetic and real-world datasets from various scientific fields. \\\\ \\hline \\textbf{Strengths:} & The paper makes significant contributions to the field of causal discovery by integrating LLMs with DSCMs. The CMA is able to propose insightful relationships between variables internal and external to the DSCMs. The paper provides a unified framework for causal discovery that combines metadata-based reasoning with data-driven modelling. The experimental results demonstrate the effectiveness of the CMA on various benchmarks. \\\\ \\hline \\textbf{Weaknesses:} & The paper relies on DSCMs, which assume a Markovian DAG. This limitation may not be optimal for all applications. The CMA requires human intervention to allow appropriate Deep Chain Graph modelling, which may not be fully automated. The paper also requires additional data for external variables, which may not be available for all applications. \\\\ \\hline \\textbf{Suggestions for Improvement:} & The paper should explore the extension of the CMA to include chain graph elements and more flexible, non-Markovian causal graphs. The CMA should be able to represent chains in a fully automated manner. The paper should also investigate techniques to enable fully automated chain graph modelling. Additionally, the CMA should be able to handle discrete variables with parents and support discrete mechanisms with a Gumbel\u2013max parametrisation. \\\\ \\hline \\textbf{Recommendation:} & The paper is well-structured and provides a clear and unified framework for causal discovery. The experimental results demonstrate the effectiveness of the CMA on various benchmarks. However, the paper should address the limitations and suggest potential future work to further improve the CMA. The paper is a significant contribution to the field of causal discovery and should be considered for publication in a reputable journal. \\\\ \\hline \\end{tabular}$$",
        "gr_score_with": 0.3408360128617363,
        "gr_score_without": 0.31909547738693467,
        "output_without": {
            "num_tokens_scored": 398,
            "num_green_tokens": 127,
            "green_fraction": 0.31909547738693467,
            "z_score": -3.2946456740069854,
            "p_value": 0.9995072707409889
        },
        "output_with": {
            "num_tokens_scored": 311,
            "num_green_tokens": 106,
            "green_fraction": 0.3408360128617363,
            "z_score": -2.1297665789001896,
            "p_value": 0.983404555407368
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$The paper proposes a novel approach to learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The authors aim to develop a framework that enables flexible composition of visual concepts from images, which is a crucial aspect of human perception and understanding of the visual world.\n\nStrengths:\n\n*   The paper provides a clear and concise overview of the problem and the proposed approach, making it easy to follow and understand.\n*   The authors provide a thorough literature review, highlighting the limitations of existing approaches and the potential benefits of their proposed method.\n*   The experimental results demonstrate the effectiveness of the proposed approach in generating images with novel concept compositions.\n*   The authors provide a detailed analysis of the results, including a comparison with prior work and a human evaluation.\n\nWeaknesses:\n\n*   The paper assumes that the pre-trained vision-language models are robust and reliable, which may not always be the case.\n*   The authors do not provide a detailed analysis of the concept axes and how they are used to generate images.\n*   The paper does not discuss the potential applications and limitations of the proposed approach.\n\nSuggestions for Improvement:\n\n*   The authors should provide a more detailed analysis of the concept axes and how they are used to generate images.\n*   The authors should discuss the potential applications and limitations of the proposed approach.\n*   The authors should provide more experimental results, including a comparison with other approaches.\n\nRecommendation:\n\n*   The paper is well-written and provides a clear and concise overview of the problem and the proposed approach.\n*   The authors provide a thorough literature review and experimental results that demonstrate the effectiveness of the proposed approach.\n*   However, the paper assumes that the pre-trained vision-language models are robust and reliable, which may not always be the case.\n*   The authors should provide a more detailed analysis of the concept axes and how they are used to generate images.\n*   The authors should discuss the potential applications and limitations of the proposed approach.\n\n$$\\begin{aligned}",
        "peer_review_with_watermark": "$$\n\\begin{aligned}\n\\textbf{Paper Summary:} &\\  The paper proposes a framework for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The proposed framework learns disentangled concept embeddings along various concept axes specified by language and enables flexible composition of concepts for image generation. The paper also proposes a method for test-time finetuning of the concept encoders to generalize to unseen concepts.\n\n\\textbf{Strengths:} &\\  The paper makes a significant contribution to the field of visual concept learning by introducing a framework that can learn disentangled concept embeddings from pre-trained vision-language models. The proposed framework is flexible and enables composition of concepts for image generation. The paper also proposes a method for test-time finetuning of the concept encoders, which is a crucial aspect of real-world applications.\n\n\\textbf{Weaknesses:} &\\  The paper has a few limitations. The proposed framework relies on pre-trained vision-language models, which may not be the best choice for all tasks. The paper also assumes that the concept axes are well-defined and discrete, which may not be the case in all real-world applications. The paper also lacks a comprehensive evaluation of the proposed framework, with only a few qualitative results presented.\n\n\\textbf{Suggestions for Improvement:} &\\  To improve the paper, the authors could consider the following: (1) Using a more robust pre-trained vision-language model that can learn more abstract and nuanced concept embeddings. (2) Introducing more robust concept axes that can capture the subtleties of visual nuances. (3) Evaluating the proposed framework more comprehensively, including more quantitative results and a more thorough evaluation of its limitations.\n\n\\textbf{Recommendation:} &\\  The paper is recommended for its contribution to the field of visual concept learning. However, the paper has a few limitations that need to be addressed. The proposed framework has the potential to be a significant improvement over existing methods, but more work is needed to evaluate its limitations and robustness.\n\n$$\n$$",
        "gr_score_with": 0.3333333333333333,
        "gr_score_without": 0.3177570093457944,
        "output_without": {
            "num_tokens_scored": 214,
            "num_green_tokens": 68,
            "green_fraction": 0.3177570093457944,
            "z_score": -2.4558405176181703,
            "p_value": 0.992972224031724
        },
        "output_with": {
            "num_tokens_scored": 285,
            "num_green_tokens": 95,
            "green_fraction": 0.3333333333333333,
            "z_score": -2.297341458681704,
            "p_value": 0.989200350754501
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\text{Paper Summary:}$$\nThe paper proposes Rapper, a two-stage Reinforced Rationale-Prompted Paradigm for enabling Natural Language Explanation (NLE) with sufficient plausible and faithful properties in Visual Question Answering (VQA). Rapper first distills language-based knowledge from Large Language Models (LLMs) and then utilizes Reinforcement Learning from NLE Feedback (RLNF) to inject visual facts into NLE generation. The proposed method achieves new state-of-the-art performance on two VL-NLE benchmarks and demonstrates reduced implausibility and hallucination.\n\n$$\\text{Strengths:}$$\n1. The paper introduces a novel two-stage paradigm that effectively integrates language-based knowledge and visual facts into NLE generation.\n2. The use of Reinforcement Learning from NLE Feedback (RLNF) is a promising approach to mitigate hallucination and implausibility in NLE generation.\n3. The proposed method achieves new state-of-the-art performance on two VL-NLE benchmarks.\n4. The paper provides a comprehensive evaluation of the proposed method, including qualitative and quantitative analysis.\n\n$$\\text{Weaknesses:}$$\n1. The paper may benefit from a more detailed explanation of the knowledge distillation process and its effect on NLE generation.\n2. The use of RLNF may be sensitive to hyperparameter tuning, and the paper could provide more insights into the hyperparameter optimization process.\n3. The paper does not provide a detailed comparison with other state-of-the-art methods in terms of their strengths and weaknesses.\n4. The qualitative evaluation section could be more detailed, with more examples and analysis of the generated NLEs.\n\n$$\\text{Suggestions for Improvement:}$$\n1. Provide a more detailed explanation of the knowledge distillation process and its effect on NLE generation.\n2. Include a more comprehensive comparison with other state-of-the-art methods in terms of their strengths and weaknesses.\n3. Conduct a more thorough analysis of the hyperparameter optimization process for RLNF.\n4. Include more qualitative evaluation examples and analysis to demonstrate the effectiveness of Rapper.\n\n$$\\text{Recommendation:}$$\nBased on the strengths and weaknesses of the paper, I recommend that it be published in a top-tier conference in NLP or VL. The paper presents a novel and effective approach to NLE generation in VQA, and its results demonstrate a significant improvement over state-of-the-art methods. With some revisions to address the weaknesses and suggestions for improvement, the paper has the potential to make a significant impact in the field.",
        "peer_review_with_watermark": "$$\n\\text{Paper Summary: }\n$$\nThe paper proposes Rapper, a two-stage Reinforced Rationale-Prompted Paradigm for enabling Natural Language Explanation (NLE) in Visual Question Answering (VQA) tasks. The first stage uses knowledge distillation to infuse LLMs with rationale-prompting, encouraging NLE to be factual and plausible. The second stage employs Reinforcement Learning from NLE Feedback (RLNF) to inject visual facts into NLE generation, allowing faithful NLE. The proposed method achieves new state-of-the-art performance on two VL-NLE benchmarks and demonstrates reduced implausibility and hallucination.\n\n$$\n\\text{Strengths: }\n$$\n1. The proposed method tackles the challenging issues of implausibility and hallucination in NLE, providing a solution for VL-NLE tasks.\n2. Rapper uniquely distills language-based knowledge from LLM and utilizes RL with natural language feedback from the VQA task, enabling the production of rationales with desirable properties.\n3. The method achieves new state-of-the-art performance on two VL-NLE benchmarks, demonstrating its effectiveness.\n\n$$\n\\text{Weaknesses: }\n$$\n1. The paper may benefit from more comprehensive evaluation of the proposed method on other VL-NLE benchmarks.\n2. Some of the experimental results, such as those in Table 2, may be unclear due to formatting issues.\n3. The paper could provide more detailed explanations of the RLNF algorithm and its components.\n\n$$\n\\text{Suggestions for Improvement: }\n$$\n1. Consider additional VL-NLE benchmarks to evaluate the proposed method.\n2. Improve the formatting of experimental results to ensure clarity.\n3. Provide more detailed explanations of the RLNF algorithm and its components.\n\n$$\n\\text{Recommendation: }\n$$\nThe proposed method, Rapper, is a significant contribution to the VL-NLE field, addressing the challenges of implausibility and hallucination in NLE. With some improvements in evaluation and presentation, Rapper has the potential to become a state-of-the-art method for VL-NLE tasks.",
        "gr_score_with": 0.9632107023411371,
        "gr_score_without": 0.8930635838150289,
        "output_without": {
            "num_tokens_scored": 346,
            "num_green_tokens": 309,
            "green_fraction": 0.8930635838150289,
            "z_score": 18.72127214770717,
            "p_value": 1.6606179145741186e-78
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 288,
            "green_fraction": 0.9632107023411371,
            "z_score": 19.879290131760182,
            "p_value": 3.0748878983785664e-88
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\\boxed{Paper Summary:}\nThe paper proposes a novel NeRF-based approach called MovingParts for dynamic scene reconstruction and part discovery from monocular input. The approach uses a hybrid representation of feature volume and neural network to achieve both high-quality reconstruction and reasonable part discovery within 30 minutes. The method consists of three modules: a canonical module, an Eulerian module, and a Lagrangian module. The Eulerian module records the motion of particles in the scene, while the Lagrangian module tracks the trajectory of particles and discovers parts by factorizing the scene motion as a composition of part-level rigid motions. The approach is evaluated on the D-NeRF 360\u25e6 synthetic dataset and achieves state-of-the-art results.\n\n$\\boxed{Strengths:}\n1. The approach uses a novel hybrid representation of feature volume and neural network to achieve both high-quality reconstruction and reasonable part discovery within 30 minutes.\n2. The method consists of three modules: a canonical module, an Eulerian module, and a Lagrangian module, which enables the discovery of parts by factorizing the scene motion as a composition of part-level rigid motions.\n3. The approach is evaluated on the D-NeRF 360\u25e6 synthetic dataset and achieves state-of-the-art results.\n4. The method is capable of overlooking motion-irrelevant characteristics in geometry and appearance and producing clean part discovery results on realistic complex scenes.\n\n$\\boxed{Weaknesses:}\n1. The approach explicitly stores the motion features in low-dimensional vectors, which makes it challenging to model motion on very long sequences.\n2. The method may not be suitable for scenes with complex motion patterns or long sequences.\n3. The approach relies on a cycle-consistency loss, which may not be effective for all types of scenes.\n4. The method may require additional post-processing to merge the over-segmented groups into a reasonable number of rigid parts.\n\n$\\boxed{Suggestions for Improvement:}\n1. Develop a unified long sequence encoding-decoding scheme to model motion on very long sequences.\n2. Improve the cycle-consistency loss to handle complex motion patterns and long sequences.\n3. Investigate the use of other loss functions or regularization techniques to improve the performance of the approach.\n4. Develop more efficient algorithms for group merging and part discovery.\n\n$\\boxed{Recommendation:}\nThe approach proposed in this paper is a significant contribution to the field of dynamic scene reconstruction and part discovery. The use of a hybrid representation of feature volume and neural network, combined with the Eulerian and Lagrangian modules, enables the discovery of parts by factorizing the scene motion as a composition of part-level rigid motions. The approach is evaluated on the D-NeRF 360\u25e6 synthetic dataset and achieves state-of-the-art results. However, there are some limitations and weaknesses of the approach, such as the explicit storage of motion features, the potential for over-segmentation, and the reliance on a cycle-consistency loss. To improve the approach, it is recommended to develop a unified long sequence encoding-decoding scheme, improve the cycle-consistency loss, and investigate the use of other loss functions or regularization techniques.",
        "peer_review_with_watermark": "$$$\\boxed{Summary:}\nThe paper proposes MovingParts, a novel NeRF-based method for dynamic scene reconstruction and part discovery from monocular image sequences. The method uses a hybrid representation of feature volume and neural network to achieve both high-quality reconstruction and reasonable part discovery within 30 minutes. The approach is inspired by fluid simulation and models motion using both Eulerian and Lagrangian views. The method achieves fast and high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n\nStrengths:\n\n1.  The proposed method is able to achieve fast and high-quality dynamic scene reconstruction and realistic rendering results.\n2.  The approach is inspired by fluid simulation and models motion using both Eulerian and Lagrangian views, which makes it robust to different types of scenes.\n3.  The method is able to discover parts in the scene, which is not achieved by previous NeRF-based methods.\n4.  The approach is able to achieve part-level representation and allow for direct downstream applications such as part tracking, object control, and scene editing.\n\nWeaknesses:\n\n1.  The method is limited to static canonical space, which may not be able to capture complex motion patterns.\n2.  The motion modeling at a specific location can be considered as a sequence decoding task, which makes it challenging to model motion on very long sequences.\n3.  The method does not provide a unified long sequence encoding-decoding scheme, which makes it challenging to model motion on very long sequences.\n\nSuggestions for Improvement:\n\n1.  To improve the robustness of the method, we can add more complex motion patterns to the static canonical space.\n2.  To improve the performance of the method on long sequences, we can implement a unified long sequence encoding-decoding scheme.\n3.  To improve the part discovery performance, we can add more features to the motion grouping module.\n\nRecommendation:\n\n1.  The proposed method is a significant contribution to the field of dynamic scene reconstruction and part discovery.\n2.  The approach is able to achieve fast and high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n3.  The method is able to discover parts in the scene, which is not achieved by previous NeRF-based methods.\n4.  The approach is able to achieve part-level representation and allow for direct downstream applications such as part tracking, object control, and scene editing.\n\n$\\boxed{Final Review:}",
        "gr_score_with": 0.9739776951672863,
        "gr_score_without": 0.9486486486486486,
        "output_without": {
            "num_tokens_scored": 370,
            "num_green_tokens": 351,
            "green_fraction": 0.9486486486486486,
            "z_score": 21.542175268883952,
            "p_value": 3.134633782617388e-103
        },
        "output_with": {
            "num_tokens_scored": 269,
            "num_green_tokens": 262,
            "green_fraction": 0.9739776951672863,
            "z_score": 19.216112611320582,
            "p_value": 1.356771512002617e-82
        }
    }
]