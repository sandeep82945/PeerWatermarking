[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. The CMA is designed to reason over a dataset and its associated data-generating process, and it is evaluated on a number of benchmarks and a real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). The experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery.\n\nStrengths:\n\n* The paper presents a novel and innovative approach to causal discovery that combines the strengths of LLMs and DSCMs.\n* The CMA is evaluated on a range of benchmarks and a real-world task, demonstrating its effectiveness in different scenarios.\n* The paper provides a clear and well-structured explanation of the CMA framework and its components.\n* The authors provide a detailed analysis of the results and insights gained from the experiments.\n\nWeaknesses:\n\n* The paper assumes that the causal graph is known a priori, which may not be the case in real-world scenarios.\n* The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. While the authors extend the framework to include chain graph elements, human intervention is necessary to allow appropriate Deep Chain Graph modelling.\n* The paper does not provide a clear explanation of how the LLMs are used to produce hypotheses and how they are integrated with the DSCMs.\n* The paper assumes that the metadata is accurate and complete, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of how the LLMs are used to produce hypotheses and how they are integrated with the DSCMs.\n* The authors should investigate techniques to enable fully automated chain graph modelling and extend deep SCMs to represent more flexible, non-Markovian causal graphs.\n* The authors should provide a more detailed analysis of the limitations of the CMA and how they can be addressed in future work.\n* The authors should provide a more detailed explanation of the results and insights gained from the experiments, including a more detailed analysis of the causal relationships between variables.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to causal discovery that combines the strengths of LLMs and DSCMs. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the CMA framework and its components, and the experimental results demonstrate its effectiveness in different scenarios. I recommend that the paper be accepted for publication, with the suggestion that the authors address the areas for improvement mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for the task of causal discovery. The CMA synergizes metadata-based reasoning capabilities of LLMs with data-driven modelling of DSCMs to reason over a dataset and its associated data-generating process. The framework is evaluated on several benchmarks and a real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). The experimental results show that the CMA outperforms previous purely data-driven or metadata-driven approaches to causal discovery.\n\nStrengths:\n1. The paper proposes a novel and innovative framework that combines LLMs with DSCMs for causal discovery, which is a timely and important area of research.\n2. The experimental results are impressive, showing that the CMA outperforms previous approaches on several benchmarks and a real-world task.\n3. The paper provides a clear and detailed description of the CMA framework, including its architecture, components, and implementation details.\n4. The authors provide a thorough analysis of the experimental results, including a comparison of the CMA with other approaches and a discussion of the implications of the findings.\n\nWeaknesses:\n1. The paper assumes a Markovian DAG, which may not be realistic in all cases. While the authors extend the framework to include chain graph elements, they found that LLMs struggle to represent chains in a fully automated manner.\n2. The paper does not provide a thorough analysis of the computational resources required to run the CMA, which may be a limitation in practice.\n3. The paper assumes that the data is available in a format that can be easily processed by the CMA, which may not always be the case in practice.\n4. The paper does not provide a clear discussion of the limitations of the CMA and potential avenues for future research.\n\nSuggestions for Improvement:\n1. Investigate techniques to enable fully automated chain graph modelling, which would allow the CMA to handle more complex causal structures.\n2. Extend the framework to represent more flexible, non-Markovian causal graphs, which would allow the CMA to handle feedback loops and other non-Markovian structures.\n3. Provide a more thorough analysis of the computational resources required to run the CMA, which would allow users to better estimate the resources required to run the framework in practice.\n4. Provide a more detailed discussion of the limitations of the CMA and potential avenues for future research.\n\nRecommendation:\nThe paper is a significant contribution to the field of causal discovery, proposing a novel and innovative framework that combines LLMs with DSCMs. The experimental results are impressive, and the paper provides a clear and detailed description of the CMA framework. While there are some limitations to the paper, including the assumption of a Markovian DAG and the lack of a thorough analysis of computational resources, the paper is a valuable contribution to the field and provides a good starting point for future research.\n\nRating:\nI would rate this paper a 9 out of 10. The paper is well-written, well-organized, and provides a significant contribution to the field of causal discovery. While there are some limitations to the paper, the experimental results are impressive, and the paper provides a clear and detailed description of the CMA framework.\n\nReviewer:\nA Research Scientist in the field of Artificial Intelligence and Machine Learning.",
        "gr_score_with": 0.42992874109263657,
        "gr_score_without": 0.3539325842696629,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 126,
            "green_fraction": 0.3539325842696629,
            "z_score": -1.7742435218724542,
            "p_value": 0.9619885609446323
        },
        "output_with": {
            "num_tokens_scored": 421,
            "num_green_tokens": 181,
            "green_fraction": 0.42992874109263657,
            "z_score": 1.2534986666779582,
            "p_value": 0.10501214200449327
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a framework for learning language-informed visual concepts from images by distilling pre-trained vision-language models. The framework consists of a set of concept encoders that extract visual concept embeddings from images along various concept axes specified by language. The concept embeddings are trained to be disentangled and compositional, allowing for flexible composition of concepts into high-quality image generations. The framework is evaluated on a range of tasks, including visual concept extraction, recomposition, and extrapolation, as well as text-based visual concept editing.\n\nStrengths:\n\n1. The paper proposes a novel framework for learning language-informed visual concepts from images, which is a significant contribution to the field of computer vision and artificial intelligence.\n2. The framework is evaluated on a range of tasks, including visual concept extraction, recomposition, and extrapolation, as well as text-based visual concept editing, which demonstrates its versatility and effectiveness.\n3. The paper provides a thorough evaluation of the framework, including quantitative and qualitative results, which provides a comprehensive understanding of its strengths and limitations.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained vision-language models, which may limit its generalizability to other domains and tasks.\n2. The framework assumes that the concept axes are specified by language, which may not always be the case in real-world applications.\n3. The paper does not provide a clear explanation of how the concept embeddings are disentangled and compositional, which may make it difficult for readers to understand the framework.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the concept embeddings are disentangled and compositional, including the mathematical formulation and the role of the text anchoring loss.\n2. The authors should provide more examples of how the framework can be applied to real-world applications, such as image editing and generation.\n3. The authors should provide a more thorough evaluation of the framework, including a comparison with other state-of-the-art methods and a discussion of its limitations and potential future directions.\n\nRecommendation:\n\nI recommend this paper for publication in a top-tier conference in the field of computer vision and artificial intelligence. The paper proposes a novel and effective framework for learning language-informed visual concepts from images, which has the potential to impact a wide range of applications. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more comprehensive and impactful.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a framework for learning language-informed visual concepts from images by distilling pre-trained vision-language models. The approach designs a set of concept encoders to extract visual concepts along language-specified concept axes from images. These encoders are trained using a combination of reconstruction and anchor losses to ensure that the extracted concept embeddings can sufficiently reconstruct the input image and are anchored to discrete text anchors in the concept embedding space. The paper also introduces a lightweight test-time finetuning procedure to adapt the encoders to novel concepts unseen during training. The approach is evaluated on various datasets and compared to prior work, demonstrating its ability to extract disentangled visual concepts and achieve better recomposition results.\n\nStrengths:\n\n* The approach is novel in using language-informed visual concepts for image generation and editing.\n* The use of pre-trained vision-language models as a distillation source is efficient and effective.\n* The introduction of anchor losses to improve disentanglement is a significant contribution.\n* The approach is evaluated thoroughly on various datasets and compared to prior work.\n\nWeaknesses:\n\n* The approach relies on pre-trained vision-language models, which may not be widely available or easily accessible.\n* The training data generation process is limited to synthetic images generated by DeepFloyd, which may not be representative of real-world images.\n* The evaluation metrics used in the paper focus on quantitative measures, but human evaluation is also necessary to assess the quality of generated images.\n* The approach assumes that language descriptions are always available for training, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n* Explore using other distillation sources, such as image captioning datasets or multimodal datasets.\n* Collect a larger and more diverse set of training images to improve the robustness of the approach.\n* Introduce additional evaluation metrics, such as human evaluation or perceptual quality metrics.\n* Investigate the use of language-informed visual concepts for other image manipulation tasks, such as image-to-image translation or image synthesis.\n\nRecommendation:\n\nThis paper is a significant contribution to the field of image generation and editing, particularly in using language-informed visual concepts for image manipulation. The approach is novel, effective, and thoroughly evaluated. However, there are some limitations and areas for improvement, which should be addressed in future work. I recommend this paper for publication in a reputable conference or journal in the field of computer vision.\n\nNote: This is a rewritten version of the original text to fit the specified format.",
        "gr_score_with": 0.4846153846153846,
        "gr_score_without": 0.3302752293577982,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 108,
            "green_fraction": 0.3302752293577982,
            "z_score": -2.573684753618667,
            "p_value": 0.9949689045764993
        },
        "output_with": {
            "num_tokens_scored": 390,
            "num_green_tokens": 189,
            "green_fraction": 0.4846153846153846,
            "z_score": 3.4109552012032327,
            "p_value": 0.00032367863388575545
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a two-stage Reinforced Rationale-Prompted Paradigm (Rapper) for natural language explanation (NLE) in visual question answering (VQA) tasks. The first stage utilizes knowledge distillation from large language models (LLMs) to infuse rationale-prompting and encourage plausible NLE. The second stage employs Reinforcement Learning from NLE Feedback (RLNF) to inject visual facts into NLE generation. The proposed method achieves state-of-the-art performance on two VL-NLE benchmarks, VQA-X and e-SNLI-VE, and demonstrates improved plausibility and faithfulness of NLE.\n\nStrengths:\n\n1. The paper presents a comprehensive approach to tackle the implausibility and hallucination problems in NLE.\n2. The proposed two-stage paradigm is well-motivated and effectively addresses the limitations of existing methods.\n3. The use of knowledge distillation and RLNF is innovative and demonstrates the effectiveness of these techniques in improving NLE.\n4. The paper provides thorough experiments and evaluation metrics, including quantitative and qualitative results, to demonstrate the superiority of Rapper.\n\nWeaknesses:\n\n1. The paper assumes that the ground-truth answer and explanation are available during training, which may not be the case in all scenarios.\n2. The filtering mechanism used in knowledge distillation may not be effective in all cases, and its impact on performance is not thoroughly evaluated.\n3. The paper does not provide a detailed analysis of the RLNF algorithm and its convergence properties.\n4. The qualitative evaluation results, although impressive, are limited to a few examples and may not be representative of the overall performance.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of different LLMs and knowledge distillation techniques on the performance of Rapper.\n2. Evaluate the robustness of Rapper to variations in the input data, such as image quality or question complexity.\n3. Provide a more detailed analysis of the RLNF algorithm, including its convergence properties and the impact of hyperparameters on performance.\n4. Conduct a more thorough qualitative evaluation, including more examples and a systematic comparison with existing methods.\n\nRecommendation:\n\nThe paper presents a well-motivated and effective approach to NLE in VQA tasks. The proposed two-stage paradigm is innovative and demonstrates the superiority of Rapper over existing methods. However, the paper assumes that the ground-truth answer and explanation are available during training, which may not be the case in all scenarios. Additionally, the filtering mechanism used in knowledge distillation may not be effective in all cases, and its impact on performance is not thoroughly evaluated. Nevertheless, the paper provides thorough experiments and evaluation metrics, and the qualitative evaluation results are impressive. Therefore, I recommend accepting the paper with minor revisions to address the mentioned weaknesses.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm for generating accurate and faithful Natural Language Explanations (NLEs) in visual question answering (VQA). The proposed method leverages knowledge distillation from Large Language Models (LLMs) and Reinforcement Learning from NLE Feedback (RLNF) to produce rationales that are both plausible and faithful. RAPPER consists of a rationale generator and a reasoning module, which are trained using a novel reward function that encourages the generation of rationales that are aligned with both visual and language knowledge. The proposed method achieves state-of-the-art performance on two benchmark datasets and demonstrates superior performance in terms of plausibility and faithfulness of NLEs.\n\nStrengths:\n\n* The proposed method leverages knowledge distillation from LLMs, which provides a novel and effective way to infuse rationales with supporting facts.\n* The use of RLNF provides a robust way to enforce the generation of rationales that are aligned with both visual and language knowledge.\n* The proposed method demonstrates superior performance in terms of plausibility and faithfulness of NLEs compared to state-of-the-art methods.\n* The method is evaluated on two benchmark datasets and demonstrates robust performance.\n\nWeaknesses:\n\n* The proposed method requires a significant amount of computational resources and training data.\n* The method assumes that the LLMs used for knowledge distillation are available and well-performing.\n* The method may not generalize well to other tasks or domains.\n* The proposed method requires a significant amount of human effort to collect and label data.\n\nSuggestions for Improvement:\n\n* Investigate the use of other types of knowledge distillation methods, such as knowledge transfer or knowledge fusion.\n* Explore the use of other types of reinforcement learning algorithms, such as Q-learning or SARSA.\n* Investigate the use of other types of reward functions, such as those that incorporate user feedback or domain knowledge.\n* Develop a method for transferring knowledge from one domain to another.\n\nRecommendation:\n\nThe proposed method demonstrates significant potential for generating accurate and faithful NLEs in VQA tasks. However, the method requires significant computational resources and training data, and may not generalize well to other tasks or domains. Therefore, I recommend that the authors:\n\n* Investigate methods for reducing the computational resources and training data required by the proposed method.\n* Explore methods for generalizing the proposed method to other tasks and domains.\n* Develop a method for transferring knowledge from one domain to another.\n* Evaluate the proposed method on a wider range of benchmark datasets and tasks.\n\nOverall, I believe that the proposed method demonstrates significant potential and deserves further investigation and development.\n\n$$$",
        "gr_score_with": 0.5573770491803278,
        "gr_score_without": 0.3869346733668342,
        "output_without": {
            "num_tokens_scored": 398,
            "num_green_tokens": 154,
            "green_fraction": 0.3869346733668342,
            "z_score": -0.5320545808955395,
            "p_value": 0.7026559033243204
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 204,
            "green_fraction": 0.5573770491803278,
            "z_score": 6.145770236779006,
            "p_value": 3.9788202339009984e-10
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. The authors propose a hybrid approach that learns motion under both the Eulerian and Lagrangian views, enabling the discovery of rigid parts in dynamic scenes. The method consists of three modules: a canonical module, an Eulerian module, and a Lagrangian module. The Eulerian module records the motion of particles passing through a given coordinate, while the Lagrangian module tracks the trajectories of specific object particles over time. The authors use a cycle-consistency loss to enforce the consistency between the two modules. They also introduce a group merging module to adaptively merge the over-segmented groups into a reasonable number of rigid parts.\n\nStrengths:\n\n* The authors provide a thorough explanation of the method and its components, making it easy to understand the approach.\n* The use of both Eulerian and Lagrangian views is a novel and effective way to model motion in dynamic scenes.\n* The cycle-consistency loss is a good way to enforce the consistency between the two modules.\n* The group merging module is an efficient way to adaptively merge the over-segmented groups.\n\nWeaknesses:\n\n* The method requires a large amount of training data, which can be a limitation in practice.\n* The authors assume that the scene is piece-wise rigid, which may not hold in all cases.\n* The method may not perform well in scenes with complex geometry or topology.\n* The authors use a hybrid feature volume and neural network representation, which can be computationally expensive.\n\nSuggestions for Improvement:\n\n* The authors can investigate the use of other representations, such as graph neural networks or attention mechanisms, to model the motion in dynamic scenes.\n* They can also explore the use of other losses, such as adversarial losses or reconstruction losses, to improve the performance of the method.\n* The authors can investigate the use of other datasets, such as the KITTI dataset, to evaluate the performance of the method in more realistic scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective method for dynamic scene reconstruction and part discovery. The use of both Eulerian and Lagrangian views is a good way to model motion in dynamic scenes, and the cycle-consistency loss is a good way to enforce the consistency between the two modules. However, the method requires a large amount of training data, and the authors assume that the scene is piece-wise rigid. The authors can investigate the use of other representations and losses to improve the performance of the method. I recommend this paper for publication in a reputable journal or conference, with the expectation that the authors will address the limitations and suggestions for improvement.\n\nFinal Verdict:\n\nThe paper presents a novel and effective method for dynamic scene reconstruction and part discovery. While the method requires a large amount of training data and assumes that the scene is piece-wise rigid, the use of both Eulerian and Lagrangian views is a good way to model motion in dynamic scenes. I recommend this paper for publication, with the expectation that the authors will address the limitations and suggestions for improvement.\n\nScore: 7.5/10\n\nReviewing Time: 2 hours\n\nReviewer's Name: [Your Name]\n\nReviewer's Affiliation: [Your Affiliation]\n\nDate: [Current Date]\n\n$$$",
        "gr_score_with": 0.5284974093264249,
        "gr_score_without": 0.3724137931034483,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 108,
            "green_fraction": 0.3724137931034483,
            "z_score": -0.9589266029707684,
            "p_value": 0.8312021390779591
        },
        "output_with": {
            "num_tokens_scored": 386,
            "num_green_tokens": 204,
            "green_fraction": 0.5284974093264249,
            "z_score": 5.153264136524973,
            "p_value": 1.2799549655500505e-07
        }
    },
    {
        "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
        "abstract": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, called Diffusion Model-Based Predictor (DMBP), to enhance the robustness of offline reinforcement learning (RL) algorithms against state observation perturbations. The proposed approach utilizes conditional diffusion models to recover the actual states from perturbed observations, which is fundamentally different from existing online robust RL methods. The framework consists of a diffusion model-based predictor that estimates the current state by reversely denoising the perturbed state, and a non-Markovian loss function that minimizes the sum entropy of denoised states over the RL trajectory to mitigate error accumulation. The authors evaluate the proposed framework on D4RL Gym benchmark and demonstrate its effectiveness in enhancing the robustness of existing offline RL algorithms against different types of attacks on state observations.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to enhancing the robustness of offline RL algorithms against state observation perturbations.\n2. The proposed framework is based on conditional diffusion models, which have been widely used in image generation and other applications.\n3. The non-Markovian loss function is a novel and effective way to mitigate error accumulation during testing.\n4. The experiments on D4RL Gym benchmark demonstrate the effectiveness of the proposed framework in enhancing the robustness of existing offline RL algorithms.\n\nWeaknesses:\n\n1. The paper assumes that the perturbed state observations are Gaussian noises, which may not be realistic in many real-world scenarios.\n2. The proposed framework may not be effective against more complex types of perturbations, such as adversarial attacks.\n3. The non-Markovian loss function may be computationally expensive to compute, especially for large RL trajectories.\n4. The paper does not provide a detailed analysis of the theoretical guarantees of the proposed framework.\n\nSuggestions for Improvement:\n\n1. The authors should investigate more realistic types of perturbations, such as non-Gaussian noises or adversarial attacks, and evaluate the effectiveness of the proposed framework against these types of perturbations.\n2. The authors should provide a more detailed analysis of the computational complexity of the non-Markovian loss function and explore ways to reduce it.\n3. The authors should provide a more detailed analysis of the theoretical guarantees of the proposed framework, such as convergence guarantees and error bounds.\n4. The authors should evaluate the proposed framework on more challenging RL tasks and environments to demonstrate its effectiveness in a wider range of scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to enhancing the robustness of offline RL algorithms against state observation perturbations. The proposed framework is based on conditional diffusion models, which have been widely used in image generation and other applications. The non-Markovian loss function is a novel and effective way to mitigate error accumulation during testing. The experiments on D4RL Gym benchmark demonstrate the effectiveness of the proposed framework in enhancing the robustness of existing offline RL algorithms. However, the paper assumes that the perturbed state observations are Gaussian noises, which may not be realistic in many real-world scenarios. The authors should investigate more realistic types of perturbations and evaluate the effectiveness of the proposed framework against these types of perturbations.\n\nOverall, the paper is well-written, and the proposed framework is well-motivated and effective. However, the authors should address the weaknesses and suggestions for improvement to make the paper more comprehensive and impactful.\n\nRecommendation: Accept.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework called Diffusion Model-Based Predictor (DMBP) for robust offline reinforcement learning (RL) against state observation perturbations in state-based tasks. DMBP leverages conditional diffusion models to predict the actual state of the environment, recovering the true state from noisy observations. To mitigate error accumulation in the denoising process, the authors propose a non-Markovian loss function that minimizes the sum entropy of denoised states along the RL trajectory. The proposed framework is evaluated on the D4RL benchmark and demonstrates significant improvements in robustness against various noise scales and adversarial attacks. Furthermore, DMBP can effectively deal with incomplete state observations with unobserved dimensions.\n\nStrengths:\n\n1. The paper addresses a significant problem in offline RL, robustness against state observation perturbations.\n2. The proposed framework, DMBP, leverages conditional diffusion models for state prediction, which is a novel and effective approach.\n3. The non-Markovian loss function proposed in this paper is a significant contribution, as it mitigates error accumulation in the denoising process.\n4. The experimental results demonstrate the effectiveness of DMBP in improving robustness against various noise scales and adversarial attacks.\n5. The paper provides a clear explanation of the proposed framework and its components, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the noise in the observation is independent and identically distributed (i.i.d.), which might not hold in real-world scenarios.\n2. The proposed framework relies heavily on the quality of the diffusion model, and its performance might degrade if the model is not well-trained.\n3. The non-Markovian loss function might require a large amount of computational resources to train, which can be a limitation for real-world applications.\n4. The paper does not provide a comprehensive comparison with other robust RL methods, which makes it difficult to evaluate the novelty and effectiveness of the proposed framework.\n5. The ablation study in the paper is limited, and more experiments are needed to fully understand the impact of the non-Markovian loss function and the diffusion model on the performance of DMBP.\n\nSuggestions for Improvement:\n\n1. Consider incorporating more robustness metrics in the evaluation of DMBP, such as the mean absolute error (MAE) or the mean squared error (MSE).\n2. Provide a more comprehensive comparison with other robust RL methods, such as robust MDPs or robust Markov decision processes.\n3. Conduct more experiments to evaluate the impact of the non-Markovian loss function and the diffusion model on the performance of DMBP.\n4. Consider incorporating more real-world datasets and environments in the evaluation of DMBP.\n5. Provide a clear explanation of the limitations of the proposed framework and the assumptions made in the paper.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of offline RL by proposing a novel framework, DMBP, for robustness against state observation perturbations. The experimental results demonstrate the effectiveness of DMBP in improving robustness against various noise scales and adversarial attacks. However, the paper has some limitations, such as the assumption of i.i.d. noise and the reliance on the quality of the diffusion model. To fully evaluate the novelty and effectiveness of the proposed framework, more experiments and comparisons with other robust RL methods are needed. I recommend this paper for publication in a reputable conference or journal in the field of RL.\n$$$",
        "gr_score_with": 0.45594713656387664,
        "gr_score_without": 0.33611111111111114,
        "output_without": {
            "num_tokens_scored": 360,
            "num_green_tokens": 121,
            "green_fraction": 0.33611111111111114,
            "z_score": -2.4744060267436274,
            "p_value": 0.9933271037081537
        },
        "output_with": {
            "num_tokens_scored": 454,
            "num_green_tokens": 207,
            "green_fraction": 0.45594713656387664,
            "z_score": 2.4333252871349256,
            "p_value": 0.007480426806738095
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven approach, RDesign, for designing RNA sequences that fold into specified tertiary structures. The authors address the challenges of RNA design by leveraging a hierarchical data-efficient representation learning framework and a comprehensive structural modeling approach. The proposed method utilizes a well-curated benchmark dataset and incorporates extracted secondary structures with base pairs as prior knowledge. The authors demonstrate the effectiveness of RDesign through extensive experiments, which outperforms existing baseline models on the recovery and Macro-F1 metrics.\n\nStrengths:\n\n1. Comprehensive approach: The authors provide a thorough treatment of the RNA design problem, addressing the challenges of data scarcity and structural complexity.\n2. Novel framework: The proposed hierarchical data-efficient representation learning framework is a significant contribution, enabling the explicit capture of intrinsic relationships within the data.\n3. Effective results: The authors demonstrate the efficacy of RDesign through extensive experiments, which outperform existing baseline models.\n4. Robustness: RDesign is shown to be robust across different RNA lengths and datasets, including Rfam and RNA-Puzzles.\n\nWeaknesses:\n\n1. Limited validation: The authors only provide qualitative results for the designed sequences' ability to fold into desired tertiary structures.\n2. Over-reliance on pre-trained models: The results reported in the paper rely heavily on pre-trained models, which may not generalize well to new datasets.\n3. Lack of explanation: The authors do not provide a clear explanation of the RDesign framework, making it difficult for readers to understand the underlying mechanics.\n4. Limited generalizability: The results reported in the paper are limited to a specific dataset and may not generalize well to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the RDesign framework, including the hierarchical data-efficient representation learning and the comprehensive structural modeling approach.\n2. Validate the designed sequences' ability to fold into desired tertiary structures using more rigorous and quantitative methods.\n3. Explore the generalizability of RDesign to other RNA design tasks and datasets.\n4. Investigate the robustness of RDesign to noise and outliers in the data.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of RNA design, proposing a novel and effective approach that addresses the challenges of data scarcity and structural complexity. While there are areas for improvement, the authors provide a comprehensive treatment of the problem and demonstrate the efficacy of RDesign through extensive experiments. I recommend this paper for publication in a reputable journal, with the understanding that the authors should address the limitations and suggestions for improvement mentioned above.",
        "gr_score_with": 0.6108108108108108,
        "gr_score_without": 0.4967741935483871,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 154,
            "green_fraction": 0.4967741935483871,
            "z_score": 3.478041718201262,
            "p_value": 0.00025254565399276333
        },
        "output_with": {
            "num_tokens_scored": 370,
            "num_green_tokens": 226,
            "green_fraction": 0.6108108108108108,
            "z_score": 8.277289019571173,
            "p_value": 6.300556118057128e-17
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning, called Sparse Entropic Wasserstein Regression (SWAP), which utilizes Entropic Wasserstein regression to mitigate noisy gradients in computing the empirical Fisher Information Matrix. The proposed method, SWAP, balances noise reduction and covariance information preservation effectively, outperforming state-of-the-art network pruning algorithms, especially when the network size or target sparsity is large, or when noisy gradients are present.\n\nStrengths:\n\n1. The proposed method, SWAP, is robust to noisy gradients, which is a significant problem in large-scale model compression.\n2. SWAP balances noise reduction and covariance information preservation effectively, outperforming state-of-the-art network pruning algorithms.\n3. The method is computationally efficient, with marginal additional computational cost compared to traditional methods.\n4. The paper provides a comprehensive theoretical analysis of the proposed method, including its geometry properties, neighborhood interpolation mechanism, and sample complexity.\n\nWeaknesses:\n\n1. The method requires careful tuning of hyperparameters, such as the regularization parameter \u03bb and the entropic regularization term \u03b5.\n2. The method may not perform well on extremely large-scale models, as it requires significant computational resources.\n3. The paper does not provide a comprehensive analysis of the method's performance on various types of neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs).\n4. The paper does not provide a detailed analysis of the method's performance on specific applications, such as image classification or natural language processing.\n\nSuggestions for Improvement:\n\n1. Provide a more comprehensive analysis of the method's performance on various types of neural networks, including CNNs and RNNs.\n2. Provide a detailed analysis of the method's performance on specific applications, such as image classification or natural language processing.\n3. Investigate the use of more advanced hyperparameter tuning methods, such as Bayesian optimization or gradient-based methods.\n4. Investigate the use of more advanced techniques, such as transfer learning or multi-task learning, to improve the method's performance on large-scale models.\n\nRecommendation:\n\nThe proposed method, SWAP, is a significant contribution to the field of neural network pruning, as it provides a robust and computationally efficient approach to mitigate noisy gradients. While there are some limitations to the method, such as the requirement for careful hyperparameter tuning, the method shows promising results on various benchmarks. Therefore, I recommend that the authors continue to develop and refine the method, and consider exploring more advanced techniques to improve its performance on large-scale models.\n\nOverall, I believe that the paper is well-written, well-organized, and provides a significant contribution to the field of neural network pruning. However, I also believe that there is room for improvement, particularly in terms of providing a more comprehensive analysis of the method's performance on various types of neural networks and specific applications.",
        "gr_score_with": 0.5013477088948787,
        "gr_score_without": 0.398876404494382,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 142,
            "green_fraction": 0.398876404494382,
            "z_score": -0.04327423224079216,
            "p_value": 0.517258534165997
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 186,
            "green_fraction": 0.5013477088948787,
            "z_score": 3.984696691953394,
            "p_value": 3.378318587876657e-05
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThis paper presents DrM, a novel visual reinforcement learning (RL) algorithm that tackles the challenges of visual RL in continuous control tasks. The authors identify a shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. To address this issue, they propose a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. The dormant ratio is a metric that measures inactivity in the RL agent's network. DrM is shown to achieve significant improvements in sample efficiency and asymptotic performance in various continuous control benchmark environments. It is also demonstrated that DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite and dexterous hand manipulation tasks in Adroit without demonstrations.\n\nStrengths:\n\n1. The paper presents a novel and effective approach to addressing the challenges of visual RL in continuous control tasks.\n2. The authors provide a clear and concise explanation of the proposed method and its underlying mechanisms.\n3. The experiments are well-designed and demonstrate the effectiveness of DrM in various benchmark environments.\n4. The paper provides a detailed analysis of the dormant ratio and its correlation with agent behavior, which is a valuable contribution to the understanding of visual RL.\n\nWeaknesses:\n\n1. The paper relies heavily on empirical results, and there is a lack of theoretical analysis of the proposed method.\n2. The authors do not provide a clear explanation of how the dormant ratio is calculated and how it is used to guide exploration-exploitation trade-offs.\n3. The paper assumes that the dormant ratio is a reliable metric for measuring inactivity in the RL agent's network, which may not be the case in all scenarios.\n4. The authors do not provide a detailed comparison of DrM with other existing visual RL algorithms, which makes it difficult to evaluate its relative performance.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed theoretical analysis of the proposed method and its underlying mechanisms.\n2. Clarify the calculation of the dormant ratio and how it is used to guide exploration-exploitation trade-offs.\n3. Conduct more experiments to evaluate the robustness of DrM in various scenarios and environments.\n4. Compare DrM with other existing visual RL algorithms and provide a detailed analysis of its relative performance.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to addressing the challenges of visual RL in continuous control tasks. While there are some weaknesses and areas for improvement, the authors provide a clear and concise explanation of the proposed method and its underlying mechanisms. The experiments are well-designed and demonstrate the effectiveness of DrM in various benchmark environments. I recommend that the paper be published in a top-tier conference in the field of RL, subject to revisions that address the suggestions provided above.\n\n$$$",
        "gr_score_with": 0.5040214477211796,
        "gr_score_without": 0.3699731903485255,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 138,
            "green_fraction": 0.3699731903485255,
            "z_score": -1.183744532012214,
            "p_value": 0.8817428974130385
        },
        "output_with": {
            "num_tokens_scored": 373,
            "num_green_tokens": 188,
            "green_fraction": 0.5040214477211796,
            "z_score": 4.100829271613733,
            "p_value": 2.058361045819475e-05
        }
    },
    {
        "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
        "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents a novel method for dynamically reducing the number of trainable parameters in deep transformer networks using Reinforcement Learning (RL). The method, called dynamic layer tying, involves training a primary network T and a driver network Q simultaneously. The driver network Q learns to select layers to tie together, reducing the number of trainable parameters, and serves as an effective regularization technique. Experimental evaluations demonstrate that the model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters, resulting in up to one order of magnitude less memory consumption during training.\n\nStrengths:\n1. The paper presents a novel and innovative approach to reducing the number of trainable parameters in deep transformer networks using RL.\n2. The method is demonstrated to be effective in reducing the number of trainable parameters while maintaining or improving the perplexity score.\n3. The paper provides a comprehensive evaluation of the method on multiple datasets and architectures, including GPT-2 and BERT.\n4. The ablation study provides valuable insights into the importance of weight tying and the role of layer 0 in the training process.\n\nWeaknesses:\n1. The paper assumes that the primary network T and the driver network Q are trained simultaneously, which may not be feasible in all scenarios.\n2. The method requires a large number of training steps, which may be computationally expensive.\n3. The paper does not provide a clear explanation of how the driver network Q learns to select layers to tie together.\n4. The paper assumes that the weights of the tied layers are identical, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of how the driver network Q learns to select layers to tie together.\n2. Investigate the possibility of training the primary network T and the driver network Q separately, rather than simultaneously.\n3. Provide more information on the computational cost of the method and how it can be optimized for large-scale applications.\n4. Investigate the possibility of applying the dynamic layer tying method to other types of neural networks, such as convolutional neural networks (CNNs).\n\nRecommendation:\nThe paper presents a novel and innovative approach to reducing the number of trainable parameters in deep transformer networks using RL. The method is demonstrated to be effective in reducing the number of trainable parameters while maintaining or improving the perplexity score. However, the paper assumes that the primary network T and the driver network Q are trained simultaneously, which may not be feasible in all scenarios. Therefore, the paper requires further investigation and optimization to make it more practical and scalable for large-scale applications.\n\nOverall, the paper is well-written, and the authors provide a comprehensive evaluation of the method on multiple datasets and architectures. However, the paper requires further improvement to address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel method for dynamically tying the weights of layers in a transformer architecture during training, which leads to a significant reduction in the number of trainable parameters. The method employs Reinforcement Learning to select layers to be tied together, which facilitates weight sharing, reduces the number of trainable parameters, and serves as an effective regularization technique. The paper demonstrates that the proposed method can achieve a 75% to 87% reduction in the number of parameters while maintaining the same level of accuracy as the full L-layer transformer. The results show that the model can be trained effectively despite rapid changes in the architecture during the training process.\n\nStrengths:\n\n* The paper presents a novel and effective method for dynamically tying the weights of layers in a transformer architecture.\n* The method is based on Reinforcement Learning, which is a powerful technique for finding optimal solutions.\n* The paper demonstrates a significant reduction in the number of trainable parameters while maintaining the same level of accuracy.\n* The results show that the model can be trained effectively despite rapid changes in the architecture during the training process.\n* The paper provides a thorough ablation study to validate the success of the method.\n\nWeaknesses:\n\n* The paper assumes that the transformer architecture is fixed, which may not be the case in real-world scenarios.\n* The paper does not explore the possibility of applying the dynamic weight tying to other neural network architectures.\n* The paper does not provide a clear explanation of why the method is effective, which may make it difficult to generalize to other scenarios.\n* The paper assumes that the Reinforcement Learning algorithm is effective, which may not be the case in all scenarios.\n* The paper does not provide a clear comparison with other state-of-the-art methods for reducing the number of trainable parameters.\n\nSuggestions for Improvement:\n\n* The paper should explore the possibility of applying the dynamic weight tying to other neural network architectures.\n* The paper should provide a clear explanation of why the method is effective, which may involve analyzing the dynamics of the training process.\n* The paper should compare the method with other state-of-the-art methods for reducing the number of trainable parameters.\n* The paper should explore the possibility of applying the dynamic weight tying to other scenarios, such as fine-tuning pre-trained models.\n* The paper should provide a clear explanation of the computational resources required to train the model, which may involve analyzing the memory usage and the number of GPU hours required.\n\nRecommendation:\n\nThe paper presents a novel and effective method for dynamically tying the weights of layers in a transformer architecture. The method is based on Reinforcement Learning, which is a powerful technique for finding optimal solutions. The paper demonstrates a significant reduction in the number of trainable parameters while maintaining the same level of accuracy. The results show that the model can be trained effectively despite rapid changes in the architecture during the training process. The paper provides a thorough ablation study to validate the success of the method. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "gr_score_with": 0.6608391608391608,
        "gr_score_without": 0.5202312138728323,
        "output_without": {
            "num_tokens_scored": 346,
            "num_green_tokens": 180,
            "green_fraction": 0.5202312138728323,
            "z_score": 4.565093325583929,
            "p_value": 2.496360895995461e-06
        },
        "output_with": {
            "num_tokens_scored": 286,
            "num_green_tokens": 189,
            "green_fraction": 0.6608391608391608,
            "z_score": 9.00430484976524,
            "p_value": 1.0851819521876068e-19
        }
    },
    {
        "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
        "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to neural subset selection, focusing on incorporating background information from the superset into the subset from a symmetric perspective. The authors introduce the concept of invariant sufficient representation, which enables the construction of neural networks that learn the relationship between the subset and superset while being invariant to permutations. The proposed method, called INSET, is a probabilistic-driven approach that achieves state-of-the-art performance in various tasks, including product recommendation, set anomaly detection, and compound selection.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the limitations of existing subset encoding methods and proposes a novel approach to address these limitations.\n2. The concept of invariant sufficient representation is well-defined and provides a clear understanding of the problem.\n3. The proposed method, INSET, is a simple yet effective approach that achieves state-of-the-art performance in various tasks.\n4. The paper provides comprehensive empirical evaluations, including comparisons with existing methods and analysis of the performance of INSET in different scenarios.\n\nWeaknesses:\n\n1. The paper assumes that the superset is a nested set of subsets, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of INSET, which may be a concern for large-scale datasets.\n3. The paper assumes that the subset and superset are represented as sets of elements, which may not be the case in all scenarios (e.g., when dealing with graphs or other structured data).\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational cost of INSET and discuss potential optimization techniques to reduce the computational overhead.\n2. Investigate the applicability of INSET to scenarios where the superset is not a nested set of subsets.\n3. Explore the use of INSET in more general scenarios, such as graph-based tasks or other structured data.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to neural subset selection, and the proposed method, INSET, achieves state-of-the-art performance in various tasks. The paper is well-written and provides a clear understanding of the problem and the proposed solution. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to neural subset selection, focusing on the aggregation of background information from the superset into the subset from a symmetric perspective. The authors address the limitation of existing methods, which overlook the valuable information contained within the superset when using neural networks to model set functions. They introduce a probabilistic perspective, demonstrating the importance of incorporating an invariant sufficient statistic of the superset into the subset of interest for effective learning. The proposed model, INSET (Invariant Representation of Subsets), incorporates an information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of INSET over conventional methods.\n\nStrengths:\n\n1. The paper addresses a significant limitation in subset encoding methods, where the output is either the subset itself or a function value associated with the subset.\n2. The authors provide a thorough theoretical analysis, demonstrating the importance of incorporating an invariant sufficient statistic of the superset into the subset of interest for effective learning.\n3. The proposed model, INSET, is a simple yet effective approach for neural subset selection, consistently outperforming previous methods according to empirical evaluations.\n4. The paper provides a clear and concise overview of the methodology, including the information aggregation module designed to merge the representations of subsets and supersets.\n\nWeaknesses:\n\n1. The paper assumes a high level of mathematical knowledge, making it challenging for readers without a strong background in probability theory and measure theory.\n2. The experimental section relies on a limited number of datasets, which may not be representative of the broader range of possible applications.\n3. The authors do not provide a clear explanation of the choice of hyperparameters and the impact of hyperparameter selection on the performance of INSET.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the mathematical concepts and notation used in the paper, making it more accessible to a wider audience.\n2. Expand the experimental section to include a more diverse range of datasets and tasks, demonstrating the versatility of INSET.\n3. Include a thorough analysis of the impact of hyperparameter selection on the performance of INSET, providing guidance on the optimal choice of hyperparameters.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to neural subset selection, addressing a significant limitation in existing methods. The thorough theoretical analysis and clear methodology make it a valuable contribution to the field. However, the paper's complexity and limited experimental section may hinder its accessibility to a broader audience. With further development and refinement, INSET has the potential to become a leading method in neural subset selection.\n\nRating: 8/10\n\nRecommendation for publication: Yes",
        "gr_score_with": 0.48860759493670886,
        "gr_score_without": 0.4,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 122,
            "green_fraction": 0.4,
            "z_score": 0.0,
            "p_value": 0.5
        },
        "output_with": {
            "num_tokens_scored": 395,
            "num_green_tokens": 193,
            "green_fraction": 0.48860759493670886,
            "z_score": 3.594710131327705,
            "p_value": 0.00016237647308981425
        }
    },
    {
        "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
        "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel batch normalization method for Spiking Neural Networks (SNNs) called Temporal Accumulated Batch Normalization (TAB). The method addresses the Temporal Covariate Shift (TCS) issue in SNNs by aligning with the neuron dynamics, specifically the accumulated membrane potential. The TAB method utilizes temporal accumulated batch statistics over an expanding window to normalize data, which effectively captures historical temporal dependencies similar to the accumulation process of the membrane potential in the LIF neuron model. The authors demonstrate the effectiveness of TAB by comparing it with other state-of-the-art methods on various datasets, including CIFAR-10, CIFAR-100, and ImageNet.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the challenges in directly training SNNs, particularly the TCS issue.\n2. The proposed TAB method is well-motivated and aligns with the neuron dynamics, which is a significant improvement over existing methods.\n3. The authors provide a theoretical connection between the TAB method and the neural dynamics, which is a valuable contribution to the field.\n4. The experimental results demonstrate the effectiveness of TAB on various datasets, including large-scale static and neuromorphic datasets.\n\nWeaknesses:\n\n1. The paper assumes a specific neuron model (LIF) and may not generalize to other neuron models.\n2. The authors do not provide a detailed explanation of the learning rules for computing the gradients in the TAB method.\n3. The paper does not discuss the computational complexity of the TAB method and its potential impact on large-scale SNNs.\n4. The authors do not provide a thorough comparison with other state-of-the-art methods on ImageNet.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the learning rules for computing the gradients in the TAB method.\n2. Discuss the computational complexity of the TAB method and its potential impact on large-scale SNNs.\n3. Provide a thorough comparison with other state-of-the-art methods on ImageNet.\n4. Investigate the generalizability of the TAB method to other neuron models.\n\nRecommendation:\n\nThe paper proposes a novel and effective batch normalization method for SNNs, which addresses the TCS issue by aligning with the neuron dynamics. The experimental results demonstrate the effectiveness of TAB on various datasets. However, the paper assumes a specific neuron model and does not provide a detailed explanation of the learning rules for computing the gradients. With some modifications and additional experiments, the paper has the potential to be a significant contribution to the field of SNNs.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel batch normalization (BN) method called Temporal Accumulated Batch Normalization (TAB) specifically designed for training Spiking Neural Networks (SNNs). SNNs have gained popularity due to their energy-efficient computing when implemented on neuromorphic hardware, but directly training SNNs is challenging due to their non-differentiable activation function and temporal delayed accumulation of outputs over time. The paper introduces TAB to address the temporal covariate shift issue by aligning with neuron dynamics, specifically the accumulated membrane potential. TAB utilizes temporal accumulated statistics for data normalization and assigns different learnable weights to each time-step to distinguish their effect on the final result. The paper provides a comprehensive overview of the proposed TAB method, including its theoretical connection to the neural dynamics of SNNs.\n\nStrengths:\n\n1. The paper proposes a novel batch normalization method specifically designed for SNNs, which addresses the temporal covariate shift issue by aligning with neuron dynamics.\n2. The paper provides a comprehensive overview of the proposed TAB method, including its theoretical connection to the neural dynamics of SNNs.\n3. The paper conducts extensive experiments on large-scale static and neuromorphic datasets, including CIFAR-10, CIFAR-100, and DVS-CIFAR10, to verify the effectiveness of the proposed TAB method.\n4. The paper compares the performance of the proposed TAB method with other batch normalization methods in the context of SNNs, including TEBN, BNTT, and tdBN.\n\nWeaknesses:\n\n1. The paper assumes a specific type of SNNs (leaky integrate-and-fire (LIF) model) and does not explore other types of SNNs.\n2. The paper does not provide a detailed analysis of the computational complexity of the proposed TAB method.\n3. The paper assumes a fixed window size for temporal accumulated statistics, which may not be optimal for all cases.\n4. The paper does not provide a detailed comparison with other state-of-the-art learning methods for SNNs.\n\nSuggestions for Improvement:\n\n1. Explore other types of SNNs (e.g., integrate-and-fire (IF) model, adaptive exponential integrate-and-fire (AEIF) model) and compare their performance with the proposed TAB method.\n2. Conduct a detailed analysis of the computational complexity of the proposed TAB method and compare it with other batch normalization methods.\n3. Investigate the effect of different window sizes on the performance of the proposed TAB method and explore adaptive window size selection methods.\n4. Conduct a more comprehensive comparison with other state-of-the-art learning methods for SNNs, including both ANN-to-SNN conversion and hybrid training methods.\n\nRecommendation:\n\nThe paper proposes a novel batch normalization method specifically designed for SNNs, which addresses the temporal covariate shift issue by aligning with neuron dynamics. The paper provides a comprehensive overview of the proposed TAB method, including its theoretical connection to the neural dynamics of SNNs. However, the paper assumes a specific type of SNNs and does not explore other types of SNNs. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed TAB method. Therefore, I recommend that the authors explore other types of SNNs, conduct a detailed analysis of the computational complexity of the proposed TAB method, and compare it with other batch normalization methods.",
        "gr_score_with": 0.42005420054200543,
        "gr_score_without": 0.39570552147239263,
        "output_without": {
            "num_tokens_scored": 326,
            "num_green_tokens": 129,
            "green_fraction": 0.39570552147239263,
            "z_score": -0.15827547107614776,
            "p_value": 0.562880132128682
        },
        "output_with": {
            "num_tokens_scored": 369,
            "num_green_tokens": 155,
            "green_fraction": 0.42005420054200543,
            "z_score": 0.786344612778118,
            "p_value": 0.21583281132684956
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, they propose S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling.\n\nStrengths:\n\n* The paper proposes a unified sampling framework (USF) that systematically studies the optional strategies for solver searching, which is a novel contribution to the field of diffusion probabilistic models.\n* The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, which is a significant insight.\n* The proposed framework allows free choices of solver strategy at each step, which provides a high degree of flexibility.\n* The authors propose a predictor-based search method (S3) that automatically optimizes the solver schedule, which is a useful tool for practitioners.\n\nWeaknesses:\n\n* The paper assumes that the reader has a good understanding of diffusion probabilistic models and their applications, which may not be the case for all readers.\n* The authors do not provide a detailed comparison with other state-of-the-art methods, which makes it difficult to evaluate the effectiveness of their proposed framework.\n* The paper does not provide a clear explanation of how the predictor-based search method (S3) works, which may make it difficult for readers to understand and replicate the results.\n* The authors do not provide a detailed analysis of the computational cost of their proposed framework, which may be a concern for practitioners who need to implement the framework in practice.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with other state-of-the-art methods to evaluate the effectiveness of their proposed framework.\n* The paper should include a clear explanation of how the predictor-based search method (S3) works, which will make it easier for readers to understand and replicate the results.\n* The authors should provide a detailed analysis of the computational cost of their proposed framework, which will help practitioners to evaluate the feasibility of implementing the framework in practice.\n* The paper should include more experimental results to demonstrate the effectiveness of their proposed framework, particularly on different types of datasets.\n\nRecommendation:\n\nThe paper proposes a novel unified sampling framework (USF) for diffusion probabilistic models, which provides a high degree of flexibility and allows for the automatic optimization of the solver schedule using a predictor-based search method. While the paper has some weaknesses, it provides a significant contribution to the field of diffusion probabilistic models, and the proposed framework has the potential to be useful for practitioners. Therefore, I recommend that the paper be accepted for publication.\n\nRating: 7/10\n\nReviewer's Expertise: The reviewer has expertise in the field of machine learning and deep learning, with a focus on diffusion probabilistic models and their applications.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The framework splits the solving process of one step into independent decisions of several components, including the choice of timestep, prediction type of the neural network, starting point of the current step, order of Taylor expansion, derivative estimation method, and usage of ODE correctors. The authors also propose S3, a predictor-based search method to automatically optimize the solver schedule to get a better time-quality trade-off of sampling. Experiments show that S3 can find outstanding solver schedules which outperform the state-of-the-art sampling methods on various datasets, including CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom.\n\nStrengths:\n\n* The paper proposes a unified framework to systematically study the optional strategies for solver searching, which can help to find better solver schedules.\n* The S3 method can automatically optimize the solver schedule, which can reduce the manual effort and improve the performance of the sampler.\n* The experiments show that S3 can outperform the state-of-the-art sampling methods on various datasets, which demonstrates the effectiveness of the proposed framework and method.\n\nWeaknesses:\n\n* The search space of the solver schedule is extremely large, which can make the search process very time-consuming.\n* The evaluation of the true performance of solver schedules can be very costly, which can limit the scalability of the S3 method.\n* The paper only reports the results on a limited number of datasets, which can limit the generalizability of the proposed framework and method.\n\nSuggestions for Improvement:\n\n* To reduce the search space, the authors can use some heuristics or constraints to prune the search space, such as using a smaller search space or using some pre-defined solver schedules as the initial population.\n* To reduce the cost of evaluating the true performance of solver schedules, the authors can use some approximation methods, such as using a smaller number of generated images or using some surrogate models to estimate the performance of solver schedules.\n* To increase the generalizability of the proposed framework and method, the authors can report the results on a larger number of datasets, including some datasets with different characteristics, such as high-resolution datasets or datasets with different types of noise.\n\nRecommendation:\n\nThe paper proposes a unified framework and a predictor-based search method to systematically study the optional strategies for solver searching. The experiments show that the proposed framework and method can outperform the state-of-the-art sampling methods on various datasets. However, the search space of the solver schedule is extremely large, which can make the search process very time-consuming. Therefore, some heuristics or constraints should be used to prune the search space, and some approximation methods should be used to reduce the cost of evaluating the true performance of solver schedules. Overall, the paper is well-written, and the proposed framework and method are well-designed. I recommend this paper for publication in a top-tier conference in the field of machine learning.",
        "gr_score_with": 0.519893899204244,
        "gr_score_without": 0.3763157894736842,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 143,
            "green_fraction": 0.3763157894736842,
            "z_score": -0.9424212936563149,
            "p_value": 0.827011505215459
        },
        "output_with": {
            "num_tokens_scored": 377,
            "num_green_tokens": 196,
            "green_fraction": 0.519893899204244,
            "z_score": 4.751843608902333,
            "p_value": 1.0078512851784376e-06
        }
    },
    {
        "title": "LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES",
        "abstract": "We study principal-agent problems in which a principal commits to an outcomedependent payment scheme\u2014called contract\u2014in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent\u2019s action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. (2023). Moreover, it can also be employed to provide a \u00d5(T ) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the problem of learning optimal contracts in hidden-action principal-agent problems. The principal and the agent interact over multiple rounds, and the principal has no knowledge about the agent's actions. The goal is to design algorithms that prescribe the principal a contract to commit to at each round in order to learn an \"approximately-optimal\" contract with high probability by using the minimum possible number of rounds.\n\nStrengths:\n\n* The paper provides a novel algorithm, called Discover-and-Cover, that learns an optimal bounded contract with high probability in a number of rounds polynomial in the size of the problem instance when the number of agent's actions is constant.\n* The algorithm solves an open problem by Zhu et al. (2023) and can also be employed to provide a \u00d5(T) regret bound in the related online learning setting.\n* The paper provides a comprehensive analysis of the algorithm, including the number of rounds required and the probability of success.\n\nWeaknesses:\n\n* The paper assumes that the number of agent's actions is constant, which may not be the case in all real-world scenarios.\n* The algorithm may not perform well in scenarios with a large number of outcomes.\n* The paper does not provide a comparison with other existing algorithms for learning optimal contracts.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the algorithm's performance in scenarios with a large number of outcomes.\n* The authors should compare the performance of the Discover-and-Cover algorithm with other existing algorithms for learning optimal contracts.\n* The authors should provide more details about the computational complexity of the algorithm and the time required to execute it.\n\nRecommendation:\n\nThe paper provides a novel and comprehensive solution to the problem of learning optimal contracts in hidden-action principal-agent problems. The Discover-and-Cover algorithm is efficient and can be employed in a variety of scenarios. However, the paper assumes a constant number of agent's actions, which may not be the case in all real-world scenarios. The authors should provide a more detailed analysis of the algorithm's performance in scenarios with a large number of outcomes and compare its performance with other existing algorithms.",
        "peer_review_with_watermark": "$$$\nPaper Title: LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES\n\nSummary:\nThe paper presents an algorithm called Discover-and-Cover for learning optimal contracts in hidden-action principal-agent problems. The algorithm is designed to learn an optimal contract with high probability by interacting with an agent over multiple rounds. The main contribution of the paper is an upper bound on the number of rounds required by the algorithm, which grows polynomially in the size of the problem instance when the number of agent's actions is constant.\n\nStrengths:\n\n1. The paper presents a novel algorithm for learning optimal contracts in hidden-action principal-agent problems.\n2. The algorithm is designed to exploit small action spaces, which is a significant improvement over previous works.\n3. The paper provides a rigorous analysis of the algorithm's performance, including an upper bound on the number of rounds required to learn an optimal contract.\n4. The algorithm can be easily exploited to achieve cumulative regret bounds in online learning settings.\n\nWeaknesses:\n\n1. The paper assumes that the agent breaks ties in favor of the principal, which may not be realistic in all scenarios.\n2. The algorithm's performance may degrade if the number of agent's actions is large.\n3. The paper does not provide a lower bound on the number of rounds required to learn an optimal contract.\n\nSuggestions for Improvement:\n\n1. Relaxing the assumption that the agent breaks ties in favor of the principal could be an interesting direction for future research.\n2. Developing a lower bound on the number of rounds required to learn an optimal contract could provide a more complete understanding of the algorithm's performance.\n3. Exploring the application of the Discover-and-Cover algorithm to other domains, such as repeated Stackelberg games, could be a promising direction for future research.\n\nRecommendation:\nThe Discover-and-Cover algorithm is a significant contribution to the field of learning optimal contracts in hidden-action principal-agent problems. The algorithm's ability to exploit small action spaces and its rigorous analysis make it a valuable tool for practitioners. However, further research is needed to address the limitations mentioned above. Overall, I recommend accepting the paper for publication in its current form, subject to minor revisions to address the suggestions for improvement.\n\nRating: 8/10\n\nReviewers should note that the rating is based on the paper's technical contributions, writing quality, and overall impact. The rating does not take into account potential minor errors or areas for improvement, which can be addressed through revisions.",
        "gr_score_with": 0.41210374639769454,
        "gr_score_without": 0.34306569343065696,
        "output_without": {
            "num_tokens_scored": 274,
            "num_green_tokens": 94,
            "green_fraction": 0.34306569343065696,
            "z_score": -1.9237281323012623,
            "p_value": 0.9728056650787306
        },
        "output_with": {
            "num_tokens_scored": 347,
            "num_green_tokens": 143,
            "green_fraction": 0.41210374639769454,
            "z_score": 0.4602342468348624,
            "p_value": 0.32267404586557813
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper presents a novel approach to address the challenges of spatio-temporal few-shot learning, which is a pressing problem in smart city applications.\n2. The authors provide a comprehensive overview of the existing literature on spatio-temporal few-shot learning and highlight the limitations of existing approaches.\n3. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework.\n\nWeaknesses:\n\n1. The paper assumes that the region prompts are available, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework.\n3. The paper does not discuss the scalability of the framework to large-scale datasets.\n4. The authors do not provide a detailed comparison of the computational cost of the proposed framework with existing approaches.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the hyperparameters and their impact on the performance of the framework.\n2. The paper should discuss the scalability of the framework to large-scale datasets and provide a detailed comparison of the computational cost with existing approaches.\n3. The authors should provide a more detailed explanation of the region prompts and how they are generated.\n4. The paper should discuss the potential applications of the proposed framework in real-world smart city scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to address the challenges of spatio-temporal few-shot learning. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework. However, the paper assumes that the region prompts are available, and the authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework. Therefore, I recommend that the authors address these limitations and provide a more detailed comparison of the computational cost with existing approaches.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary:**\n\nThe paper proposes a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework addresses the challenges of data scarcity in many cities and regions by leveraging pre-trained models and adaptive prompting techniques. GPD recasts spatio-temporal few-shot learning as pre-training a generative diffusion model that generates tailored neural networks guided by prompts. The framework is model-agnostic and has been extensively evaluated on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The results demonstrate that GPD consistently outperforms state-of-the-art baselines with an average improvement of 7.87% on four datasets.\n\n**Strengths:**\n\n1. **Innovative Approach**: The paper proposes a novel generative pre-training framework that addresses the challenges of data scarcity in spatio-temporal modeling.\n2. **Model-Agnostic**: GPD is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n3. **Effective Knowledge Transfer**: The framework enables effective knowledge transfer across source and target cities, which is essential for spatio-temporal few-shot learning.\n4. **Extensive Evaluations**: The paper has been extensively evaluated on multiple real-world datasets, demonstrating the effectiveness of the framework.\n5. **Flexible Prompting**: GPD allows for flexible prompting techniques, which can be tailored to the specific characteristics of the target city.\n\n**Weaknesses:**\n\n1. **Limited Dataset**: While the paper has been extensively evaluated on multiple real-world datasets, the number of datasets used is still limited.\n2. **Prompt Selection**: The selection of prompts is crucial in the GPD framework. However, the paper does not provide a systematic method for prompt selection.\n3. **Conditioning Strategy**: The paper explores different conditioning strategies, but the most effective one is not explicitly identified.\n4. **Model Complexity**: The model used in the paper is complex, which may limit its applicability in practical scenarios.\n5. **Future Work**: While the paper provides a valuable contribution to the field, there is still much work to be done in terms of improving the framework, especially in terms of prompt selection and model complexity.\n\n**Suggestions for Improvement:**\n\n1. **More Comprehensive Evaluations**: The paper should be evaluated on more datasets to demonstrate its generalizability.\n2. **Systematic Prompt Selection**: A systematic method for prompt selection should be developed to ensure that the most effective prompts are used.\n3. **More Robust Conditioning Strategy**: The most effective conditioning strategy should be identified and validated.\n4. **Simplification of the Model**: The model used in the paper should be simplified to make it more applicable in practical scenarios.\n5. **Further Investigation**: Further investigation is needed to explore the limitations and potential biases of the framework.\n\n**Recommendation:**\n\nI highly recommend this paper for publication in the conference. The paper provides a valuable contribution to the field of spatio-temporal few-shot learning, and the results demonstrate the effectiveness of the framework. With some minor adjustments to address the limitations mentioned above, the paper has the potential to make a significant impact in the field.\n\n$$$",
        "gr_score_with": 0.5167037861915368,
        "gr_score_without": 0.42758620689655175,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 124,
            "green_fraction": 0.42758620689655175,
            "z_score": 0.9589266029707684,
            "p_value": 0.16879786092204085
        },
        "output_with": {
            "num_tokens_scored": 449,
            "num_green_tokens": 232,
            "green_fraction": 0.5167037861915368,
            "z_score": 5.0478041415865995,
            "p_value": 2.2345847855656362e-07
        }
    },
    {
        "title": "HAZARD CHALLENGE: EMBODIED DECISION MAK-",
        "abstract": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire , flood , and wind , and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents\u2019 decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://sites.google. com/view/hazard-challenge.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes the HAZARD challenge, a novel benchmark for evaluating embodied decision-making in dynamically changing environments. The HAZARD challenge consists of three unexpected disaster scenarios: fire, flood, and wind, and requires agents to rescue valuable items from these continuously evolving and perilous circumstances. The authors develop a comprehensive benchmark, HAZARD, that enables the evaluation of autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning, rule-based, and search-based methods. They also propose an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.\n\nStrengths:\n1. The paper proposes a novel and challenging benchmark for embodied decision-making in dynamically changing environments.\n2. The authors develop a comprehensive benchmark, HAZARD, that enables the evaluation of autonomous agents' decision-making capabilities across various pipelines.\n3. The paper explores the use of large language models (LLMs) in embodied decision-making tasks and provides a detailed analysis of their performance.\n4. The authors provide a thorough evaluation of the HAZARD benchmark using various baseline methods and LLM-based agents.\n\nWeaknesses:\n1. The paper focuses only on object rescue tasks and does not explore other actions that agents can take to mitigate environmental changes.\n2. The authors do not provide a thorough analysis of the limitations of the HAZARD benchmark and how it can be improved.\n3. The paper assumes that the agents have perfect perception and do not consider the impact of perception errors on the decision-making process.\n4. The authors do not provide a clear comparison of the performance of different LLMs and how they can be improved.\n\nSuggestions for Improvement:\n1. The authors should explore other actions that agents can take to mitigate environmental changes, such as using an extinguisher to put out fires.\n2. The authors should provide a thorough analysis of the limitations of the HAZARD benchmark and how it can be improved.\n3. The authors should consider the impact of perception errors on the decision-making process and develop methods to handle these errors.\n4. The authors should provide a clear comparison of the performance of different LLMs and how they can be improved.\n\nRecommendation:\nThe paper proposes a novel and challenging benchmark for embodied decision-making in dynamically changing environments. The authors develop a comprehensive benchmark, HAZARD, that enables the evaluation of autonomous agents' decision-making capabilities across various pipelines. The paper explores the use of large language models (LLMs) in embodied decision-making tasks and provides a detailed analysis of their performance. However, the paper has some limitations, such as focusing only on object rescue tasks and not exploring other actions that agents can take to mitigate environmental changes. Therefore, I recommend that the authors address these limitations in future work.\n\nRating: 7/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel challenge, HAZARD, designed to assess the decision-making abilities of embodied agents in dynamic and uncertain environments. The challenge consists of three disaster scenarios: fire, flood, and wind, and requires agents to rescue valuable objects from these continuously evolving and perilous circumstances. The authors develop a simulation system on top of the ThreeDWorld platform, enabling simulations of these disaster scenarios, and provide a dataset for evaluating and analyzing the performance of various decision-making pipelines, including large language model (LLM) agents. The paper presents an in-depth analysis of the challenges posed by perception and reasoning for existing methods, especially LLM-based agents, and explores the potential of LLMs in decision-making tasks.\n\nStrengths:\n\n1. The paper proposes a novel and challenging benchmark, HAZARD, that simulates real-world disaster scenarios and requires agents to make complex decisions.\n2. The authors provide a comprehensive evaluation of various decision-making pipelines, including LLM-based agents, and analyze their performance in different scenarios.\n3. The paper explores the potential of LLMs in decision-making tasks and highlights their strengths and weaknesses.\n4. The authors provide a detailed description of the simulation system and dataset, making it easy for readers to reproduce the experimental results.\n5. The paper highlights the importance of considering perception and reasoning challenges in decision-making tasks, especially in dynamic and uncertain environments.\n\nWeaknesses:\n\n1. The paper focuses only on object rescue tasks and does not consider more complex tasks, such as mitigating environmental changes.\n2. The evaluation metrics used in the paper, such as Value, Step, and Damage, may not be sufficient to fully capture the performance of decision-making pipelines.\n3. The paper does not provide a detailed analysis of the robustness and generalizability of LLM-based agents in different scenarios.\n4. The authors do not discuss potential limitations and biases of the dataset and simulation system.\n5. The paper does not provide a clear roadmap for future work and potential applications of the HAZARD challenge.\n\nSuggestions for Improvement:\n\n1. The authors should consider expanding the HAZARD challenge to include more complex tasks, such as mitigating environmental changes.\n2. The evaluation metrics used in the paper should be expanded to include more comprehensive metrics, such as accuracy, robustness, and generalizability.\n3. The paper should provide a more detailed analysis of the robustness and generalizability of LLM-based agents in different scenarios.\n4. The authors should discuss potential limitations and biases of the dataset and simulation system and provide a plan to address these issues.\n5. The paper should provide a clear roadmap for future work and potential applications of the HAZARD challenge.\n\nRecommendation:\n\nThe paper presents a novel and challenging benchmark, HAZARD, that simulates real-world disaster scenarios and requires agents to make complex decisions. The authors provide a comprehensive evaluation of various decision-making pipelines, including LLM-based agents, and analyze their performance in different scenarios. While the paper highlights some limitations and biases, it provides a solid foundation for future work and potential applications. I recommend accepting the paper for publication, subject to some minor revisions to address the suggestions provided above.",
        "gr_score_with": 0.5796703296703297,
        "gr_score_without": 0.504885993485342,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 155,
            "green_fraction": 0.504885993485342,
            "z_score": 3.7512936603190696,
            "p_value": 8.79622509144623e-05
        },
        "output_with": {
            "num_tokens_scored": 364,
            "num_green_tokens": 211,
            "green_fraction": 0.5796703296703297,
            "z_score": 6.997154052553427,
            "p_value": 1.3060701213371328e-12
        }
    },
    {
        "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
        "abstract": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel formulation of retrosynthesis in terms of stochastic processes to account for the uncertainty of chemical reactions. The authors introduce a new evaluation metric called successful synthesis probability (SSP) that captures the probability that at least one synthesis plan will work in the lab. They also propose a novel greedy algorithm called retro-fallback that maximizes the SSP by greedily optimizing the probability that at least one synthesis plan can be executed in the lab. The authors demonstrate the effectiveness of retro-fallback using in-silico benchmarks and compare it with existing algorithms such as MCTS and retro*.\n\nStrengths:\n\n1. The paper proposes a novel and well-motivated approach to retrosynthesis, which takes into account the uncertainty of chemical reactions.\n2. The authors introduce a new evaluation metric called SSP that captures the probability that at least one synthesis plan will work in the lab.\n3. The paper proposes a novel greedy algorithm called retro-fallback that maximizes the SSP by greedily optimizing the probability that at least one synthesis plan can be executed in the lab.\n4. The authors demonstrate the effectiveness of retro-fallback using in-silico benchmarks and compare it with existing algorithms such as MCTS and retro*.\n\nWeaknesses:\n\n1. The paper assumes that the uncertainty of chemical reactions can be modeled using stochastic processes, which may not be accurate in all cases.\n2. The authors do not provide a clear explanation of how to estimate the parameters of the stochastic processes used in the algorithm.\n3. The paper assumes that the SSP metric is a good evaluation metric for retrosynthesis, but it is not clear if this is the case in all scenarios.\n4. The authors do not provide a thorough comparison of retro-fallback with other algorithms that take into account uncertainty, such as probabilistic planning algorithms.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how to estimate the parameters of the stochastic processes used in the algorithm.\n2. Provide a more thorough comparison of retro-fallback with other algorithms that take into account uncertainty, such as probabilistic planning algorithms.\n3. Investigate the use of other evaluation metrics for retrosynthesis, such as the expected cost or length of the synthesis plan.\n4. Provide a more detailed analysis of the computational complexity of the algorithm and how it scales with the size of the search graph.\n\nRecommendation:\n\nThe paper proposes a novel and well-motivated approach to retrosynthesis, which takes into account the uncertainty of chemical reactions. The authors introduce a new evaluation metric called SSP that captures the probability that at least one synthesis plan will work in the lab. The paper also proposes a novel greedy algorithm called retro-fallback that maximizes the SSP by greedily optimizing the probability that at least one synthesis plan can be executed in the lab. The authors demonstrate the effectiveness of retro-fallback using in-silico benchmarks and compare it with existing algorithms such as MCTS and retro*. However, the paper assumes that the uncertainty of chemical reactions can be modeled using stochastic processes, which may not be accurate in all cases. The authors also do not provide a clear explanation of how to estimate the parameters of the stochastic processes used in the algorithm. Therefore, I recommend that the authors provide a more detailed explanation of how to estimate the parameters of the stochastic processes used in the algorithm and provide a more thorough comparison of retro-fallback with other algorithms that take into account uncertainty.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel formulation of retrosynthesis, a task in chemistry, to account for the uncertainty in the feasibility and buyability of chemical reactions. The authors introduce a stochastic process model to capture this uncertainty and develop a new evaluation metric, successful synthesis probability (SSP), which measures the probability that at least one synthesis plan will work. They also propose a novel algorithm, retro-fallback, which maximizes the SSP by greedily selecting the molecule that has the highest expected value of the successful synthesis probability. The authors evaluate retro-fallback experimentally and show that it outperforms existing algorithms, including retro* and MCTS, in terms of maximizing the SSP.\n\nStrengths:\n\n1. The paper provides a novel and meaningful formulation of the retrosynthesis problem, which captures the uncertainty in the feasibility and buyability of chemical reactions.\n2. The authors develop a new evaluation metric, successful synthesis probability (SSP), which measures the probability that at least one synthesis plan will work.\n3. The paper proposes a novel algorithm, retro-fallback, which maximizes the SSP by greedily selecting the molecule that has the highest expected value of the successful synthesis probability.\n4. The authors provide an extensive experimental evaluation of retro-fallback, comparing it to existing algorithms, including retro* and MCTS.\n5. The paper provides a clear and concise presentation of the algorithm and its evaluation, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the feasibility and buyability of chemical reactions can be modeled as stochastic processes, which may not always be the case.\n2. The authors rely on the assumption that the heuristic function used in retro-fallback is admissible, which may not always be the case.\n3. The paper does not provide a theoretical guarantee that retro-fallback's worst-case performance is better than that of retro*.\n4. The authors assume that the number of samples used in retro-fallback is sufficient to approximate the true SSP, which may not always be the case.\n5. The paper does not provide a clear explanation of how to choose the parameters of the stochastic process model.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how to choose the parameters of the stochastic process model, including the choice of kernel and the hyperparameters.\n2. The paper should provide a theoretical guarantee that retro-fallback's worst-case performance is better than that of retro*.\n3. The authors should provide more experimental results to evaluate the performance of retro-fallback in different scenarios, including cases where the feasibility and buyability of chemical reactions are uncertain.\n4. The paper should provide a clear explanation of how to relax the assumption that the heuristic function used in retro-fallback is admissible.\n5. The authors should provide more details on how to implement the algorithm and its evaluation, including the choice of data structures and the optimization techniques used.\n\nRecommendation:\n\nThe paper provides a novel and meaningful formulation of the retrosynthesis problem, which captures the uncertainty in the feasibility and buyability of chemical reactions. The authors develop a new evaluation metric, successful synthesis probability (SSP), which measures the probability that at least one synthesis plan will work. The paper proposes a novel algorithm, retro-fallback, which maximizes the SSP by greedily selecting the molecule that has the highest expected value of the successful synthesis probability. The authors provide an extensive experimental evaluation of retro-fallback, comparing it to existing algorithms, including retro* and MCTS. However, the paper assumes that the feasibility and buyability of chemical reactions can be modeled as stochastic processes, which may not always be the case. Therefore, I recommend that the authors provide more details on how to relax this assumption and provide a clear explanation of how to choose the parameters of the stochastic process model. I also recommend that the authors provide more experimental results to evaluate the performance of retro-fallback in different scenarios, including cases where the feasibility and buyability of chemical reactions are uncertain.\n\nOverall, the paper provides a significant contribution to the field of retrosynthesis, and I believe that it has the potential to be published in a top-tier conference. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more robust and impactful.",
        "gr_score_with": 0.5721784776902887,
        "gr_score_without": 0.425,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 119,
            "green_fraction": 0.425,
            "z_score": 0.8539125638299665,
            "p_value": 0.1965767188370015
        },
        "output_with": {
            "num_tokens_scored": 381,
            "num_green_tokens": 218,
            "green_fraction": 0.5721784776902887,
            "z_score": 6.860183469510533,
            "p_value": 3.4386082528060323e-12
        }
    },
    {
        "title": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
        "abstract": "Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input. TUVF provides a category-level texture representation disentangled from 3D shapes. Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning. Note that all samples are rendered under 1024\u00d71024 resolution; zoom-in is recommended. ABSTRACT",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for generating high-quality and disentangled textures on 3D object shapes, called Texture UV Radiance Fields (TUVF). The method leverages a canonical UV sphere space to represent textures, allowing for efficient rendering and disentanglement from geometry. TUVF is trained using a self-supervised canonical surface auto-encoder and a texture mapping network, and is evaluated on various datasets and metrics. The results show significant improvements over state-of-the-art approaches in terms of texture synthesis, swapping, and editing.\n\nStrengths:\n\n* The paper presents a novel and efficient method for generating high-quality textures on 3D object shapes.\n* The use of a canonical UV sphere space allows for efficient rendering and disentanglement from geometry.\n* The method is evaluated on various datasets and metrics, showing significant improvements over state-of-the-art approaches.\n* The paper provides a clear and concise explanation of the method and its components.\n\nWeaknesses:\n\n* The method assumes one-to-one dense mapping between the UV sphere and the 3D object shape, which may not always hold in real-world scenarios.\n* The evaluation metrics used in the paper are limited to LPIPSg, LPIPSt, FID, and KID, which may not fully capture the quality and diversity of the generated textures.\n* The paper does not provide a thorough comparison with other state-of-the-art approaches in terms of computational efficiency and scalability.\n\nSuggestions for Improvement:\n\n* To address the limitation of one-to-one dense mapping, the authors could explore methods for handling multiple or partial correspondences between the UV sphere and the 3D object shape.\n* To improve the evaluation metrics, the authors could consider using additional metrics such as texture similarity, diversity, and realism, as well as user studies to assess the quality and usability of the generated textures.\n* To improve the computational efficiency and scalability of the method, the authors could explore methods for reducing the number of points required for rendering, such as using hierarchical sampling or sparse point clouds.\n\nRecommendation:\n\nThe paper presents a novel and efficient method for generating high-quality textures on 3D object shapes, and the results show significant improvements over state-of-the-art approaches. However, the method has limitations and could be improved by addressing the issues mentioned above. Overall, the paper is well-written and well-organized, and the authors should be commended for their contributions to the field of texture synthesis and rendering.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach, Texture UV Radiance Fields (TUVF), for high-quality and disentangled texture generation on a given 3D shape. TUVF uses a canonical UV sphere space to generate textures, which allows for category-level texture representation disentangled from 3D shapes. The proposed method uses a Canonical Surface Auto-encoder to establish correspondence between the UV sphere and the 3D shape, and then uses a texture feature generator to produce textures on the UV sphere. The textures are then rendered using a point-based radiance field, and the entire pipeline is supervised by an adversarial loss. The paper demonstrates the effectiveness of TUVF through quantitative and qualitative evaluations on several datasets, including CompCars, Photoshape, and DiffusionCats.\n\nStrengths:\n\n1. The proposed approach is novel and innovative, using a canonical UV sphere space to generate textures, which allows for category-level texture representation disentangled from 3D shapes.\n2. The paper provides thorough experimental evaluations, including quantitative and qualitative results, which demonstrate the effectiveness of TUVF.\n3. The proposed method is able to generate high-quality and realistic textures, and achieves substantial improvements over state-of-the-art approaches to texture swapping and editing applications.\n4. The paper provides a comprehensive overview of the related work, including mesh-based approaches, NeRF-based approaches, and texture representations.\n\nWeaknesses:\n\n1. The paper assumes one-to-one dense mapping between the UV sphere and the 3D shape, which may not always hold in real-world scenarios.\n2. The proposed method relies on the use of a canonical UV sphere space, which may not be suitable for all types of textures or shapes.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed method, which may be a limitation for large-scale applications.\n4. The paper does not provide a comprehensive evaluation of the proposed method on a wide range of datasets, which may limit its generalizability.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more sophisticated neural rendering architectures, such as ray transformers, to enhance the results.\n2. Incorporate advanced data-driven priors, such as diffusion models, to mitigate distortions and improve the quality of the generated textures.\n3. Investigate the use of more complex textures, such as textures with multiple components or textures with varying levels of complexity.\n4. Provide a more detailed analysis of the computational complexity of the proposed method, including the use of optimization techniques to reduce computational overhead.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to texture generation, using a canonical UV sphere space to generate textures. The paper provides thorough experimental evaluations, including quantitative and qualitative results, which demonstrate the effectiveness of TUVF. While there are some limitations to the proposed method, including the assumption of one-to-one dense mapping and the reliance on the use of a canonical UV sphere space, the paper provides a comprehensive overview of the related work and provides a detailed analysis of the proposed method. I recommend the paper for publication, with the suggestion that the authors address the limitations and provide additional evaluation on a wider range of datasets.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.49507389162561577,
        "gr_score_without": 0.4157303370786517,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 148,
            "green_fraction": 0.4157303370786517,
            "z_score": 0.6058392513710811,
            "p_value": 0.27231075115542647
        },
        "output_with": {
            "num_tokens_scored": 406,
            "num_green_tokens": 201,
            "green_fraction": 0.49507389162561577,
            "z_score": 3.9103773351327447,
            "p_value": 4.6076024781871715e-05
        }
    },
    {
        "title": "CODERS FOR INTRINSICALLY LOW-DIMENSIONAL DATA",
        "abstract": "Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied such as natural images are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a theoretical analysis of Wasserstein Autoencoders (WAEs) for learning unknown distributions, particularly when the data exhibits an intrinsically low-dimensional structure in the high-dimensional feature space. The authors propose a framework to analyze the error rates of WAEs, characterized by the Minkowski dimension of the support of the target distribution. They derive an oracle inequality to bound the excess risk in terms of misspecification, generalization, and optimization errors. The excess risk bounds are obtained by balancing model-misspecification and stochastic errors to find proper network architectures in terms of the number of samples.\n\nStrengths:\n\n1. The paper presents a comprehensive theoretical analysis of WAEs, which is a significant contribution to the field.\n2. The authors propose a novel framework to analyze the error rates of WAEs, which is based on the Minkowski dimension of the support of the target distribution.\n3. The paper provides a detailed proof of the main result, which is a significant achievement in theoretical computer science.\n4. The authors discuss the implications of the theoretical results, including encoding, decoding, and data generation guarantees.\n\nWeaknesses:\n\n1. The paper assumes that the data distribution is realizable, which is a strong assumption.\n2. The authors assume that the true generator and encoder exist, which is not always the case in practice.\n3. The paper does not provide a clear discussion of the optimization error term, which is a significant challenge in practical applications.\n4. The authors do not provide a comparison with other existing methods, such as Generative Adversarial Networks (GANs).\n\nSuggestions for Improvement:\n\n1. The authors should relax the assumption of realizability and provide a more realistic analysis of the data distribution.\n2. The paper should discuss the optimization error term in more detail and provide a clear analysis of its impact on the error rates.\n3. The authors should provide a comparison with other existing methods, such as GANs, to demonstrate the effectiveness of WAEs.\n4. The paper should provide more experimental results to support the theoretical analysis.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of theoretical computer science, particularly in the analysis of WAEs. However, the authors should address the weaknesses mentioned above to make the paper more comprehensive and practical. With some revisions, the paper has the potential to be a top-tier publication in the field.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper aims to address the shortcomings in the current literature on Wasserstein Autoencoders (WAEs) by providing an error analysis when the data lies in a low-dimensional structure in the high-dimensional feature space. The authors propose a framework to control the expected excess risk of the WAE problem and show that the error rates scale as O\u0303(n\u2212 1\n2+d\u00b5 ), where d\u00b5 is the upper Minkowski dimension of the support of the target distribution.\n\nStrengths:\n\n1. The authors address a significant gap in the current literature on WAEs by providing a framework to control the expected excess risk when the data lies in a low-dimensional structure.\n2. The proposed framework allows the authors to analyze the accuracy of encoding and decoding guarantees, which is essential for understanding the performance of WAEs.\n3. The authors provide explicit bounds on the expected excess risk, which can be used to determine the appropriate network architectures in terms of the number of samples.\n\nWeaknesses:\n\n1. The authors make strong assumptions on the smoothness of the true models and the latent space, which may not hold in practice.\n2. The proposed framework requires the estimation of the intrinsic dimension of the data, which can be challenging in practice.\n3. The authors do not provide empirical evidence to support their theoretical findings, which makes it difficult to assess the practical significance of their results.\n\nSuggestions for Improvement:\n\n1. The authors should provide empirical evidence to support their theoretical findings and demonstrate the practical significance of their results.\n2. The authors should relax the strong assumptions on the smoothness of the true models and the latent space to make the framework more applicable in practice.\n3. The authors should provide more detailed explanations on how to estimate the intrinsic dimension of the data and how to determine the appropriate network architectures in terms of the number of samples.\n\nRecommendation:\n\nThe paper provides a significant contribution to the literature on WAEs by addressing a significant gap in the current literature. However, the authors should address the limitations and challenges mentioned above to make the framework more applicable in practice. With some revisions, the paper can be published in a reputable journal in the field of machine learning.\n\nRating: 7/10\n\n$$$$",
        "gr_score_with": 0.5353535353535354,
        "gr_score_without": 0.41566265060240964,
        "output_without": {
            "num_tokens_scored": 332,
            "num_green_tokens": 138,
            "green_fraction": 0.41566265060240964,
            "z_score": 0.5825439294326857,
            "p_value": 0.2801001803333296
        },
        "output_with": {
            "num_tokens_scored": 297,
            "num_green_tokens": 159,
            "green_fraction": 0.5353535353535354,
            "z_score": 4.761482665275299,
            "p_value": 9.608790418104929e-07
        }
    },
    {
        "title": "TOOL-AUGMENTED REWARD MODELING",
        "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements1.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to reward modeling, called Themis, which integrates external tools into the reward modeling process to enhance interpretability, reliability, and effectiveness. Themis enables reward models to interact with diverse external sources, including calculators and search engines, to construct task-specific tool engagement and reasoning traces. The authors present a comprehensive dataset, TARA, comprising 13,604 training datasets and 1,469 test sets, each consisting of a question, a positive answer, and a negative answer, along with detailed tool invocation processes. They evaluate Themis across eight distinct tasks and demonstrate a significant improvement of 17.7% compared to conventional reward models, outperforming Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation.\n\nStrengths:\n\n1. The paper presents a novel approach to reward modeling that integrates external tools, which is a significant departure from traditional reward models.\n2. The authors provide a comprehensive dataset, TARA, which is a valuable resource for the research community.\n3. The experimental results demonstrate the effectiveness of Themis in enhancing interpretability, reliability, and effectiveness.\n4. The paper discusses the importance of transparency and interpretability in reward modeling and presents a framework that addresses these challenges.\n\nWeaknesses:\n\n1. The paper assumes that the external tools are available and can be integrated seamlessly into the reward modeling process, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the limitations and challenges of integrating external tools into the reward modeling process.\n3. The paper relies heavily on the performance of the GPT-4 model, which may not be representative of all language models.\n4. The authors do not provide a detailed discussion of the potential biases and limitations of the TARA dataset.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the limitations and challenges of integrating external tools into the reward modeling process.\n2. They should discuss the potential biases and limitations of the TARA dataset and provide a more comprehensive evaluation of the dataset.\n3. The authors should provide a more detailed discussion of the implications of their approach on the transparency and interpretability of reward models.\n4. They should explore the potential applications of Themis in real-world scenarios and discuss the potential challenges and limitations of deploying the approach in practice.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to reward modeling that integrates external tools. The authors provide a comprehensive dataset and experimental results that demonstrate the effectiveness of Themis. However, the paper assumes that the external tools are available and can be integrated seamlessly into the reward modeling process, which may not be the case in all scenarios. The authors should provide a more detailed analysis of the limitations and challenges of integrating external tools into the reward modeling process and discuss the potential biases and limitations of the TARA dataset. Overall, the paper is well-written, and the authors have made a significant contribution to the field of reward modeling.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper presents Themis, a tool-augmented reward modeling framework that integrates external tools into the reasoning process of large language models (LLMs). The framework allows RMs to make dynamic decisions about which APIs to call, when to invoke them, and how to incorporate the results into the reasoning process. The authors propose a novel dataset, Tool-Augmented Reward dAtaset (TARA), which includes interactions with seven distinct external tools. The framework is evaluated across eight diverse tasks, and the results demonstrate a significant improvement of 17.7% compared to conventional RMs.\n\nStrengths:\n1. The paper presents a novel and innovative approach to reward modeling by incorporating external tools into the reasoning process of LLMs.\n2. The framework is well-defined, and the architecture is clearly explained.\n3. The dataset, TARA, is comprehensive and includes interactions with seven distinct external tools.\n4. The experimental results are impressive, and the framework outperforms conventional RMs in multiple tasks.\n\nWeaknesses:\n1. The paper assumes that the external tools are readily available and can be easily integrated into the framework. However, in real-world applications, this might not be the case.\n2. The framework relies heavily on the quality and availability of the external tools, which might not always be the case.\n3. The paper does not provide a thorough analysis of the potential biases and errors that might arise from using external tools.\n4. The framework is evaluated only in a single-task setting, and its performance in multi-task settings is not evaluated.\n\nSuggestions for Improvement:\n1. The authors should provide more details about the potential biases and errors that might arise from using external tools and how they can be mitigated.\n2. The framework should be evaluated in multiple-task settings to assess its performance in real-world applications.\n3. The authors should provide more information about the quality and availability of the external tools and how they can be improved.\n4. The framework should be compared to other state-of-the-art reward modeling approaches to assess its performance in a more comprehensive manner.\n\nRecommendation:\nThe paper presents a novel and innovative approach to reward modeling by incorporating external tools into the reasoning process of LLMs. The framework is well-defined, and the experimental results are impressive. However, the paper assumes that the external tools are readily available and can be easily integrated into the framework, which might not be the case in real-world applications. Therefore, the framework needs to be evaluated in multiple-task settings and compared to other state-of-the-art reward modeling approaches to assess its performance in a more comprehensive manner.\n\nOverall, the paper presents a promising approach to reward modeling, and its evaluation in multiple-task settings and comparison to other state-of-the-art approaches will provide a more comprehensive understanding of its performance and potential applications.\n\n$$$",
        "gr_score_with": 0.3761467889908257,
        "gr_score_without": 0.30303030303030304,
        "output_without": {
            "num_tokens_scored": 363,
            "num_green_tokens": 110,
            "green_fraction": 0.30303030303030304,
            "z_score": -3.7712361663282548,
            "p_value": 0.9999187795774066
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 123,
            "green_fraction": 0.3761467889908257,
            "z_score": -0.8804710999221765,
            "p_value": 0.8106979227679965
        }
    },
    {
        "title": "ADOPD: A LARGE-SCALE DOCUMENT PAGE DECOM-",
        "abstract": "Recent research in document image understanding is impeded by the scarcity of high-quality data. We introduce ADoPD, a large-scale dataset for document page decomposition. ADoPD distinguishes itself by utilizing a novel data-driven document taxonomy discovery method for data collection. Our approach takes advantage of both large-scale pretrained models and human-in-the-loop process, ensuring diversity and balance in our data collection. Leveraging our data-driven document taxonomy, we collected and densely annotated labels for document images, covering four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Specifically, for each image, the annotations include human-labeled entity masks, text bounding boxes, as well as automatically generated tags and captions. We provide detailed experimental analyses to validate our data-driven document taxonomy method and experimentally analyze the four tasks based on different models. We believe that ADoPD has the potential to become a cornerstone dataset to support future research on document image understanding.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces ADOPD, a large-scale dataset for document page decomposition. The dataset is constructed using a novel data-driven document taxonomy discovery method, which leverages large-scale pretrained models and human-in-the-loop process to ensure diversity and balance in data collection. The dataset consists of four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq, and is annotated with human-labeled entity masks, text bounding boxes, and automatically generated tags and captions. The authors propose a data-driven approach for constructing document taxonomies during data collection and safeguard the ADOPD through outlier detection and human-in-the-loop. The paper also presents a systematic process covering data collection, taxonomy analysis, model-assisted data annotation, and HITL processes.\n\nStrengths:\n\n1. The paper introduces a novel data-driven document taxonomy discovery method, which is a significant contribution to the field of document image understanding.\n2. The dataset is large-scale and diverse, covering four document image understanding tasks, and is annotated with human-labeled entity masks, text bounding boxes, and automatically generated tags and captions.\n3. The authors propose a systematic process covering data collection, taxonomy analysis, model-assisted data annotation, and HITL processes, which is a comprehensive approach to document page decomposition.\n4. The paper presents a thorough analysis of the dataset and experimental comparisons across four tasks, which provides valuable insights into the effectiveness of the dataset.\n\nWeaknesses:\n\n1. The paper assumes that the use of large-scale pretrained models is sufficient to ensure diversity and balance in data collection, which may not be the case in real-world scenarios.\n2. The authors do not provide a clear explanation of how the data-driven document taxonomy discovery method is applied to the dataset, which makes it difficult to understand the process.\n3. The paper does not provide a thorough analysis of the limitations of the dataset, such as the potential for bias in the data collection process.\n4. The authors do not provide a clear explanation of how the model-assisted data annotation process is applied to the dataset, which makes it difficult to understand the process.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how the data-driven document taxonomy discovery method is applied to the dataset, including the specific steps and algorithms used.\n2. Provide a thorough analysis of the limitations of the dataset, including the potential for bias in the data collection process.\n3. Provide a clear explanation of how the model-assisted data annotation process is applied to the dataset, including the specific steps and algorithms used.\n4. Consider providing a more detailed analysis of the performance of the dataset on different tasks and evaluation metrics.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to document page decomposition, and the dataset is a significant contribution to the field. However, the paper assumes that the use of large-scale pretrained models is sufficient to ensure diversity and balance in data collection, and does not provide a clear explanation of how the data-driven document taxonomy discovery method is applied to the dataset. Therefore, I recommend that the authors provide a clear explanation of these issues and provide a more detailed analysis of the limitations of the dataset.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents ADoPD, a large-scale dataset for document page decomposition. ADoPD is constructed using a novel data-driven document taxonomy discovery method and leverages both large-scale pretrained models and human-in-the-loop processes to ensure diversity and balance in data collection. The dataset includes 4 tasks: DOC2MASK, DOC2BOX, DOC2TAG, and DOC2SEQ, and is annotated with human-labeled entity masks, text bounding boxes, tags, and captions. The authors propose a collaborative approach to enhance overall efficiency and provide detailed experimental analyses to validate the effectiveness of ADoPD.\n\nStrengths:\n\n* ADoPD is a well-structured and comprehensive dataset for document page decomposition, which is a challenging task.\n* The authors propose a novel data-driven document taxonomy discovery method, which is a significant contribution to the field.\n* The dataset is annotated with human-labeled entity masks, text bounding boxes, tags, and captions, which provides valuable insights into document understanding.\n* The authors provide detailed experimental analyses to validate the effectiveness of ADoPD.\n\nWeaknesses:\n\n* The paper lacks a detailed analysis of the performance of ADoPD compared to other datasets and baselines.\n* The authors do not provide a detailed evaluation of the quality and consistency of the annotations.\n* The paper does not discuss the potential biases and limitations of the dataset.\n* The authors do not provide a detailed analysis of the generalization performance of ADoPD.\n\nSuggestions for Improvement:\n\n* Provide a detailed analysis of the performance of ADoPD compared to other datasets and baselines.\n* Evaluate the quality and consistency of the annotations and provide a detailed analysis of the results.\n* Discuss the potential biases and limitations of the dataset and provide a detailed analysis of the results.\n* Provide a detailed analysis of the generalization performance of ADoPD.\n\nRecommendation:\n\nThe paper presents a valuable contribution to the field of document understanding and provides a well-structured and comprehensive dataset. However, to further improve the paper, the authors should provide a detailed analysis of the performance of ADoPD compared to other datasets and baselines, evaluate the quality and consistency of the annotations, discuss the potential biases and limitations of the dataset, and provide a detailed analysis of the generalization performance of ADoPD. With these improvements, the paper would be more comprehensive and provide valuable insights into document understanding.\n\nRating: 8/10\n\nRecommendation: Accept with revisions.",
        "gr_score_with": 0.4339622641509434,
        "gr_score_without": 0.3580645161290323,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 111,
            "green_fraction": 0.3580645161290323,
            "z_score": -1.507151411220547,
            "p_value": 0.9341140742946301
        },
        "output_with": {
            "num_tokens_scored": 265,
            "num_green_tokens": 115,
            "green_fraction": 0.4339622641509434,
            "z_score": 1.1285321907949182,
            "p_value": 0.12954761492454253
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of widely used, open-source language models (LLMs) to prompt formatting choices, finding that performance spreads across tasks, models, and number of few-shot examples. They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats, allowing for the efficient exploration of the space. The authors demonstrate that FORMATSPREAD can be used to estimate the performance spread across prompt formatting choices, and that this spread is large regardless of model choice, even when increasing model size, number of few-shots, or using instruction tuning.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of using these models in real-world applications.\n2. The authors propose a novel grammar that defines the space of plausible prompt formats and semantic equivalence between formats, which allows for the efficient exploration of the space.\n3. The paper introduces FORMATSPREAD, an algorithm that can be used to estimate the performance spread across prompt formatting choices, which is a valuable tool for researchers and practitioners.\n4. The authors provide a thorough evaluation of the algorithm on a large set of tasks and models, demonstrating its effectiveness in estimating performance spread.\n\nWeaknesses:\n\n1. The paper assumes that the grammar is manually constructed, which may not be practical for large-scale applications.\n2. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation for large-scale applications.\n3. The paper focuses on classification tasks and does not explore other types of tasks, such as open-ended text generation.\n4. The authors do not provide a detailed analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nSuggestions for Improvement:\n\n1. Develop a more automated method for constructing the grammar, which could be based on machine learning algorithms or other techniques.\n2. Provide a more detailed analysis of the computational cost of FORMATSPREAD and explore ways to reduce it.\n3. Explore the use of FORMATSPREAD on other types of tasks, such as open-ended text generation.\n4. Conduct a more thorough analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of natural language processing and language modeling, and the authors should be commended for their thorough analysis of the sensitivity of LLMs to prompt formatting choices. However, to make the paper more impactful, the authors should address the limitations mentioned above and provide a more detailed analysis of the algorithm's computational cost and robustness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a method called FORMATSPREAD to efficiently search the space of plausible prompt formats under a user-specified computational budget for a given task. The authors aim to characterize the space of prompt formats and find that it is largely non-monotonic, with few atomic features being predictors of performance alone. They also show that the separability of format embeddings is highly correlated with observed performance spread. The paper includes several experiments to demonstrate the effectiveness of FORMATSPREAD, including evaluations on multiple models and tasks, as well as a comparison with traditional methods.\n\nStrengths:\n\n* The paper presents a novel method for efficiently searching the space of prompt formats, which is a critical component of language model performance.\n* The experiments are thorough and demonstrate the effectiveness of FORMATSPREAD on multiple models and tasks.\n* The paper provides a clear and concise explanation of the method and its implementation.\n* The authors also discuss the limitations of the method and potential future work.\n\nWeaknesses:\n\n* The paper assumes a specific grammar for prompt formats, which may not be applicable to all tasks or models.\n* The experiments focus primarily on classification tasks, and it would be beneficial to explore the method on other types of tasks, such as open-ended text generation.\n* The paper does not provide a detailed comparison with traditional methods, such as gradient-based prompt engineering.\n* Some of the results, such as the high spread observed in exact prefix matching, may be influenced by degeneration, which is a known issue in language models.\n\nSuggestions for Improvement:\n\n* Provide a more detailed comparison with traditional methods, such as gradient-based prompt engineering.\n* Explore the method on other types of tasks, such as open-ended text generation.\n* Consider incorporating additional features, such as input length, into the grammar for prompt formats.\n* Provide more details on the implementation of FORMATSPREAD, such as the specific algorithms used for Bayesian optimization.\n* Consider investigating the relationship between FORMATSPREAD and other methods for prompt optimization, such as prompt engineering.\n\nRecommendation:\n\nThe paper presents a novel and effective method for efficiently searching the space of prompt formats, which is a critical component of language model performance. The experiments demonstrate the effectiveness of FORMATSPREAD on multiple models and tasks, and the paper provides a clear and concise explanation of the method and its implementation. However, the paper assumes a specific grammar for prompt formats, and the experiments focus primarily on classification tasks. To address these limitations, the authors could provide a more detailed comparison with traditional methods, explore the method on other types of tasks, and consider incorporating additional features into the grammar for prompt formats. Overall, the paper is a significant contribution to the field of language model optimization, and it has the potential to be a valuable tool for researchers and practitioners working with language models.",
        "gr_score_with": 0.6127167630057804,
        "gr_score_without": 0.5056497175141242,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 179,
            "green_fraction": 0.5056497175141242,
            "z_score": 4.057554392235678,
            "p_value": 2.4794635155228624e-05
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 212,
            "green_fraction": 0.6127167630057804,
            "z_score": 8.076703576033106,
            "p_value": 3.3270516652244595e-16
        }
    },
    {
        "title": "REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS",
        "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to learning robust world models in the reward-free setting. The authors address the problem of generating curricula for training robust agents without access to a reward function during exploration. They introduce WAKER, an algorithm for actively sampling environments for exploration during reward-free training based on the estimated error of the world model in each environment. The authors demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.\n\nStrengths:\n\n1. The paper addresses a novel problem in the field of reinforcement learning, specifically the generation of curricula for training robust agents in the reward-free setting.\n2. The authors provide a clear and concise definition of the problem and a thorough analysis of the related work.\n3. The proposed algorithm, WAKER, is well-motivated and easy to understand, and the authors provide a detailed explanation of its components and how they work together.\n4. The experimental results are comprehensive and demonstrate the effectiveness of WAKER in improving robustness, efficiency, and generalisation.\n\nWeaknesses:\n\n1. The paper assumes that the world model learns a suitable representation model, which may not always be the case in practice.\n2. The authors rely on an intrinsically motivated policy to adequately explore the state-action space across a range of environments, which may pose a challenge for scalability to more complex environments.\n3. The paper does not provide a clear explanation of how the world model is updated during the training process, and how the error estimates are used to bias sampling towards more uncertain environments.\n4. The experimental results are limited to a few domains and tasks, and it is not clear how WAKER would perform in more complex or diverse environments.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the world model is updated during the training process, and how the error estimates are used to bias sampling towards more uncertain environments.\n2. Investigate the use of function approximation to estimate uncertainty throughout large parameter spaces, to improve scalability to more complex environments.\n3. Provide more experimental results to demonstrate the effectiveness of WAKER in a wider range of domains and tasks.\n4. Discuss the potential applications of WAKER in more complex or diverse environments, and how it could be adapted to handle such scenarios.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of reinforcement learning by proposing a novel approach to learning robust world models in the reward-free setting. The authors provide a clear and concise definition of the problem, a thorough analysis of the related work, and a well-motivated and easy-to-understand algorithm. The experimental results demonstrate the effectiveness of WAKER in improving robustness, efficiency, and generalisation. However, there are some limitations and weaknesses that need to be addressed in future work. Overall, I recommend accepting the paper for publication, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces the problem of generating curricula in the reward-free setting to train robust world models. It proposes an algorithm, WAKER, that selects environments for data collection based on the estimated error of the world model for each environment. The paper also introduces the Minimax World Model Error problem, which optimises an upper bound on the objective of the Reward-Free Minimax Regret problem. The authors conduct experiments on several pixel-based continuous control domains and demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.\n\nStrengths:\n\n* The paper introduces a novel problem and approach to reward-free world model learning, which is an important contribution to the field of reinforcement learning.\n* The Minimax World Model Error problem provides a theoretical motivation for the proposed algorithm, WAKER.\n* The experiments demonstrate the effectiveness of WAKER in several pixel-based continuous control domains.\n* The paper provides a clear and concise presentation of the methodology and results.\n\nWeaknesses:\n\n* The paper assumes the existence of a suitable representation model, which may not always be the case in practice.\n* The Minimax World Model Error problem assumes that the true latent dynamics function, T, is known, which may not be the case in practice.\n* The paper relies on an intrinsically motivated policy to adequately explore the state-action space, which may pose a challenge for scalability to more complex environments.\n* The paper does not provide a clear evaluation of the robustness of the world models, and the experiments only report the performance on downstream tasks.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the suitability of the representation model and the Minimax World Model Error problem in practice.\n* Provide a more detailed evaluation of the robustness of the world models, including the error between the transitions predicted by the world model and real transitions.\n* Investigate the use of function approximation to estimate uncertainty throughout large parameter spaces, as opposed to the discrete buffer used in this work.\n* Investigate the use of WAKER for reward-free pretraining, followed by task-specific fine-tuning.\n\nRecommendation:\n\nOverall, the paper provides an important contribution to the field of reinforcement learning, and the experiments demonstrate the effectiveness of the proposed algorithm, WAKER. However, the paper has some limitations and areas for improvement, which should be addressed in future work. I recommend the paper for publication, but with the above suggestions for improvement.",
        "gr_score_with": 0.45325779036827196,
        "gr_score_without": 0.359375,
        "output_without": {
            "num_tokens_scored": 384,
            "num_green_tokens": 138,
            "green_fraction": 0.359375,
            "z_score": -1.6250000000000022,
            "p_value": 0.9479187205847807
        },
        "output_with": {
            "num_tokens_scored": 353,
            "num_green_tokens": 160,
            "green_fraction": 0.45325779036827196,
            "z_score": 2.0425132179534673,
            "p_value": 0.020550324816663502
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method constructs a directed graph to represent unlabeled data and uses a diffusion process to quantify the influence of each candidate subset. A greedy algorithm is introduced to select the subset with maximum influence, which approximates the vast unlabeled data. The paper provides theoretical analysis and empirical evaluations on various benchmarks, demonstrating the superiority of IDEAL over previous state-of-the-art selective annotation baselines.\n\nStrengths:\n\n1. The paper presents a novel approach to selective annotations, addressing the challenges of balancing diversity and representativeness.\n2. The method is end-to-end, unsupervised, and efficient, reducing unnecessary costs at inference time.\n3. The paper provides theoretical analysis, establishing an upper limit for the global optimal solution and demonstrating the reliability of the greedy search algorithm.\n4. Empirical evaluations demonstrate the superiority of IDEAL across various benchmarks, including classification, multi-choice, dialogue, and generation tasks.\n\nWeaknesses:\n\n1. The paper assumes access to a large-scale unlabeled data pool, which may not be feasible in practice.\n2. The method relies on a similarity-based prompt retrieval method, which may not be optimal in all cases.\n3. The paper does not provide a comprehensive comparison with other graph-based methods, such as K-Means and MFL.\n4. The case study on automatic annotation is limited to a small set of datasets and may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for constructing the directed graph, such as using different similarity metrics or graph neural networks.\n2. Evaluate the performance of IDEAL on other datasets and tasks, including those with varying levels of complexity and domain adaptation.\n3. Compare IDEAL with other graph-based methods, such as K-Means and MFL, to demonstrate its superiority.\n4. Provide a more detailed analysis of the time complexity and scalability of IDEAL, especially for large datasets.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach to selective annotations, addressing the challenges of balancing diversity and representativeness. The theoretical analysis and empirical evaluations demonstrate the superiority of IDEAL over previous state-of-the-art selective annotation baselines. While there are some limitations and areas for improvement, the paper provides a valuable contribution to the field of in-context learning and large language models. I recommend acceptance of the paper, with the suggested improvements and additional evaluations to further strengthen the results.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThe paper proposes a novel method, Influence-Driven Selective Annotations Empower In-Context Learners (IDEAL), for reducing annotation costs while improving the quality of in-context examples in large language models. The method utilizes influence-driven selective annotations to empower in-context learners, which aims to identify a subset of data that acts as a proxy and closely approximates the vast unlabeled dataset. The authors provide theoretical analysis to establish an upper limit for the global optimal solution and demonstrate that their greedy search algorithm selects a subset with influence at least as substantial as a specific proportion of the optimal solution's influence. Empirical evaluations show that the proposed method achieves superior performance while largely reducing the time required for subset selection.\n\n**Strengths**\n\n1. The paper proposes a novel method for reducing annotation costs while improving the quality of in-context examples in large language models.\n2. The method utilizes influence-driven selective annotations, which is a novel approach in the field of in-context learning.\n3. The authors provide theoretical analysis to establish an upper limit for the global optimal solution, which provides a rigorous foundation for the method.\n4. The empirical evaluations show that the proposed method achieves superior performance while largely reducing the time required for subset selection.\n5. The paper explores the potential of selective annotations in in-context learning, which is a promising area of research.\n\n**Weaknesses**\n\n1. The paper assumes that the unlabeled data is embedded in a vector space using Sentence-BERT, which may not be applicable to all datasets.\n2. The method relies on the construction of a directed graph, which may be computationally expensive for large datasets.\n3. The authors do not provide a detailed analysis of the time complexity of the method.\n4. The paper does not explore the potential of other prompt retrieval methods.\n5. The authors do not provide a detailed analysis of the robustness of the method to noise in the input data.\n\n**Suggestions for Improvement**\n\n1. The authors should provide a detailed analysis of the time complexity of the method and explore potential optimization techniques.\n2. The authors should explore other prompt retrieval methods and provide a detailed analysis of their performance.\n3. The authors should provide a detailed analysis of the robustness of the method to noise in the input data.\n4. The authors should explore the potential of selective annotations in other areas of natural language processing.\n5. The authors should provide more detailed experiments to validate the claims of the paper.\n\n**Recommendation**\n\nThe paper proposes a novel method for reducing annotation costs while improving the quality of in-context examples in large language models. The method utilizes influence-driven selective annotations, which is a novel approach in the field of in-context learning. The authors provide theoretical analysis to establish an upper limit for the global optimal solution, which provides a rigorous foundation for the method. The empirical evaluations show that the proposed method achieves superior performance while largely reducing the time required for subset selection. The paper explores the potential of selective annotations in in-context learning, which is a promising area of research. Therefore, I recommend this paper for publication in a reputable conference or journal.",
        "gr_score_with": 0.47648902821316613,
        "gr_score_without": 0.391644908616188,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 150,
            "green_fraction": 0.391644908616188,
            "z_score": -0.33376821066546764,
            "p_value": 0.6307227633298782
        },
        "output_with": {
            "num_tokens_scored": 319,
            "num_green_tokens": 152,
            "green_fraction": 0.47648902821316613,
            "z_score": 2.7886169573832147,
            "p_value": 0.0026466815253382665
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of the approach. While there are some limitations and weaknesses, the paper is well-written and well-organized, and the results are impressive. I recommend this paper for publication in a top-tier conference.\n\nGrade: B+\n\nReviewer's Comments:\n\nThe paper is well-written and well-organized, and the results are impressive. However, there are some limitations and weaknesses that need to be addressed. The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios. The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications. The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field, and I recommend it for publication in a top-tier conference. However, the authors need to address the limitations and weaknesses mentioned above to make the paper more robust and convincing.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy, Win-Win, for efficient training and inference of high-resolution vision transformers. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special tricks. The authors show that Win-Win is effective when using relative positional embedding, such as rotary embeddings, and is 4 times faster to train than a full-resolution network. They demonstrate the generality of their contribution by applying Win-Win to two dense prediction tasks with high-resolution data, semantic segmentation and optical flow estimation, and achieving state-of-the-art performance on the Spring benchmark.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* Win-Win is effective in reducing the training cost and memory usage while achieving comparable performance to full-resolution training.\n* The authors demonstrate the generality of their contribution by applying Win-Win to two dense prediction tasks with high-resolution data.\n* The paper provides a thorough comparison of Win-Win with other state-of-the-art methods, including tiling and attention sparsification.\n\nWeaknesses:\n\n* The paper assumes that the input image is divisible into rectangular windows, which may not be the case for all images.\n* The authors do not provide a thorough analysis of the impact of window size and number of windows on the performance of Win-Win.\n* The paper relies on relative positional embedding, which may not be suitable for all vision tasks.\n* The authors do not provide a thorough comparison of Win-Win with other attention mechanisms, such as local attention and sparse attention.\n\nSuggestions for Improvement:\n\n* Investigate the impact of window size and number of windows on the performance of Win-Win.\n* Analyze the effectiveness of Win-Win on other vision tasks, such as image classification and object detection.\n* Investigate the use of other attention mechanisms, such as local attention and sparse attention, in conjunction with Win-Win.\n* Provide a thorough comparison of Win-Win with other state-of-the-art methods, including tiling and attention sparsification.\n\nRecommendation:\n\nThe paper is well-written and presents a novel and efficient strategy for training high-resolution vision transformers. However, the paper assumes that the input image is divisible into rectangular windows, and the authors do not provide a thorough analysis of the impact of window size and number of windows on the performance of Win-Win. Therefore, the paper is suitable for publication in a conference or journal, but the authors should address the limitations and suggestions for improvement before submission.\n\nRecommendation Score: 7/10\n$$$",
        "gr_score_with": 0.6075581395348837,
        "gr_score_without": 0.5104712041884817,
        "output_without": {
            "num_tokens_scored": 382,
            "num_green_tokens": 195,
            "green_fraction": 0.5104712041884817,
            "z_score": 4.407325727672964,
            "p_value": 5.2327372170582e-06
        },
        "output_with": {
            "num_tokens_scored": 344,
            "num_green_tokens": 209,
            "green_fraction": 0.6075581395348837,
            "z_score": 7.858024338956194,
            "p_value": 1.9511974483555113e-15
        }
    },
    {
        "title": "THRESHOLD-CONSISTENT MARGIN LOSS FOR OPEN- WORLD DEEP METRIC LEARNING",
        "abstract": "Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate (FAR) and false reject rate (FRR) across test classes and data distributions. We define this issue in DML as threshold inconsistency. In real-world applications, such inconsistency often complicates the threshold selection process when deploying commercial image retrieval systems. To measure this inconsistency, we propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. Using the OPIS metric, we find that achieving high accuracy levels in a DML model does not automatically guarantee threshold consistency. In fact, our investigation reveals a Pareto frontier in the high-accuracy regime, where existing methods to improve accuracy often lead to degradation in threshold consistency. To address this trade-off, we introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. Extensive experiments demonstrate TCM\u2019s effectiveness in enhancing threshold consistency while preserving accuracy, simplifying the threshold selection process in practical DML settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel solution to address the issue of threshold inconsistency in deep metric learning (DML), which occurs when different test classes and distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy. The authors introduce a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) to quantify the threshold inconsistency of a DML model. They also propose a simple yet effective regularization technique called Threshold-Consistent Margin (TCM) loss, which promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. The TCM loss can be combined with any base losses and backbone architecture to improve threshold consistency in DML.\n\nStrengths:\n\n1. The paper addresses a significant issue in DML, namely threshold inconsistency, which is crucial in real-world applications.\n2. The authors introduce a novel metric, OPIS, to quantify threshold inconsistency, which is more straightforward and easier to compute compared to existing methods.\n3. The proposed TCM loss is simple and effective in improving threshold consistency while maintaining accuracy.\n4. The authors demonstrate the effectiveness of TCM loss on various image retrieval benchmarks, achieving state-of-the-art performance in both threshold consistency and accuracy.\n\nWeaknesses:\n\n1. The paper assumes a sufficient number of samples per class for the OPIS and \u03f5-OPIS metrics, which may not be feasible in few-shot evaluation scenarios.\n2. The TCM loss may fail when there's a significant distribution shift between the training and test sets or when strong label noise is present.\n3. The paper does not provide a comprehensive comparison with existing methods that address threshold inconsistency, such as posthoc calibration methods and conformal prediction.\n4. The authors do not provide a detailed analysis of the computational complexity of TCM loss, which may be a concern for large-scale datasets.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of using OPIS and \u03f5-OPIS metrics in few-shot evaluation scenarios and propose modifications to address the issue.\n2. Provide a more comprehensive comparison with existing methods that address threshold inconsistency, including posthoc calibration methods and conformal prediction.\n3. Conduct a more detailed analysis of the computational complexity of TCM loss and propose modifications to reduce the computational cost.\n4. Investigate the robustness of TCM loss to distribution shifts and label noise, and propose modifications to address these issues.\n\nRecommendation:\n\nThe paper proposes a novel and effective solution to address threshold inconsistency in DML. However, it requires further investigation and modifications to address the limitations mentioned above. With some additional work, the paper has the potential to make a significant impact in the field of DML.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents a novel approach to address the issue of threshold inconsistency in deep metric learning (DML) for image retrieval tasks. The authors define threshold inconsistency as the phenomenon where different test classes and distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy. To measure this inconsistency, they introduce a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. They also propose a simple yet effective regularization technique called Threshold-Consistent Margin (TCM) loss, which promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs.\n\nStrengths:\n\n1. The paper presents a clear and concise definition of threshold inconsistency, a long-standing issue in DML.\n2. The authors introduce a novel metric, OPIS, to quantify threshold inconsistency, which is a significant contribution to the field.\n3. The TCM loss is a simple yet effective regularization technique that can be combined with any base loss and backbone architecture.\n4. The paper presents a comprehensive evaluation of the proposed method on four standard image retrieval benchmarks, demonstrating its effectiveness in improving threshold consistency while maintaining accuracy.\n\nWeaknesses:\n\n1. The paper assumes that the embedding space is hyperspherical, which may not hold in all cases.\n2. The TCM loss may not be effective in cases where the distribution shift between training and testing datasets is significant.\n3. The paper does not provide a detailed analysis of the trade-off between accuracy and threshold consistency.\n\nSuggestions for Improvement:\n\n1. The authors could provide a more detailed analysis of the trade-off between accuracy and threshold consistency, including a more in-depth exploration of the Pareto frontier.\n2. They could provide more experimental results on the robustness of the TCM loss to distribution shift between training and testing datasets.\n3. The authors could provide more insights into how the TCM loss can be adapted for other types of embedding spaces.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of DML, addressing a long-standing issue in image retrieval tasks. The proposed method is simple yet effective, and its evaluation on four standard benchmarks demonstrates its effectiveness in improving threshold consistency while maintaining accuracy. The authors' work has the potential to impact a wide range of applications, including computer vision, natural language processing, and recommendation systems.\n\nRating: 8.5/10\n\nRecommendation for publication: Yes\n\nNote: The rating and recommendation are based on the overall quality, significance, and impact of the paper, as well as its technical soundness and contributions to the field.",
        "gr_score_with": 0.4719387755102041,
        "gr_score_without": 0.4,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 156,
            "green_fraction": 0.4,
            "z_score": 0.0,
            "p_value": 0.5
        },
        "output_with": {
            "num_tokens_scored": 392,
            "num_green_tokens": 185,
            "green_fraction": 0.4719387755102041,
            "z_score": 2.907370998419186,
            "p_value": 0.0018224032062600956
        }
    },
    {
        "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
        "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which builds upon the previous work of COMBINER (Guo et al., 2023). RECOMBINER addresses several limitations of COMBINER, including the use of factorized priors and posterior approximations that lack flexibility, the inability to adapt to local deviations from global patterns in the data, and the sensitivity to modeling choices and the variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization for neural network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The proposed method, RECOMBINER, is evaluated on various data modalities, including image, audio, video, and 3D protein structure data, and demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\nStrengths:\n\n1. The paper proposes several novel and non-trivial extensions to the previous work of COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices.\n2. The authors provide a comprehensive evaluation of the proposed method on various data modalities, including image, audio, video, and 3D protein structure data.\n3. The paper demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n4. The authors provide a clear and concise explanation of the proposed method and its extensions, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The encoding time complexity of RECOMBINER is high, which may limit its practical application.\n2. The paper does not provide a detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n3. The authors do not provide a clear explanation of how the proposed method can be extended to other data modalities.\n4. The paper does not discuss the potential applications of RECOMBINER in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n2. The paper should provide a clear explanation of how the proposed method can be extended to other data modalities.\n3. The authors should discuss the potential applications of RECOMBINER in real-world scenarios.\n4. The paper should provide a more detailed analysis of the encoding time complexity of RECOMBINER and propose potential solutions to reduce it.\n\nRecommendation:\n\nThe paper proposes a novel and competitive data compression method called RECOMBINER, which addresses several limitations of the previous work of COMBINER. The authors provide a comprehensive evaluation of the proposed method on various data modalities and demonstrate competitive results with the best INR-based methods. However, the encoding time complexity of RECOMBINER is high, which may limit its practical application. Therefore, I recommend that the authors provide a more detailed comparison with other state-of-the-art compression methods, discuss the potential applications of RECOMBINER in real-world scenarios, and propose potential solutions to reduce the encoding time complexity.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which is an extension of the previous method COMBINER. RECOMBINER addresses several limitations of COMBINER, including its use of factorized Gaussian variational posteriors, its inability to effectively adapt to local deviations from global patterns in the data, and its sensitivity to modeling choices and variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization of the INR weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The authors conduct extensive experiments on various data modalities, including image, audio, video, and 3D protein structure data, and demonstrate that RECOMBINER achieves competitive results with state-of-the-art codecs.\n\nStrengths:\n\n1. The paper proposes a novel and effective method for data compression, which is an important problem in computer vision and machine learning.\n2. The authors provide a thorough analysis of the limitations of previous methods and propose several non-trivial extensions to address these limitations.\n3. The experiments conducted on various data modalities demonstrate the effectiveness of RECOMBINER in achieving competitive results with state-of-the-art codecs.\n4. The authors provide a clear and concise description of their method and its components, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes a strong background in probability and statistics, particularly in variational inference and Bayesian neural networks.\n2. The experiments conducted on high-resolution data show that RECOMBINER can produce block artifacts, which is a common issue in data compression methods that use patch-based representations.\n3. The paper does not provide a thorough analysis of the time complexity of RECOMBINER and its comparison with other state-of-the-art codecs.\n4. The authors mention that RECOMBINER is sensitive to the choice of hyperparameters, particularly the trade-off parameter \u03b2, and provide a heuristic method for tuning \u03b2, but a more systematic approach is needed.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the time complexity of RECOMBINER and its comparison with other state-of-the-art codecs.\n2. Develop a more systematic approach for tuning the trade-off parameter \u03b2 and other hyperparameters of RECOMBINER.\n3. Investigate the use of more advanced techniques, such as approximate message passing or deep generative models, to address the block artifacts issue in high-resolution data compression.\n4. Conduct more experiments on a wider range of data modalities and provide a more thorough analysis of the strengths and weaknesses of RECOMBINER.\n\nRecommendation:\n\nThe paper is well-written and provides a novel and effective method for data compression. However, it assumes a strong background in probability and statistics and does not provide a thorough analysis of the time complexity of RECOMBINER and its comparison with other state-of-the-art codecs. The authors should address these limitations in future work and provide a more systematic approach for tuning the trade-off parameter \u03b2 and other hyperparameters of RECOMBINER.",
        "gr_score_with": 0.5394736842105263,
        "gr_score_without": 0.45507246376811594,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 157,
            "green_fraction": 0.45507246376811594,
            "z_score": 2.0880381657533253,
            "p_value": 0.018397196027388044
        },
        "output_with": {
            "num_tokens_scored": 380,
            "num_green_tokens": 205,
            "green_fraction": 0.5394736842105263,
            "z_score": 5.549814284864966,
            "p_value": 1.4298660996310888e-08
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework combines the strengths of LLMs, which can perform high-level abstract reasoning, and automated reasoners, which can perform precise low-level reasoning. The authors present a formal calculus, LEMUR, that operates over a configuration, which is either a distinguished symbol or a tuple of a program, an assumption, and a list of properties. The calculus performs oracle calls to LLMs to propose new properties and revise them. The authors also present a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n\nStrengths:\n\n1. The paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification.\n2. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners.\n3. The paper presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n4. The authors demonstrate the practicality of LEMUR by instantiating it as a fully automated framework and evaluating its efficiency on standard benchmark sets.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs can generate correct and useful properties, which may not always be the case.\n2. The authors do not provide a thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n3. The paper assumes that the automated reasoners can perform precise low-level reasoning, which may not always be the case.\n4. The authors do not provide a clear explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n2. The paper should provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n3. The authors should provide a more detailed description of the LLMs used in the paper and their limitations.\n4. The paper should provide a more thorough discussion of the strengths and weaknesses of the LEMUR framework compared to other automated program verification approaches.\n\nRecommendation:\n\nThe paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners. The paper also presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure. However, the paper assumes that the LLMs can generate correct and useful properties, which may not always be the case. Therefore, the paper should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks and provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nIn summary, the paper has the potential to make a significant contribution to the field of automated program verification, but it requires further evaluation and refinement to be considered a complete and reliable solution.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework, LEMUR, that combines Large Language Models (LLMs) and automated reasoners for automated program verification. The framework, LEMUR, is designed to leverage the strengths of both LLMs and automated reasoners to overcome the limitations of each approach. The authors present a formal calculus, LEMUR, that combines LLMs and automated reasoners, and provide a practical algorithmic instantiation of the calculus. They demonstrate the efficiency of LEMUR on standard benchmark sets and investigate its ability to solve hard benchmarks from the Software-Verification Competition 2023. The authors also discuss the current limitations of LEMUR and propose future research directions to extend the framework.\n\nStrengths:\n\n1. The paper presents a novel framework, LEMUR, that combines LLMs and automated reasoners for automated program verification, addressing the limitations of each approach.\n2. The authors provide a formal calculus, LEMUR, that combines LLMs and automated reasoners, and present a practical algorithmic instantiation of the calculus.\n3. The paper demonstrates the efficiency of LEMUR on standard benchmark sets and investigates its ability to solve hard benchmarks from the Software-Verification Competition 2023.\n4. The authors discuss the current limitations of LEMUR and propose future research directions to extend the framework.\n\nWeaknesses:\n\n1. The paper assumes that the verifier, V, is sound, but does not provide any evidence to support this assumption.\n2. The authors do not provide any empirical evaluation of the proposed framework, LEMUR, on a large-scale benchmark set.\n3. The paper does not discuss any countermeasures to handle the potential issues with LLMs, such as bias, fairness, and robustness.\n4. The authors do not provide any detailed discussion on how to handle the complexity of programs with multiple loops.\n\nSuggestions for Improvement:\n\n1. Provide empirical evaluation of the proposed framework, LEMUR, on a large-scale benchmark set to demonstrate its efficiency and effectiveness.\n2. Discuss countermeasures to handle the potential issues with LLMs, such as bias, fairness, and robustness.\n3. Provide detailed discussion on how to handle the complexity of programs with multiple loops.\n4. Consider extending the framework, LEMUR, to handle more complex properties, such as temporal properties.\n\nRecommendation:\n\nThis paper makes a significant contribution to the area of automated program verification by proposing a novel framework, LEMUR, that combines LLMs and automated reasoners. However, there are some limitations and weaknesses that need to be addressed. With some improvements, this paper has the potential to be published in a top-tier conference.",
        "gr_score_with": 0.3724137931034483,
        "gr_score_without": 0.3069620253164557,
        "output_without": {
            "num_tokens_scored": 316,
            "num_green_tokens": 97,
            "green_fraction": 0.3069620253164557,
            "z_score": -3.3759668094834967,
            "p_value": 0.999632216038866
        },
        "output_with": {
            "num_tokens_scored": 290,
            "num_green_tokens": 108,
            "green_fraction": 0.3724137931034483,
            "z_score": -0.9589266029707684,
            "p_value": 0.8312021390779591
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks with small initialization. The authors consider a training dataset with well-separated input vectors and provide a careful analysis of the neurons' directional dynamics. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nStrengths:\n\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5, which is a significant improvement over previous work.\n3. The paper provides a quantitative bound on the time it takes for good alignment to be achieved, which is O(logn\u221a\u00b5).\n4. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nWeaknesses:\n\n1. The paper assumes that the training dataset has well-separated input vectors, which may not be a realistic assumption in practice.\n2. The authors use a balanced initialization scheme, which may not be a common initialization scheme in practice.\n3. The paper does not provide a detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n2. The paper should be extended to consider more realistic initialization schemes and training datasets.\n3. The authors should provide more numerical experiments to validate their theoretical findings.\n\nRecommendation:\n\nThis paper provides a significant contribution to the understanding of the dynamics of gradient flow for training two-layer ReLU networks with small initialization. The authors' analysis is thorough and provides a quantitative bound on the time it takes for good alignment to be achieved. However, the paper assumes a balanced initialization scheme and well-separated input vectors, which may not be realistic in practice. Therefore, I recommend that the authors extend their analysis to more realistic initialization schemes and training datasets.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper studies the problem of training a two-layer ReLU network with small initialization using gradient flow. It considers a binary classification task with a dataset where any pair of input vectors with the same label are positively correlated, and any pair with different labels are negatively correlated. The analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows the authors to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data. After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank.\n\nStrengths:\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. It shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer.\n3. A careful analysis of the neurons' directional dynamics allows the authors to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data.\n4. The paper provides a numerical experiment on the MNIST dataset that illustrates the theoretical findings.\n\nWeaknesses:\n1. The analysis relies heavily on the assumption that the data are well-separated, which may not be a realistic assumption in many cases.\n2. The paper assumes that the second-layer weights are initialized with a balanced distribution, which may not be a realistic assumption in many cases.\n3. The analysis of the alignment phase relies on a finite but small initialization scale \u03f5, which may not be a realistic assumption in many cases.\n4. The paper does not provide a thorough analysis of the convergence rate of the loss after the early alignment phase.\n\nSuggestions for Improvement:\n1. To make the analysis more realistic, the authors could consider relaxing the assumption of well-separated data and consider more general data distributions.\n2. To make the analysis more realistic, the authors could consider relaxing the assumption of balanced second-layer weights and consider more general initialization schemes.\n3. To make the analysis more realistic, the authors could consider analyzing the alignment phase with a finite but not necessarily small initialization scale \u03f5.\n4. To improve the analysis, the authors could consider providing a thorough analysis of the convergence rate of the loss after the early alignment phase.\n\nRecommendation:\nThe paper provides a thorough analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. The analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows the authors to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data. However, the analysis relies heavily on the assumption that the data are well-separated, which may not be a realistic assumption in many cases. Therefore, I recommend that the authors consider relaxing this assumption and providing a more general analysis that applies to a wider range of data distributions.\n\nOverall, the paper provides a thorough and rigorous analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. While the analysis relies heavily on the assumption that the data are well-separated, which may not be a realistic assumption in many cases, the paper provides a thorough and rigorous analysis that is a significant contribution to the field of deep learning.\n\n$$$",
        "gr_score_with": 0.45689655172413796,
        "gr_score_without": 0.41194029850746267,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 138,
            "green_fraction": 0.41194029850746267,
            "z_score": 0.4460997367454705,
            "p_value": 0.3277626014843311
        },
        "output_with": {
            "num_tokens_scored": 348,
            "num_green_tokens": 159,
            "green_fraction": 0.45689655172413796,
            "z_score": 2.1665561421854203,
            "p_value": 0.015134357314086567
        }
    },
    {
        "title": "VONET: UNSUPERVISED VIDEO OBJECT LEARNING WITH PARALLEL U-NET ATTENTION",
        "abstract": "Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents VONet, a novel approach for unsupervised video object learning. VONet is inspired by MONet, a scene decomposition model for single images, but extends it to handle video sequences. The key innovation in VONet is its parallel attention process, which generates attention masks for all slots simultaneously from a U-Net. Additionally, VONet incorporates an object-wise sequential VAE framework to achieve temporal consistency of objects between adjacent video frames. The paper conducts extensive evaluations on five MOVI datasets, showcasing VONet's superiority over several strong baselines.\n\nStrengths:\n\n1. The paper presents a novel and efficient approach for unsupervised video object learning, which outperforms several strong baselines.\n2. The parallel attention process in VONet is a significant innovation, allowing for simultaneous attention mask generation for all slots.\n3. The object-wise sequential VAE framework is a novel and effective way to achieve temporal consistency of objects between adjacent video frames.\n4. The paper provides a comprehensive evaluation of VONet on five MOVI datasets, demonstrating its effectiveness in handling videos of diverse complexities.\n\nWeaknesses:\n\n1. The paper assumes that the number of slots is known a priori, which may not be the case in practice.\n2. The paper does not provide a detailed analysis of the computational complexity of VONet.\n3. The paper does not provide a clear explanation of how the context vectors are acquired.\n4. The paper does not provide a comparison with other state-of-the-art methods for unsupervised video object learning.\n\nSuggestions for Improvement:\n\n1. Provide a detailed analysis of the computational complexity of VONet and discuss potential optimizations.\n2. Provide a clear explanation of how the context vectors are acquired and how they are used in the parallel attention process.\n3. Compare VONet with other state-of-the-art methods for unsupervised video object learning and discuss the advantages and disadvantages of each approach.\n4. Provide a more detailed evaluation of VONet on a wider range of datasets and scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for unsupervised video object learning, which outperforms several strong baselines. However, there are some weaknesses and limitations that need to be addressed. With some additional analysis and evaluation, VONet has the potential to become a leading method in the field of unsupervised video object learning.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces VONet, an unsupervised video object learning method that generates high-quality object representations from video frames. VONet is inspired by MONet, a scene decomposition model for single images, and extends it to video frames using a parallel attention process. The method generates attention masks for all slots simultaneously using a U-Net architecture and a context vector, and uses an object-wise sequential VAE framework to achieve temporal consistency. VONet is evaluated on five public datasets and outperforms several state-of-the-art unsupervised video object learning methods in terms of both FG-ARI and mIoU metrics.\n\nStrengths:\n\n1. VONet's parallel attention process allows for efficient inference and generation of attention masks for all slots simultaneously, which is a significant improvement over traditional recurrent attention processes.\n2. The use of context vectors and object-wise sequential VAE framework effectively tackles the challenge of temporal consistency in video object learning.\n3. VONet's ability to generate high-quality object representations is demonstrated across five datasets, including complex video scenes with textured objects and diverse shapes and sizes.\n4. The paper provides a comprehensive evaluation and comparison with state-of-the-art unsupervised video object learning methods, including SCALOR, ViMON, SIMONe, SAVI, and STEVE.\n\nWeaknesses:\n\n1. The paper assumes that the number of slots is fixed and does not discuss how to adapt to varying numbers of objects in a scene.\n2. The use of a replay buffer to enable training from past slot states may not be suitable for all scenarios, and alternative methods should be explored.\n3. The paper does not discuss how to handle failures modes, such as over-segmentation and incomplete object understanding, and how to improve the KLD loss.\n4. The paper assumes that the video frames are resized to 128x128, which may not be suitable for all scenarios, and alternative resolutions should be explored.\n\nSuggestions for Improvement:\n\n1. Explore alternative methods for adapting to varying numbers of objects in a scene, such as using a dynamic number of slots or using a different attention mechanism.\n2. Investigate alternative methods for enabling training from past slot states, such as using a different replay buffer or using a different training strategy.\n3. Explore alternative methods for handling failures modes, such as using extra losses to penalize the use of too many slots or using pretrained knowledge about the appearances of everyday objects.\n4. Explore alternative resolutions and input sizes to evaluate VONet's performance on different scenarios.\n\nRecommendation:\n\nThe paper provides a comprehensive and well-evaluated method for unsupervised video object learning, and its strengths and contributions make it a valuable contribution to the field. However, the paper's weaknesses and limitations should be addressed in future work. Therefore, I recommend this paper for publication, pending revisions to address the mentioned weaknesses and limitations.\n\n$$$",
        "gr_score_with": 0.5083532219570406,
        "gr_score_without": 0.4360655737704918,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 133,
            "green_fraction": 0.4360655737704918,
            "z_score": 1.285692601083507,
            "p_value": 0.09927518223191267
        },
        "output_with": {
            "num_tokens_scored": 419,
            "num_green_tokens": 213,
            "green_fraction": 0.5083532219570406,
            "z_score": 4.5273411424595364,
            "p_value": 2.9865235983658843e-06
        }
    },
    {
        "title": "FLOW MATCHING ON GENERAL GEOMETRIES",
        "abstract": "We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world nonEuclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Riemannian Flow Matching (RFM), a novel framework for training continuous normalizing flows on manifolds. RFM builds upon the Flow Matching framework and learns a continuous normalizing flow by regressing an implicitly defined target vector field that pushes a base distribution towards a target distribution. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. RFM offers several advantages over previous approaches, including simulation-free training on simple geometries, no need for divergence computation, and exact conditional vector fields. The authors demonstrate the effectiveness of RFM on various manifolds, including simple geometries, high-dimensional tori, and manifolds with non-trivial curvature and boundaries.\n\nStrengths:\n\n1. The paper presents a novel and highly scalable approach for training continuous normalizing flows on manifolds.\n2. RFM is completely simulation-free and introduces zero approximation errors on simple geometries with closed-form geodesics.\n3. The authors demonstrate the effectiveness of RFM on various manifolds, including simple geometries, high-dimensional tori, and manifolds with non-trivial curvature and boundaries.\n4. The paper introduces a new premetric for defining target vector fields, which is simple and efficient to compute.\n5. RFM does not require divergence computation, which makes it more efficient and scalable than previous approaches.\n\nWeaknesses:\n\n1. The paper assumes that the manifold is smooth and connected, which may not be the case in practice.\n2. The authors do not provide a detailed analysis of the computational complexity of RFM, which may be important for large-scale applications.\n3. The paper assumes that the target distribution is known, which may not be the case in practice.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods for training continuous normalizing flows on manifolds.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of RFM and its scalability.\n2. Investigate the case where the manifold is not smooth and connected.\n3. Explore the possibility of learning the target distribution from data.\n4. Provide a more detailed comparison with other state-of-the-art methods for training continuous normalizing flows on manifolds.\n5. Investigate the use of RFM for other applications, such as image and video generation.\n\nRecommendation:\n\nThe paper presents a novel and highly scalable approach for training continuous normalizing flows on manifolds. The authors demonstrate the effectiveness of RFM on various manifolds, including simple geometries, high-dimensional tori, and manifolds with non-trivial curvature and boundaries. While there are some weaknesses and limitations, the paper provides a valuable contribution to the field of generative modeling on manifolds. I recommend accepting this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Riemannian Flow Matching (RFM), a novel framework for training continuous normalizing flows on general Riemannian manifolds. RFM builds upon the Flow Matching framework and introduces a simple yet powerful approach for learning generative models on manifolds. The key contribution of RFM is the construction of a premetric for defining target vector fields, which allows for simulation-free training on simple geometries and efficient computation on general geometries. The paper also introduces spectral distances as an alternative to geodesic distances for general Riemannian manifolds, which offers benefits such as robustness to topological noise and smoothness. Experimental results on various datasets, including earth and climate science, protein structures, and high-dimensional tori, demonstrate the effectiveness and scalability of RFM.\n\nStrengths:\n\n1. RFM is a highly scalable approach for training continuous normalizing flows on manifolds, with zero approximation errors on simple geometries.\n2. The framework is simple and intuitive, with a clear mathematical formulation and efficient computation on general geometries.\n3. RFM introduces spectral distances as an alternative to geodesic distances, which offers benefits such as robustness to topological noise and smoothness.\n4. The paper provides a comprehensive experimental evaluation on various datasets, demonstrating the effectiveness and scalability of RFM.\n\nWeaknesses:\n\n1. The paper assumes a certain level of mathematical background in Riemannian geometry, which may make it inaccessible to some readers.\n2. The experimental results are limited to a few datasets, and it would be beneficial to include more diverse and challenging examples.\n3. The paper does not provide a detailed analysis of the computational complexity of RFM, which is an important consideration for large-scale applications.\n4. The framework relies on the construction of a premetric, which may require manual tuning or adaptation for specific applications.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed mathematical derivation of RFM, including a clear explanation of the premetric construction and its properties.\n2. Include more experimental results on diverse and challenging datasets, such as those with non-trivial boundaries or high-dimensional structures.\n3. Conduct a thorough analysis of the computational complexity of RFM, including a comparison with other state-of-the-art methods.\n4. Develop a more systematic approach for constructing premetrics, which could make RFM more applicable to a wider range of applications.\n\nRecommendation:\n\nOverall, the paper provides a significant contribution to the field of generative modeling on manifolds, with a novel and highly scalable approach that is simple to implement and efficient to compute. However, the paper could benefit from a more detailed mathematical derivation, more experimental results, and a more systematic approach for constructing premetrics. With these improvements, RFM has the potential to become a widely used and influential framework in the field.",
        "gr_score_with": 0.4755784061696658,
        "gr_score_without": 0.39296187683284456,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 134,
            "green_fraction": 0.39296187683284456,
            "z_score": -0.26529461297122975,
            "p_value": 0.6046087429465818
        },
        "output_with": {
            "num_tokens_scored": 389,
            "num_green_tokens": 185,
            "green_fraction": 0.4755784061696658,
            "z_score": 3.0427544685340693,
            "p_value": 0.0011721176017211836
        }
    },
    {
        "title": "MODERNTCN: A MODERN PURE CONVOLUTION STRUCTURE FOR GENERAL TIME SERIES ANALYSIS",
        "abstract": "Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel time series analysis model called ModernTCN, which is a modern pure convolution structure for general time series analysis. The authors aim to bring convolution back to the arena of time series analysis by modernizing the traditional TCN and conducting time series related modifications to make it more suitable for time series tasks. The proposed model achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection. The authors also demonstrate the efficiency superiority of ModernTCN compared to state-of-the-art Transformer-based models.\n\nStrengths:\n1. The paper provides a thorough analysis of the limitations of traditional convolution-based models in time series analysis and proposes a novel solution to overcome these limitations.\n2. The authors demonstrate the excellent task-generality of ModernTCN, achieving consistent state-of-the-art performance on multiple mainstream time series analysis tasks.\n3. The proposed model provides a better balance of efficiency and performance compared to state-of-the-art Transformer-based models.\n4. The authors conduct comprehensive experiments to evaluate the performance of ModernTCN on various time series analysis tasks and provide detailed analysis of the results.\n\nWeaknesses:\n1. The paper assumes that the input time series data is stationary, which may not be the case in many real-world applications.\n2. The authors do not provide a detailed explanation of the time series related modifications made to the traditional TCN block.\n3. The paper does not provide a clear comparison with other convolution-based models that have been proposed in the literature.\n4. The authors do not provide a detailed analysis of the computational complexity of ModernTCN.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation of the time series related modifications made to the traditional TCN block.\n2. The paper should include a comparison with other convolution-based models that have been proposed in the literature.\n3. The authors should provide a detailed analysis of the computational complexity of ModernTCN.\n4. The paper should include a discussion of the potential applications of ModernTCN in real-world time series analysis tasks.\n\nRecommendation:\nThe paper proposes a novel and efficient time series analysis model that achieves consistent state-of-the-art performance on multiple mainstream time series analysis tasks. The authors provide a thorough analysis of the limitations of traditional convolution-based models and propose a novel solution to overcome these limitations. However, the paper assumes that the input time series data is stationary, which may not be the case in many real-world applications. The authors should provide a more detailed explanation of the time series related modifications made to the traditional TCN block and include a comparison with other convolution-based models that have been proposed in the literature. Despite these limitations, the paper is well-written and provides a valuable contribution to the field of time series analysis.\n\nOverall, I would recommend this paper for publication in a top-tier conference or journal in the field of time series analysis. However, the authors should address the limitations mentioned above to make the paper more comprehensive and robust.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel convolution-based model, ModernTCN, for general time series analysis. By modernizing the traditional TCN block with time series related modifications, ModernTCN achieves consistent state-of-the-art performance on five mainstream analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection, while maintaining the efficiency advantage of convolution-based models.\n\nStrengths:\n1. The paper presents a novel approach to modernize the traditional TCN block for time series analysis, which is a significant contribution to the field.\n2. The experimental results show that ModernTCN achieves consistent state-of-the-art performance on five mainstream analysis tasks, demonstrating its excellent task-generality.\n3. The paper provides a detailed analysis of the ERF of ModernTCN, which explains why the model can provide better performance than previous convolution-based models.\n4. The authors conduct ablation studies to validate the effectiveness of their design, which provides evidence for the necessity and effectiveness of the decoupled modification to the ConvFFN module.\n\nWeaknesses:\n1. The paper assumes that the input time series is multivariate, which may not be the case for all time series analysis tasks.\n2. The authors do not provide a detailed analysis of the computational complexity of ModernTCN, which may be a concern for large-scale time series analysis tasks.\n3. The paper does not discuss the interpretability of the model, which is an important aspect of time series analysis.\n\nSuggestions for Improvement:\n1. To make the model more generalizable, the authors can consider incorporating additional features, such as seasonality, trend, and noise, into the model.\n2. To reduce the computational complexity of the model, the authors can consider using pruning or quantization techniques to reduce the number of parameters.\n3. To improve the interpretability of the model, the authors can consider using visualization tools, such as heatmaps, to show the importance of each feature in the model.\n\nRecommendation:\nThe paper presents a novel approach to modernize the traditional TCN block for time series analysis, which is a significant contribution to the field. The experimental results show that ModernTCN achieves consistent state-of-the-art performance on five mainstream analysis tasks, demonstrating its excellent task-generality. However, the paper assumes that the input time series is multivariate, which may not be the case for all time series analysis tasks. Therefore, I recommend that the authors consider incorporating additional features, such as seasonality, trend, and noise, into the model to make it more generalizable. Additionally, the authors should consider using pruning or quantization techniques to reduce the number of parameters and improve the interpretability of the model using visualization tools, such as heatmaps.\n$$$\nPlease note that the review is written in a standard peer review format, which is typically 1-2 pages long, and includes a summary of the paper, strengths, weaknesses, suggestions for improvement, and a recommendation.",
        "gr_score_with": 0.5571428571428572,
        "gr_score_without": 0.44375,
        "output_without": {
            "num_tokens_scored": 320,
            "num_green_tokens": 142,
            "green_fraction": 0.44375,
            "z_score": 1.5975241260567346,
            "p_value": 0.05507446209297346
        },
        "output_with": {
            "num_tokens_scored": 350,
            "num_green_tokens": 195,
            "green_fraction": 0.5571428571428572,
            "z_score": 6.000991981489791,
            "p_value": 9.805783836714955e-10
        }
    },
    {
        "title": "NAVIGATING TEXT-TO-IMAGE CUSTOMIZATION: FROM",
        "abstract": "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied finetuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces LyCORIS, an open-source library that provides a wide range of fine-tuning methodologies for Stable Diffusion, a leading open-source text-to-image generative model. The library offers a spectrum of methodologies, including LoRA, LoHa, and LoKr, among others. The authors also propose a comprehensive evaluation framework that incorporates a diverse suite of metrics to capture key aspects of fine-tuning, including concept fidelity, text-image alignment, image diversity, and preservation of the base model's style. Through extensive experiments, the authors compare the performances of different fine-tuning algorithms implemented in LyCORIS and assess the impacts of various hyperparameters, offering insights into how these factors influence the results.\n\nStrengths:\n\n1. The paper introduces a comprehensive evaluation framework that captures key aspects of fine-tuning, making it a valuable contribution to the field.\n2. The authors provide a wide range of fine-tuning methodologies in the LyCORIS library, making it a useful resource for researchers and practitioners.\n3. The paper includes extensive experiments that compare the performances of different fine-tuning algorithms and assess the impacts of various hyperparameters.\n4. The authors provide a clear and concise explanation of the methods and experiments, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes a certain level of background knowledge in deep learning and generative models, which may make it challenging for readers without this background to fully understand the content.\n2. The evaluation framework proposed in the paper is comprehensive, but it may be challenging to implement and evaluate, especially for researchers without extensive experience in fine-tuning and evaluation of generative models.\n3. The paper focuses on the Stable Diffusion model, which may limit its applicability to other generative models.\n4. The authors do not explore the task of generating images with multiple learned concepts, which is an important aspect of fine-tuning and evaluation.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the background knowledge required to understand the content, including a brief introduction to deep learning and generative models.\n2. Include a more detailed explanation of the evaluation framework and its implementation, including code snippets and examples.\n3. Explore the applicability of the LyCORIS library and evaluation framework to other generative models, including models with different architectures and capabilities.\n4. Include a more detailed discussion of the limitations and challenges of the paper, including the assumption of a certain level of background knowledge and the focus on a single generative model.\n\nRecommendation:\n\nThe paper is a valuable contribution to the field of fine-tuning and evaluation of generative models, and it provides a comprehensive evaluation framework and a wide range of fine-tuning methodologies in the LyCORIS library. However, it assumes a certain level of background knowledge and focuses on a single generative model, which may limit its applicability and accessibility. Overall, I recommend this paper to researchers and practitioners with a strong background in deep learning and generative models, and I suggest that the authors provide more detailed explanations and examples to make the content more accessible to a wider audience.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents LyCORIS, an open-source library that provides a wide selection of fine-tuning methodologies for Stable Diffusion, a text-to-image generative model. LyCORIS offers a comprehensive framework for the systematic assessment of varied fine-tuning techniques, including hyperparameter adjustments and evaluation with different prompt types across various concept categories. The library includes a diverse suite of metrics, capturing key aspects such as concept fidelity, text-image alignment, image diversity, and preservation of the base model's style.\n\nStrengths:\n\n* LyCORIS provides a comprehensive framework for the systematic assessment of varied fine-tuning techniques, including hyperparameter adjustments and evaluation with different prompt types across various concept categories.\n* The library includes a diverse suite of metrics, capturing key aspects such as concept fidelity, text-image alignment, image diversity, and preservation of the base model's style.\n* The paper presents a thorough analysis of the impact of the choice of the algorithm and their configuration on model performance, revealing their relative strengths and limitations.\n\nWeaknesses:\n\n* The paper assumes that the reader has a good understanding of the underlying concepts of text-to-image generative models, diffusion models, and LoRA.\n* The evaluation metrics used in the paper may not be comprehensive or robust, as the authors acknowledge that the quality of the generated images cannot be effectively measured.\n* The paper does not provide a detailed comparison of the performance of LyCORIS with other fine-tuning libraries or methods.\n\nSuggestions for Improvement:\n\n* Provide a more detailed introduction to the underlying concepts of text-to-image generative models, diffusion models, and LoRA, as well as the motivation for developing LyCORIS.\n* Expand the evaluation metrics to include more comprehensive and robust measures of image quality, such as visual similarity metrics or human evaluation.\n* Conduct a more detailed comparison of the performance of LyCORIS with other fine-tuning libraries or methods, including those that are widely used in the research community.\n\nRecommendation:\n\nThe paper presents a comprehensive and well-structured framework for the systematic assessment of varied fine-tuning techniques for text-to-image generative models. The library, LyCORIS, provides a diverse suite of metrics and includes a wide selection of fine-tuning methodologies. However, the paper assumes that the reader has a good understanding of the underlying concepts, and the evaluation metrics may not be comprehensive or robust. Therefore, the recommendation is to expand the evaluation metrics and conduct a more detailed comparison of the performance of LyCORIS with other fine-tuning libraries or methods.\n\n$$$",
        "gr_score_with": 0.6585365853658537,
        "gr_score_without": 0.49246231155778897,
        "output_without": {
            "num_tokens_scored": 398,
            "num_green_tokens": 196,
            "green_fraction": 0.49246231155778897,
            "z_score": 3.765309341722265,
            "p_value": 8.317150613596575e-05
        },
        "output_with": {
            "num_tokens_scored": 287,
            "num_green_tokens": 189,
            "green_fraction": 0.6585365853658537,
            "z_score": 8.94040794980537,
            "p_value": 1.9386815794960535e-19
        }
    },
    {
        "title": "ENHANCING NEURAL TRAINING VIA A CORRELATED DYNAMICS MODEL",
        "abstract": "As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this, we introduce correlation mode decomposition (CMD). This algorithm clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables CMD to efficiently represent the training dynamics of complex networks, like ResNets and Transformers, using only a few modes. Moreover, test set generalization is enhanced. We introduce an efficient CMD variant, designed to run concurrently with training. Our experiments indicate that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification. Our modeling can improve training efficiency and lower communication overhead, as shown by our preliminary experiments in the context of federated learning.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process. The authors introduce Correlation Mode Decomposition (CMD), a data-driven approach that efficiently models training dynamics. They show that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification and propose an efficient CMD variant that can be performed online during training. The authors also pioneer the use of modeled training dynamics to reduce communication overhead in distributed learning.\n\nStrengths:\n\n1. The paper presents a novel and effective approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process.\n2. The authors provide a comprehensive experimental evaluation, showcasing the benefits of their approach in various architectures and learning tasks.\n3. The paper proposes an efficient CMD variant that can be performed online during training, reducing computational overhead and memory requirements.\n4. The authors demonstrate the efficiency of their approach in Federated Learning, improving communication overhead without compromising performance.\n\nWeaknesses:\n\n1. The paper assumes a fixed number of modes, which may not be optimal for all cases. The authors could explore more adaptive methods to determine the number of modes.\n2. The experimental evaluation focuses primarily on image classification tasks. The authors could explore the applicability of their approach to other learning tasks, such as regression or reinforcement learning.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed method. The authors could provide more insights into the computational requirements of their approach.\n\nSuggestions for Improvement:\n\n1. Explore more adaptive methods to determine the number of modes, such as using techniques from density estimation or clustering.\n2. Evaluate the proposed approach on a broader range of learning tasks, including regression and reinforcement learning.\n3. Provide a more detailed analysis of the computational complexity of the proposed method, including a discussion of the memory and computational requirements.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to model neural network parameter dynamics, leveraging the underlying correlated behavior of the training process. The authors provide a comprehensive experimental evaluation, showcasing the benefits of their approach in various architectures and learning tasks. While there are some limitations to the paper, the proposed approach has the potential to improve the efficiency and effectiveness of neural network training. Therefore, I recommend accepting this paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThis paper proposes a novel approach, Correlation Mode Decomposition (CMD), for modeling neural network training dynamics. The authors observe that neural network parameters exhibit intrinsic correlations over time, which are exploited to cluster the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables CMD to efficiently represent the training dynamics of complex networks using only a few modes, enhancing test set generalization. The authors also introduce an efficient CMD variant, designed to run concurrently with training, and demonstrate its effectiveness in reducing communication overhead in federated learning.\n\n**Strengths**\n\n1. The authors present a novel observation about the intrinsic correlations between neural network parameters, which is a significant contribution to the field.\n2. The proposed CMD approach is able to effectively model the training dynamics of complex networks using only a few modes, making it computationally efficient.\n3. The authors demonstrate the effectiveness of CMD in enhancing test set generalization and reducing communication overhead in federated learning.\n4. The paper provides a comprehensive evaluation of the proposed method, including comparisons with state-of-the-art approaches.\n\n**Weaknesses**\n\n1. The authors assume that the neural network parameters exhibit intrinsic correlations, which may not always be the case. It would be beneficial to provide more evidence for this assumption.\n2. The proposed CMD approach relies on the accuracy of the reference weights, which may not always be accurate. It would be beneficial to provide more information about how the reference weights are selected.\n3. The authors do not provide a thorough analysis of the computational complexity of the proposed method, which may be a limitation for large-scale applications.\n4. The paper could benefit from more detailed comparisons with existing approaches, including both theoretical and experimental evaluations.\n\n**Suggestions for Improvement**\n\n1. Provide more evidence for the assumption that neural network parameters exhibit intrinsic correlations.\n2. Develop more robust methods for selecting reference weights, such as using multiple reference weights or incorporating additional information.\n3. Perform a more thorough analysis of the computational complexity of the proposed method, including its scalability for large-scale applications.\n4. Provide more detailed comparisons with existing approaches, including both theoretical and experimental evaluations.\n\n**Recommendation**\n\nThis paper presents a novel and effective approach for modeling neural network training dynamics. While there are some limitations, the proposed CMD approach has significant potential for improving the performance and efficiency of neural network training. I recommend this paper for publication, with some minor revisions to address the suggested improvements.",
        "gr_score_with": 0.4277286135693215,
        "gr_score_without": 0.2833333333333333,
        "output_without": {
            "num_tokens_scored": 300,
            "num_green_tokens": 85,
            "green_fraction": 0.2833333333333333,
            "z_score": -4.124789556921527,
            "p_value": 0.9999814463056153
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 145,
            "green_fraction": 0.4277286135693215,
            "z_score": 1.042131164872459,
            "p_value": 0.14867543571541209
        }
    },
    {
        "title": "RingAttention with Blockwise Transformers for Near-Infinite Context",
        "abstract": "Transformers have emerged as the architecture of choice for many state-ofthe-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Blockwise RingAttention, which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach, RingAttention with Blockwise Transformers, to reduce the memory requirements of Transformers and enable them to handle long sequences. The authors leverage blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices, overlapping the communication of key-value blocks with the computation of blockwise attention. This approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers. The authors demonstrate the effectiveness of their approach through extensive experiments on language modeling and reinforcement learning tasks.\n\nStrengths:\n\n1. The paper presents a novel approach to reduce the memory requirements of Transformers, which is a significant contribution to the field.\n2. The authors provide a thorough analysis of the memory requirements of Transformers and identify the key challenges in scaling up the context length.\n3. The paper presents a clear and concise description of the RingAttention with Blockwise Transformers approach and its implementation.\n4. The authors provide extensive experimental results to demonstrate the effectiveness of their approach.\n5. The paper discusses the implications of the approach and its potential applications in various fields.\n\nWeaknesses:\n\n1. The paper assumes a high-level understanding of Transformers and their architecture, which may not be familiar to readers who are new to the field.\n2. The authors do not provide a detailed comparison with other approaches to reducing memory requirements in Transformers.\n3. The paper assumes a specific hardware setup (GPUs and TPUs) and does not discuss the potential challenges in implementing the approach on other hardware platforms.\n4. The authors do not provide a detailed analysis of the computational complexity of the RingAttention with Blockwise Transformers approach.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other approaches to reducing memory requirements in Transformers.\n2. Discuss the potential challenges in implementing the approach on other hardware platforms.\n3. Provide a more detailed analysis of the computational complexity of the RingAttention with Blockwise Transformers approach.\n4. Consider providing more experimental results to demonstrate the effectiveness of the approach on different tasks and datasets.\n5. Consider discussing the potential applications of the approach in various fields and providing more insights into its implications.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to reducing the memory requirements of Transformers. The authors provide a thorough analysis of the memory requirements of Transformers and identify the key challenges in scaling up the context length. The paper presents a clear and concise description of the RingAttention with Blockwise Transformers approach and its implementation. The authors provide extensive experimental results to demonstrate the effectiveness of their approach. However, the paper assumes a high-level understanding of Transformers and their architecture, and does not provide a detailed comparison with other approaches to reducing memory requirements in Transformers. Therefore, I recommend that the authors revise the paper to address these issues and provide a more comprehensive analysis of the approach.\n\nOverall, the paper is well-written, and the authors provide a clear and concise description of the RingAttention with Blockwise Transformers approach. The paper presents a novel and effective approach to reducing the memory requirements of Transformers, and the authors provide extensive experimental results to demonstrate the effectiveness of their approach. However, the paper assumes a high-level understanding of Transformers and their architecture, and does not provide a detailed comparison with other approaches to reducing memory requirements in Transformers. Therefore, I recommend that the authors revise the paper to address these issues and provide a more comprehensive analysis of the approach.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach, RingAttention with Blockwise Transformers, to address the memory constraints of Transformers when dealing with long sequences. The proposed method leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices, fully overlapping the communication of key-value blocks with the computation of blockwise attention. The authors demonstrate the effectiveness of their approach through extensive experiments on language modeling and reinforcement learning tasks, showcasing improved performance and scalability.\n\nStrengths:\n\n1. The paper addresses a significant challenge in the field of Transformers, namely, the memory constraints imposed by individual devices when dealing with long sequences.\n2. The proposed RingAttention with Blockwise Transformers approach is innovative and effective, enabling the training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers.\n3. The authors provide comprehensive experiments on language modeling and reinforcement learning tasks, demonstrating the improved performance and scalability of their approach.\n4. The paper contributes to the field of memory-efficient computation for Transformers, an area of ongoing research and development.\n\nWeaknesses:\n\n1. The paper assumes that the block size is sufficient to overlap communication and computation, which may not always be the case, and requires further analysis and evaluation.\n2. The authors rely on existing memory-efficient techniques, such as blockwise computation of self-attention and feedforward, without providing a comprehensive comparison of their approach with other memory-efficient methods.\n3. The paper does not provide a thorough analysis of the communication overheads and their impact on the overall performance of the proposed approach.\n4. The authors do not discuss the potential limitations and challenges of their approach, such as the need for careful tuning of the block size and the potential impact on model performance.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive analysis of the communication overheads and their impact on the overall performance of the proposed approach.\n2. The paper should include a thorough comparison of the proposed approach with other memory-efficient methods, highlighting the strengths and weaknesses of each approach.\n3. The authors should discuss the potential limitations and challenges of their approach, such as the need for careful tuning of the block size and the potential impact on model performance.\n4. The paper should provide more details on the implementation and evaluation of the proposed approach, including the choice of hyperparameters and the evaluation metrics used.\n\nRecommendation:\n\nThe paper proposes an innovative and effective approach to address the memory constraints of Transformers when dealing with long sequences. However, the paper assumes that the block size is sufficient to overlap communication and computation, which may not always be the case, and requires further analysis and evaluation. The authors should provide a more comprehensive analysis of the communication overheads and their impact on the overall performance of the proposed approach, as well as a thorough comparison with other memory-efficient methods. With these improvements, the paper would make a significant contribution to the field of memory-efficient computation for Transformers.\n\nScore: 7/10",
        "gr_score_with": 0.5263157894736842,
        "gr_score_without": 0.41304347826086957,
        "output_without": {
            "num_tokens_scored": 322,
            "num_green_tokens": 133,
            "green_fraction": 0.41304347826086957,
            "z_score": 0.47776654295295323,
            "p_value": 0.3164081882821973
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 180,
            "green_fraction": 0.5263157894736842,
            "z_score": 4.768316485434156,
            "p_value": 9.288589327276183e-07
        }
    },
    {
        "title": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS",
        "abstract": "Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https://github.com/THUDM/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256\u00d7256 and CelebA-HQ 256\u00d7256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256\u00d7256 generation without any guidance. RDM can achieve a FID of 1.99 (and a class-balanced FID of 1.87) if with classifier-free guidance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel cascaded diffusion model, called Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion models via blurring diffusion and block noise. The RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM, and DiT by a large margin. The authors also analyze the reasons of the difficulty of noise scheduling in high-resolution diffusion models in frequency domain and introduce the block noise to bridge the gap.\n\nStrengths:\n\n* The paper provides a thorough analysis of the frequency spectrum of images with different resolutions and introduces the concept of block noise, which is a significant contribution to the field of diffusion models.\n* The proposed RDM model achieves state-of-the-art performance on CelebA-HQ and ImageNet 256\u00d7256, demonstrating its effectiveness in high-resolution image synthesis.\n* The authors provide a detailed explanation of the RDM architecture, including the blurring diffusion and block noise components, which is easy to follow and understand.\n\nWeaknesses:\n\n* The paper assumes that the noise schedule is resolution-dependent, which may not be the case in all scenarios. Further investigation is needed to confirm this assumption.\n* The authors do not provide a theoretical derivation of the optimal noise schedule, which is a significant limitation of the paper.\n* The paper relies heavily on the EDM framework and implementation, which may limit its applicability to other diffusion models.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of the frequency spectrum analysis and how it relates to the block noise concept.\n* Further investigation is needed to confirm the assumption that the noise schedule is resolution-dependent.\n* The authors should provide a more thorough comparison of the RDM model with other diffusion models, including those that do not use the EDM framework.\n* The paper should include more experiments on higher resolutions and different datasets to demonstrate the robustness and generalizability of the RDM model.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of diffusion models. However, the limitations mentioned above should be addressed in future work. With further investigation and experimentation, the RDM model has the potential to become a state-of-the-art diffusion model for high-resolution image synthesis.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Relay Diffusion Model (RDM), a new cascaded framework for high-resolution image synthesis. RDM aims to address the limitations of previous diffusion models in training efficiency and noise scheduling. By analyzing the frequency spectrum of natural images and the diffusion process, the authors introduce block noise to bridge the gap between low-resolution and high-resolution images. RDM starts diffusion from the result of the last stage, rather than conditioning on it and starting from pure noise. This approach enables seamless continuation of the diffusion process across different resolutions and models without restarting from pure noise or low-resolution conditioning.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the frequency spectrum of natural images and the diffusion process, which helps to understand the limitations of previous diffusion models.\n2. The introduction of block noise is a novel and effective solution to address the noise scheduling issue in high-resolution image synthesis.\n3. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM, and DiT by a large margin.\n4. The paper provides a comprehensive evaluation of RDM on different metrics, including FID, sFID, IS, Precision, and Recall.\n\nWeaknesses:\n\n1. The paper assumes that the block noise is effective in all scenarios, but it may not be suitable for all types of images or datasets.\n2. The training and sampling steps of RDM may be affected by the quality of the low-resolution images, which may lead to artifacts in the high-resolution images.\n3. The paper does not provide a theoretical framework for deriving the optimal noise schedule, which may limit the applicability of RDM to other datasets or scenarios.\n\nSuggestions for Improvement:\n\n1. Further investigation on the effectiveness of block noise in different scenarios and datasets.\n2. Evaluation of RDM on other metrics, such as visual quality and diversity.\n3. Development of a theoretical framework for deriving the optimal noise schedule.\n\nRecommendation:\n\nThe paper proposes a novel and effective solution to address the limitations of previous diffusion models in high-resolution image synthesis. RDM achieves state-of-the-art results on CelebA-HQ and ImageNet 256\u00d7256 and provides a comprehensive evaluation on different metrics. However, further investigation on the effectiveness of block noise and development of a theoretical framework for deriving the optimal noise schedule are necessary to fully understand the potential of RDM.\n\n$$$",
        "gr_score_with": 0.4764705882352941,
        "gr_score_without": 0.4085714285714286,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 143,
            "green_fraction": 0.4085714285714286,
            "z_score": 0.3273268353539886,
            "p_value": 0.3717103488476262
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 162,
            "green_fraction": 0.4764705882352941,
            "z_score": 2.87824837800531,
            "p_value": 0.001999450761052404
        }
    },
    {
        "title": "A TOPOLOGICAL PERSPECTIVE ON DEMYSTIFYING GNN-BASED LINK PREDICTION PERFORMANCE",
        "abstract": "Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies aim to improve the overall LP performance of GNNs, none have explored its varying performance across different nodes and its underlying reasons. To this end, we aim to demystify which nodes will perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit poorer LP performance, our empirical findings provide nuances to this viewpoint and prompt us to propose a better metric, Topological Concentration (TC), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that TC has a higher correlation with LP performance than other node-level topological metrics like degree and subgraph density, offering a better way to identify low-performing nodes than using degree. With TC, we discover a novel topological distribution shift issue in which newly joined neighbors of a node tend to become less interactive with that node\u2019s existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To make the computation of TC scalable, We further propose Approximated Topological Concentration (ATC) and theoretically/empirically justify its efficacy in approximating TC and reducing the computation complexity. Given the positive correlation between node TC and its LP performance, we explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing and discuss its effectiveness with limitations. Our code is publicly available at https://github.com/submission2024/Topo Concentration.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Topological Concentration (TC), a novel metric to measure the topological interaction between a node's local subgraph and its neighbors' subgraphs. The authors demonstrate that TC has a higher correlation with link prediction (LP) performance than other node-level topological metrics, such as degree and subgraph density. They also discover a novel topological distribution shift (TDS) issue, where newly joined neighbors of a node become less interactive with its existing neighbors, compromising the generalizability of node embeddings for LP. To address this issue, the authors propose Approximated Topological Concentration (ATC), a scalable version of TC. They also explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing process.\n\nStrengths:\n\n1. The paper proposes a novel and effective metric, TC, which characterizes the node LP performance from a topological perspective.\n2. The authors demonstrate the superiority of TC over other common node topological properties, such as degree and subgraph density.\n3. The paper discovers a novel TDS issue, which highlights the importance of considering the dynamic nature of node interactions in LP.\n4. The authors propose a scalable version of TC, ATC, which reduces the computational complexity while maintaining high correlations with LP performance.\n5. The paper explores the potential of boosting LP performance via enhancing TC, which provides a new direction for future research.\n\nWeaknesses:\n\n1. The paper assumes that the link formation between a node pair depends on the intersection between their local subgraphs, which may not always be the case in real-world networks.\n2. The authors do not provide a clear explanation of how the TDS issue arises and how it can be addressed.\n3. The paper assumes that the graph diffusion can be efficiently executed via power iteration, which may not be the case for large-scale networks.\n4. The authors do not provide a thorough analysis of the causal relationship between TC and LP performance.\n5. The paper does not discuss the potential limitations and challenges of using TC and ATC in real-world applications.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the link formation mechanism and how it relates to the intersection between local subgraphs.\n2. Offer a more comprehensive analysis of the TDS issue, including its causes and potential solutions.\n3. Investigate the causal relationship between TC and LP performance using techniques such as causal inference or structural equation modeling.\n4. Discuss the potential limitations and challenges of using TC and ATC in real-world applications, such as scalability and computational complexity.\n5. Explore other potential applications of TC and ATC, such as node classification or community detection.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of link prediction by proposing a novel and effective metric, TC. However, it also raises several questions and challenges that need to be addressed in future research. Therefore, I recommend that the authors revise the paper to address these weaknesses and suggestions for improvement, and submit it to a top-tier conference in the field of machine learning or network science.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper aims to demystify the varying levels of embedding quality and link prediction (LP) performance across different nodes within a graph from a topological perspective. The authors propose Topological Concentration (TC), a new metric to measure the quality of node embeddings and its correlation with LP performance. They demonstrate the superiority of TC in characterizing node LP performance and identifying low-performing nodes compared to other common node topological properties like degree and subgraph density. Additionally, the authors discover a novel topological distribution shift (TDS) issue and propose an approximated version of TC (ATC) to reduce computation complexity. The authors also explore the potential of boosting GNNs' LP performance via enhancing TC by re-weighting edges in message-passing.\n\nStrengths:\n\n* The paper tackles an important and under-explored problem in GNN-based link prediction, i.e., the varying levels of embedding quality and LP performance across different nodes within a graph.\n* The authors propose a new and effective metric, Topological Concentration (TC), to characterize node LP performance and its correlation with TC.\n* The paper provides a thorough empirical evaluation of TC and its approximated version (ATC), demonstrating their superiority in characterizing node LP performance and identifying low-performing nodes.\n* The authors also discover a novel topological distribution shift (TDS) issue and propose potential solutions to mitigate its negative impact on LP performance.\n\nWeaknesses:\n\n* The paper assumes a strong connection between link formation and subgraph interaction, which might not be universally applicable to all graphs and LP tasks.\n* The authors rely heavily on empirical evaluation and visualization to support their claims, which might not be sufficient to establish a strong theoretical foundation for TC and its approximated version (ATC).\n* The paper does not provide a clear explanation of why TC and its approximated version (ATC) are effective in characterizing node LP performance and identifying low-performing nodes.\n* The authors do not provide a thorough comparison of TC and its approximated version (ATC) with other common node topological properties like degree and subgraph density.\n\nSuggestions for Improvement:\n\n* Provide a more thorough theoretical explanation of why TC and its approximated version (ATC) are effective in characterizing node LP performance and identifying low-performing nodes.\n* Provide a clear explanation of the connection between link formation and subgraph interaction, and how TC and its approximated version (ATC) capture this connection.\n* Provide a more thorough comparison of TC and its approximated version (ATC) with other common node topological properties like degree and subgraph density.\n* Consider adding more experimental results and visualization to support the authors' claims and provide a more comprehensive evaluation of TC and its approximated version (ATC).\n\nRecommendation:\n\nThe paper provides a thorough and effective approach to characterizing node LP performance and identifying low-performing nodes in GNN-based link prediction. The authors propose a new and effective metric, Topological Concentration (TC), and its approximated version (ATC), and provide a thorough empirical evaluation to support their claims. However, the paper assumes a strong connection between link formation and subgraph interaction, and relies heavily on empirical evaluation and visualization to support its claims. To improve the paper, the authors should provide a clearer explanation of why TC and its approximated version (ATC) are effective in characterizing node LP performance and identifying low-performing nodes, and provide a more thorough comparison with other common node topological properties like degree and subgraph density.",
        "gr_score_with": 0.5409356725146199,
        "gr_score_without": 0.37471264367816093,
        "output_without": {
            "num_tokens_scored": 435,
            "num_green_tokens": 163,
            "green_fraction": 0.37471264367816093,
            "z_score": -1.0765704024436231,
            "p_value": 0.8591638835734398
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 185,
            "green_fraction": 0.5409356725146199,
            "z_score": 5.3202049675445915,
            "p_value": 5.1825204838082034e-08
        }
    },
    {
        "title": "TIONS WITH TRANSITIONAL DICTIONARY LEARNING",
        "abstract": "This paper introduces a novel Transitional Dictionary Learning (TDL) framework that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. We propose a game-theoretic diffusion model to decompose the input into visual parts using the dictionaries learned by the Expectation Maximization (EM) algorithm, implemented as the online prototype clustering, based on the decomposition results. Additionally, two metrics, clustering information gain, and heuristic shape score are proposed to evaluate the model. Experiments are conducted on three abstract compositional visual object datasets, which require the model to utilize the compositionality of data instead of simply exploiting visual features. Then, three tasks on symbol grounding to predefined classes of parts and relations, as well as transfer learning to unseen classes, followed by a human evaluation, were carried out on these datasets. The results show that the proposed method discovers compositional patterns, which significantly outperforms the state-of-the-art unsupervised part segmentation methods that rely on visual features from pre-trained backbones. Furthermore, the proposed metrics are consistent with human evaluations.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel Transitional Dictionary Learning (TDL) framework that learns symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. The TDL framework uses an Expectation Maximization (EM) algorithm to iteratively update dictionaries that store hidden representations of symbolic knowledge. The authors also introduce two metrics, Clustering Information Gain (CIG) and Heuristic Shape Score (HSS), to evaluate the learned representation. Experiments are conducted on three abstract compositional visual object datasets, and the results show that the proposed method significantly outperforms state-of-the-art unsupervised part segmentation methods that rely on visual features.\n\nStrengths:\n\n1. The TDL framework is a novel approach to learning symbolic knowledge from visual inputs, which is a challenging problem in computer vision.\n2. The use of EM algorithm to iteratively update dictionaries is a clever approach to learning symbolic knowledge.\n3. The introduction of CIG and HSS as evaluation metrics is a significant contribution to the field of computer vision.\n4. The experiments on three abstract compositional visual object datasets demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n\n1. The TDL framework requires a large number of training samples, which may not be feasible in all scenarios.\n2. The EM algorithm used in the TDL framework may get stuck in local optima, which can affect the performance of the method.\n3. The CIG and HSS metrics may not capture all aspects of the learned representation, which can lead to biased evaluation results.\n4. The experiments on the three datasets may not be exhaustive, and more datasets should be used to evaluate the proposed method.\n\nSuggestions for Improvement:\n\n1. Investigate the use of other optimization algorithms, such as stochastic gradient descent or Adam, to improve the performance of the TDL framework.\n2. Explore the use of transfer learning to adapt the TDL framework to new datasets or scenarios.\n3. Develop more comprehensive evaluation metrics that capture all aspects of the learned representation.\n4. Conduct experiments on more datasets to evaluate the robustness and generalizability of the proposed method.\n\nRecommendation:\n\nThe proposed TDL framework is a significant contribution to the field of computer vision, and it has the potential to learn symbolic knowledge from visual inputs. However, the method requires further investigation and improvement to make it more robust and generalizable. I recommend that the authors address the weaknesses and suggestions for improvement mentioned above and conduct more experiments to evaluate the proposed method.\n\nOverall, I rate this paper 8/10 for its novelty, significance, and potential impact on the field of computer vision. However, I deduct 2 points for the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel Transitional Dictionary Learning (TDL) framework that learns symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. The TDL framework uses an Expectation Maximization (EM) algorithm to iteratively update dictionaries that store hidden representations of symbolic knowledge. The authors also introduce two metrics, clustering information gain and heuristic shape score, to evaluate the learned representation. Experiments are conducted on three abstract compositional visual object datasets, and the results show that the proposed method outperforms state-of-the-art unsupervised part segmentation methods.\n\nStrengths:\n\n1. The paper proposes a novel framework that bridges the gap between neural and symbolic intelligence.\n2. The TDL framework uses an EM algorithm to iteratively update dictionaries, which is a well-established technique in machine learning.\n3. The authors introduce two novel metrics, clustering information gain and heuristic shape score, to evaluate the learned representation.\n4. The experiments are conducted on three abstract compositional visual object datasets, which is a challenging and realistic setting.\n5. The results show that the proposed method outperforms state-of-the-art unsupervised part segmentation methods.\n\nWeaknesses:\n\n1. The paper assumes a linear composition of visual parts, which may not hold in all cases.\n2. The EM algorithm used in the TDL framework may get stuck in local optima.\n3. The clustering information gain and heuristic shape score metrics may not capture all aspects of the learned representation.\n4. The paper does not provide a thorough comparison with existing methods that use different types of learning (e.g., supervised learning).\n5. The human evaluation study has a relatively small sample size and may not be representative of all possible scenarios.\n\nSuggestions for Improvement:\n\n1. Consider using a more general composition model, such as a non-linear or hierarchical composition model.\n2. Use techniques such as variational inference or stochastic optimization to improve the EM algorithm.\n3. Consider using additional metrics or evaluation protocols to assess the learned representation.\n4. Conduct a more thorough comparison with existing methods that use different types of learning.\n5. Increase the sample size of the human evaluation study to improve its reliability and generalizability.\n\nRecommendation:\n\nOverall, the paper presents a novel and interesting framework that bridges the gap between neural and symbolic intelligence. While there are some limitations and suggestions for improvement, the proposed method outperforms state-of-the-art unsupervised part segmentation methods and demonstrates promising results. Therefore, I recommend accepting the paper with minor revisions to address the suggestions for improvement.\n\nRating: 8/10\n\nReviewers' Comments:\n\n1. The paper is well-written and easy to follow.\n2. The proposed framework is novel and well-motivated.\n3. The experiments are well-conducted and provide a good assessment of the proposed method.\n4. The paper could benefit from a more thorough comparison with existing methods.\n5. The human evaluation study could be improved by increasing the sample size and using additional metrics or evaluation protocols.\n\nConflict of Interest:\n\nI declare no conflict of interest.",
        "gr_score_with": 0.4188861985472155,
        "gr_score_without": 0.35,
        "output_without": {
            "num_tokens_scored": 400,
            "num_green_tokens": 140,
            "green_fraction": 0.35,
            "z_score": -2.041241452319315,
            "p_value": 0.9793865833314181
        },
        "output_with": {
            "num_tokens_scored": 413,
            "num_green_tokens": 173,
            "green_fraction": 0.4188861985472155,
            "z_score": 0.7834548185980483,
            "p_value": 0.21668003785959916
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel regularization technique, zero-mean regularization, for spectral contrastive learning (SpeCL). The authors argue that SpeCL has two limitations: (i) it requires the orthogonality of representations between negative pairs, and (ii) it does not account for wrong connections in the positive-pair graph. To address these limitations, they introduce an additive factor, \u03c4, into the term of SpeCL involving negative pairs. This modification is equivalent to regularizing the mean of representations to be zero, referred to as zero-mean regularization. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem and the proposed solution.\n2. The authors provide a thorough theoretical analysis of the benefits of zero-mean regularization in both unsupervised and supervised scenarios.\n3. The paper presents extensive experimental results to verify the effectiveness of zero-mean regularization on various benchmark datasets.\n4. The authors provide a clear explanation of the implications of zero-mean regularization on the structure of the learned representations.\n\nWeaknesses:\n\n1. The paper assumes a specific form of the positive-pair graph, which may not be realistic in all scenarios.\n2. The authors do not provide a clear explanation of how to choose the value of \u03c4 in practice.\n3. The paper does not discuss the computational complexity of the proposed method.\n4. The authors do not provide a comparison with other regularization techniques.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how to choose the value of \u03c4 in practice.\n2. Discuss the computational complexity of the proposed method and provide a comparison with other regularization techniques.\n3. Provide more experimental results on different datasets and scenarios to further verify the effectiveness of zero-mean regularization.\n4. Discuss the implications of zero-mean regularization on the interpretability of the learned representations.\n\nRecommendation:\n\nThe paper presents a novel and effective regularization technique for spectral contrastive learning. The authors provide a thorough theoretical analysis and extensive experimental results to demonstrate the effectiveness of zero-mean regularization. The paper is well-written and easy to follow. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust. I recommend this paper for publication in a top-tier conference in the field of machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThis paper introduces zero-mean regularization, a modification to the spectral contrastive loss, which enhances the discriminativeness of representations learned through contrastive learning. The authors argue that the spectral contrastive loss, while being theoretically sound, has two limitations: (i) it requires orthogonality of representations between negative pairs, which may not be desirable, and (ii) it does not address the issue of wrong connections in the positive-pair graph. The proposed zero-mean regularization addresses these limitations by introducing an additional factor, \u03c4, into the negative part of the spectral contrastive loss, which relaxes the orthogonality constraint and modifies pairwise similarities between representations. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections. Specifically, they prove that zero-mean regularization can tighten the error bound on target domains in unsupervised domain adaptation and mitigate label noise in supervised learning with noisy labels.\n\nStrengths:\n\n1. Theoretical soundness: The authors provide a thorough theoretical analysis of the spectral contrastive loss and its limitations, which is a significant contribution to the field of contrastive learning.\n2. Novel modification: The introduction of zero-mean regularization is a novel modification to the spectral contrastive loss, which addresses the limitations of the original loss function.\n3. Experimental validation: The authors provide extensive experimental results, which demonstrate the effectiveness of zero-mean regularization on various benchmark datasets.\n\nWeaknesses:\n\n1. Restrictive assumptions: The authors make restrictive assumptions, such as assuming that the representation dimensionality is greater than or equal to the number of classes, which may not be feasible in all scenarios.\n2. Limited scope: The paper focuses primarily on spectral contrastive loss, which may not be applicable to other contrastive learning algorithms.\n3. Lack of empirical comparison: The authors do not provide a comprehensive empirical comparison with other state-of-the-art algorithms, which makes it difficult to evaluate the true effectiveness of zero-mean regularization.\n\nSuggestions for Improvement:\n\n1. Relax restrictive assumptions: The authors should relax the restrictive assumptions made in the paper, such as assuming that the representation dimensionality is greater than or equal to the number of classes, to make the results more applicable to a wider range of scenarios.\n2. Expand scope: The authors should expand the scope of the paper to include other contrastive learning algorithms, such as Barlow Twins loss and VICReg loss, to demonstrate the generality of zero-mean regularization.\n3. Provide more comprehensive empirical comparison: The authors should provide a more comprehensive empirical comparison with other state-of-the-art algorithms, including Barlow Twins loss and VICReg loss, to evaluate the true effectiveness of zero-mean regularization.\n\nRecommendation:\n\nThis paper introduces a novel modification to the spectral contrastive loss, which addresses the limitations of the original loss function. The experimental results demonstrate the effectiveness of zero-mean regularization on various benchmark datasets. While there are some limitations to the paper, such as restrictive assumptions and limited scope, the contribution is significant, and the results are compelling. Therefore, I recommend accepting this paper for publication, subject to the authors addressing the limitations mentioned above.$$$",
        "gr_score_with": 0.4049382716049383,
        "gr_score_without": 0.2912912912912913,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 97,
            "green_fraction": 0.2912912912912913,
            "z_score": -4.049308199791282,
            "p_value": 0.9999743153708819
        },
        "output_with": {
            "num_tokens_scored": 405,
            "num_green_tokens": 164,
            "green_fraction": 0.4049382716049383,
            "z_score": 0.20286020648339484,
            "p_value": 0.41962214907911
        }
    },
    {
        "title": "AVERAGE REWARD MARKOV DECISION PROCESSES",
        "abstract": "We resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of \u00d5(|S||A|tmix\u03b5)* and a lower bound of \u03a9(|S||A|tmix\u03b5). In these expressions, |S| and |A| denote the cardinalities of the state and action spaces respectively, tmix serves as a uniform upper limit for the total variation mixing times, and \u03b5 signifies the error tolerance. Therefore, a notable gap of tmix still remains to be bridged. Our primary contribution is the development of an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5). This marks the first algorithm and analysis to reach the literature\u2019s lower bound. Our new algorithm draws inspiration from ideas in Li et al. (2020), Jin & Sidford (2021), and Wang et al. (2023). Additionally, we conduct numerical experiments to validate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a theoretical contribution to the area of reinforcement learning (RL) by providing the first provably optimal sample complexity guarantee for a tabular RL environment in which a controller wishes to maximize the long-run average reward governed by a Markov decision process (MDP). The authors resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. They develop an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5), which marks the first algorithm and analysis to reach the literature's lower bound.\n\nStrengths:\n\n1. The paper provides a significant contribution to the field of reinforcement learning by resolving the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP).\n2. The authors develop an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5), which marks the first algorithm and analysis to reach the literature's lower bound.\n3. The paper provides a comprehensive literature review, discussing the relevant works and the gaps in the existing literature.\n4. The authors provide a clear and concise explanation of the methodology and the technical details of the algorithm.\n\nWeaknesses:\n\n1. The paper assumes a uniformly ergodic MDP, which may not be a realistic assumption in many real-world scenarios.\n2. The authors do not provide any numerical experiments to validate their theoretical findings, which makes it difficult to assess the practicality of the algorithm.\n3. The paper does not discuss the computational complexity of the algorithm, which is an important consideration in many real-world applications.\n4. The authors do not provide any discussion on the potential applications of the algorithm, which makes it difficult to assess its potential impact.\n\nSuggestions for Improvement:\n\n1. The authors should provide numerical experiments to validate their theoretical findings and assess the practicality of the algorithm.\n2. The authors should discuss the computational complexity of the algorithm and provide a detailed analysis of the time and space complexity.\n3. The authors should discuss the potential applications of the algorithm and provide a clear explanation of how it can be used in real-world scenarios.\n4. The authors should provide a more detailed explanation of the technical details of the algorithm and the assumptions made in the paper.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of reinforcement learning and resolves the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP). However, the paper assumes a uniformly ergodic MDP, which may not be a realistic assumption in many real-world scenarios. The authors should provide numerical experiments to validate their theoretical findings and discuss the computational complexity of the algorithm. Overall, the paper is well-written and provides a clear and concise explanation of the methodology and the technical details of the algorithm.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper provides a theoretical contribution to reinforcement learning (RL) by establishing the first provably optimal sample complexity guarantee for tabular RL environments in which a controller wishes to maximize the long-run average reward governed by a Markov decision process (MDP). The authors resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. They develop an estimator for the optimal policy of average reward MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5). This marks the first algorithm and analysis to reach the literature's lower bound.\n\nStrengths:\n\n1. The paper provides a significant contribution to the field of reinforcement learning, specifically in the area of sample complexity for uniformly ergodic Markov decision processes.\n2. The authors develop a novel algorithm for learning an optimal policy for uniformly ergodic MDPs with a sample complexity of \u00d5(|S||A|tmix\u03b5), which is the first algorithm to achieve the literature's lower bound.\n3. The paper conducts numerical experiments to validate the theoretical findings, demonstrating the optimality of the proposed algorithm's sample complexity dependence on both \u03f5 and tminorize.\n4. The authors provide a detailed analysis of the sample complexity of uniformly ergodic DMDPs, which is of independent theoretical significance.\n\nWeaknesses:\n\n1. The paper assumes a strong form of MDP mixing known as uniform ergodicity, which may not be suitable for all practical applications.\n2. The assumption of uniform ergodicity may yield conservative upper bounds on sample complexity in situations where suboptimal policies induce Markov chains with especially large mixing times.\n3. The proposed algorithm requires a priori knowledge of an upper bound on the uniform mixing time for the transition kernel P, which may be challenging to obtain in practice.\n4. The paper does not provide a detailed comparison with existing algorithms for uniformly ergodic MDPs, which may limit its impact.\n\nSuggestions for Improvement:\n\n1. Investigate the possibility of relaxing the assumption of uniform ergodicity to weaker forms of mixing, such as weak communication or periodic chains.\n2. Develop a more practical method for obtaining an upper bound on the uniform mixing time for the transition kernel P, which may involve using empirical estimates or adaptive methods.\n3. Provide a more detailed comparison with existing algorithms for uniformly ergodic MDPs, including their sample complexity guarantees and practical performance.\n4. Investigate the possibility of extending the proposed algorithm to more general settings, such as general state-space MDPs or MDPs with continuous state and action spaces.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of reinforcement learning, specifically in the area of sample complexity for uniformly ergodic Markov decision processes. However, it assumes a strong form of MDP mixing known as uniform ergodicity, which may not be suitable for all practical applications. To address this limitation, the authors should investigate the possibility of relaxing the assumption of uniform ergodicity to weaker forms of mixing. Furthermore, the paper would benefit from a more detailed comparison with existing algorithms for uniformly ergodic MDPs and a more practical method for obtaining an upper bound on the uniform mixing time for the transition kernel P.",
        "gr_score_with": 0.43765281173594134,
        "gr_score_without": 0.380327868852459,
        "output_without": {
            "num_tokens_scored": 305,
            "num_green_tokens": 116,
            "green_fraction": 0.380327868852459,
            "z_score": -0.7012868733182765,
            "p_value": 0.7584379979887306
        },
        "output_with": {
            "num_tokens_scored": 409,
            "num_green_tokens": 179,
            "green_fraction": 0.43765281173594134,
            "z_score": 1.5543665654287904,
            "p_value": 0.06004850186537856
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors examine two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis, and (2) bounds that may also depend on the learning algorithm. They prove mathematically that no generalization bound of the first type can be uniformly tight in the overparameterized setting, and establish a trade-off between learning performance and estimability for the second type of bounds.\n\nStrengths:\n\n* The paper provides a rigorous and formal analysis of the notion of uniformly tight generalization bounds.\n* The authors introduce a framework for understanding the limitations of generalization bounds in the overparameterized setting.\n* The paper presents several theorems and lemmas that provide a deeper understanding of the relationships between learning performance, estimability, and generalization bounds.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and theoretical computer science.\n* The authors do not provide a comprehensive review of existing generalization bounds and their limitations, which may make it difficult for readers to understand the significance of their results.\n* The paper's conclusions and implications may be subject to interpretation, and the authors acknowledge that different scholarly positions exist on this matter.\n\nSuggestions for Improvement:\n\n* The authors could provide a more accessible introduction to the mathematical framework and notation used in the paper.\n* The paper could benefit from a more comprehensive review of existing generalization bounds and their limitations, which would provide context for the authors' results.\n* The authors could provide more concrete examples and illustrations to demonstrate the implications of their results and make the paper more engaging for readers.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of machine learning and theoretical computer science by shedding light on the limitations of generalization bounds in the overparameterized setting. The authors' formal analysis and rigorous proofs provide a solid foundation for understanding the relationships between learning performance, estimability, and generalization bounds. However, the paper's assumptions and conclusions may be subject to interpretation, and the authors could benefit from providing a more comprehensive review of existing generalization bounds and their limitations. Overall, the paper is well-written and well-structured, and it is a valuable contribution to the field.\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research:\n\n* Investigate the implications of the authors' results for existing generalization bounds and their limitations.\n* Explore the relationships between learning performance, estimability, and generalization bounds in more detail.\n* Develop new generalization bounds that take into account the limitations and trade-offs identified by the authors.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper investigates the possibility of obtaining generalization bounds that are uniformly tight for neural networks in the overparameterized setting. The authors show that for certain types of generalization bounds, it is mathematically impossible to obtain bounds that are uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and population distributions. Specifically, they prove that bounds that depend solely on the training set, the learned hypothesis, and the hypothesis class cannot be uniformly tight, while bounds that depend on the learning algorithm can be tight for some algorithms, but not for all.\n\nStrengths:\n\n1. The paper provides a rigorous and thorough investigation of the possibility of obtaining generalization bounds that are uniformly tight for neural networks in the overparameterized setting.\n2. The authors provide clear and concise definitions of the concepts involved, making it easy for readers to follow their arguments.\n3. The paper includes multiple theorems and lemmas, which provides a clear and structured presentation of their results.\n4. The authors provide a detailed discussion of the implications of their results, including the limitations of certain types of generalization bounds and the potential for future research directions.\n\nWeaknesses:\n\n1. The paper assumes a certain level of background knowledge in machine learning and learning theory, which may make it difficult for readers without a strong foundation in these areas to follow their arguments.\n2. Some of the technical details, such as the proofs of the theorems, are presented in an appendix, which may make it difficult for readers to fully understand the results without consulting the appendix.\n3. The paper does not provide a clear comparison to existing generalization bounds, which may make it difficult for readers to understand the significance of their results.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed introduction to the concepts involved, including definitions and background information, to make it easier for readers without a strong foundation in machine learning and learning theory to follow their arguments.\n2. Consider providing a more detailed comparison to existing generalization bounds, including an analysis of their strengths and limitations, to provide context for the reader's understanding of the results.\n3. Consider providing more technical details, such as the proofs of the theorems, in the main text, rather than in an appendix, to make it easier for readers to fully understand the results.\n\nRecommendation:\n\nThe paper provides a rigorous and thorough investigation of the possibility of obtaining generalization bounds that are uniformly tight for neural networks in the overparameterized setting, and their results have significant implications for future research directions in this area. While there are some limitations to the paper, including the assumption of a certain level of background knowledge and the presentation of technical details in an appendix, the paper is well-written and provides clear and concise definitions of the concepts involved. Overall, the paper is well-suited for publication in a top-tier conference or journal in machine learning or learning theory.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.47790055248618785,
        "gr_score_without": 0.35911602209944754,
        "output_without": {
            "num_tokens_scored": 362,
            "num_green_tokens": 130,
            "green_fraction": 0.35911602209944754,
            "z_score": -1.587821979064637,
            "p_value": 0.9438367000210964
        },
        "output_with": {
            "num_tokens_scored": 362,
            "num_green_tokens": 173,
            "green_fraction": 0.47790055248618785,
            "z_score": 3.0254445817312643,
            "p_value": 0.0012413394856129762
        }
    },
    {
        "title": "DIFFUSIONNAG: PREDICTOR-GUIDED NEURAL ARCHI-",
        "abstract": "Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 20\u00d7 when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called DiffusionNAG, which is a paradigm shift from existing Neural Architecture Search (NAS) methods. DiffusionNAG is a conditional Neural Architecture Generation (NAG) framework based on diffusion models, which enables the generation of desired neural architectures with the guidance of parameterized predictors. The framework consists of a base diffusion generative model that generates architectures following the distribution of a search space, and a conditional diffusion framework that leverages a predictor to guide the generation process toward architectures that satisfy specific objectives. The proposed framework offers several advantages, including efficient search, superior utilization of predictors, and easy adaptability across diverse tasks. The paper demonstrates the effectiveness of DiffusionNAG through extensive experiments on two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to neural architecture generation, which is a significant contribution to the field of NAS.\n2. The framework is well-structured, and the authors provide a clear explanation of the proposed method and its components.\n3. The paper demonstrates the effectiveness of DiffusionNAG through extensive experiments on two predictor-based NAS scenarios, which provides strong evidence for the validity of the proposed approach.\n4. The authors provide a thorough comparison with existing NAS methods, which highlights the advantages of DiffusionNAG.\n5. The paper is well-written, and the authors provide a clear and concise summary of the related work.\n\nWeaknesses:\n\n1. The paper assumes that the predictors are accurate and reliable, which may not always be the case in practice.\n2. The authors do not provide a detailed analysis of the computational cost of the proposed framework, which may be a limitation in certain scenarios.\n3. The paper assumes that the search space is known and can be represented as a directed acyclic graph, which may not always be the case in practice.\n4. The authors do not provide a detailed comparison with other diffusion-based generative models, which may be a limitation in certain scenarios.\n5. The paper assumes that the predictors are trained on a sufficient amount of data, which may not always be the case in practice.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of the proposed framework and its limitations.\n2. The paper should include a more detailed comparison with other diffusion-based generative models and their limitations.\n3. The authors should provide a more detailed explanation of how the predictors are trained and validated.\n4. The paper should include a more detailed discussion of the potential applications of DiffusionNAG and its limitations.\n5. The authors should provide a more detailed explanation of how the proposed framework can be adapted to different search spaces and scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to neural architecture generation, which is a significant contribution to the field of NAS. The framework is well-structured, and the authors provide a clear explanation of the proposed method and its components. The paper demonstrates the effectiveness of DiffusionNAG through extensive experiments on two predictor-based NAS scenarios, which provides strong evidence for the validity of the proposed approach. However, the paper assumes that the predictors are accurate and reliable, which may not always be the case in practice. Therefore, the authors should provide a more detailed analysis of the computational cost of the proposed framework and its limitations.\n\nIn conclusion, the paper is well-written, and the authors provide a clear and concise summary of the related work. However, the paper has some limitations, and the authors should address these limitations in future work. Overall, the paper is a significant contribution to the field of NAS, and it has the potential to impact the development of neural architectures in the future.\n\nRecommendation: Accept\n\n$$$",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel approach called DiffusionNAG, which is a paradigm shift from existing Neural Architecture Search (NAS) methods. DiffusionNAG is a conditional Neural Architecture Generation (NAG) framework based on diffusion models, which can efficiently generate task-optimal architectures with the guidance of a predictor. The framework introduces a score network to ensure the generation of valid architectures. Extensive experiments demonstrate that DiffusionNAG outperforms existing NAS methods, especially effective in large search spaces.\n\nStrengths:\n\n1.  DiffusionNAG is a novel approach that addresses the limitations of existing NAS methods, including high search costs and inefficient use of property predictors.\n2.  The framework is based on diffusion models, which have shown success in a variety of domains, including image, audio, and graph generation.\n3.  The introduction of a score network ensures the generation of valid architectures, which is a significant improvement over existing NAS methods.\n4.  Extensive experiments demonstrate the effectiveness of DiffusionNAG in both Transferable NAS and BO-based NAS scenarios.\n\nWeaknesses:\n\n1.  The paper assumes a certain level of prior knowledge about the search space, which may not be available in all scenarios.\n2.  The framework relies heavily on the quality of the predictor, which can be a limitation if the predictor is not well-trained or is biased.\n3.  The paper does not provide a thorough comparison of DiffusionNAG with existing NAS methods, particularly in terms of computational resources required.\n\nSuggestions for Improvement:\n\n1.  Provide a more thorough comparison of DiffusionNAG with existing NAS methods, including a detailed analysis of computational resources required.\n2.  Investigate the impact of predictor quality on the performance of DiffusionNAG.\n3.  Explore the use of DiffusionNAG in scenarios where prior knowledge about the search space is limited.\n\nRecommendation:\n\nThe paper proposes a novel approach to Neural Architecture Generation, which addresses some of the limitations of existing NAS methods. While there are some concerns about the framework's reliance on prior knowledge and predictor quality, the experimental results demonstrate the effectiveness of DiffusionNAG in both Transferable NAS and BO-based NAS scenarios. With some further development and experimentation, DiffusionNAG has the potential to become a leading approach to NAS.\n\n$$",
        "gr_score_with": 0.5686900958466453,
        "gr_score_without": 0.4909090909090909,
        "output_without": {
            "num_tokens_scored": 385,
            "num_green_tokens": 189,
            "green_fraction": 0.4909090909090909,
            "z_score": 3.641095406272096,
            "p_value": 0.00013574025102158923
        },
        "output_with": {
            "num_tokens_scored": 313,
            "num_green_tokens": 178,
            "green_fraction": 0.5686900958466453,
            "z_score": 6.091947232721404,
            "p_value": 5.57727277456203e-10
        }
    },
    {
        "title": "FUNCTIONAL INTERPOLATION FOR RELATIVE POSI-",
        "abstract": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5\u2019s RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel functional relative position encoding method called FIRE (Functional Interpolation for Relative Positional Encoding) to improve the length generalization of Transformers. FIRE uses a learnable continuous function to map input positions to biases and a progressive interpolation technique to transform arbitrary input lengths into a bounded domain. The authors theoretically prove that FIRE can represent popular position encodings such as T5\u2019s RPE, Alibi, and Kerple, and empirically show strong length generalization behavior on various benchmarks. The paper also introduces two additional transformations to FIRE for further improvement: amplifying the differences among local positions and thresholding the normalizer for better short sequence modeling.\n\nStrengths:\n\n* The paper proposes a novel functional relative position encoding method that can represent popular position encodings such as T5\u2019s RPE, Alibi, and Kerple.\n* The authors provide theoretical and empirical evidence for the effectiveness of FIRE in improving length generalization.\n* The paper introduces two additional transformations to FIRE for further improvement, which can be applied to other position encoding methods.\n* The authors provide a comprehensive evaluation of FIRE on various benchmarks, including language modeling, long text benchmark, and zero-shot long-context question answering.\n\nWeaknesses:\n\n* The paper only studies decoder models and does not analyze the role of other components of Transformer and other training components (data, optimizer) in length generalization.\n* The paper assumes that the maximum sequence length can be predicted for auto-regressive generation, which may not be feasible in practice.\n* The paper does not provide a detailed comparison with other methods that use interpolation techniques, such as RoPE with position interpolation.\n* The paper assumes that the normalizer is the query position index, which may not be the best choice for all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should study the role of other components of Transformer and other training components (data, optimizer) in length generalization.\n* The authors should provide a detailed comparison with other methods that use interpolation techniques, such as RoPE with position interpolation.\n* The authors should experiment with different normalizers, such as a pre-defined max context length, to evaluate their performance.\n* The authors should provide more details on the implementation of FIRE, such as the specific architecture of the MLP and the choice of hyperparameters.\n\nRecommendation:\n\nThe paper proposes a novel functional relative position encoding method that can improve the length generalization of Transformers. The authors provide theoretical and empirical evidence for the effectiveness of FIRE, and the paper introduces two additional transformations to FIRE for further improvement. However, the paper has some limitations, such as only studying decoder models and assuming that the maximum sequence length can be predicted. Overall, the paper is well-written and provides a comprehensive evaluation of FIRE on various benchmarks. I recommend accepting the paper with some minor revisions to address the suggestions for improvement.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel functional relative position encoding method called FIRE (Functional Interpolation for Relative Positional Encoding) to improve the generalization of Transformers to longer contexts. FIRE uses a learnable continuous function to map input positions to biases, and a progressive interpolation technique to normalize the query-key relative distance by the query position index, resulting in a bounded input to the position encoding function for all input sequence lengths. The paper presents theoretical and empirical results showing the effectiveness of FIRE in improving length generalization.\n\nStrengths:\n\n1. FIRE offers a flexible and adaptive approach to learn diverse position encoding biases in long context settings, making it a promising method for improving Transformer generalization.\n2. The paper presents a theoretical proof that FIRE can represent several popular position encoding methods, including T5's RPE, Alibi, and Kerple, in a parameter-efficient manner.\n3. Empirical results show that FIRE achieves strong length generalization behavior, improving over existing methods in zero-shot and finetuning settings on a wide range of datasets and benchmarks.\n\nWeaknesses:\n\n1. The paper focuses on decoder-only models, and it would be interesting to explore whether FIRE can be applied to encoder-only models.\n2. The role of other components of Transformer and other training components (data, optimizer) in length generalization is not analyzed in this paper.\n3. While FIRE achieves strong performance on long sequence lengths, it is not clear whether it sacrifices model quality on short sequence lengths.\n\nSuggestions for Improvement:\n\n1. Explore whether FIRE can be applied to encoder-only models and how it would perform in comparison to other methods.\n2. Investigate the role of other components of Transformer and other training components in length generalization.\n3. Evaluate the performance of FIRE on a wider range of tasks and datasets, including those with short sequence lengths.\n\nRecommendation:\n\nThe paper presents a novel and effective method for improving Transformer generalization to longer contexts. While there are some limitations to the current study, the results are promising, and further investigation is warranted. I recommend this paper for publication in a top-tier conference, with the understanding that the authors address the limitations mentioned above.\n\nRating: 8/10",
        "gr_score_with": 0.4672364672364672,
        "gr_score_without": 0.4325068870523416,
        "output_without": {
            "num_tokens_scored": 363,
            "num_green_tokens": 157,
            "green_fraction": 0.4325068870523416,
            "z_score": 1.264221214848674,
            "p_value": 0.10307531915796764
        },
        "output_with": {
            "num_tokens_scored": 351,
            "num_green_tokens": 164,
            "green_fraction": 0.4672364672364672,
            "z_score": 2.5713004384784126,
            "p_value": 0.005065869456437423
        }
    },
    {
        "title": "BEYOND SPATIO-TEMPORAL REPRESENTATIONS: EVOLVING FOURIER TRANSFORM FOR TEMPORAL GRAPHS",
        "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph\u2019s structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel spectral transform, called Evolving Graph Fourier Transform (EFT), for temporal graphs. The EFT is designed to capture the evolving spectra of temporal graphs, which is essential for downstream tasks such as link prediction and node classification. The authors propose a theoretical framework based on an optimization perspective, where they define a notion of signal variation on the dynamic graph similar to the variation of signals on static graphs. They then derive the EFT as a solution to the optimization problem of the variational characterization with pseudospectrum relaxations. The authors also provide a computational efficient form of the EFT, which can be obtained by sampling T snapshots of the dynamic graph signal at uniform time intervals.\n\nStrengths:\n\n1. The paper proposes a novel spectral transform, EFT, which is designed to capture the evolving spectra of temporal graphs.\n2. The authors provide a theoretical framework based on an optimization perspective, which is well-motivated and easy to follow.\n3. The EFT is computationally efficient, which is essential for real-world applications.\n4. The authors provide experimental results on both continuous and discrete temporal graph datasets, which demonstrate the effectiveness of the EFT.\n\nWeaknesses:\n\n1. The paper assumes a fixed node set, which may not be realistic in many real-world scenarios.\n2. The authors do not consider generic signed and directed graphs, which may limit the applicability of the EFT.\n3. The paper does not provide a detailed comparison with other existing methods for temporal graph analysis.\n4. The authors do not provide a clear explanation of how the EFT can be used for downstream tasks such as link prediction and node classification.\n\nSuggestions for Improvement:\n\n1. The authors should consider extending the EFT to generic signed and directed graphs.\n2. The paper should provide a more detailed comparison with other existing methods for temporal graph analysis.\n3. The authors should provide a clear explanation of how the EFT can be used for downstream tasks such as link prediction and node classification.\n4. The paper should provide more experimental results on different types of temporal graph datasets.\n\nRecommendation:\n\nThe paper proposes a novel spectral transform, EFT, which is designed to capture the evolving spectra of temporal graphs. The authors provide a theoretical framework based on an optimization perspective, which is well-motivated and easy to follow. The EFT is computationally efficient, which is essential for real-world applications. The authors provide experimental results on both continuous and discrete temporal graph datasets, which demonstrate the effectiveness of the EFT. However, the paper assumes a fixed node set, which may not be realistic in many real-world scenarios. The authors do not consider generic signed and directed graphs, which may limit the applicability of the EFT. The paper does not provide a detailed comparison with other existing methods for temporal graph analysis. The authors do not provide a clear explanation of how the EFT can be used for downstream tasks such as link prediction and node classification. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of the EFT.\n\nRating: 7/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents a novel spectral transform, Evolving Graph Fourier Transform (EFT), for capturing evolving spectra on temporal graphs. The authors motivate their work by the inadequacy of existing methods, which are computationally expensive due to the temporal aspect along with the graph vertex domain. They propose a pseudo-spectrum relaxation that decomposes the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs.\n\nStrengths:\n1. The paper provides a comprehensive theoretical framework for the evolving graph Fourier transform, grounded on variational principles and pseudospectrum relaxations.\n2. The proposed EFT transform is computationally efficient, reducing the complexity from O(T^3) to O(T + T log(T)).\n3. The paper presents a novel approach to transform temporal graphs into the frequency domain, making it effective for downstream tasks on evolving graphs.\n4. The authors demonstrate the practical effectiveness of EFT on large-scale and standard temporal graph benchmarks.\n\nWeaknesses:\n1. The paper assumes that the node set V is fixed, i.e., no new nodes are added. It is unclear how the method would perform in scenarios with node editions.\n2. The paper does not consider generic signed and directed graphs, limiting its applicability.\n3. The paper assumes that the graph structure is changing with time, but it is unclear how the method would perform in scenarios with constant graph structure.\n\nSuggestions for Improvement:\n1. Investigate the performance of the method in scenarios with node editions and constant graph structure.\n2. Generalize the Laplacian and the resulting transform to generic signed and directed graphs, leveraging techniques proposed in (Mercado et al., 2016; Cucuringu et al., 2021).\n3. Explore the potential of EFT as a spectral representation of the evolving graph in downstream graph representation learning models.\n\nRecommendation:\nThe paper presents a novel and effective approach to transform temporal graphs into the frequency domain. The proposed EFT transform is computationally efficient and effective for downstream tasks on evolving graphs. However, the paper assumes a fixed node set and constant graph structure, limiting its applicability. Therefore, I recommend that the authors investigate the performance of the method in more general scenarios and explore its applicability to generic signed and directed graphs. With these improvements, the paper has the potential to make a significant contribution to the field of graph representation learning.",
        "gr_score_with": 0.4233983286908078,
        "gr_score_without": 0.3680555555555556,
        "output_without": {
            "num_tokens_scored": 288,
            "num_green_tokens": 106,
            "green_fraction": 0.3680555555555556,
            "z_score": -1.106588015946783,
            "p_value": 0.8657639545181361
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 152,
            "green_fraction": 0.4233983286908078,
            "z_score": 0.9049538685360012,
            "p_value": 0.18274491348742727
        }
    },
    {
        "title": "LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS",
        "abstract": "Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this paper, we explore a perplexing tendency of diffusion models: they often display the infinite Lipschitz property of the network with respect to time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it. The Lipschitz singularities pose a threat to the stability and accuracy during both the training and inference processes of diffusion models. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. To address this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates the Lipschitz singularities of the diffusion model near the zero point of timesteps. Remarkably, our technique yields a substantial improvement in performance. Moreover, as a byproduct of our method, we achieve a dramatic reduction in the Fr\u00e9chet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive experiments on diverse datasets validate our theory and method. Our work may advance the understanding of the general diffusion process, and also provide insights for the design of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the issue of Lipschitz singularities in diffusion models, which have been shown to be a dominant class of generative models. The authors provide theoretical proofs and empirical results to confirm that the noise prediction and v-prediction diffusion models often exhibit infinite Lipschitz constants near the zero point. This issue poses a threat to the stability and accuracy of the diffusion process, particularly during training and inference processes. To address this challenge, the authors propose a novel approach called Early Timestep-shared Diffusion Model (E-TSDM), which alleviates the Lipschitz singularities by sharing the timestep conditions in the interval with large Lipschitz constants. Experimental results demonstrate that E-TSDM outperforms the baseline on various datasets, including unconditional generation, conditional generation, and fast sampling.\n\nStrengths:\n\n* The paper provides a thorough theoretical analysis of the Lipschitz singularities issue in diffusion models, which is a significant contribution to the field.\n* The authors propose a novel approach called E-TSDM, which is easy to implement and effective in alleviating the Lipschitz singularities.\n* Experimental results demonstrate the superiority of E-TSDM over the baseline on various datasets.\n* The paper explores the generalizability of E-TSDM on different noise schedules and continuous-time diffusion models, which is a significant extension of the approach.\n\nWeaknesses:\n\n* The paper assumes that the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0, which may not be the case for all noise schedules.\n* The authors do not provide a detailed analysis of the impact of E-TSDM on large-scale text-to-image generative models, which is a significant limitation of the approach.\n* The paper does not provide a thorough comparison of E-TSDM with other potential methods to address the challenge of large Lipschitz constants, which is a significant limitation of the approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the impact of E-TSDM on large-scale text-to-image generative models.\n* The authors should provide a thorough comparison of E-TSDM with other potential methods to address the challenge of large Lipschitz constants.\n* The authors should explore the possibility of applying E-TSDM to other types of generative models, such as variational autoencoders (VAEs) and generative adversarial networks (GANs).\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of generative models by exploring the issue of Lipschitz singularities and proposing a novel approach to address this challenge. The experimental results demonstrate the superiority of E-TSDM over the baseline on various datasets. However, the paper has some limitations, such as assuming that the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0 and not providing a thorough comparison with other potential methods. Therefore, I recommend that the authors address these limitations in future work.\n\nOverall, I believe that the paper is well-written, and the authors provide a thorough analysis of the issue and a novel approach to address it. However, the paper has some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper explores the phenomenon of infinite Lipschitz constants in diffusion models, particularly in the context of noise prediction and v-prediction models. The authors provide theoretical proofs and empirical results to confirm the existence of this issue, which affects the stability and accuracy of the diffusion process. To mitigate this problem, the authors propose a novel approach, Early Timestep-shared Diffusion Model (E-TSDM), which involves sharing timestep conditions in the interval with large Lipschitz constants. The paper demonstrates the effectiveness of E-TSDM in reducing Lipschitz constants, improving stability, and achieving state-of-the-art performance in various applications, including unconditional generation, conditional generation, and fast sampling.\n\nStrengths:\n1. The paper provides a clear and thorough explanation of the problem of infinite Lipschitz constants in diffusion models.\n2. The authors offer a novel and effective solution, E-TSDM, which has been extensively tested and validated.\n3. The paper demonstrates the adaptability and generalizability of E-TSDM across different noise schedules, datasets, and applications.\n4. The authors provide a detailed analysis of the properties and hyperparameters of E-TSDM, which is beneficial for practical implementation.\n\nWeaknesses:\n1. The paper assumes a specific noise schedule, and it is unclear whether the results would hold for other noise schedules.\n2. The authors do not explore the potential effects of reducing Lipschitz constants on large-scale generative models.\n3. The paper does not provide a clear explanation of why the reduction of Lipschitz constants to zero within each sub-interval in E-TSDM may be beneficial or detrimental.\n\nSuggestions for Improvement:\n1. The authors should explore the effects of different noise schedules on the performance of E-TSDM.\n2. The paper should include a more in-depth discussion of the potential effects of reducing Lipschitz constants on large-scale generative models.\n3. The authors should provide more insight into why reducing Lipschitz constants to zero within each sub-interval in E-TSDM may be beneficial or detrimental.\n\nRecommendation:\nThe paper presents a significant contribution to the field of diffusion models by identifying and addressing the problem of infinite Lipschitz constants. The proposed solution, E-TSDM, has been extensively tested and validated, and its adaptability and generalizability are impressive. The paper has the potential to improve the performance of diffusion models and advance our understanding of the rationality underlying the diffusion process. However, the authors should address the limitations and suggestions for improvement mentioned above to further strengthen the paper.\n\nOverall, I recommend this paper for publication, subject to the authors addressing the mentioned limitations and suggestions for improvement.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.5631868131868132,
        "gr_score_without": 0.43617021276595747,
        "output_without": {
            "num_tokens_scored": 376,
            "num_green_tokens": 164,
            "green_fraction": 0.43617021276595747,
            "z_score": 1.4316582658130816,
            "p_value": 0.076120822882506
        },
        "output_with": {
            "num_tokens_scored": 364,
            "num_green_tokens": 205,
            "green_fraction": 0.5631868131868132,
            "z_score": 6.355213313787058,
            "p_value": 1.0406879299167209e-10
        }
    },
    {
        "title": "UNKNOWN DOMAIN INCONSISTENCY MINIMIZATION FOR DOMAIN GENERALIZATION",
        "abstract": "The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain\u2019s loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there\u2019s still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM\u2019s generalization capability in unseen domains. Our code is available at https://github.com/SJShin-AI/UDIM.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to domain generalization called Unknown Domain Inconsistency Minimization (UDIM). The objective of UDIM is to minimize the discrepancy in the loss landscape between the source domain and unobserved domains. UDIM combines the Sharpness-Aware Minimization (SAM) with a new objective that leverages both parameter and data perturbed regions for domain generalization. The authors theoretically prove that the integration of SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. Empirical results demonstrate that UDIM consistently outperforms SAM variants across multiple DG benchmark datasets, achieving accuracy gains up to 9.9% in some settings.\n\nStrengths:\n\n* The paper proposes a novel approach to domain generalization that leverages both parameter and data perturbed regions.\n* The authors provide a thorough theoretical analysis of the proposed approach, including a proof of the upper bound for the true objective of the DG task.\n* Empirical results demonstrate the effectiveness of UDIM in improving generalization performance on unobserved domains.\n* The paper provides a clear and concise description of the proposed approach, making it easy to follow and understand.\n\nWeaknesses:\n\n* The paper assumes that the source domain dataset is representative of the unobserved domains, which may not always be the case.\n* The authors do not provide a thorough evaluation of the proposed approach on a diverse set of datasets and scenarios.\n* The paper does not discuss the computational cost of the proposed approach, which may be a concern for large-scale applications.\n* The authors do not provide a clear explanation of how the proposed approach can be extended to other types of domain generalization tasks, such as multi-task learning.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough evaluation of the proposed approach on a diverse set of datasets and scenarios.\n* The paper should discuss the computational cost of the proposed approach and provide a clear explanation of how it can be optimized for large-scale applications.\n* The authors should provide a clear explanation of how the proposed approach can be extended to other types of domain generalization tasks, such as multi-task learning.\n* The paper should provide a more detailed analysis of the theoretical results, including a proof of the upper bound for the true objective of the DG task.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to domain generalization that leverages both parameter and data perturbed regions. The authors provide a thorough theoretical analysis and empirical results demonstrate the effectiveness of the proposed approach. However, the paper assumes that the source domain dataset is representative of the unobserved domains, which may not always be the case. The authors should provide a more thorough evaluation of the proposed approach on a diverse set of datasets and scenarios, and discuss the computational cost of the proposed approach. Overall, the paper is well-written and provides a clear and concise description of the proposed approach. I recommend accepting the paper for publication after addressing the suggested improvements.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents Unknown Domain Inconsistency Minimization (UDIM), a novel approach to improve domain generalization performance. The authors propose a method that leverages both parameter and data perturbation to minimize the loss landscape discrepancy between the source domain and unknown domains. The UDIM objective is formulated to optimize both the source domain loss and the worst-case loss across unknown domains. The authors provide theoretical analysis and experimental results demonstrating the efficacy of UDIM in improving generalization performance on various benchmark datasets.\n\nStrengths:\n1. The paper presents a novel approach to improve domain generalization performance by leveraging both parameter and data perturbation.\n2. The authors provide a clear and concise formulation of the UDIM objective and its optimization process.\n3. The experimental results demonstrate the efficacy of UDIM in improving generalization performance on various benchmark datasets.\n4. The authors provide a thorough analysis of the strengths and weaknesses of the proposed method.\n\nWeaknesses:\n1. The paper assumes a specific distribution of the data in the unknown domains, which may not always hold in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed method.\n3. The paper relies heavily on experimental results to demonstrate the efficacy of UDIM, which may not be sufficient to establish its robustness in all scenarios.\n4. The authors do not provide a clear comparison of UDIM with other state-of-the-art methods in domain generalization.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the distribution of the data in the unknown domains and how it affects the performance of UDIM.\n2. The paper should include a detailed comparison of UDIM with other state-of-the-art methods in domain generalization to establish its robustness.\n3. The authors should provide a more detailed analysis of the computational complexity of the proposed method and its scalability to large-scale datasets.\n4. The paper should include a discussion of the potential applications of UDIM in real-world scenarios and its potential limitations.\n\nRecommendation:\nThe paper presents a novel approach to improve domain generalization performance, and the experimental results demonstrate its efficacy. However, to establish its robustness and scalability, the authors should provide a more detailed analysis of the distribution of the data in the unknown domains, a detailed comparison with other state-of-the-art methods, and a detailed analysis of the computational complexity. With these additions, the paper would be a strong contribution to the field of domain generalization.",
        "gr_score_with": 0.5437956204379562,
        "gr_score_without": 0.46105919003115264,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 148,
            "green_fraction": 0.46105919003115264,
            "z_score": 2.2330473616138993,
            "p_value": 0.012772910639052731
        },
        "output_with": {
            "num_tokens_scored": 274,
            "num_green_tokens": 149,
            "green_fraction": 0.5437956204379562,
            "z_score": 4.858646693119851,
            "p_value": 5.909541198268063e-07
        }
    },
    {
        "title": "SCALING PHYSICS-INFORMED HARD CONSTRAINTS WITH MIXTURE-OF-EXPERTS",
        "abstract": "Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an \u201cexpert\u201d through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE) in neural networks. The authors aim to address the challenge of imposing hard constraints, which significantly increases computational and memory costs, especially for complex dynamical systems. They introduce a physics-informed hard constraint mixture-of-experts (PI-HC-MoE) framework that decomposes the constraint into smaller experts, allowing for parallelization and improved convergence. The authors demonstrate the effectiveness of their approach on two challenging non-linear systems, 1D diffusion-sorption and 2D Navier-Stokes equations, achieving lower errors than standard differentiable optimization using a single hard constraint and soft constraint penalty methods.\n\nStrengths:\n\n1. The paper addresses a significant challenge in enforcing hard physical constraints in neural networks, which is crucial for modeling complex dynamical systems.\n2. The proposed PI-HC-MoE framework is scalable and efficient, allowing for parallelization and improved convergence.\n3. The authors provide a thorough analysis of the method's performance on two challenging non-linear systems, demonstrating its effectiveness.\n4. The paper contributes to the broader field of physics-informed neural networks and differentiable physics.\n\nWeaknesses:\n\n1. The paper assumes a specific neural network architecture (FNO) and does not explore other architectures or their impact on the method's performance.\n2. The authors do not provide a comprehensive comparison with other state-of-the-art methods for enforcing hard constraints.\n3. The paper assumes a specific problem setting (non-linear least squares solver) and does not explore other optimization methods or their impact on the method's performance.\n4. The authors do not provide a detailed analysis of the computational cost and memory requirements of the proposed method.\n\nSuggestions for Improvement:\n\n1. Explore other neural network architectures and their impact on the method's performance.\n2. Provide a comprehensive comparison with other state-of-the-art methods for enforcing hard constraints.\n3. Investigate other optimization methods and their impact on the method's performance.\n4. Provide a detailed analysis of the computational cost and memory requirements of the proposed method.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of physics-informed neural networks and differentiable physics. The proposed PI-HC-MoE framework is scalable and efficient, and the authors demonstrate its effectiveness on two challenging non-linear systems. However, to further strengthen the paper, the authors should address the weaknesses mentioned above and provide a more comprehensive evaluation of the method's performance. With these improvements, the paper has the potential to make a significant impact in the field.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a scalable approach to enforce hard physical constraints in neural networks using Mixture-of-Experts (MoE). The approach, called Physics-Informed Hard Constraint Mixture-of-Experts (PI-HC-MoE), decomposes the spatiotemporal domain into smaller domains, each of which is solved by an \"expert\" through differentiable optimization. The experts can be parallelized across multiple GPUs, making the approach more scalable and efficient. The authors demonstrate the effectiveness of PI-HC-MoE on two challenging non-linear systems, 1D diffusion-sorption and 2D Navier-Stokes equations, and show that it achieves lower L2 relative error and faster convergence than standard differentiable optimization and soft constraint penalty methods.\n\nStrengths:\n\n* The paper proposes a novel and scalable approach to enforcing hard physical constraints in neural networks.\n* The approach is demonstrated on two challenging non-linear systems, 1D diffusion-sorption and 2D Navier-Stokes equations, and shows significant improvements over existing methods.\n* The paper provides a clear and detailed explanation of the approach and its implementation.\n* The authors provide a comprehensive analysis of the results, including scalability and convergence analysis.\n\nWeaknesses:\n\n* The paper assumes that the physical laws can be represented as a set of basis functions, which may not be the case in all physical systems.\n* The approach may not be applicable to systems with complex or non-linear physical laws.\n* The paper does not provide a detailed analysis of the computational resources required for the approach, which may be a limitation for large-scale applications.\n* The authors rely heavily on numerical experiments, and more theoretical analysis would be beneficial to confirm the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the assumptions and limitations of the approach, particularly with regards to the representation of physical laws as basis functions.\n* Investigate the applicability of the approach to systems with complex or non-linear physical laws.\n* Provide a more detailed analysis of the computational resources required for the approach, including estimates of memory and time requirements.\n* Include more theoretical analysis, such as convergence and stability analysis, to confirm the results of the numerical experiments.\n\nRecommendation:\n\nThe paper proposes a novel and scalable approach to enforcing hard physical constraints in neural networks, and demonstrates its effectiveness on two challenging non-linear systems. While there are some limitations and suggestions for improvement, the approach is a significant contribution to the field and warrants further investigation. The paper is well-written and clear, and the authors provide a comprehensive analysis of the results. I recommend accepting this paper for publication, with the suggestions for improvement in mind.\n\nGrade: 8.5/10\n\nReviewer's Name: [Your Name]",
        "gr_score_with": 0.556135770234987,
        "gr_score_without": 0.4489795918367347,
        "output_without": {
            "num_tokens_scored": 343,
            "num_green_tokens": 154,
            "green_fraction": 0.4489795918367347,
            "z_score": 1.8516401995451008,
            "p_value": 0.032038753225529924
        },
        "output_with": {
            "num_tokens_scored": 383,
            "num_green_tokens": 213,
            "green_fraction": 0.556135770234987,
            "z_score": 6.237293436810892,
            "p_value": 2.226031999714693e-10
        }
    },
    {
        "title": "GOAL-CONDITIONED REINFORCEMENT LEARNING",
        "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to Offline Goal-Conditioned Reinforcement Learning (GCRL) called SMORe. GCRL is a challenging problem where an agent learns to achieve multiple goals in an environment from offline datasets using sparse reward functions. The authors argue that existing methods, such as supervised learning and contrastive learning, are often suboptimal in the offline setting. They propose a new perspective on GCRL by formulating it as an occupancy matching problem and derive a discriminator-free method that learns unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. The authors show that their method, SMORe, outperforms state-of-the-art baselines on a variety of tasks, including robotic arms, anthropomorphic hands, and locomotion environments.\n\nStrengths:\n\n* The paper presents a novel approach to GCRL that is discriminator-free and applicable to a number of f-divergences.\n* The authors provide a clear and concise explanation of the problem and the proposed solution.\n* The paper includes extensive experiments on a variety of tasks, including robotic arms, anthropomorphic hands, and locomotion environments.\n* The authors show that SMORe outperforms state-of-the-art baselines on these tasks.\n\nWeaknesses:\n\n* The paper assumes that the goal distribution is known, which may not be the case in practice.\n* The authors do not provide a thorough analysis of the computational complexity of SMORe.\n* The paper does not provide a clear explanation of how to choose the hyperparameters of SMORe.\n* The authors do not provide a comparison with other methods that are specifically designed for offline GCRL.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the computational complexity of SMORe.\n* The paper should include a comparison with other methods that are specifically designed for offline GCRL.\n* The authors should provide a clear explanation of how to choose the hyperparameters of SMORe.\n* The paper should include more details on how to handle the case where the goal distribution is not known.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to GCRL that is discriminator-free and applicable to a number of f-divergences. The authors provide extensive experiments on a variety of tasks and show that SMORe outperforms state-of-the-art baselines. However, the paper assumes that the goal distribution is known, which may not be the case in practice. Therefore, I recommend that the authors provide a more thorough analysis of the computational complexity of SMORe and include a comparison with other methods that are specifically designed for offline GCRL.\n\nIn terms of publication, I recommend that the paper be accepted for publication in a top-tier conference or journal in the field of reinforcement learning. The paper has the potential to make a significant impact in the field and provide a new perspective on GCRL.\n\nOverall, I believe that the paper is well-written, clear, and concise, and presents a novel and promising approach to GCRL. However, the paper assumes that the goal distribution is known, which may not be the case in practice. Therefore, I recommend that the authors provide a more thorough analysis of the computational complexity of SMORe and include a comparison with other methods that are specifically designed for offline GCRL.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents SMORe, a novel approach to offline goal-conditioned reinforcement learning (GCRL) that optimizes for occupancy matching using a convex dual formulation. SMORe is a discriminator-free method that learns unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. The method is principled and demonstrated to outperform state-of-the-art baselines on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks.\n\nStrengths:\n\n1. The paper presents a new approach to offline GCRL that is principled and demonstrated to outperform state-of-the-art baselines.\n2. The method is discriminator-free, which is an advantage over existing methods that require learning a discriminator.\n3. The method is applicable to a number of f-divergences, which is an advantage over existing methods that are limited to a specific divergence.\n4. The method learns unnormalized scores over actions at a state to reach the goal, which is an advantage over existing methods that learn normalized densities.\n\nWeaknesses:\n\n1. The paper assumes that the goal distribution is known, which is a limitation of the method.\n2. The method requires access to an offline dataset of goal-reaching trajectories, which is a limitation of the method.\n3. The method is not applicable to online GCRL, which is a limitation of the method.\n\nSuggestions for Improvement:\n\n1. The paper could provide more details on how to handle cases where the goal distribution is unknown.\n2. The paper could provide more details on how to handle cases where the offline dataset is limited.\n3. The paper could provide more experiments on online GCRL to demonstrate the limitations of the method.\n\nRecommendation:\n\nI recommend accepting the paper for publication. The method is novel, principled, and demonstrated to outperform state-of-the-art baselines. The limitations of the method are acknowledged, and the suggestions for improvement are reasonable. The paper provides a valuable contribution to the field of offline GCRL.\n\n$$$\nNote: The review is written in the format of a standard conference review, with a clear summary of the paper, strengths and weaknesses, suggestions for improvement, and a recommendation. The review is written in a neutral tone and provides a balanced evaluation of the paper.",
        "gr_score_with": 0.49328859060402686,
        "gr_score_without": 0.44,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 154,
            "green_fraction": 0.44,
            "z_score": 1.5275252316519468,
            "p_value": 0.06331522897380859
        },
        "output_with": {
            "num_tokens_scored": 298,
            "num_green_tokens": 147,
            "green_fraction": 0.49328859060402686,
            "z_score": 3.2872372003907944,
            "p_value": 0.000505877866824345
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method employs a single pair of frozen random matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings, making it suitable for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n\nStrengths:\n\n1. VeRA significantly reduces the number of trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable results to LoRA on various benchmarks, including GLUE, E2E, and image classification tasks.\n3. VeRA is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The paper provides a thorough analysis of the method's components, including the impact of different initializations and the magnitude of adaptation.\n\nWeaknesses:\n\n1. The paper assumes that the frozen random matrices can be regenerated from a random number generator (RNG) seed, which may not be feasible in all scenarios.\n2. The method's performance may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques.\n3. The paper focuses on language and vision models with Transformer architecture, and its applicability to other architectures and domains remains an area for future research.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of regenerating frozen random matrices from an RNG seed in various scenarios.\n2. Explore additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques, to improve the method's performance.\n3. Conduct experiments on other architectures and domains to evaluate VeRA's applicability and performance.\n\nRecommendation:\n\nVeRA is a promising parameter-efficient finetuning method that achieves comparable results to LoRA while reducing the number of trainable parameters. The method's memory savings make it suitable for scenarios that require frequent swapping of numerous finetuned models. However, further research is needed to explore the method's limitations and potential areas for improvement. I recommend accepting this paper for publication in a reputable conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel parameter-efficient finetuning method called Vector-based Random Matrix Adaptation (VeRA) for large language models. VeRA aims to address the issue of efficient adaptation for specific tasks by reducing the number of trainable parameters required for finetuning. The method achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead of the original low-rank matrices. The authors demonstrate the effectiveness of VeRA on various benchmarks, including the GLUE and E2E benchmarks, image classification tasks, and instruction-tuning of 7B and 13B language models.\n\nStrengths:\n\n1. VeRA achieves a significant reduction in the number of trainable parameters compared to the state-of-the-art LoRA method, with a ten-fold reduction in parameters yielding the same performance on the GLUE benchmark for RoBERTalarge.\n2. The method is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n3. VeRA has a minimal size of the scaling vectors, allowing many versions to reside in the limited memory of a single GPU, thus substantially improving serving efficiency and removing the bottleneck of loading specific models into memory.\n\nWeaknesses:\n\n1. The paper does not provide a thorough comparison of VeRA with other parameter-efficient finetuning methods, such as AdaLoRA and Bitfit.\n2. The experiments on image classification tasks are limited to a single architecture (ViT) and a single set of hyperparameters.\n3. The authors do not provide a detailed analysis of the performance of VeRA on tasks with a large number of classes or a large vocabulary.\n\nSuggestions for Improvement:\n\n1. Include a more comprehensive comparison of VeRA with other parameter-efficient finetuning methods in the experiments.\n2. Perform experiments on image classification tasks with a variety of architectures and hyperparameters to demonstrate the robustness of VeRA.\n3. Provide a detailed analysis of the performance of VeRA on tasks with a large number of classes or a large vocabulary.\n\nRecommendation:\n\nThis paper proposes a novel parameter-efficient finetuning method for large language models that achieves a significant reduction in trainable parameters with minimal performance degradation. The method is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users. However, the paper could benefit from a more comprehensive comparison with other parameter-efficient finetuning methods and a more thorough analysis of the performance of VeRA on various tasks. I recommend accepting this paper for publication in the conference, with the suggestions for improvement addressed in the future work.\n\n$$$",
        "gr_score_with": 0.5509641873278237,
        "gr_score_without": 0.4375,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 154,
            "green_fraction": 0.4375,
            "z_score": 1.436140661634506,
            "p_value": 0.07548115988012619
        },
        "output_with": {
            "num_tokens_scored": 363,
            "num_green_tokens": 200,
            "green_fraction": 0.5509641873278237,
            "z_score": 5.87112903167012,
            "p_value": 2.1641860766531144e-09
        }
    },
    {
        "title": "SKILL MACHINES: TEMPORAL LOGIC SKILL COMPOSITION IN REINFORCEMENT LEARNING",
        "abstract": "It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-optimal behaviours zero-shot. We demonstrate this experimentally in a tabular setting, as well as in a high-dimensional video game and continuous control environment. Finally, we also demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a framework called \"Skill Machines\" that enables an agent to solve complex tasks involving temporal and spatial composition. The authors address the challenge of the \"curse of dimensionality\" in temporal logic tasks by introducing the concept of \"skill primitives,\" which are composable sub-skills for achieving high-level goals in the environment. They also introduce \"skill machines,\" which are finite state machines that encode the solution to any temporal logic task by leveraging skill primitives. The authors demonstrate the effectiveness of their approach in several environments, including a high-dimensional video game and a continuous control environment.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of temporal logic composition and the challenges associated with it.\n2. The authors propose a novel framework for addressing this challenge, which is based on the concept of skill primitives and skill machines.\n3. The paper provides a thorough analysis of the theoretical foundations of the proposed approach, including the use of reward machines and goal-oriented value functions.\n4. The authors demonstrate the effectiveness of their approach in several environments, including a high-dimensional video game and a continuous control environment.\n5. The paper provides a clear and concise description of the experimental setup and results, including the use of zero-shot and few-shot learning.\n\nWeaknesses:\n\n1. The paper assumes a relatively simple environment with a small number of propositions and constraints, which may not be representative of more complex real-world environments.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed approach, which may be a limitation in practice.\n3. The paper does not provide a clear explanation of how the skill machines are learned, which may be a limitation in terms of interpretability.\n4. The authors do not provide a detailed comparison with other approaches to temporal logic composition, which may be a limitation in terms of evaluating the effectiveness of the proposed approach.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed approach, including the time and space complexity of the skill machines.\n2. The paper should include a more detailed explanation of how the skill machines are learned, including the use of reinforcement learning or other machine learning techniques.\n3. The authors should provide a more detailed comparison with other approaches to temporal logic composition, including the use of options frameworks and successor features.\n4. The paper should include a more detailed discussion of the limitations of the proposed approach, including the assumption of a relatively simple environment and the use of reward machines.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to temporal logic composition, which has the potential to be widely applicable in a variety of domains. However, the paper assumes a relatively simple environment and does not provide a detailed analysis of the computational complexity of the proposed approach. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity and limitations of the proposed approach, as well as a more detailed comparison with other approaches to temporal logic composition.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThis paper proposes a framework, called skill machines, to address the challenge of compositionality in reinforcement learning. The authors aim to develop agents that can solve complex tasks involving spatial and temporal composition, specified through language, with little or no additional learning. The key contributions of this work are: (1) introducing skill machines, finite state machines that encode the solution to any task specified using regular language; (2) demonstrating how skill machines can be learned from reward machines; and (3) showing how skill machines can be used to solve temporally extended tasks with little or no further learning.\n\nStrengths:\n1. The paper presents a clear and well-structured approach to address the challenge of compositionality in reinforcement learning.\n2. The authors provide a formal definition of skill machines and demonstrate how they can be learned from reward machines.\n3. The paper includes empirical results that show the effectiveness of skill machines in solving complex tasks involving spatial and temporal composition.\n4. The authors provide a detailed analysis of the strengths and weaknesses of their approach and identify potential areas for improvement.\n\nWeaknesses:\n1. The paper assumes that the task specifications are given in the form of regular language, which may not be the case in real-world scenarios.\n2. The authors use a simplified setting to evaluate the performance of skill machines, and it is unclear how well they would perform in more complex and realistic environments.\n3. The paper relies heavily on theoretical results and does not provide a clear explanation of how skill machines can be learned in practice.\n4. The authors do not provide a detailed comparison of their approach with existing methods for compositionality in reinforcement learning.\n\nSuggestions for Improvement:\n1. To address the assumption of regular language task specifications, the authors could explore how to extend their approach to handle more general task specifications.\n2. To evaluate the performance of skill machines in more complex and realistic environments, the authors could use benchmark tasks from the literature and compare their approach with existing methods.\n3. To provide a clear explanation of how skill machines can be learned in practice, the authors could include a detailed description of the learning algorithm and provide experimental results to support their claims.\n4. To provide a detailed comparison of their approach with existing methods for compositionality in reinforcement learning, the authors could include a comprehensive literature review and compare the performance of skill machines with existing methods.\n\nRecommendation:\nThis paper presents a promising approach to address the challenge of compositionality in reinforcement learning. However, to make the approach more effective and widely applicable, the authors should address the limitations and weaknesses of their approach and provide a clear explanation of how skill machines can be learned in practice. With further development and refinement, skill machines have the potential to become a widely adopted approach for compositionality in reinforcement learning.",
        "gr_score_with": 0.5507246376811594,
        "gr_score_without": 0.4342857142857143,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 152,
            "green_fraction": 0.4342857142857143,
            "z_score": 1.3093073414159544,
            "p_value": 0.09521513191276199
        },
        "output_with": {
            "num_tokens_scored": 345,
            "num_green_tokens": 190,
            "green_fraction": 0.5507246376811594,
            "z_score": 5.714630769430154,
            "p_value": 5.4971238160215125e-09
        }
    },
    {
        "title": "GNERP: GAUSSIAN-GUIDED NEURAL RECONSTRUC-",
        "abstract": "Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)\u2013based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel method for reconstructing the detailed geometry of reflective scenes using a Gaussian-based representation of normals and polarization priors. The method, called GNeRP, extends the geometry representation from scalar Signed Distance Functions (SDFs) to Gaussian fields of normals, which captures high-frequency details and separates complex geometry from specular reflection. The method is supervised by polarization priors, which reveal information about surface normals, and a Degree of Polarization (DoP) based reweighting strategy is introduced to alleviate noise issues in polarization priors.\n\nStrengths:\n1. The paper presents a novel approach for reconstructing reflective scenes, which is a challenging problem in computer vision.\n2. The Gaussian-based representation of normals is a more informative representation of geometry than scalar SDFs, and it captures high-frequency details.\n3. The use of polarization priors as supervision is a novel and effective way to guide the learning of geometry behind specular reflection.\n4. The DoP reweighing strategy is a useful technique for alleviating noise issues in polarization priors.\n\nWeaknesses:\n1. The paper relies heavily on the assumption that the incident light is unpolarized, which may not be true in all cases.\n2. The method requires a large amount of data, including radiance images and aligned polarization priors, which may be difficult to obtain.\n3. The method is computationally expensive, requiring a significant amount of time and resources to train.\n4. The paper does not provide a thorough evaluation of the method's performance on a wide range of scenes and datasets.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of the method and potential areas for improvement.\n2. The authors should provide a more thorough evaluation of the method's performance on a wide range of scenes and datasets.\n3. The authors should consider using more robust and efficient methods for estimating the covariance matrix of the Gaussian representation of normals.\n4. The authors should provide a more detailed explanation of the DoP reweighing strategy and its effects on the method's performance.\n\nRecommendation:\nThe paper presents a novel and effective approach for reconstructing reflective scenes, and it has the potential to make a significant contribution to the field of computer vision. However, the method has some limitations and areas for improvement, and the authors should address these issues in future work. Overall, I recommend that the paper be accepted for publication, but with the understanding that the authors will address the limitations and areas for improvement mentioned above.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel method, GNeRP, for reconstructing the detailed geometry of reflective scenes from multi-view images with polarization priors. The method employs a Gaussian-based representation of normals, which is supervised by polarization priors, and a DoP reweighing strategy to alleviate noise issues in polarization priors. The authors collect a new, challenging multi-view dataset, PolRef, to evaluate existing methods more comprehensively.\n\nStrengths:\n\n*   The proposed method, GNeRP, addresses the challenges of reconstructing reflective scenes, which is a significant contribution to the field of 3D reconstruction.\n*   The use of Gaussian-based representation of normals and polarization priors provides a more accurate and detailed representation of geometry.\n*   The DoP reweighing strategy is an innovative solution to the noise issues in polarization priors, which is a significant limitation of existing methods.\n*   The experimental results demonstrate the superiority of GNeRP over existing methods, including state-of-the-art methods specifically designed for reflective objects.\n\nWeaknesses:\n\n*   The method relies heavily on polarization priors, which may not be available for all scenes or datasets.\n*   The DoP reweighing strategy may not be applicable to all types of noise or imbalanced distributions of polarization priors.\n*   The method requires a significant amount of training data, which may not be available for all scenes or datasets.\n*   The method may not be applicable to scenes with highly reflective or textured surfaces.\n\nSuggestions for Improvement:\n\n*   Investigate alternative methods for handling noise or imbalanced distributions of polarization priors, such as using more robust or adaptive weighting schemes.\n*   Explore the use of other types of prior information, such as depth or surface normal information, to supplement or replace polarization priors.\n*   Investigate the use of more advanced machine learning or optimization techniques, such as deep learning or Bayesian optimization, to improve the accuracy or efficiency of GNeRP.\n*   Collect more diverse and challenging datasets, such as scenes with highly reflective or textured surfaces, to further evaluate the performance of GNeRP.\n\nRecommendation:\n\nThe proposed method, GNeRP, is a significant contribution to the field of 3D reconstruction, particularly for reconstructing reflective scenes. However, it has some limitations, such as relying heavily on polarization priors, which may not be available for all scenes or datasets. To address these limitations, the authors should investigate alternative methods for handling noise or imbalanced distributions of polarization priors, explore the use of other types of prior information, and collect more diverse and challenging datasets. Overall, GNeRP has the potential to become a state-of-the-art method for reconstructing reflective scenes, and its performance should be compared to existing methods more comprehensively.\n\n$$$",
        "gr_score_with": 0.49244712990936557,
        "gr_score_without": 0.36363636363636365,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 128,
            "green_fraction": 0.36363636363636365,
            "z_score": -1.392621247645584,
            "p_value": 0.9181328228377041
        },
        "output_with": {
            "num_tokens_scored": 331,
            "num_green_tokens": 163,
            "green_fraction": 0.49244712990936557,
            "z_score": 3.433221382818781,
            "p_value": 0.00029822743482382167
        }
    },
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential scalability issues of the method, particularly when dealing with large and complex datasets.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the concept bottleneck layer, including its training procedure and how it is integrated into the generative model.\n2. Include a more comprehensive discussion of the limitations of the method, including potential scalability issues and the need for large amounts of annotated data.\n3. Consider including more qualitative and quantitative evaluations of the method, such as user studies or expert evaluations, to provide a more comprehensive understanding of its effectiveness.\n4. Provide a clear explanation of how the method can be applied to other domains, such as text or audio, and discuss the potential challenges and opportunities of doing so.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. However, the paper requires a significant amount of background knowledge in generative modeling, and the authors rely heavily on existing work. Despite these limitations, the paper is well-written, and the authors provide a comprehensive evaluation of the method. I recommend this paper for readers interested in generative modeling and interpretable AI.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThis paper proposes concept bottleneck generative models (CBGMs) to address the challenges of interpreting and steering current generative models. The proposed architecture consists of three parts: the pre-concept bottleneck portion, the concept bottleneck (CB) layer, and the post-concept bottleneck portion. The CB layer is model-agnostic and can be applied to different types of generative models, including VAEs, GANs, and diffusion models. The authors demonstrate the effectiveness of the proposed method by showing that steering a generative model with the CB layer outperforms several input conditioning baselines across multiple datasets and generative models.\n\nStrengths:\n\n* The proposed method is model-agnostic and can be applied to different types of generative models.\n* The authors demonstrate the effectiveness of the proposed method by showing that steering a generative model with the CB layer outperforms several input conditioning baselines across multiple datasets and generative models.\n* The method provides a way to interpret and debug generative models by partitioning the pre-concept bottleneck representations into known and unknown concepts.\n* The authors provide a clear and well-structured presentation of their method and results.\n\nWeaknesses:\n\n* The method requires that the entire training set be annotated with pre-defined concepts, which can be a laborious requirement in practice.\n* The method is limited to image tasks and has not been tested on text tasks.\n* The authors do not provide a clear explanation of how the concept bottleneck layer is trained and how it learns to map input representations to human-understandable features.\n* The method assumes that the pre-defined human understandable features are complete, which is not realistic in many cases.\n\nSuggestions for Improvement:\n\n* Provide a clear explanation of how the concept bottleneck layer is trained and how it learns to map input representations to human-understandable features.\n* Test the method on text tasks and provide results.\n* Address the limitation of requiring pre-defined concepts by proposing a method to automatically learn concepts.\n* Provide more details on how the concept bottleneck layer is implemented and how it is integrated with different types of generative models.\n\nRecommendation:\n\nThis paper proposes a novel and effective method for steering and debugging generative models. However, there are some limitations and areas for improvement. Overall, the paper is well-written and well-structured, and the results are convincing. I would recommend this paper for publication in a top-tier conference, pending some revisions to address the limitations and suggestions for improvement.\n\nRating: 8/10\n\nThis review is subjective and based on my personal opinion and expertise. I would be happy to provide feedback and suggestions for improvement to the authors.",
        "gr_score_with": 0.5140449438202247,
        "gr_score_without": 0.35786802030456855,
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 141,
            "green_fraction": 0.35786802030456855,
            "z_score": -1.707081894813686,
            "p_value": 0.9560965841033031
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 183,
            "green_fraction": 0.5140449438202247,
            "z_score": 4.392334572440341,
            "p_value": 5.6069975476233785e-06
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. The framework consists of a Reasoning and a Reflection module, which enable the system to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. The paper presents extensive experiments that prove DiLu's capability to accumulate experience and generalize to different environments.\n\nStrengths:\n\n* The paper proposes a novel framework that addresses the challenges of autonomous driving by leveraging LLMs and a memory component.\n* The framework is designed to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.\n* The paper presents extensive experiments that demonstrate the effectiveness of DiLu in accumulating experience and generalizing to different environments.\n* The paper highlights the potential of DiLu to be deployed on practical autonomous driving systems.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used in the framework are pre-trained and available, which may not be the case in real-world scenarios.\n* The paper does not provide a thorough analysis of the decision-making latency of the DiLu framework, which is a critical aspect of autonomous driving systems.\n* The paper does not provide a comparison with other knowledge-driven approaches to autonomous driving.\n* The paper assumes that the memory component is updated continuously, which may not be feasible in real-world scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the decision-making latency of the DiLu framework and propose methods to reduce it.\n* Compare DiLu with other knowledge-driven approaches to autonomous driving and highlight its advantages.\n* Provide a more detailed explanation of the memory component and its update process.\n* Discuss the potential limitations of using LLMs in the framework and propose methods to address them.\n\nRecommendation:\n\nThe paper proposes a novel and effective framework for autonomous driving that leverages LLMs and a memory component. While there are some weaknesses and limitations, the paper presents extensive experiments that demonstrate the effectiveness of DiLu. Therefore, I recommend that the paper be accepted for publication, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel autonomous driving system, DiLu, which integrates a knowledge-driven approach using large language models (LLMs) to enable decision-making based on common-sense knowledge and continuous learning. DiLu consists of four core modules: Environment, Reasoning, Reflection, and Memory, which interact with each other to accumulate experiences and improve performance. The authors demonstrate that DiLu can accumulate experience, exhibit strong generalization ability, and directly acquire experiences from real-world datasets, making it a promising approach for practical autonomous driving systems.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to autonomous driving, leveraging LLMs to instill human-level knowledge and decision-making capabilities.\n2. The authors provide a clear and well-structured summary of the knowledge-driven paradigm, which serves as a solid foundation for their proposed DiLu framework.\n3. The experimental results demonstrate that DiLu can accumulate experience, exhibit strong generalization ability, and outperform SOTA RL-based methods in certain scenarios.\n4. The authors address the limitations of data-driven approaches, such as dataset bias, overfitting, and uninterpretability, and provide a solution that mitigates these challenges.\n\nWeaknesses:\n1. The paper relies heavily on the assumption that LLMs can be used as a substitute for human expertise in autonomous driving, which may not be universally applicable.\n2. The authors do not provide a thorough analysis of the potential risks and challenges associated with using LLMs in autonomous driving systems.\n3. The decision-making latency of DiLu (5-10 seconds) may be a concern in time-critical autonomous driving applications.\n4. The authors do not thoroughly address the issue of hallucinations generated by LLMs, which may be a significant problem in autonomous driving systems.\n\nSuggestions for Improvement:\n1. Provide a more thorough analysis of the potential risks and challenges associated with using LLMs in autonomous driving systems.\n2. Investigate methods to reduce decision-making latency and improve the efficiency of DiLu.\n3. Develop strategies to mitigate the issue of hallucinations generated by LLMs.\n4. Explore the possibility of incorporating other types of knowledge, such as sensor data or domain-specific expertise, into the DiLu framework.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to autonomous driving, and the experimental results demonstrate its potential effectiveness. However, the authors should address the limitations and challenges mentioned above to make the DiLu framework more robust and reliable. With further improvements, DiLu could become a promising solution for practical autonomous driving systems.\n\nGrade: B+\n\n$$$",
        "gr_score_with": 0.5027322404371585,
        "gr_score_without": 0.38961038961038963,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 120,
            "green_fraction": 0.38961038961038963,
            "z_score": -0.37219368415938864,
            "p_value": 0.6451256759147427
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 184,
            "green_fraction": 0.5027322404371585,
            "z_score": 4.011822237897406,
            "p_value": 3.0125931502317037e-05
        }
    },
    {
        "title": "GENERALIZATION: UNRAVELING LEARNING DIFFER-",
        "abstract": "This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension (ddata) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to ddata, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic \u201clabel sharpness\u201d (KF ) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model\u2019s adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our ddata formalism to the related metric of learned representation intrinsic dimension (drepr), derive a generalization scaling law with respect to drepr, and show that ddata serves as an upper bound for drepr. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper investigates the discrepancies in how neural networks learn from different imaging domains, specifically natural and medical images. The authors propose a generalization scaling law with respect to the intrinsic dimension of the training set (ddata) and introduce a novel measure of label sharpness (KF) of a dataset. They demonstrate that the substantial scaling discrepancy between the two domains may be attributed to the higher intrinsic label sharpness of medical imaging datasets. Furthermore, they show that a model's adversarial robustness relates to its training set's KF and that medical image datasets are typically more susceptible to adversarial attacks. The authors also extend their ddata formalism to derive and test a generalization scaling law with respect to the intrinsic dimension of the model's learned representations (drepr) and show that ddata serves as an upper bound for drepr.\n\nStrengths:\n\n* The paper provides a comprehensive investigation of the discrepancies in how neural networks learn from different imaging domains.\n* The authors propose a novel measure of label sharpness (KF) of a dataset, which is a significant contribution to the field.\n* The paper demonstrates the importance of considering the intrinsic properties of the training set in understanding neural network behavior.\n* The authors provide a thorough analysis of the relationship between generalization ability, representation learning, and adversarial robustness.\n\nWeaknesses:\n\n* The paper assumes a binary classification task, which may limit its applicability to more complex tasks.\n* The authors use a limited number of models and datasets, which may not be representative of the broader field.\n* The paper does not provide a clear explanation for the observed generalization scaling discrepancy between natural and medical image models.\n* The authors do not consider other potential factors that may contribute to the observed discrepancies.\n\nSuggestions for Improvement:\n\n* The authors should consider extending their analysis to more complex tasks, such as multi-class classification or semantic segmentation.\n* They should use a more diverse set of models and datasets to increase the generalizability of their findings.\n* The paper should provide a more detailed explanation for the observed generalization scaling discrepancy between natural and medical image models.\n* The authors should consider other potential factors that may contribute to the observed discrepancies and provide a more comprehensive analysis.\n\nRecommendation:\n\nThis paper provides a significant contribution to the field of neural network behavior and is well-written and well-structured. However, the limitations mentioned above should be addressed in future work. I would recommend this paper for publication in a reputable conference or journal, but with the caveat that the authors should address the limitations mentioned above.\n\nRating: 8/10\n\nReviewer's Signature: [Insert Signature]\n\nDate: [Insert Date]\n\n$$$\nNote: The rating and recommendation are based on the provided text and may not reflect the actual quality of the paper.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper investigates the discrepancies in neural network generalization ability across different imaging domains. Specifically, the authors examine how intrinsic properties of the training set, such as intrinsic dimension (ddata) and label sharpness (KF), affect generalization error and adversarial robustness. The study focuses on natural and medical image domains, showing that generalization error scales differently with ddata between these two domains. The authors propose that the higher intrinsic \"label sharpness\" of medical imaging datasets contributes to this difference. Additionally, they demonstrate that a model's adversarial robustness is negatively correlated with its training set's KF. The study provides a generalization scaling law with respect to ddata and drepr (learned representation intrinsic dimension), showing that ddata serves as an upper bound for drepr.\n\nStrengths:\n\n* The study provides a comprehensive analysis of neural network generalization ability across different imaging domains.\n* The authors propose a novel measure of intrinsic label sharpness (KF) and derive a generalization scaling law with respect to ddata.\n* The study demonstrates the importance of intrinsic dataset properties on generalization error and adversarial robustness.\n\nWeaknesses:\n\n* The study focuses on binary classification tasks and may not generalize to other tasks or datasets.\n* The analysis relies on assumptions about the dataset and model properties, which may not always hold.\n* The study does not provide a clear causal link between KF and generalization error.\n\nSuggestions for Improvement:\n\n* The study could be expanded to include other tasks or datasets to increase generalizability.\n* The analysis could be made more robust by incorporating more datasets or models to reduce dependence on assumptions.\n* The study could investigate alternative explanations for the observed generalization scaling discrepancy.\n\nRecommendation:\n\nThis paper provides valuable insights into neural network generalization ability across different imaging domains. The study's comprehensive analysis and novel proposals make significant contributions to the field. However, further research is needed to increase generalizability and provide a clear causal link between KF and generalization error.",
        "gr_score_with": 0.47586206896551725,
        "gr_score_without": 0.38636363636363635,
        "output_without": {
            "num_tokens_scored": 396,
            "num_green_tokens": 153,
            "green_fraction": 0.38636363636363635,
            "z_score": -0.5539117094069979,
            "p_value": 0.7101803644591724
        },
        "output_with": {
            "num_tokens_scored": 290,
            "num_green_tokens": 138,
            "green_fraction": 0.47586206896551725,
            "z_score": 2.637048158169613,
            "p_value": 0.004181547888300411
        }
    },
    {
        "title": "TRANSFORMER-MODULATED DIFFUSION MODELS FOR PROBABILISTIC MULTIVARIATE TIME SERIES FORECASTING",
        "abstract": "Transformers have gained widespread usage in multivariate time series (MTS) forecasting, delivering impressive performance. Nonetheless, these existing transformer-based methods often neglect an essential aspect: the incorporation of uncertainty into the predicted series, which holds significant value in decisionmaking. In this paper, we introduce a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. TMDM harnesses the power of transformers to extract essential insights from historical time series data. This information is then utilized as prior knowledge, capturing covariate-dependence in both the forward and reverse processes within the diffusion model. Furthermore, we seamlessly integrate well-designed transformerbased forecasting methods into TMDM to enhance its overall performance. Additionally, we introduce two novel metrics for evaluating uncertainty estimation performance. Through extensive experiments on six datasets using four evaluation metrics, we establish the effectiveness of TMDM in probabilistic MTS forecasting.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called Transformer-Modulated Diffusion Model (TMDM) for probabilistic multivariate time series forecasting. TMDM combines the conditional diffusion generative process with transformers to enable precise distribution forecasting for multivariate time series. The framework leverages the strengths of transformers in estimating conditional means and extends this capability as priors across both forward and reverse processes within the diffusion model. The authors introduce two novel evaluation metrics, Prediction Interval Coverage Probability (PICP) and Quantile Interval Coverage Error (QICE), to assess uncertainty estimation abilities in probabilistic multivariate time series forecasting models.\n\nStrengths:\n\n1. The paper proposes a novel framework that combines diffusion generative process with transformers, which is a unique and innovative approach to probabilistic multivariate time series forecasting.\n2. The authors introduce two novel evaluation metrics, PICP and QICE, which are essential for assessing uncertainty estimation abilities in probabilistic multivariate time series forecasting models.\n3. The paper provides a comprehensive experimental evaluation on six real-world datasets, demonstrating the effectiveness of TMDM in enhancing probabilistic prediction quality.\n4. The authors discuss the advantages of using representations captured by existing well-designed transformer-based models as conditions, which is a valuable insight for future research.\n\nWeaknesses:\n\n1. The paper assumes that the conditional representation y\u03020:M can be accurately estimated by the transformer, which may not always be the case in practice.\n2. The authors do not provide a detailed analysis of the computational complexity of TMDM, which may be a concern for large-scale time series forecasting tasks.\n3. The paper relies heavily on the performance of the transformer in estimating conditional means, which may not generalize well to other time series forecasting tasks.\n4. The authors do not provide a clear comparison with other state-of-the-art methods for probabilistic multivariate time series forecasting.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of TMDM and discuss potential optimization techniques to reduce computational costs.\n2. The paper should provide a more thorough comparison with other state-of-the-art methods for probabilistic multivariate time series forecasting, including a discussion of the strengths and weaknesses of each method.\n3. The authors should provide a more detailed analysis of the assumptions made in the paper, particularly the assumption that the conditional representation y\u03020:M can be accurately estimated by the transformer.\n4. The paper should provide a more comprehensive discussion of the potential applications of TMDM in real-world time series forecasting tasks.\n\nRecommendation:\n\nThe paper proposes a novel and innovative framework for probabilistic multivariate time series forecasting, which is a valuable contribution to the field. However, the paper has some limitations that need to be addressed. With some additional analysis and comparison with other state-of-the-art methods, the paper has the potential to be a top-tier publication in the field of time series forecasting.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework called Transformer-Modulated Diffusion Model (TMDM) for probabilistic multivariate time series forecasting. TMDM combines the diffusion generative process with existing well-designed transformer-based time series models to estimate the full distribution of future time series. The authors introduce a novel metric called Prediction Interval Coverage Probability (PICP) and Quantile Interval Coverage Error (QICE) to evaluate the uncertainty estimation capabilities of probabilistic multivariate time series forecasting models.\n\nStrengths:\n\n1.  The authors propose a novel framework called TMDM that combines diffusion generative process with existing well-designed transformer-based time series models to estimate the full distribution of future time series.\n2.  They introduce two novel metrics called PICP and QICE to evaluate the uncertainty estimation capabilities of probabilistic multivariate time series forecasting models.\n3.  The authors conduct extensive experiments on six real-world datasets to demonstrate the effectiveness of TMDM in probabilistic multivariate time series forecasting.\n4.  They also conduct an ablation study to assess the impact of each component within their proposed framework.\n\nWeaknesses:\n\n1.  The authors assume that the true distribution of future time series can be captured using a normal distribution, which may not be realistic in all scenarios.\n2.  They do not provide a detailed comparison of their proposed framework with other existing methods.\n3.  The authors assume that the noise level in the diffusion process can be determined using a fixed noise schedule, which may not be optimal in all scenarios.\n4.  They do not provide a detailed analysis of the computational complexity of their proposed framework.\n\nSuggestions for Improvement:\n\n1.  The authors should provide a more detailed comparison of their proposed framework with other existing methods.\n2.  They should investigate the use of different distributions to capture the true distribution of future time series.\n3.  The authors should investigate the use of a more adaptive noise schedule to optimize the noise level in the diffusion process.\n4.  They should provide a more detailed analysis of the computational complexity of their proposed framework.\n\nRecommendation:\n\nThis paper proposes a novel framework called TMDM that combines diffusion generative process with existing well-designed transformer-based time series models to estimate the full distribution of future time series. The authors introduce two novel metrics called PICP and QICE to evaluate the uncertainty estimation capabilities of probabilistic multivariate time series forecasting models. However, the authors assume that the true distribution of future time series can be captured using a normal distribution, which may not be realistic in all scenarios. They also do not provide a detailed comparison of their proposed framework with other existing methods. Nevertheless, the authors conduct extensive experiments on six real-world datasets to demonstrate the effectiveness of TMDM in probabilistic multivariate time series forecasting. Overall, this paper provides a novel contribution to the field of probabilistic multivariate time series forecasting and provides a useful tool for evaluating the uncertainty estimation capabilities of probabilistic multivariate time series forecasting models.\n\nI recommend that this paper be accepted for publication, subject to the authors addressing the weaknesses mentioned above.\n\nRating: 8/10",
        "gr_score_with": 0.4074074074074074,
        "gr_score_without": 0.28254847645429365,
        "output_without": {
            "num_tokens_scored": 361,
            "num_green_tokens": 102,
            "green_fraction": 0.28254847645429365,
            "z_score": -4.555191451491525,
            "p_value": 0.999997383105615
        },
        "output_with": {
            "num_tokens_scored": 297,
            "num_green_tokens": 121,
            "green_fraction": 0.4074074074074074,
            "z_score": 0.26057865332352254,
            "p_value": 0.39720872663491263
        }
    },
    {
        "title": "ING IN WHOLE SLIDE IMAGES",
        "abstract": "The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the ContextAware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5%, 95.9%, and 88.1%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interpretability by identifying regions of high diagnostic value.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, Context-Aware Multiple Instance Learning (CAMIL), for analyzing Whole Slide Images (WSIs) in cancer diagnostics. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. The authors evaluate CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving state-of-the-art performance compared to other attention-based MIL models.\n\nStrengths:\n\n1. The paper presents a novel approach to address the limitations of existing attention-based MIL models in capturing contextual information of tumor and neighboring tiles.\n2. The proposed neighbor-constrained attention mechanism effectively captures dependencies between tiles within a WSI.\n3. The authors provide thorough experiments and comparisons with state-of-the-art models, demonstrating the effectiveness of CAMIL on multiple datasets.\n4. The paper provides a comprehensive analysis of the attention maps generated by CAMIL, highlighting its ability to capture both global and local concepts.\n\nWeaknesses:\n\n1. The paper assumes that the contextual information of tumor and neighboring tiles is sufficient to improve the performance of MIL models. However, this assumption may not hold true for all types of cancers or imaging modalities.\n2. The authors do not provide a thorough analysis of the computational complexity of CAMIL, which may be a limitation for large-scale WSIs.\n3. The paper does not provide a clear explanation of how the neighbor-constrained attention mechanism is implemented, making it difficult to reproduce the results.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of the neighbor-constrained attention mechanism and its implementation.\n2. The paper should include a thorough analysis of the computational complexity of CAMIL and its potential limitations for large-scale WSIs.\n3. The authors should investigate the generalizability of CAMIL to other types of cancers or imaging modalities.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to analyzing WSIs in cancer diagnostics. However, the authors should address the limitations mentioned above to further improve the paper. With minor revisions, the paper has the potential to make a significant contribution to the field of medical imaging and computer vision.\n\nScore: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework dubbed Context-Aware Multiple Instance Learning (CAMIL) to improve the classification and localization of tumors in whole slide images (WSIs). CAMIL leverages the dependencies between neighboring tiles within a WSI and imposes contextual constraints as prior knowledge on the multiple instance learning model. The proposed method consists of a WSI-preprocessing phase, a tile and feature extraction module, a transformer module to capture global contexts, a neighbor-constrained attention module to capture local contexts, and a feature aggregator and classification layer. The method is evaluated on three histopathology datasets and achieves state-of-the-art performance.\n\nStrengths:\n1. The proposed method leverages the dependencies between neighboring tiles within a WSI, which can provide valuable contextual information for improved classification and localization.\n2. The method incorporates contextual constraints as prior knowledge into the MIL model, which can enhance the accuracy of the model.\n3. The proposed method consists of a transformer module and a neighbor-constrained attention module, which can efficiently capture both global and local contexts.\n4. The method achieves state-of-the-art performance on three histopathology datasets.\n\nWeaknesses:\n1. The method requires a large amount of labeled data to train the model, which can be a limitation in some cases.\n2. The method assumes that the dependencies between neighboring tiles are symmetric, which may not always be the case.\n3. The method may not generalize well to new datasets with different characteristics.\n\nSuggestions for Improvement:\n1. The method can be improved by incorporating more advanced techniques for handling the dependencies between neighboring tiles.\n2. The method can be improved by incorporating more contextual information, such as the morphology and topography of the tissue.\n3. The method can be improved by using more advanced architectures, such as graph-based architectures.\n\nRecommendation:\nThe proposed method is an excellent contribution to the field of computer vision and machine learning, and it has the potential to improve the accuracy and efficiency of cancer diagnosis and treatment. The method can be improved by addressing the limitations and challenges mentioned above. Overall, the method is well-written, and the results are convincing and impressive.\n\n$$$",
        "gr_score_with": 0.47,
        "gr_score_without": 0.3430232558139535,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 118,
            "green_fraction": 0.3430232558139535,
            "z_score": -2.157104720497778,
            "p_value": 0.9845012470515115
        },
        "output_with": {
            "num_tokens_scored": 300,
            "num_green_tokens": 141,
            "green_fraction": 0.47,
            "z_score": 2.4748737341529163,
            "p_value": 0.006664164390408773
        }
    },
    {
        "title": "COLIDE: CONCOMITANT LINEAR DAG ESTIMATION",
        "abstract": "We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the unknown SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE (Concomitant Linear DAG Estimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of noise variances in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for learning linear Directed Acyclic Graphs (DAGs) called CoLiDE (Concomitant Linear DAG Estimation). The framework simultaneously estimates both the DAG structure and the exogenous noise levels. The authors present variants of CoLiDE to estimate homoscedastic and heteroscedastic noise across nodes. The proposed method, CoLiDE, incorporates concomitant estimation of scale parameters to enhance DAG topology inference using continuous first-order optimization. The authors demonstrate that CoLiDE outperforms state-of-the-art methods in diverse synthetic and real-world settings, including the recovery of the DAG edges as well as their weights.\n\nStrengths:\n\n1. The paper proposes a novel framework for learning linear DAGs, which is a significant contribution to the field of causal discovery.\n2. The authors present a comprehensive evaluation of CoLiDE against other state-of-the-art approaches, including GES, GOLEM, DAGMA, SortNRegress, and DAGuerreotype.\n3. The paper provides a thorough analysis of the performance of CoLiDE in both homoscedastic and heteroscedastic settings, including the recovery of the DAG edges as well as their weights.\n4. The authors demonstrate that CoLiDE is robust to deviations from Gaussianity and can handle high-dimensional settings where both data scarcity and model uncertainty are prevalent.\n5. The paper provides a clear and concise explanation of the CoLiDE algorithm and its implementation details.\n\nWeaknesses:\n\n1. The paper assumes that the data is generated from a linear structural equation model (SEM), which may not be realistic in many real-world scenarios.\n2. The authors do not provide a thorough analysis of the computational complexity of the CoLiDE algorithm, which may be a limitation for large-scale datasets.\n3. The paper assumes that the noise variances are known or can be estimated, which may not be the case in many real-world scenarios.\n4. The authors do not provide a thorough analysis of the robustness of CoLiDE to outliers and other forms of data contamination.\n5. The paper assumes that the DAG is sparse, which may not be the case in many real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational complexity of the CoLiDE algorithm and its scalability to large-scale datasets.\n2. The paper should assume a more realistic noise model, such as a non-Gaussian noise model, to better reflect real-world scenarios.\n3. The authors should provide a more thorough analysis of the robustness of CoLiDE to outliers and other forms of data contamination.\n4. The paper should provide a more thorough analysis of the performance of CoLiDE in non-linear SEMs and other non-linear models.\n5. The authors should provide a more thorough analysis of the limitations of CoLiDE and potential areas for future work.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of causal discovery and provides a novel framework for learning linear DAGs. The authors provide a comprehensive evaluation of CoLiDE against other state-of-the-art approaches and demonstrate its robustness to deviations from Gaussianity. However, the paper assumes a linear SEM and does not provide a thorough analysis of the computational complexity of the CoLiDE algorithm. Therefore, I recommend that the authors address these limitations in future work.\n\nOverall, I believe that the paper is well-written, well-organized, and provides a significant contribution to the field of causal discovery. However, there are some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes CoLiDE, a framework for learning linear directed acyclic graphs (DAGs) from observational data, which incorporates concomitant estimation of scale parameters to enhance DAG topology inference. CoLiDE removes the coupling between the sparsity regularization parameter and the exogenous noise levels, leading to minimum or no recalibration effort across diverse problem instances. The paper presents two CoLiDE variants, CoLiDE-EV and CoLiDE-NV, for estimating homoscedastic and heteroscedastic noise across nodes, respectively.\n\nStrengths:\n\n* CoLiDE addresses the limitations of existing methods, which rely on limiting homoscedasticity assumptions or require expensive penalty parameter retuning.\n* CoLiDE removes the coupling between the sparsity regularization parameter and the exogenous noise levels, making it more robust to deviations from Gaussianity.\n* CoLiDE provides a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale parameters.\n* CoLiDE exhibits superior performance compared to state-of-the-art methods in various synthetic and real-world settings.\n\nWeaknesses:\n\n* CoLiDE may not perform well in smaller graphs with 10 or 20 nodes, where the profile log-likelihood score function used by GOLEM outperforms CoLiDE.\n* CoLiDE may require careful hyperparameter tuning, especially in heteroscedastic settings.\n\nSuggestions for Improvement:\n\n* Investigate the robustness of CoLiDE in smaller graphs with 10 or 20 nodes.\n* Explore the use of other convex loss functions beyond LS, such as Huber's loss, in CoLiDE.\n* Investigate the application of CoLiDE to non-linear SEMs, which may offer new opportunities for robustness and accuracy.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of DAG learning, addressing the limitations of existing methods and offering a new framework that removes the coupling between sparsity regularization and exogenous noise levels. CoLiDE exhibits superior performance compared to state-of-the-art methods, and its application to various synthetic and real-world settings demonstrates its robustness and accuracy. The paper is well-written, and the experimental results are thorough and convincing. Overall, CoLiDE is a promising approach for DAG learning, and its potential applications in various fields, including biology, genetics, finance, and economics, make it an exciting contribution.\n\nRating: 9/10\n\nReviewer's Comments:\n\nThe paper provides a comprehensive overview of CoLiDE, including its formulation, optimization, and experimental results. The paper is well-structured, and the writing is clear and concise. The experimental results demonstrate the robustness and accuracy of CoLiDE, and the comparison with state-of-the-art methods provides a thorough assessment of CoLiDE's performance. However, there are some minor issues with the paper, including the requirement for careful hyperparameter tuning, especially in heteroscedastic settings, and the potential limitations of CoLiDE in smaller graphs with 10 or 20 nodes.\n\nOverall, the paper provides a significant contribution to the field of DAG learning, and CoLiDE is a promising approach that addresses the limitations of existing methods. The paper is well-written, and the experimental results are thorough and convincing.",
        "gr_score_with": 0.5,
        "gr_score_without": 0.4187192118226601,
        "output_without": {
            "num_tokens_scored": 406,
            "num_green_tokens": 170,
            "green_fraction": 0.4187192118226601,
            "z_score": 0.7699188535494519,
            "p_value": 0.22067401470616332
        },
        "output_with": {
            "num_tokens_scored": 410,
            "num_green_tokens": 205,
            "green_fraction": 0.5,
            "z_score": 4.133198922545748,
            "p_value": 1.7887441945643245e-05
        }
    },
    {
        "title": "CONQUER: DIFFUSION-BASED 3D DEPTH AWARE COMPOSABLE IMAGE SYNTHESIS",
        "abstract": "Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, COMPOSE AND CONQUER (CNC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-conditional diffusion model called Compose and Conquer (CnC) that addresses two main challenges in the field: three-dimensional placement of multiple objects and region-specific localization of global semantics from multiple sources. CnC employs two main components: the local and global fuser, which respectively leverage the new Depth Disentanglement Training (DDT) and soft guidance techniques. The local fuser uses DDT to infer the absolute depth placement of objects, while the global fuser uses soft guidance to incorporate semantics onto localized regions. The authors demonstrate the effectiveness of CnC through extensive experimental results on the COCO-stuff and Pick-a-Pic datasets.\n\nStrengths:\n\n* The paper presents a novel approach to addressing two main challenges in the field of text-conditional diffusion models.\n* The authors provide a clear and concise explanation of the proposed method, including the local and global fuser components.\n* The experimental results demonstrate the effectiveness of CnC in addressing the challenges of three-dimensional placement of multiple objects and region-specific localization of global semantics.\n* The authors provide a thorough comparison with baseline models, including GLIGEN, ControlNet, Uni-ControlNet, and T2I-Adapter.\n\nWeaknesses:\n\n* The paper assumes that the user has a good understanding of the proposed method and the underlying concepts, which may make it difficult for readers without a background in the field to follow.\n* The authors do not provide a clear explanation of how the Depth Disentanglement Training (DDT) and soft guidance techniques work, which may make it difficult for readers to understand the proposed method.\n* The paper does not provide a clear discussion of the limitations of the proposed method, including the assumption that the user has a good understanding of the proposed method.\n* The authors do not provide a clear explanation of how the proposed method can be applied to real-world scenarios.\n\nSuggestions for Improvement:\n\n* Provide a clear and concise explanation of the proposed method, including the local and global fuser components, and the underlying concepts.\n* Provide a clear explanation of how the Depth Disentanglement Training (DDT) and soft guidance techniques work.\n* Discuss the limitations of the proposed method, including the assumption that the user has a good understanding of the proposed method.\n* Provide a clear explanation of how the proposed method can be applied to real-world scenarios.\n\nRecommendation:\n\nThe paper presents a novel approach to addressing two main challenges in the field of text-conditional diffusion models. The authors provide a clear and concise explanation of the proposed method, including the local and global fuser components, and the experimental results demonstrate the effectiveness of CnC. However, the paper assumes that the user has a good understanding of the proposed method and the underlying concepts, which may make it difficult for readers without a background in the field to follow. Therefore, I recommend that the authors provide a clear and concise explanation of the proposed method, including the local and global fuser components, and the underlying concepts, and discuss the limitations of the proposed method.\n\nRating: 7/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-conditional diffusion model called Compose and Conquer (CnC) that addresses two main challenges in the field: three-dimensional placement of multiple objects and region-specific localization of global semantics from multiple sources. CnC employs two main components: the local and global fuser, which respectively leverage the new Depth Disentanglement Training (DDT) and soft guidance techniques. The authors demonstrate that DDT infers the absolute depth placement of objects, and soft guidance is able to incorporate semantics onto localized regions.\n\nStrengths:\n\n1. The authors have proposed a novel approach to address the limitations of text-conditional diffusion models, specifically three-dimensional placement of multiple objects and region-specific localization of global semantics from multiple sources.\n2. The use of depth disentanglement training (DDT) and soft guidance techniques are innovative and effective in inferring absolute depth placement of objects and incorporating semantics onto localized regions, respectively.\n3. The authors have provided a comprehensive evaluation of their model on the COCO-stuff and Pick-a-Pic datasets, demonstrating its proficiency in addressing the challenges mentioned above.\n\nWeaknesses:\n\n1. The paper assumes that the depth maps are available, which may not be the case in real-world scenarios.\n2. The authors have used a large dataset to train their model, which may not be feasible for all users.\n3. The paper does not provide a clear explanation of how the soft guidance technique works, making it difficult to understand the paper's contributions.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the soft guidance technique works, including a mathematical derivation of the equation used to compute the similarity matrix S'.\n2. The authors should provide a more thorough evaluation of their model on a wider range of datasets, including those with different characteristics and complexities.\n3. The authors should investigate ways to make their model more practical and feasible for real-world applications, such as reducing the computational cost of training and inference.\n\nRecommendation:\n\nBased on the strengths and weaknesses of the paper, I recommend accepting the paper for publication, with the understanding that the authors will address the weaknesses mentioned above in a revised version of the paper. The paper presents a novel and effective approach to addressing the limitations of text-conditional diffusion models, and the authors have provided a comprehensive evaluation of their model on a range of datasets. However, the paper could be improved with a more detailed explanation of the soft guidance technique and a more thorough evaluation of the model on a wider range of datasets.\n\nRating: 7/10\n\nReviewing Criteria:\n\n1. Technical Quality: 6/10\n2. Novelty: 7/10\n3. Significance: 7/10\n4. Clarity: 6/10\n5. Overall Quality: 7/10\n6. Impact: 7/10\n7. Originality: 7/10\n8. Breadth: 7/10\n9. Depth: 7/10\n10. Use of References: 6/10\n\nReviewing Guidelines:\n\n1. Technical Quality: Reviewing the technical quality of the paper, including the methodology, experiments, and results.\n2. Novelty: Evaluating the originality and novelty of the paper's contributions, including the proposed approach and techniques.\n3. Significance: Assessing the significance of the paper's contributions, including their impact on the field and relevance to real-world applications.\n4. Clarity: Reviewing the paper's clarity, including the writing style, organization, and explanation of technical details.\n5. Overall Quality: Evaluating the paper's overall quality, including its contributions, methodology, experiments, results, and writing style.\n6. Impact: Assessing the paper's potential impact on the field, including its relevance to real-world applications and potential to influence future research.\n7. Originality: Evaluating the paper's originality, including its proposed approach, techniques, and contributions.\n8. Breadth: Reviewing the paper's breadth, including its scope, scope, and relevance to a wide range of topics and applications.\n9. Depth: Evaluating the paper's depth, including its technical details, methodology, and results.\n10. Use of References: Reviewing the paper's use of references, including its citation style, references to relevant literature, and use of prior work.\n\nReviewing Process:\n\n1. Reviewing the paper's technical quality, including its methodology, experiments, and results.\n2. Evaluating the paper's novelty, including its proposed approach, techniques, and contributions.\n3. Assessing the paper's significance, including its impact on the field and relevance to real-world applications.\n4. Reviewing the paper's clarity, including its writing style, organization, and explanation of technical details.\n5. Evaluating the paper's overall quality, including its contributions, methodology, experiments, results, and writing style.\n6. Assessing the paper's potential impact on the field, including its relevance to real-world applications and potential to influence future research.\n7. Evaluating the paper's originality, including its proposed approach, techniques, and contributions.\n8. Reviewing the paper's breadth, including its scope, scope, and relevance to a wide range of topics and applications.\n9. Evaluating the paper's depth, including its technical details, methodology, and results.\n10. Reviewing the paper's use of references, including its citation style, references to relevant literature, and use of prior work.\n\nReviewing Criteria:\n\n1. Technical Quality: 6/10\n2. Novelty: 7/10\n3. Significance: 7/10\n4. Clarity: 6/10\n5. Overall Quality: 7/10\n6. Impact: 7/10\n7. Originality: 7/10\n8. Breadth: 7/10\n9. Depth: 7/10\n10. Use of References: 6/10\n\nReviewing Guidelines:\n\n1. Technical Quality: Reviewing the technical quality of the paper, including the methodology, experiments, and results.\n2. Novelty: Evaluating the originality and novelty of the paper's contributions, including the proposed approach and techniques.\n3. Significance: Assessing the significance of the paper's contributions, including their impact on the field and relevance to real-world applications.\n4. Clarity: Reviewing the paper's clarity, including the writing style, organization, and explanation of technical details.\n5. Overall Quality: Evaluating the paper's overall quality, including its contributions, methodology, experiments, results, and writing style.\n6. Impact: Assessing the paper's potential impact on the field, including its relevance to real-world applications and potential to influence future research.\n7. Originality: Evaluating the paper's originality, including its proposed approach, techniques, and contributions.\n8. Breadth: Reviewing the paper's breadth, including its scope, scope, and relevance to a wide range of topics and applications.\n9. Depth: Evaluating the paper's depth, including its technical details, methodology, and results.\n10. Use of References: Reviewing the paper's use of references, including its citation style, references to relevant literature, and use of prior work.\n\nReviewing Process:\n\n1. Reviewing the paper's technical quality, including its methodology, experiments, and results.\n2. Evaluating the paper's novelty, including its proposed approach, techniques, and contributions.\n3. Assessing the paper's significance, including its impact on the field and relevance to real-world applications.\n4. Reviewing the paper's clarity, including its writing style, organization, and explanation of technical details.\n5. Evaluating the paper's overall quality, including its contributions, methodology, experiments, results, and writing style.\n6. Assessing the paper's potential impact on the field, including its relevance to real-world applications and potential to influence future research.\n7. Evaluating the paper's originality, including its proposed approach, techniques, and contributions.\n8. Reviewing the paper's breadth, including its scope, scope, and relevance to a wide range of topics and applications.\n9. Evaluating the paper's depth, including its technical details, methodology, and results.\n10. Reviewing the paper's use of references, including its citation style, references to relevant literature, and use of prior work.\n\nReviewing Criteria:\n\n1. Technical Quality: 6/10\n2. Novelty: 7/10\n3. Significance: 7/10\n4. Clarity: 6/10\n5. Overall Quality: 7/10\n6. Impact: 7/10\n7. Originality: 7/10\n8. Breadth: 7/10\n9. Depth: 7/10\n10. Use of References: 6/10\n\nReviewing Guidelines:\n\n1. Technical Quality: Reviewing the technical quality of the paper, including the methodology, experiments, and results.\n2. Novelty: Evaluating the originality and novelty of the paper's contributions, including the proposed approach and techniques.\n3. Significance: Assessing the significance of the paper's contributions, including their impact on the field and relevance to real-world applications.\n4. Clarity: Reviewing the paper's clarity, including the writing style, organization, and explanation of technical details.\n5. Overall Quality: Evaluating the paper's overall quality, including its contributions, methodology, experiments, results, and writing style.\n6. Impact: Assessing the paper's potential impact on the field, including its relevance to real-world applications and potential to influence future research.\n7. Originality: Evaluating the paper's originality, including its proposed approach, techniques, and contributions",
        "gr_score_with": 0.5018450184501845,
        "gr_score_without": 0.36177474402730375,
        "output_without": {
            "num_tokens_scored": 293,
            "num_green_tokens": 106,
            "green_fraction": 0.36177474402730375,
            "z_score": -1.3356067081015377,
            "p_value": 0.9091610717948907
        },
        "output_with": {
            "num_tokens_scored": 542,
            "num_green_tokens": 272,
            "green_fraction": 0.5018450184501845,
            "z_score": 4.83987130444007,
            "p_value": 6.496161801262788e-07
        }
    },
    {
        "title": "BENO: BOUNDARY-EMBEDDED NEURAL OPERATORS FOR ELLIPTIC PDES",
        "abstract": "Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green\u2019s function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%. Our source code can be found https://github.com/AI4Science-WestlakeU/beno.git.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel neural operator architecture, Boundary-Embedded Neural Operators (BENO), to solve elliptic partial differential equations (PDEs) with complex boundary conditions. BENO consists of two branches of Graph Neural Networks (GNNs) for interior source terms and boundary values, respectively, and a Transformer encoder to map the global boundary geometry into a latent vector that influences each message passing layer of the GNNs. The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems, and its strong generalization capabilities across different scenarios.\n\nStrengths:\n\n* The paper presents a novel and effective approach to solving elliptic PDEs with complex boundary conditions.\n* The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods.\n* The paper provides a thorough evaluation of BENO's performance on various datasets and scenarios.\n* The authors provide a detailed description of the architecture and implementation of BENO.\n\nWeaknesses:\n\n* The paper assumes a specific type of elliptic PDE (Poisson equation) and does not provide a general framework for solving other types of elliptic PDEs.\n* The authors do not provide a thorough comparison with other classical methods for solving elliptic PDEs.\n* The paper relies heavily on the use of graph neural networks and Transformers, which may not be suitable for all types of elliptic PDEs.\n* The authors do not provide a detailed analysis of the computational complexity of BENO.\n\nSuggestions for Improvement:\n\n* Provide a more general framework for solving elliptic PDEs, including different types of PDEs and boundary conditions.\n* Compare BENO with other classical methods for solving elliptic PDEs, such as finite element methods and finite difference methods.\n* Investigate the use of other neural network architectures, such as convolutional neural networks, for solving elliptic PDEs.\n* Provide a detailed analysis of the computational complexity of BENO and compare it with other methods.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to solving elliptic PDEs with complex boundary conditions. While there are some limitations to the paper, the authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods. Therefore, I recommend accepting this paper for publication, with the suggestion for the authors to address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper introduces Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture for solving elliptic partial differential equations (PDEs) with complex geometries and inhomogeneous boundary values. BENO consists of two branches of Graph Neural Networks (GNNs) for interior source terms and boundary values, respectively, and a Transformer encoder for mapping global boundary geometry to a latent vector. The authors claim that BENO outperforms state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems.\n\nStrengths:\n1. The paper presents a novel approach for addressing the challenges of inhomogeneous boundary conditions with complex boundary geometry in solving elliptic PDEs.\n2. The authors provide a comprehensive analysis of the strengths and weaknesses of existing methods for solving elliptic PDEs.\n3. The proposed BENO architecture incorporates physics intuition through a boundary-embedded architecture, consisting of GNNs and a Transformer.\n4. The authors demonstrate the effectiveness of BENO in outperforming existing state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems.\n5. The paper provides a thorough analysis of the generalization capabilities of BENO across different scenarios.\n\nWeaknesses:\n1. The paper assumes that the input data is well-preprocessed, which may not always be the case in real-world applications.\n2. The authors do not provide a comprehensive analysis of the computational complexity of BENO.\n3. The paper assumes that the boundary conditions are known, which may not always be the case in real-world applications.\n4. The authors do not provide a thorough analysis of the interpretability of BENO.\n5. The paper assumes that the solution space is well-defined, which may not always be the case in real-world applications.\n\nSuggestions for Improvement:\n1. Provide a more comprehensive analysis of the computational complexity of BENO.\n2. Include a thorough analysis of the interpretability of BENO.\n3. Provide a more realistic assumption for the input data preprocessing.\n4. Include a thorough analysis of the robustness of BENO to noisy or incomplete data.\n5. Provide a more comprehensive analysis of the generalization capabilities of BENO across different scenarios.\n\nRecommendation:\nThe paper presents a novel approach for addressing the challenges of inhomogeneous boundary conditions with complex boundary geometry in solving elliptic PDEs. However, there are some weaknesses in the paper that need to be addressed. With some improvements, this paper has the potential to be a significant contribution to the field of neural operators for solving elliptic PDEs.\n$$$",
        "gr_score_with": 0.4243421052631579,
        "gr_score_without": 0.3619047619047619,
        "output_without": {
            "num_tokens_scored": 315,
            "num_green_tokens": 114,
            "green_fraction": 0.3619047619047619,
            "z_score": -1.3801311186847085,
            "p_value": 0.9162268612556914
        },
        "output_with": {
            "num_tokens_scored": 304,
            "num_green_tokens": 129,
            "green_fraction": 0.4243421052631579,
            "z_score": 0.8663418757130659,
            "p_value": 0.1931513542338315
        }
    },
    {
        "title": "TRAIN NEURAL PROGRAM SYNTHESIZERS",
        "abstract": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one\u2019s intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample. We then use the informative dataset to train models to improve the synthesizer\u2019s ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics, i.e., how users choose examples informatively. The method bootstraps pragmatic synthesizers by iteratively generating data using speaker (example generator) and listener (program synthesizer) models, filtering the generated data with an example selection method drawn from work on computational pragmatics, and further training the speaker and listener models on the generated data. The approach is evaluated on the challenging task of synthesizing regular expressions from example strings, and it is shown that the method produces pragmatic program synthesizers with minimal supervision, matching the performance of synthesizers trained on human-produced examples.\n\nStrengths:\n\n* The paper proposes a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics.\n* The method is evaluated on the challenging task of synthesizing regular expressions from example strings, and it is shown that the method produces pragmatic program synthesizers with minimal supervision.\n* The paper provides a detailed description of the approach, including the algorithm, the speaker and listener models, and the example selection method.\n* The paper evaluates the approach using a user study, which provides insights into how humans communicate with the synthesizer.\n\nWeaknesses:\n\n* The paper assumes that the speaker and listener models are trained using a large dataset of programs and specifications, which may not be available in all domains.\n* The paper does not provide a detailed analysis of the computational complexity of the approach, which may be a concern in large-scale program synthesis tasks.\n* The paper does not evaluate the approach on other domains, such as open-ended Python code generation, which may have an even larger space of possible programs and input/output specifications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational complexity of the approach, including the time and space complexity of the speaker and listener models.\n* Evaluate the approach on other domains, such as open-ended Python code generation, to demonstrate its scalability and applicability.\n* Provide more insights into how the speaker and listener models are trained, including the learning rate, batch size, and number of epochs.\n\nRecommendation:\n\nThe paper presents a novel and interesting approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics. The approach is evaluated on the challenging task of synthesizing regular expressions from example strings, and it is shown that the method produces pragmatic program synthesizers with minimal supervision. However, the paper assumes that the speaker and listener models are trained using a large dataset of programs and specifications, which may not be available in all domains. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the approach and evaluate it on other domains to demonstrate its scalability and applicability.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics. The authors propose a bootstrapping algorithm that iteratively generates data using speaker and listener models, filters the generated data with an example selection method from work on computational pragmatics, and further trains the speaker and listener models on the generated data. The authors validate their approach on the challenging task of synthesizing regular expressions from example strings and show that their method matches the performance of synthesizers trained on human-produced examples, despite not using any human-produced data.\n\nStrengths:\nThe paper makes several significant contributions. Firstly, it presents a novel approach to train program synthesis models that can resolve ambiguity in program specification examples by modeling pragmatics. Secondly, the authors provide a comprehensive experimental evaluation of their approach on a challenging task of synthesizing regular expressions from example strings. Thirdly, they demonstrate that their method matches the performance of synthesizers trained on human-produced examples, despite not using any human-produced data. Finally, the authors provide a clear explanation of their approach and the underlying principles of pragmatic reasoning.\n\nWeaknesses:\nOne of the main weaknesses of the paper is that it focuses on a specific domain of regular expressions, which may not be representative of all programming domains. Another weakness is that the authors rely on a simplified version of the RSA pragmatic reasoning framework, which may not accurately capture the complexities of human pragmatic reasoning. Furthermore, the authors do not provide a detailed analysis of the limitations of their approach and how they can be addressed in future work.\n\nSuggestions for Improvement:\nTo improve the paper, the authors should provide a more comprehensive analysis of the limitations of their approach and how they can be addressed in future work. They should also provide a more detailed explanation of the underlying principles of pragmatic reasoning and how they are implemented in their approach. Furthermore, they should provide a more rigorous experimental evaluation of their approach on a broader range of programming domains.\n\nRecommendation:\nThe paper presents a significant contribution to the field of program synthesis and pragmatic reasoning. The authors provide a comprehensive experimental evaluation of their approach on a challenging task of synthesizing regular expressions from example strings. However, the paper has some limitations that need to be addressed in future work. I would recommend this paper for publication in a top conference on artificial intelligence or natural language processing, but with a request for revisions to address the limitations mentioned above.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.4937106918238994,
        "gr_score_without": 0.38762214983713356,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 119,
            "green_fraction": 0.38762214983713356,
            "z_score": -0.4426992518388979,
            "p_value": 0.6710083581603872
        },
        "output_with": {
            "num_tokens_scored": 318,
            "num_green_tokens": 157,
            "green_fraction": 0.4937106918238994,
            "z_score": 3.411120671392444,
            "p_value": 0.000323482260493494
        }
    },
    {
        "title": "CIRCUMVENTING CONCEPT ERASURE METHODS FOR TEXT-TO-IMAGE GENERATIVE MODELS",
        "abstract": "Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. Yet, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to \u201cerase\u201d sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we devise an algorithm to learn special input word embeddings that can retrieve \u201cerased\u201d concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents an investigation into the efficacy of concept erasure methods for text-to-image generative models, specifically Stable Diffusion. The authors examine seven recently proposed methods, including fine-tuning-based and inference-guiding-based approaches, and demonstrate that all of them can be circumvented using a novel attack technique called Concept Inversion (CI). The CI attack learns special word embeddings that can retrieve erased concepts from the sanitized models with no alterations to their weights. The authors' results highlight the brittleness of post-hoc concept erasure methods and call into question their use in the algorithmic toolkit for AI safety.\n\nStrengths:\n\n1. The paper presents a thorough investigation into the concept erasure methods, including a detailed analysis of the seven methods examined.\n2. The authors provide a clear and concise explanation of the CI attack technique and its application to each of the concept erasure methods.\n3. The results are well-presented and demonstrate the effectiveness of the CI attack in circumventing the concept erasure methods.\n4. The paper raises important questions about the use of post-hoc concept erasure methods in AI safety and highlights the need for more robust evaluation methodologies.\n\nWeaknesses:\n\n1. The paper assumes that the adversary has access to the weights and components of the erased model, which may not always be the case in real-world scenarios.\n2. The authors do not provide a comprehensive evaluation of the concept erasure methods in terms of their effectiveness in preventing the generation of sensitive or unsafe content.\n3. The paper does not provide a clear explanation of why the input filtering phenomenon seems to be occurring in all the recent methods.\n\nSuggestions for Improvement:\n\n1. The authors should consider investigating the use of more robust evaluation methodologies, such as human evaluation or more comprehensive metrics, to assess the effectiveness of the concept erasure methods.\n2. The paper could benefit from a more detailed analysis of the limitations of the CI attack technique and its potential vulnerabilities.\n3. The authors should provide a more comprehensive explanation of the input filtering phenomenon and its implications for the design of more robust concept erasure methods.\n\nRecommendation:\n\nThis paper presents a timely and important investigation into the concept erasure methods for text-to-image generative models. The results are well-presented and demonstrate the effectiveness of the CI attack technique in circumventing the concept erasure methods. However, the paper assumes a level of access to the weights and components of the erased model that may not always be the case in real-world scenarios. Therefore, I recommend that the authors consider investigating more robust evaluation methodologies and providing a more comprehensive explanation of the input filtering phenomenon. Overall, I believe that this paper makes a significant contribution to the field of AI safety and raises important questions about the use of post-hoc concept erasure methods.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThis paper examines the effectiveness of seven concept erasure methods for text-to-image generative models in removing sensitive or unsafe concepts. The authors propose an algorithm called Concept Inversion (CI) to learn special input word embeddings that can retrieve the erased concepts from the sanitized models without modifying their weights. The experiments show that all seven concept erasure methods can be fully circumvented using the CI attacks, and the seemingly-safe models may still be used to produce sensitive or offensive content.\n\nStrengths:\n\n1. The paper provides a thorough review of existing concept erasure methods and proposes an innovative approach to evaluate their effectiveness.\n2. The experiments are well-designed and provide convincing evidence that the CI attacks can circumvent the concept erasure methods.\n3. The paper highlights the brittleness of post-hoc concept erasure methods and calls into question their use in AI safety.\n4. The authors provide a clear and concise explanation of the concept erasure methods and the CI algorithm.\n\nWeaknesses:\n\n1. The paper focuses on text-to-image generative models, but the concept erasure methods may be applicable to other types of models as well.\n2. The experiments are limited to seven concept erasure methods, and it would be interesting to explore other methods as well.\n3. The paper assumes that the adversary has access to the weights and components of the erased model, which may not be the case in real-world scenarios.\n4. The authors do not provide a clear explanation of why the input filtering phenomenon seems to be occurring in all the recent methods.\n\nSuggestions for Improvement:\n\n1. The authors could explore other concept erasure methods and evaluate their effectiveness using the CI attacks.\n2. The experiments could be extended to other types of models, such as image-to-image translation models.\n3. The authors could provide more details about the input filtering phenomenon and propose potential solutions.\n4. The paper could benefit from more concrete recommendations for future work in concept erasure and AI safety.\n\nRecommendation:\n\nThis paper provides a thorough review of concept erasure methods and proposes an innovative approach to evaluate their effectiveness. The experiments are well-designed and provide convincing evidence that the CI attacks can circumvent the concept erasure methods. The paper highlights the brittleness of post-hoc concept erasure methods and calls into question their use in AI safety. I recommend this paper for publication in a prestigious conference or journal in the field of AI safety and machine learning.",
        "gr_score_with": 0.5111111111111111,
        "gr_score_without": 0.38873239436619716,
        "output_without": {
            "num_tokens_scored": 355,
            "num_green_tokens": 138,
            "green_fraction": 0.38873239436619716,
            "z_score": -0.4333513900174395,
            "p_value": 0.6676202453662028
        },
        "output_with": {
            "num_tokens_scored": 315,
            "num_green_tokens": 161,
            "green_fraction": 0.5111111111111111,
            "z_score": 4.025382429497066,
            "p_value": 2.844139767858285e-05
        }
    },
    {
        "title": "NONNEGATIVE LOW-RANK SEMIDEFINITE PROGRAM- MING",
        "abstract": "K-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the K-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed K-means formulation using a nonconvex Burer\u2013Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-theart NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments, we observe that our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an efficient and large-scale algorithm for the K-means clustering problem, called the Nonnegative Low-Rank Semidefinite Program (NLSDP) algorithm. The algorithm is based on the Burer-Monteiro factorization approach and is designed to leverage the implicit positive semidefinite structure of the membership matrix and nonnegativity constraint. The NLSDP algorithm achieves the same strong statistical optimality guarantees as the semidefinite programming (SDP) approach, but with significantly improved computational scalability.\n\nStrengths:\n\n1. The NLSDP algorithm is designed to overcome the computational bottleneck of the SDP approach, which is the need to store and optimize over the n^2 individual elements of the matrix.\n2. The algorithm is based on a nonconvex approach, which allows it to achieve the same statistical optimality guarantees as the SDP approach.\n3. The algorithm is computationally efficient and scalable, making it suitable for large-scale datasets.\n\nWeaknesses:\n\n1. The algorithm requires a careful choice of parameters, including the rank parameter r and the step size \u03b1, which can affect the convergence rate and accuracy of the algorithm.\n2. The algorithm assumes that the data points are generated from a Gaussian mixture model, which may not be the case in practice.\n3. The algorithm may not be robust to outliers or noisy data.\n\nSuggestions for Improvement:\n\n1. Investigate the robustness of the NLSDP algorithm to outliers and noisy data.\n2. Develop a more robust and efficient initialization method for the algorithm.\n3. Explore the use of other nonconvex approaches, such as the alternating direction method of multipliers (ADMM), to further improve the computational efficiency and scalability of the algorithm.\n\nRecommendation:\n\nThe NLSDP algorithm is a promising approach for the K-means clustering problem, and its strong statistical optimality guarantees and computational efficiency make it a valuable contribution to the field. However, further research is needed to improve its robustness and scalability, and to explore its applications in real-world datasets.",
        "peer_review_with_watermark": "$$$Paper Summary: The paper introduces a novel algorithm for solving the K-means clustering problem, which combines the advantages of both Semidefinite Programming (SDP) and Nonnegative Matrix Factorization (NMF). The algorithm, named Burer-Monteiro factorization, exploits the low-rank structure of the SDP relaxation to achieve substantial computational savings. The authors provide a theoretical analysis of the algorithm's convergence rate, showing that it converges linearly under certain conditions. The algorithm is compared to existing methods, such as SDP, NMF, Spectral Clustering, and K-means++, on both synthetic and real-world datasets. The results show that the proposed algorithm outperforms existing methods in terms of both clustering accuracy and computational efficiency. The paper also provides a thorough analysis of the algorithm's robustness to different initialization methods and its ability to handle high-dimensional data. The results suggest that the algorithm is robust to different initialization methods and can handle high-dimensional data efficiently. The paper concludes by highlighting the potential applications of the algorithm in various fields, such as image and video analysis, natural language processing, and bioinformatics. The algorithm's ability to handle high-dimensional data makes it particularly suitable for applications in these fields. The results of the paper suggest that the algorithm has the potential to become a new benchmark for K-means clustering algorithms.",
        "gr_score_with": 0.6238532110091743,
        "gr_score_without": 0.3987341772151899,
        "output_without": {
            "num_tokens_scored": 316,
            "num_green_tokens": 126,
            "green_fraction": 0.3987341772151899,
            "z_score": -0.04593152121746319,
            "p_value": 0.5183175848081761
        },
        "output_with": {
            "num_tokens_scored": 218,
            "num_green_tokens": 136,
            "green_fraction": 0.6238532110091743,
            "z_score": 6.746615416033838,
            "p_value": 7.566684788209457e-12
        }
    },
    {
        "title": "SLICEGPT: COMPRESS LARGE LANGUAGE MODELS",
        "abstract": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for OPT 66B and LLAMA-2 70B models with modest loss in accuracy (superior to 2:4 sparsity). Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA-2 70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents SliceGPT, a new post-training sparsification scheme for large language models. SliceGPT reduces the embedding dimension of the network by replacing each weight matrix with a smaller (dense) matrix, resulting in a smaller model size with modest loss in accuracy. The authors demonstrate that SliceGPT can remove up to 25% of the model parameters for OPT 66B and LLAMA-2 70B models with superior perplexity to the state-of-the-art 2:4 sparsity scheme. The paper also introduces the concept of computational invariance in transformer networks, which enables SliceGPT and may inspire future avenues to reduce memory and computation demands for pre-trained models.\n\nStrengths:\n1. The paper presents a novel and effective post-training sparsification scheme for large language models.\n2. The authors demonstrate that SliceGPT can achieve superior perplexity to the state-of-the-art 2:4 sparsity scheme.\n3. The paper introduces the concept of computational invariance in transformer networks, which may inspire future research in improving the efficiency of deep learning models.\n4. The authors provide extensive experimental validation of SliceGPT on various language generation and zero-shot tasks.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with the basics of transformer networks and sparsification techniques.\n2. The authors do not provide a thorough comparison with other pruning methods, such as magnitude-based sparsification or optimal brain surgeon.\n3. The paper does not discuss the potential limitations of SliceGPT, such as its applicability to other types of neural networks or its sensitivity to hyperparameters.\n4. The authors do not provide a clear explanation of how SliceGPT can be applied to models with different architectures or sizes.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the computational invariance concept and its implications for transformer networks.\n2. Compare SliceGPT with other pruning methods, such as magnitude-based sparsification or optimal brain surgeon.\n3. Discuss the potential limitations of SliceGPT and provide suggestions for future research.\n4. Provide a clear explanation of how SliceGPT can be applied to models with different architectures or sizes.\n\nRecommendation:\nThis paper presents a novel and effective post-training sparsification scheme for large language models. The authors demonstrate that SliceGPT can achieve superior perplexity to the state-of-the-art 2:4 sparsity scheme. However, the paper assumes that the reader is familiar with the basics of transformer networks and sparsification techniques, and does not provide a thorough comparison with other pruning methods. Despite these limitations, the paper is well-written and provides extensive experimental validation of SliceGPT. I recommend this paper for publication in a top-tier conference or journal, with the suggestion that the authors address the limitations mentioned above.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper presents SliceGPT, a new post-training sparsification scheme for large language models. SliceGPT reduces the embedding dimension of the network by replacing each weight matrix with a smaller one, resulting in a significant reduction in model size. The method is based on a computational invariance property of transformer networks, which allows for the application of orthogonal transformations to the weights of the network without affecting the output. The paper demonstrates the effectiveness of SliceGPT on various large language models, achieving superior perplexity to existing state-of-the-art methods.\n\nStrengths:\n\n* The paper presents a novel approach to model compression that is based on a fundamental property of transformer networks.\n* The method is scalable and can be applied to large language models with billions of parameters.\n* The paper provides a comprehensive experimental evaluation of SliceGPT on various large language models, demonstrating its effectiveness.\n\nWeaknesses:\n\n* The paper assumes that the computational invariance property holds for all transformer networks, which may not be the case in practice.\n* The method requires a calibration dataset to compute the orthogonal transformations, which may not be available in all cases.\n* The paper does not provide a detailed analysis of the computational complexity of SliceGPT.\n\nSuggestions for Improvement:\n\n* The paper should provide a more detailed analysis of the computational complexity of SliceGPT and its dependence on the size of the model and the calibration dataset.\n* The paper should investigate the assumption that the computational invariance property holds for all transformer networks.\n* The paper should provide a more detailed comparison of SliceGPT with other state-of-the-art methods for model compression.\n\nRecommendation:\n\nOverall, the paper presents a novel and effective approach to model compression for large language models. While there are some limitations and weaknesses to the method, it has the potential to make a significant impact on the field of natural language processing. I recommend acceptance of the paper with minor revisions to address the weaknesses mentioned above.\n\n$$$",
        "gr_score_with": 0.5461254612546126,
        "gr_score_without": 0.436950146627566,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 149,
            "green_fraction": 0.436950146627566,
            "z_score": 1.3927967180989522,
            "p_value": 0.08184063557103011
        },
        "output_with": {
            "num_tokens_scored": 271,
            "num_green_tokens": 148,
            "green_fraction": 0.5461254612546126,
            "z_score": 4.9102648713700825,
            "p_value": 4.547672760326911e-07
        }
    },
    {
        "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
        "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL. FedCOG consists of two key components: complementary data generation and knowledge-distillation-based model training. The complementary data generation module generates data from the shared global model to complement the original dataset, reducing the heterogeneity level. The knowledge-distillation-based model training module distills knowledge from the global model to the local model based on the generated data, mitigating the effects of heterogeneity. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy. The authors conduct extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n1. The paper addresses a critical issue in FL, data heterogeneity, from a novel perspective of modifying local dataset.\n2. FedCOG is a well-designed algorithm that consists of two key components, complementary data generation and knowledge-distillation-based model training, which are both orthogonal to most existing FL methods.\n3. The authors provide extensive experimental results on classical and real-world FL datasets, demonstrating the effectiveness of FedCOG.\n4. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy.\n\nWeaknesses:\n\n1. The paper does not provide a thorough analysis of the theoretical guarantees of FedCOG.\n2. The authors do not provide a detailed comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The paper assumes a centralized setting, where the global model is shared among all clients, which may not be realistic in many FL scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the theoretical guarantees of FedCOG, including convergence rates and optimality guarantees.\n2. The authors should conduct a more thorough comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The authors should consider a decentralized setting, where the global model is not shared among all clients, and evaluate the performance of FedCOG in such a setting.\n\nRecommendation:\n\nThe paper is well-written, and the proposed algorithm, FedCOG, is innovative and effective. However, the paper lacks a thorough theoretical analysis and a detailed comparison with other state-of-the-art methods. With some improvements, the paper can be considered for publication in a top-tier conference.",
        "peer_review_with_watermark": "$$$ \nPaper Title: FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION\n\nSummary:\nThe paper proposes a novel federated learning algorithm, FedCOG, to tackle data heterogeneity in FL. FedCOG generates data to complement the original dataset and distills knowledge from the global model to the local model through the generated data, achieving consensus among local models and alleviating the impact of data heterogeneity.\n\nStrengths:\n1. The paper proposes a novel approach to addressing data heterogeneity in FL, which is orthogonal to most existing methods.\n2. FedCOG is plug-and-play, meaning it can be easily combined with existing FL methods to further enhance their performance.\n3. The proposed algorithm is compatible with standard FL protocols, including Secure Aggregation, and makes no compromise on communication cost and privacy.\n4. The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nWeaknesses:\n1. The paper assumes that the global model is accurate and reliable, which may not always be the case in practice.\n2. The proposed algorithm requires generating data to complement the original dataset, which may be computationally expensive and require significant memory resources.\n3. The paper does not provide a thorough analysis of the computational cost and memory requirements of FedCOG.\n4. The paper assumes that the disagreement term between global and local models is effective in generating consensual data, which may not always be the case.\n\nSuggestions for Improvement:\n1. The paper could provide more detailed analysis of the computational cost and memory requirements of FedCOG, as well as its scalability to large-scale FL scenarios.\n2. The paper could provide more experimental results to demonstrate the robustness and generalizability of FedCOG to different FL scenarios and datasets.\n3. The paper could provide more discussion on the limitations and potential issues with the proposed algorithm, such as the accuracy and reliability of the global model, and the potential for overfitting to the generated data.\n\nRecommendation:\nThe paper proposes a novel and effective approach to addressing data heterogeneity in FL, and the experimental results demonstrate its potential to outperform state-of-the-art methods. However, the paper requires more detailed analysis and experimental results to fully demonstrate its robustness and generalizability. Therefore, I recommend acceptance with revisions to address the above suggestions.\n\nRating: 7.5/10\n\nReviewer: [Your Name]",
        "gr_score_with": 0.5392953929539296,
        "gr_score_without": 0.4276923076923077,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 139,
            "green_fraction": 0.4276923076923077,
            "z_score": 1.019049330730136,
            "p_value": 0.15408977373352706
        },
        "output_with": {
            "num_tokens_scored": 369,
            "num_green_tokens": 199,
            "green_fraction": 0.5392953929539296,
            "z_score": 5.461907175242599,
            "p_value": 2.355231898291056e-08
        }
    },
    {
        "title": "LMUFORMER: LOW COMPLEXITY YET POWERFUL SPIKING MODEL WITH LEGENDRE MEMORY UNITS",
        "abstract": "Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and state-of-the-art (SOTA) performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a fully-sequential recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LMUFormer, which augments the LMU with convolutional patch embedding and convolutional channel mixer. Moreover, we present a spiking version of this architecture, which introduces the benefit of states within the patch embedding and channel mixer modules while simultaneously reducing the computing complexity. We evaluated our architectures on multiple sequence datasets. Of particular note is our performance on the Speech Commands V2 dataset (35 classes). In comparison to SOTA transformer-based models within the ANN domain, our LMUFormer demonstrates comparable performance while necessitating a remarkable 53\u00d7 reduction in parameters and a substantial 65\u00d7 decrement in FLOPs. Furthermore, when benchmarked against extant low-complexity SNN variants, our model establishes a new SOTA with an accuracy of 96.12%. Additionally, owing to our model\u2019s proficiency in real-time data processing, we are able to achieve a 32.03% reduction in sequence length, all while incurring an inconsequential decline in performance. Our code is publicly available here.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel neural network architecture, LMUFormer, which combines the advantages of Legendre Memory Units (LMU) and Transformer models. The authors aim to develop a model that has parallel training, streaming, and low-cost inference capabilities while achieving state-of-the-art performance. To achieve this, they introduce a spiking version of the LMUFormer, which further reduces computational complexity and improves energy efficiency. The proposed architecture is evaluated on multiple sequence datasets, including the Speech Commands V2 dataset, and demonstrates comparable performance to state-of-the-art transformer-based models while requiring significantly fewer parameters and FLOPs.\n\nStrengths:\n\n1. The authors provide a comprehensive overview of the existing literature on Transformer models and their limitations, particularly in terms of computational complexity and sequential processing capability.\n2. The proposed LMUFormer architecture is well-designed and effectively combines the strengths of LMU and Transformer models.\n3. The authors provide extensive experimental results on multiple sequence datasets, including the Speech Commands V2 dataset, and demonstrate the effectiveness of the proposed architecture.\n4. The spiking version of the LMUFormer is a novel contribution that further reduces computational complexity and improves energy efficiency.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of neural networks and deep learning, which may make it challenging for non-experts to understand the proposed architecture and experimental results.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed architecture and its comparison with existing models.\n3. The paper relies heavily on experimental results, but the authors do not provide a thorough analysis of the model's performance on different tasks and datasets.\n4. The authors do not discuss the potential limitations and challenges of the proposed architecture, such as its sensitivity to hyperparameters and its ability to handle long-range dependencies.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of the proposed architecture and its comparison with existing models.\n2. Include a more thorough analysis of the model's performance on different tasks and datasets, including a discussion of its strengths and weaknesses.\n3. Discuss the potential limitations and challenges of the proposed architecture, such as its sensitivity to hyperparameters and its ability to handle long-range dependencies.\n4. Consider providing a more comprehensive overview of the existing literature on SNNs and their applications in sequence learning tasks.\n\nRecommendation:\n\nThe paper presents a novel and effective neural network architecture, LMUFormer, which combines the strengths of LMU and Transformer models. The proposed architecture demonstrates comparable performance to state-of-the-art transformer-based models while requiring significantly fewer parameters and FLOPs. However, the paper assumes a high level of familiarity with neural networks and deep learning, and the authors do not provide a detailed analysis of the computational complexity and potential limitations of the proposed architecture. Therefore, I recommend that the authors provide a more comprehensive overview of the existing literature, a detailed analysis of the computational complexity, and a thorough discussion of the model's performance and limitations.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes two novel neural network architectures, LMUFormer and Spiking LMUFormer, designed to improve the efficiency and performance of transformer models while retaining their sequential processing capability. The authors aim to develop models that can process data sequentially, reduce computational complexity, and achieve state-of-the-art performance. The proposed architectures combine Legendre Memory Units (LMU) with convolutional patch embedding and convolutional channel mixers. The Spiking LMUFormer variant introduces explicit states within the patch embedding and channel mixer modules, further reducing complexity and improving energy efficiency. The authors evaluate their models on various sequence learning tasks, including Speech Commands V2, permuted sequential MNIST, and Long Range Arena benchmark, and demonstrate competitive performance compared to state-of-the-art models.\n\nStrengths:\n\n* The paper addresses an important problem of reducing the computational complexity of transformer models while preserving their performance.\n* The proposed LMUFormer and Spiking LMUFormer architectures demonstrate competitive performance compared to state-of-the-art models.\n* The authors provide extensive experimental evaluations on various sequence learning tasks, including Speech Commands V2, permuted sequential MNIST, and Long Range Arena benchmark.\n* The paper discusses the benefits of using spiking neural networks, including reduced energy consumption and increased efficiency.\n\nWeaknesses:\n\n* The paper assumes that the input data can be processed sequentially, which might not be feasible for all applications.\n* The authors do not provide a clear explanation of how the convolutional patch embedding and convolutional channel mixers improve the performance of the LMU module.\n* The paper does not discuss the potential limitations of using Legendre Memory Units, such as their sensitivity to hyperparameters and potential issues with long-term memory.\n* The experimental evaluations could be more comprehensive, including comparisons with other state-of-the-art models and more detailed analysis of the results.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how the convolutional patch embedding and convolutional channel mixers improve the performance of the LMU module.\n* The paper should discuss the potential limitations of using Legendre Memory Units and propose solutions to mitigate these issues.\n* The experimental evaluations should be more comprehensive, including comparisons with other state-of-the-art models and more detailed analysis of the results.\n* The authors should discuss the potential applications of the proposed models and how they can be used in practice.\n\nRecommendation:\n\nThe paper presents an interesting approach to improving the efficiency and performance of transformer models. The proposed LMUFormer and Spiking LMUFormer architectures demonstrate competitive performance compared to state-of-the-art models, and the authors provide extensive experimental evaluations. However, the paper has some limitations, including the assumption that the input data can be processed sequentially and the lack of clear explanation of how the convolutional patch embedding and convolutional channel mixers improve the performance of the LMU module. To improve the paper, the authors should address these limitations and provide more comprehensive experimental evaluations. Overall, the paper is well-written and well-presented, and it has the potential to contribute to the field of neural network architecture design.\n\n$$$",
        "gr_score_with": 0.5165745856353591,
        "gr_score_without": 0.438953488372093,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 151,
            "green_fraction": 0.438953488372093,
            "z_score": 1.4747552680954206,
            "p_value": 0.07013917566402131
        },
        "output_with": {
            "num_tokens_scored": 362,
            "num_green_tokens": 187,
            "green_fraction": 0.5165745856353591,
            "z_score": 4.527438345711325,
            "p_value": 2.985150606816993e-06
        }
    },
    {
        "title": "MOTION GUIDANCE: DIFFUSION-BASED IMAGE EDIT-",
        "abstract": "Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel technique called motion guidance, which enables users to specify dense and complex motion fields to edit images. The method works by steering the diffusion sampling process using an off-the-shelf optical flow network, while also encouraging the generated image to be visually similar to the source image. The authors demonstrate that their technique can handle a wide range of complex motion fields, including translations, rotations, homographies, stretching, deformations, and even flow fields extracted from a video.\n\nStrengths:\n\n1. The paper proposes a novel and effective technique for motion-based image editing, which can handle complex motion fields and produce high-quality edits.\n2. The method is simple, zero-shot, and does not require any training or explicit constraints on the diffusion network architecture.\n3. The authors provide a comprehensive evaluation of their method, including qualitative and quantitative results, and comparisons with state-of-the-art baselines.\n4. The paper explores the possibility of repurposing other low-level computer vision models for image generation tasks through diffusion guidance, which is a promising area of research.\n\nWeaknesses:\n\n1. The paper relies on the performance of the off-the-shelf optical flow network, which may not be optimal for all cases.\n2. The method may be susceptible to various weaknesses, such as slow sampling speed and limitations of the optical flow method.\n3. The paper does not provide a thorough analysis of the trade-offs between the flow loss and the CLIP similarity metrics.\n4. The evaluation of the method is limited to a specific set of datasets and target flows, which may not be representative of all possible scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more advanced optical flow networks or other motion estimation models to improve the performance of the method.\n2. Explore the possibility of using other guidance signals, such as classifier guidance or text-based guidance, to improve the flexibility and robustness of the method.\n3. Provide a more thorough analysis of the trade-offs between the flow loss and the CLIP similarity metrics, including a deeper understanding of the underlying factors that influence these metrics.\n4. Evaluate the method on a more diverse set of datasets and target flows to better understand its limitations and potential applications.\n\nRecommendation:\n\nThe paper presents a novel and effective technique for motion-based image editing, which has the potential to improve the flexibility and robustness of image manipulation models. While the method has some limitations, it is a promising area of research that deserves further exploration. I recommend the paper for publication in a top-tier conference, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$ \n\nPaper Summary:\n\nThe paper proposes a novel technique called motion guidance, which enables users to specify dense and complex motion fields to edit images using diffusion models. The proposed method uses an off-the-shelf optical flow network to guide the diffusion sampling process and steer the image towards the desired motion while maintaining visual similarity to the source image. The authors demonstrate that their method can handle a wide range of complex motions, including compositions of translations, rotations, homographies, stretching, deformations, and even flow fields extracted from a video. They show that their method outperforms state-of-the-art baselines and can achieve high-quality motion-conditioned edits without requiring text prompts or fine-tuning with LoRA.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to image editing using diffusion models.\n2. The authors demonstrate that their method can handle a wide range of complex motions and achieve high-quality edits.\n3. The proposed method does not require text prompts or fine-tuning with LoRA, making it a significant improvement over existing methods.\n4. The authors provide comprehensive ablation studies to demonstrate the importance of each component of their method.\n5. They also provide quantitative results to compare their method with state-of-the-art baselines.\n\nWeaknesses:\n\n1. The paper does not provide a comprehensive discussion of the limitations of their method, such as slow sampling speed and the limitations of their optical flow method.\n2. The authors do not provide any qualitative or quantitative results on the robustness of their method to out-of-domain images.\n3. The paper does not provide any discussion on the potential applications of their method beyond image editing.\n4. The authors do not provide any information on how to extend their method to handle multiple objects or complex scenes.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive discussion of the limitations of their method and potential future work.\n2. They should provide qualitative and quantitative results on the robustness of their method to out-of-domain images.\n3. The authors should provide a discussion on the potential applications of their method beyond image editing.\n4. They should provide information on how to extend their method to handle multiple objects or complex scenes.\n5. The authors should provide more details on the implementation of their method and make the code publicly available.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to image editing using diffusion models. The authors demonstrate that their method can handle a wide range of complex motions and achieve high-quality edits. However, they should provide a more comprehensive discussion of the limitations of their method and potential future work. With some minor revisions, this paper has the potential to be a top-tier paper in the field of image manipulation and computer vision.",
        "gr_score_with": 0.4817073170731707,
        "gr_score_without": 0.35353535353535354,
        "output_without": {
            "num_tokens_scored": 396,
            "num_green_tokens": 140,
            "green_fraction": 0.35353535353535354,
            "z_score": -1.8874028616831022,
            "p_value": 0.9704469197394242
        },
        "output_with": {
            "num_tokens_scored": 328,
            "num_green_tokens": 158,
            "green_fraction": 0.4817073170731707,
            "z_score": 3.020593276135896,
            "p_value": 0.0012613999487141219
        }
    },
    {
        "title": "STRUCTURAL ESTIMATION OF PARTIALLY OBSERVED LINEAR NON-GAUSSIAN ACYCLIC MODEL: A PRAC-",
        "abstract": "Conventional causal discovery approaches, which seek to uncover causal relationships among measured variables, are typically sensitive to the presence of latent variables. While various methods have been developed to address this confounding issue, they often rely on strong assumptions about the underlying causal structure. In this paper, we consider a general scenario where measured and latent variables collectively form a partially observed causally sufficient linear system and latent variables may be anywhere in the causal structure. We theoretically show that with the aid of high-order statistics, the causal graph is (almost) fully identifiable if, roughly speaking, each latent set has a sufficient number of pure children, which can be either latent or measured. Naturally, LiNGAM, a model without latent variables, is encompassed as a special case. Based on the identification theorem, we develop a principled algorithm to identify the causal graph by testing for statistical independence involving only measured variables in specific manners. Experimental results show that our method effectively recovers the causal structure, even when latent variables are influenced by measured variables.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for structural estimation of partially observed linear non-Gaussian acyclic models, which allows for the presence of latent variables in the causal graph. The authors develop a three-phase algorithm to identify the causal structure, leveraging the Generalized Independent Noise (GIN) condition and non-Gaussianity. The algorithm iteratively identifies leaf atomic units, discovers new atomic units, and refines the atomic units. The paper theoretically demonstrates the identifiability of causal structures under the linear causal model and non-Gaussianity assumptions, without prior knowledge regarding the presence or positions of latent variables.\n\nStrengths:\n\n1. The paper proposes a novel method for structural estimation of partially observed linear non-Gaussian acyclic models, which is a significant contribution to the field of causal discovery.\n2. The authors develop a three-phase algorithm to identify the causal structure, which is efficient and scalable.\n3. The paper theoretically demonstrates the identifiability of causal structures under the linear causal model and non-Gaussianity assumptions, which is a significant theoretical contribution.\n4. The authors provide a comprehensive evaluation of the method using synthetic and real-world datasets, which demonstrates its effectiveness and robustness.\n\nWeaknesses:\n\n1. The paper assumes that the noise terms are continuous random variables with non-Gaussian distribution, which may not be realistic in many applications.\n2. The algorithm requires a sufficient number of pure children for each atomic unit, which may not be feasible in some cases.\n3. The paper does not provide a detailed analysis of the computational complexity of the algorithm, which may be a concern for large-scale datasets.\n4. The authors do not provide a clear comparison with other state-of-the-art methods for structural estimation of partially observed linear non-Gaussian acyclic models.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the algorithm and discuss its scalability for large-scale datasets.\n2. The paper should provide a clearer comparison with other state-of-the-art methods for structural estimation of partially observed linear non-Gaussian acyclic models.\n3. The authors should provide more experimental results to demonstrate the robustness of the method to different types of noise and latent variables.\n4. The paper should provide a more detailed discussion of the assumptions made in the linear causal model and non-Gaussianity assumptions.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of causal discovery, and the authors have developed a novel and efficient method for structural estimation of partially observed linear non-Gaussian acyclic models. However, there are some limitations and suggestions for improvement that need to be addressed. With some revisions, the paper can be a strong candidate for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents a novel approach for structural estimation of partially observed linear non-Gaussian acyclic models. The authors consider a general scenario where measured and latent variables collectively form a partially observed causally sufficient linear system. The paper theoretically shows that with the aid of high-order statistics, the causal graph is (almost) fully identifiable if each latent set has a sufficient number of pure children. A principled algorithm is developed to identify the causal graph by testing for statistical independence involving only measured variables in specific manners. The authors demonstrate the effectiveness of their method on both synthetic and real-world datasets.\n\nStrengths:\nThe paper makes several significant contributions. Firstly, it considers a broad spectrum of causal structures, regardless of the presence and positions of latent variables. The authors establish their theoretical identifiability under milder graphical conditions. The paper presents a feasible iterative algorithm to identify the causal graph, which leverages the Generalized Independent Noise (GIN) condition in specific manners. The authors demonstrate the effectiveness of their method on both synthetic and real-world datasets. The paper also includes extensive experimental results, which demonstrate the competitiveness of their approach.\n\nWeaknesses:\nOne of the major weaknesses of the paper is that it relies on several strong assumptions. The authors assume that each latent variable has at least two pure variable children. The paper also relies on the presence of high-order statistics in the data. The authors assume that the noise terms are continuous random variables with non-Gaussian distribution. The paper does not provide a clear justification for these assumptions. Another weakness of the paper is that it does not provide a clear description of how to choose the sufficient number of pure children for each atomic unit. The authors mention that this is an open problem for future research.\n\nSuggestions for Improvement:\nTo improve the paper, the authors should provide a clear justification for their assumptions. They should also provide more details on how to choose the sufficient number of pure children for each atomic unit. The authors should also consider providing more experimental results on real-world datasets. The paper could also benefit from more discussions on the limitations of their approach. The authors should also consider providing more details on how their approach can be applied in practice.\n\nRecommendation:\nThe paper presents a novel and effective approach for structural estimation of partially observed linear non-Gaussian acyclic models. The authors demonstrate the competitiveness of their approach on both synthetic and real-world datasets. However, the paper relies on several strong assumptions, and the authors should provide more details on how to choose the sufficient number of pure children for each atomic unit. Overall, the paper is well-written, and the authors should consider publishing it in a top-tier conference in the field of causal discovery.",
        "gr_score_with": 0.4868804664723032,
        "gr_score_without": 0.36875,
        "output_without": {
            "num_tokens_scored": 320,
            "num_green_tokens": 118,
            "green_fraction": 0.36875,
            "z_score": -1.141088661469096,
            "p_value": 0.8730834853920632
        },
        "output_with": {
            "num_tokens_scored": 343,
            "num_green_tokens": 167,
            "green_fraction": 0.4868804664723032,
            "z_score": 3.284457020621668,
            "p_value": 0.000510895407331679
        }
    },
    {
        "title": "DISTRIBUTIONAL MINIMAX PROBLEMS",
        "abstract": "In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose mean-field Langevin averaged gradient (MFL-AG), a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging, and establish average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Furthermore, we propose mean-field Langevin anchored best response (MFL-ABR), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. Finally, we study applications to zero-sum Markov games and conduct simulations demonstrating long-term optimality.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes two novel mean-field Langevin dynamics algorithms, namely Mean-Field Langevin Averaged Gradient (MFL-AG) and Mean-Field Langevin Anchored Best Response (MFL-ABR), for entropy-regularized minimax problems. The authors extend the mean-field Langevin dynamics to the minimax setting, which is a crucial step towards understanding the behavior of multiple learning agents. The proposed algorithms have several strengths, including global convergence guarantees, uniform-in-time analysis of propagation of chaos, and efficient computation. The authors also provide numerical experiments to demonstrate the effectiveness of the proposed algorithms.\n\nStrengths:\n\n1. The paper proposes novel mean-field Langevin dynamics algorithms for entropy-regularized minimax problems, which is a significant contribution to the field.\n2. The authors provide a thorough analysis of the proposed algorithms, including average-iterate convergence, uniform-in-time analysis of propagation of chaos, and last-iterate convergence.\n3. The paper provides a clear and concise presentation of the proposed algorithms and their analysis, making it easy to follow for readers.\n4. The authors provide numerical experiments to demonstrate the effectiveness of the proposed algorithms, which is essential for verifying the theoretical results.\n\nWeaknesses:\n\n1. The paper assumes that the functional derivatives of the objective function are uniformly bounded, which may not be the case in practice.\n2. The authors assume that the regularizers are strongly convex, which may not be the case in practice.\n3. The paper assumes that the objective function is convex-concave, which may not be the case in practice.\n4. The authors do not provide a detailed comparison with existing algorithms, which would be helpful for readers to understand the strengths and weaknesses of the proposed algorithms.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the assumptions made in the paper, including the uniform boundedness of the functional derivatives and the strong convexity of the regularizers.\n2. The authors should provide a more detailed comparison with existing algorithms, including a discussion of the strengths and weaknesses of the proposed algorithms.\n3. The authors should provide more numerical experiments to demonstrate the effectiveness of the proposed algorithms, including a comparison with existing algorithms.\n4. The authors should provide a more detailed discussion of the applications of the proposed algorithms, including multi-agent reinforcement learning.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough analysis of the proposed algorithms. The authors make a significant contribution to the field by proposing novel mean-field Langevin dynamics algorithms for entropy-regularized minimax problems. However, the paper assumes several strong conditions, which may not be the case in practice. The authors should provide a more detailed analysis of the assumptions made in the paper and provide more numerical experiments to demonstrate the effectiveness of the proposed algorithms. With these improvements, the paper would be a strong candidate for publication in a top-tier conference.\n\nOverall, I would recommend this paper for publication in a top-tier conference with some revisions.\n\nRating: 8/10\n\nReviewer's Expertise: The reviewer has expertise in the field of machine learning and optimization, with a focus on mean-field dynamics and multi-agent reinforcement learning. The reviewer has read and reviewed several papers on these topics and has a deep understanding of the strengths and weaknesses of the proposed algorithms.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents two symmetric mean-field Langevin dynamics algorithms, MFL-AG and MFL-ABR, for entropy-regularized minimax problems. MFL-AG is a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging, and MFL-ABR is a symmetric double-loop algorithm based on best response dynamics. The authors establish average-iterate convergence of MFL-AG and linear convergence of the outer loop of MFL-ABR. They also propose a two-step iterative scheme for finding the regularized Markov perfect equilibrium in zero-sum Markov games. The authors demonstrate the effectiveness of their algorithms through numerical experiments.\n\nStrengths:\n\n* The paper presents a new symmetric mean-field Langevin dynamics algorithm, MFL-AG, which achieves average-iterate convergence to the mixed Nash equilibrium.\n* The authors establish a new uniform-in-time analysis of propagation of chaos that accounts for dependence on history using novel perturbative techniques.\n* The paper proposes a double-loop algorithm, MFL-ABR, which achieves linear convergence of the outer loop.\n* The authors demonstrate the effectiveness of their algorithms through numerical experiments.\n\nWeaknesses:\n\n* The authors assume that the functional derivatives of the objective function are uniformly bounded, which may not be the case in practice.\n* The paper only presents a single-loop algorithm, MFL-AG, and a double-loop algorithm, MFL-ABR, and does not explore other possible algorithmic structures.\n* The authors do not provide a detailed comparison of the computational efficiency of their algorithms with other existing methods.\n\nSuggestions for Improvement:\n\n* Relax the assumption of uniformly bounded functional derivatives and explore other conditions under which the algorithms converge.\n* Investigate other possible algorithmic structures, such as multi-loop or hierarchical algorithms.\n* Provide a detailed comparison of the computational efficiency of MFL-AG and MFL-ABR with other existing methods.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of mean-field Langevin dynamics by introducing symmetric algorithms for entropy-regularized minimax problems. The authors demonstrate the effectiveness of their algorithms through numerical experiments. However, further work is needed to relax the assumption of uniformly bounded functional derivatives and to explore other possible algorithmic structures. With minor modifications, this paper has the potential to be published in a top-tier conference.",
        "gr_score_with": 0.41496598639455784,
        "gr_score_without": 0.32602739726027397,
        "output_without": {
            "num_tokens_scored": 365,
            "num_green_tokens": 119,
            "green_fraction": 0.32602739726027397,
            "z_score": -2.8847734414021455,
            "p_value": 0.9980415228329028
        },
        "output_with": {
            "num_tokens_scored": 294,
            "num_green_tokens": 122,
            "green_fraction": 0.41496598639455784,
            "z_score": 0.5238095238095227,
            "p_value": 0.3002055143074528
        }
    },
    {
        "title": "TOWARDS ROBUST OUT-OF-DISTRIBUTION GENERALIZATION BOUNDS VIA SHARPNESS",
        "abstract": "Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by \"robustness\" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for \"flat minima leads to better OOD generalization\". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to understanding Out-of-Distribution (OOD) generalization by leveraging the concept of robustness. The authors introduce a new framework for OOD generalization bounds that considers algorithmic robustness, which is shown to be a key factor in determining the model's ability to generalize to unseen data. The paper establishes a rigorous connection between robustness and sharpness, a geometric property of the loss landscape, and uses this connection to derive a sharpness-based OOD generalization bound. The authors also provide empirical evidence to support their findings, demonstrating that flat minima lead to better OOD generalization and that the proposed bound is tighter than existing non-robust guarantees.\n\nStrengths:\n\n* The paper provides a comprehensive overview of the existing literature on OOD generalization and identifies the need for a more robust approach to addressing this problem.\n* The authors propose a novel framework for OOD generalization bounds that considers algorithmic robustness, which is a key factor in determining the model's ability to generalize to unseen data.\n* The paper establishes a rigorous connection between robustness and sharpness, which provides a deeper understanding of the OOD generalization problem.\n* The authors provide empirical evidence to support their findings, demonstrating that flat minima lead to better OOD generalization and that the proposed bound is tighter than existing non-robust guarantees.\n\nWeaknesses:\n\n* The paper assumes a specific type of neural network architecture, which may limit the applicability of the proposed framework to other types of models.\n* The authors use a simplified definition of robustness, which may not capture the full complexity of the concept.\n* The paper does not provide a clear explanation of how the proposed framework can be extended to more complex scenarios, such as multi-task learning or transfer learning.\n* The authors rely heavily on theoretical results, but the empirical evidence provided is limited to a few experiments, which may not be sufficient to fully validate the proposed framework.\n\nSuggestions for Improvement:\n\n* The authors should consider extending the proposed framework to more complex scenarios, such as multi-task learning or transfer learning.\n* The paper should provide a more detailed explanation of how the proposed framework can be applied to different types of neural network architectures.\n* The authors should provide more empirical evidence to support their findings, including experiments with different types of models and datasets.\n* The paper should provide a clearer explanation of the limitations of the proposed framework and how they can be addressed.\n\nRecommendation:\n\nThe paper provides a novel and comprehensive approach to understanding OOD generalization, and the proposed framework has the potential to improve the robustness of machine learning models. However, the paper assumes a specific type of neural network architecture, and the empirical evidence provided is limited. Therefore, I recommend that the authors extend the proposed framework to more complex scenarios and provide more empirical evidence to support their findings.\n\nOverall, I believe that the paper makes a significant contribution to the field of OOD generalization and provides a novel and comprehensive approach to addressing this problem. However, the paper requires further development and experimentation to fully validate the proposed framework.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to understanding Out-of-Distribution (OOD) generalization by leveraging the concept of robustness. The authors introduce a robust OOD generalization bound that effectively captures the algorithmic robustness in the presence of shifting data distributions. The bound is shown to be tighter than existing works, particularly in the overparameterized regime. Additionally, the paper establishes a connection between robustness and sharpness, a geometric property of the loss landscape, and demonstrates that sharpness has a negative impact on generalization.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the concept of OOD generalization and its importance in machine learning.\n2. The authors introduce a novel approach to understanding OOD generalization by leveraging robustness, which is a more interpretable and informative concept than existing methods.\n3. The paper establishes a connection between robustness and sharpness, a geometric property of the loss landscape, and demonstrates that sharpness has a negative impact on generalization.\n4. The experimental results demonstrate the effectiveness of the proposed approach in capturing OOD generalization.\n\nWeaknesses:\n\n1. The paper assumes that the loss function is convex, which is not always the case in practice.\n2. The authors assume that the Hessian matrix of the loss function is invertible, which is not always the case in practice.\n3. The experimental results are limited to a few specific datasets and tasks, and it is not clear whether the proposed approach generalizes to other tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide more details on how to relax the assumption of convexity in the loss function.\n2. The authors should provide more details on how to relax the assumption of invertibility in the Hessian matrix.\n3. The authors should provide more experimental results on a broader range of datasets and tasks to demonstrate the generalizability of the proposed approach.\n\nRecommendation:\n\nThe paper is well-written and provides a clear and concise introduction to the concept of OOD generalization. The authors introduce a novel approach to understanding OOD generalization by leveraging robustness, which is a more interpretable and informative concept than existing methods. The experimental results demonstrate the effectiveness of the proposed approach in capturing OOD generalization. However, the paper has some limitations, such as the assumption of convexity in the loss function and invertibility in the Hessian matrix. Therefore, I recommend that the authors provide more details on how to relax these assumptions and provide more experimental results on a broader range of datasets and tasks.\n\nOverall, I believe that the paper makes a significant contribution to the field of OOD generalization and is well-suited for publication in a top-tier machine learning conference.\n\n$$$$",
        "gr_score_with": 0.5134228187919463,
        "gr_score_without": 0.43454038997214484,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 156,
            "green_fraction": 0.43454038997214484,
            "z_score": 1.3358842821245729,
            "p_value": 0.09079354958357871
        },
        "output_with": {
            "num_tokens_scored": 298,
            "num_green_tokens": 153,
            "green_fraction": 0.5134228187919463,
            "z_score": 3.9967128551513973,
            "p_value": 3.211406524509568e-05
        }
    },
    {
        "title": "TEXTFIELD3D: TOWARDS ENHANCING OPEN- VOCABULARY 3D GENERATION WITH NOISY TEXT FIELDS",
        "abstract": "Generative models have shown remarkable progress in 3D aspect. Recent works learn 3D representation explicitly under text-3D guidance. However, limited text3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing openvocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D.Specifically, rather than using the text prompts as input directly, we suggest to inject dynamic noise into the latent space of given text prompts, i.e., Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the appropriate range of textual latent space that is expanded by NTFs. To this end, an NTFGen module is proposed to model general text latent code in noisy fields. Meanwhile, an NTFBind module is proposed to align viewinvariant image latent code to noisy fields, further supporting image-conditional 3D generation. To guide the conditional generation in both geometry and texture, multi-modal discrimination is constructed with a text-3D discriminator and a text2.5D discriminator. Compared to previous methods, TextField3D includes three merits: 1) large vocabulary, 2) text consistency, and 3) low latency. Extensive experiments demonstrate that our method achieves a potential open-vocabulary 3D generation capability.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel 3D generative model called TextField3D, which aims to enhance the open-vocabulary capability of 3D generative models. The model introduces Noisy Text Fields (NTFs) to boost the latent mapping between V-L concepts and 3D representations. The authors also propose multi-modal discrimination to enhance the supervision of 3D generation. The overall framework is designed to allow for efficient generation of various 3D contents with complicated text prompts.\n\nStrengths:\n\n1. The paper presents a novel approach to enhancing the open-vocabulary capability of 3D generative models.\n2. The authors propose a new module, NTFGen, which generates noisy text latent code to enhance the mapping of V-L pre-trained knowledge and 3D training data.\n3. The paper introduces a multi-modal discrimination framework that includes both text-3D and text-2.5D discriminators to supervise the generation quality and text consistency.\n4. The authors provide extensive experimental results to demonstrate the effectiveness of their proposed method.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained models, which may not be available for all users.\n2. The authors assume that the scale of 3D data is steadily increasing, which may not be the case in reality.\n3. The paper does not provide a detailed analysis of the limitations of their proposed method, such as its dependence on the vocabulary of training data.\n4. The authors do not provide a clear explanation of how their proposed method can be used in real-world applications.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the limitations of their proposed method and how they plan to address them.\n2. The paper should include a more detailed discussion of the potential applications of TextField3D and how it can be used in real-world scenarios.\n3. The authors should provide a more comprehensive evaluation of their proposed method, including a comparison with other state-of-the-art methods.\n4. The paper should include a more detailed explanation of the NTFBind module and how it is used in the proposed method.\n\nRecommendation:\n\nThe paper presents a novel approach to enhancing the open-vocabulary capability of 3D generative models. While it has some limitations, the proposed method shows promising results and has the potential to be used in real-world applications. I recommend that the authors address the weaknesses and suggestions for improvement mentioned above and provide a more comprehensive evaluation of their proposed method. With further refinement, TextField3D has the potential to become a state-of-the-art method for 3D generative modeling.\n\nScore: 7.5/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper introduces TextField3D, a conditional 3D generative model that tackles the limitation of text-3D generation with noisy text fields (NTFs). NTFs are designed to inject dynamic noise into the latent space of given text prompts, allowing for a larger vocabulary and text consistency in 3D generation. The model consists of an NTFGen module to generate noisy text latent codes, an NTFBind module to bind view-invariant image latent codes to NTFs, and multi-modal discrimination to guide the generation quality and text consistency.\n\nStrengths:\n1. The proposed approach addresses the limitation of text-3D generation with NTFs, which can handle noisy text prompts and generate open-vocabulary 3D objects.\n2. The NTFGen module is designed to generate noisy text latent codes, which can improve text control consistency and generation quality.\n3. The NTFBind module is introduced to bind view-invariant image latent codes to NTFs, which can improve the generation consistency of novel views.\n4. The multi-modal discrimination approach is used to guide the generation quality and text consistency, which can improve the overall performance of the model.\n\nWeaknesses:\n1. The model still depends on the vocabulary of training data, which may not fully match the general capability of V-L supervised methods.\n2. The NTFGen module may not be effective in generating noisy text latent codes for certain text prompts, which can limit the performance of the model.\n3. The NTFBind module may not be able to handle complex text prompts, which can limit the performance of the model.\n4. The multi-modal discrimination approach may not be effective in guiding the generation quality and text consistency for certain text prompts.\n\nSuggestions for Improvement:\n1. Investigate the effect of different noise levels and distributions on the performance of NTFGen module.\n2. Explore the use of other text embedding models, such as BERT or RoBERTa, to improve the performance of NTFGen module.\n3. Investigate the effect of different hyperparameters, such as \u03bbpc and \u03bbgen, on the performance of the model.\n4. Explore the use of other evaluation metrics, such as Inception Score or Fr\u00e9chet Inception Distance, to evaluate the performance of the model.\n\nRecommendation:\nThe paper presents a novel approach to text-3D generation with NTFs, which can handle noisy text prompts and generate open-vocabulary 3D objects. However, there are some limitations to the model, and further investigation is needed to improve its performance. With some modifications and improvements, the model can be a strong contender in the field of text-3D generation.\n\n$$$",
        "gr_score_with": 0.5428571428571428,
        "gr_score_without": 0.39710144927536234,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 137,
            "green_fraction": 0.39710144927536234,
            "z_score": -0.1098967455659645,
            "p_value": 0.5437543682131186
        },
        "output_with": {
            "num_tokens_scored": 315,
            "num_green_tokens": 171,
            "green_fraction": 0.5428571428571428,
            "z_score": 5.175491695067657,
            "p_value": 1.1365577121900409e-07
        }
    },
    {
        "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
        "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The model, called the recurrent circuit model, uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with a modulated global signal to sample from the prior at a rate prescribed by the period of the global oscillator. The model can flexibly use sensory and contextual sources of information to encode task-specific posteriors. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n\nStrengths:\n\n1. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions.\n2. The model is flexible and can be used to encode task-specific posteriors using sensory and contextual sources of information.\n3. The authors demonstrate that the model can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n4. The paper provides a thorough analysis of the model's performance and compares it to traditional diffusion models.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the dendritic tree, which may not be biologically realistic.\n2. The model's performance is evaluated on a limited set of tasks and distributions, and it is unclear how well it generalizes to other scenarios.\n3. The paper does not provide a clear explanation of how the model's parameters are learned, which is a critical component of any machine learning model.\n4. The paper's conclusion is somewhat overstated, as the model's performance is not significantly better than traditional diffusion models in all cases.\n\nSuggestions for Improvement:\n\n1. Consider exploring more biologically realistic architectures for the dendritic tree, such as those with variable branching factors or depths.\n2. Evaluate the model's performance on a wider range of tasks and distributions to assess its generalizability.\n3. Provide a more detailed explanation of how the model's parameters are learned, including any optimization algorithms or techniques used.\n4. Be more cautious in the paper's conclusion, as the model's performance is not uniformly better than traditional diffusion models.\n\nRecommendation:\n\nI recommend accepting this paper for publication, subject to the above suggestions for improvement. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions, and the authors demonstrate its flexibility and performance on a range of tasks. However, the paper's assumptions and limitations should be carefully considered and addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit implementation of across-task inference, where a common prior is encoded in a recurrent neural circuit with dendritic nonlinearities optimized for denoising. The stochastic dynamics of this circuit provide samples from the prior at the peak of an ongoing local oscillation, while additional inputs (carrying sensory or contextual information) switch the dynamics to sampling from the corresponding posterior. This solution offers a first circuit model for reusing priors across tasks, as seen behaviorally, and has measurable neural signatures based on neural population recordings in animals.\n\nStrengths:\n\n1.  The paper provides a novel and biologically-inspired approach to probabilistic inference, which is a crucial aspect of cognitive and motor functions.\n2.  The authors propose a modular architecture that allows for flexible inference, reusing the same prior information across multiple tasks.\n3.  The paper provides a thorough analysis of the circuit implementation, including the effects of dendritic tree morphology on learning speed and the ability to sample from complex distributions.\n4.  The authors provide a detailed mathematical framework for the circuit implementation, including the derivation of posterior sampling dynamics.\n\nWeaknesses:\n\n1.  The paper relies on simplifying assumptions, such as the use of a sinusoidal oscillation and a Gaussian likelihood, which may not accurately capture the complexity of real-world data.\n2.  The authors do not provide a comprehensive evaluation of the circuit implementation, including experiments on more complex tasks and datasets.\n3.  The paper relies on simulations and does not provide empirical evidence from neural population recordings in animals.\n4.  The authors do not discuss the computational complexity of the circuit implementation, including the number of parameters and the computational requirements.\n\nSuggestions for Improvement:\n\n1.  Provide a more comprehensive evaluation of the circuit implementation, including experiments on more complex tasks and datasets.\n2.  Discuss the computational complexity of the circuit implementation and provide estimates of the number of parameters and computational requirements.\n3.  Provide empirical evidence from neural population recordings in animals to support the circuit implementation.\n4.  Relax the simplifying assumptions and provide a more accurate mathematical framework for the circuit implementation.\n\nRecommendation:\n\nThe paper provides a novel and biologically-inspired approach to probabilistic inference, which has the potential to improve our understanding of cognitive and motor functions. However, the paper relies on simplifying assumptions and does not provide a comprehensive evaluation of the circuit implementation. Therefore, I recommend that the authors provide more empirical evidence and relax the simplifying assumptions to improve the validity and generality of the circuit implementation.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.4782608695652174,
        "gr_score_without": 0.39285714285714285,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 132,
            "green_fraction": 0.39285714285714285,
            "z_score": -0.267261241912425,
            "p_value": 0.6053659869328596
        },
        "output_with": {
            "num_tokens_scored": 345,
            "num_green_tokens": 165,
            "green_fraction": 0.4782608695652174,
            "z_score": 2.9672121302810415,
            "p_value": 0.0015025677218210556
        }
    },
    {
        "title": "A LIE GROUP APPROACH TO RIEMANNIAN BATCH NORMALIZATION",
        "abstract": "Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization layers induced by these Lie groups are then proposed for SPD neural networks. We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography (EEG) classification. The code is available at https://github.com/GitZH-Chen/LieBN.git.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. The proposed framework, LieBN, can effectively normalize both the sample and population statistics. The authors generalize the existing Lie groups on SPD manifolds and showcase their framework on the parameterized Lie groups of SPD manifolds. The effectiveness of LieBN is demonstrated through extensive experiments on radar recognition, human action recognition, and EEG classification.\n\nStrengths:\n\n1. The paper proposes a novel framework for batch normalization over Lie groups, which is a significant contribution to the field of machine learning.\n2. The authors provide a thorough theoretical analysis of the proposed framework, including the properties of the Gaussian distribution on Lie groups and the derivation of the normalization operations.\n3. The paper demonstrates the effectiveness of LieBN through extensive experiments on various applications, including radar recognition, human action recognition, and EEG classification.\n4. The authors provide a clear and concise explanation of the proposed framework and its applications.\n\nWeaknesses:\n\n1. The paper assumes that the underlying manifold is a Lie group, which may not be the case in all applications.\n2. The authors do not provide a comprehensive comparison with existing Riemannian normalization methods, which may limit the generalizability of the proposed framework.\n3. The paper assumes that the weights of the neural network are convex, which may not be the case in all applications.\n4. The authors do not provide a detailed analysis of the computational complexity of the proposed framework.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive comparison with existing Riemannian normalization methods to demonstrate the effectiveness of the proposed framework.\n2. The paper should be modified to handle non-convex weights in the neural network.\n3. The authors should provide a detailed analysis of the computational complexity of the proposed framework to demonstrate its efficiency.\n4. The paper should be modified to handle non-Lie group manifolds.\n\nRecommendation:\n\nThe paper proposes a novel framework for batch normalization over Lie groups, which is a significant contribution to the field of machine learning. However, the paper assumes that the underlying manifold is a Lie group, which may not be the case in all applications. Therefore, the authors should provide a more comprehensive comparison with existing Riemannian normalization methods and modify the paper to handle non-convex weights and non-Lie group manifolds. With these modifications, the paper will be more generalizable and effective in various applications.\n\nOverall, the paper is well-written and provides a clear explanation of the proposed framework and its applications. However, the paper requires further modifications to make it more comprehensive and effective.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called LieBN, which enables batch normalization over Lie groups. The proposed framework is based on a generalization of the concept of batch normalization to manifold-valued measurements, specifically to Lie groups. The authors provide a theoretical framework for controlling both the sample and population statistics of a distribution over a Lie group. They also demonstrate the effectiveness of their framework through extensive experiments on SPD neural networks, comparing their results with previous state-of-the-art methods. The proposed framework is shown to outperform previous methods in terms of accuracy and efficiency.\n\nStrengths:\n\n* The paper provides a comprehensive review of previous works on Riemannian batch normalization and highlights the limitations of previous methods.\n* The authors provide a novel framework for batch normalization over Lie groups, which is a significant contribution to the field of machine learning.\n* The proposed framework is shown to outperform previous methods in terms of accuracy and efficiency.\n* The authors provide extensive experimental results, including comparisons with previous state-of-the-art methods.\n\nWeaknesses:\n\n* The paper assumes that the distribution over the Lie group is a Gaussian distribution, which may not always be the case in practice.\n* The authors do not provide a detailed analysis of the computational complexity of their proposed framework.\n* The paper assumes that the left-invariant metric is used, but it is not clear why this is a reasonable assumption in practice.\n* The authors do not provide a detailed discussion of the theoretical guarantees of their proposed framework.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of their proposed framework.\n* The authors should provide a more detailed discussion of the theoretical guarantees of their proposed framework.\n* The authors should provide a discussion of the assumptions made in the paper, such as the Gaussian distribution assumption.\n* The authors should provide a discussion of the potential applications of their proposed framework beyond SPD neural networks.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of machine learning, providing a novel framework for batch normalization over Lie groups. The proposed framework is shown to outperform previous methods in terms of accuracy and efficiency. However, the paper assumes that the distribution over the Lie group is a Gaussian distribution, which may not always be the case in practice. Additionally, the authors do not provide a detailed analysis of the computational complexity of their proposed framework. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of their proposed framework and a more detailed discussion of the theoretical guarantees of their proposed framework.\n\n$$$",
        "gr_score_with": 0.44866920152091255,
        "gr_score_without": 0.34782608695652173,
        "output_without": {
            "num_tokens_scored": 299,
            "num_green_tokens": 104,
            "green_fraction": 0.34782608695652173,
            "z_score": -1.841549442134555,
            "p_value": 0.9672294589234645
        },
        "output_with": {
            "num_tokens_scored": 263,
            "num_green_tokens": 118,
            "green_fraction": 0.44866920152091255,
            "z_score": 1.6111147531389567,
            "p_value": 0.05357735668337619
        }
    },
    {
        "title": "MAGNITUDE INVARIANT PARAMETRIZATIONS IMPROVE HYPERNETWORK LEARNING",
        "abstract": "Hypernetworks, neural networks that predict the parameters of another neural network, are powerful models that have been successfully used in diverse applications from image generation to multi-task learning. Unfortunately, existing hypernetworks are often challenging to train. Training typically converges far more slowly than for non-hypernetwork models, and the rate of convergence can be very sensitive to hyperparameter choices. In this work, we identify a fundamental and previously unidentified problem that contributes to the challenge of training hypernetworks: a magnitude proportionality between the inputs and outputs of the hypernetwork. We demonstrate both analytically and empirically that this can lead to unstable optimization, thereby slowing down convergence, and sometimes even preventing any learning. We present a simple solution to this problem using a revised hypernetwork formulation that we call Magnitude Invariant Parametrizations (MIP). We demonstrate the proposed solution on several hypernetwork tasks, where it consistently stabilizes training and achieves faster convergence. Furthermore, we perform a comprehensive ablation study including choices of activation function, normalization strategies, input dimensionality, and hypernetwork architecture; and find that MIP improves training in all scenarios. We also provide easy-to-use code that can turn existing networks into MIP-based hypernetworks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel solution to address the training instability issue in hypernetworks, which are neural networks that predict the parameters of another neural network. The authors identify a previously unstudied hypernetwork design problem, known as the \"magnitude proportionality problem,\" where the scale of hypernetwork inputs affects the scale of predicted parameters, leading to unstable training and slow convergence. They propose a revised hypernetwork formulation, called Magnitude Invariant Parametrizations (MIP), which effectively eliminates the influence of the scale of hypernetwork inputs on the scale of predicted parameters. The authors demonstrate the effectiveness of MIP through extensive experiments on several hypernetwork tasks, including Bayesian neural networks, Hypermorph, and Scale-Space hypernetworks.\n\nStrengths:\n\n1. The authors provide a clear and concise explanation of the magnitude proportionality problem and its impact on hypernetwork training.\n2. The proposed MIP solution is simple and elegant, and its effectiveness is demonstrated through extensive experiments.\n3. The authors provide a comprehensive analysis of the MIP solution, including its benefits and limitations.\n4. The paper is well-organized and easy to follow, with clear sections and subsections.\n\nWeaknesses:\n\n1. The paper assumes a specific hypernetwork architecture, which may not be applicable to all hypernetwork tasks.\n2. The authors focus on a limited set of hypernetwork tasks and architectures, and it is unclear whether the MIP solution will generalize to other tasks and architectures.\n3. The paper does not provide a detailed comparison with other normalization strategies, which may be relevant for hypernetwork training.\n4. The authors do not provide a clear explanation of why MIP is more effective than other normalization strategies.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other normalization strategies, including batch and layer normalization.\n2. The paper should include more experiments on different hypernetwork architectures and tasks to demonstrate the generalizability of the MIP solution.\n3. The authors should provide a more detailed explanation of why MIP is more effective than other normalization strategies.\n4. The paper should include a discussion of the limitations of the MIP solution and potential future work.\n\nRecommendation:\n\nThe paper is well-written and provides a clear and concise explanation of the magnitude proportionality problem and its impact on hypernetwork training. The proposed MIP solution is simple and effective, and its benefits are demonstrated through extensive experiments. However, the paper assumes a specific hypernetwork architecture and focuses on a limited set of tasks and architectures. To improve the paper, the authors should provide a more detailed comparison with other normalization strategies and include more experiments on different hypernetwork architectures and tasks.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper addresses the training instability issue in hypernetworks, which are neural networks that predict the parameters of another neural network. The authors identify a fundamental problem that contributes to this challenge: a magnitude proportionality between the inputs and outputs of the hypernetwork. This proportionality leads to large gradient magnitude fluctuations, resulting in unstable training dynamics and slow convergence. To overcome this issue, the authors propose a revised hypernetwork formulation called Magnitude Invariant Parametrizations (MIP). They demonstrate the effectiveness of MIP on several hypernetwork tasks, showing that it stabilizes training and achieves faster convergence compared to the standard hypernetwork formulation.\n\nStrengths:\n\n1. The authors provide a thorough analysis of the proportionality issue in hypernetworks, which is a fundamental problem that has not been identified before.\n2. The proposed solution, MIP, is simple and easy to implement, and it does not introduce additional training or inference costs.\n3. The authors provide extensive experimental results that demonstrate the effectiveness of MIP on various hypernetwork tasks and architectures.\n4. The paper provides a comprehensive review of related work on hypernetworks, parameter initialization, normalization techniques, and adaptive optimization.\n\nWeaknesses:\n\n1. The paper assumes that biases are initialized to zero, which only holds at initialization. However, the authors mention that biases remain near zero during early training, which might not be the case for all models.\n2. The paper only considers fully connected layers and does not explore other architectures, such as convolutional layers.\n3. The authors only evaluate their method on a limited set of tasks and architectures, which might not be representative of all possible scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide more details on how to extend their method to other architectures, such as convolutional layers.\n2. The paper should provide more information on the robustness of MIP to different initialization schemes and hyperparameter settings.\n3. The authors should provide a more detailed analysis of the proportionality issue and how it affects training dynamics.\n\nRecommendation:\n\nThis paper provides a valuable contribution to the field of hypernetworks, and the proposed solution, MIP, is a significant improvement over the standard hypernetwork formulation. The authors provide extensive experimental results that demonstrate the effectiveness of MIP on various tasks and architectures. However, the paper has some limitations, and the authors should provide more details on how to extend their method to other architectures and how to improve its robustness. Overall, I recommend this paper for publication in a top-tier conference on machine learning.\n\nRating: 8/10\n\nReview:\n\nThis paper provides a thorough analysis of the proportionality issue in hypernetworks and proposes a simple and effective solution, MIP. The authors provide extensive experimental results that demonstrate the effectiveness of MIP on various tasks and architectures. However, the paper has some limitations, and the authors should provide more details on how to extend their method to other architectures and how to improve its robustness. Overall, I recommend this paper for publication in a top-tier conference on machine learning.\n\nNote:\n\nThis review is based on the provided paper and does not take into account any external information.",
        "gr_score_with": 0.45758354755784064,
        "gr_score_without": 0.34441087613293053,
        "output_without": {
            "num_tokens_scored": 331,
            "num_green_tokens": 114,
            "green_fraction": 0.34441087613293053,
            "z_score": -2.064420700779922,
            "p_value": 0.9805110772874559
        },
        "output_with": {
            "num_tokens_scored": 389,
            "num_green_tokens": 178,
            "green_fraction": 0.45758354755784064,
            "z_score": 2.3182891188831,
            "p_value": 0.010216806691008795
        }
    },
    {
        "title": "OUT-OF-DOMAIN KNOWLEDGE DISTILLATION",
        "abstract": "Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers\u2019 training data and real-world scenarios (student domain). The degradation stems from the portions of teachers\u2019 knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students\u2019 performance. Hence, selectively transferring teachers\u2019 appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off the learning process between OOD knowledge distillation and domain-specific information learning via mixup learning. Extensive experiments in 3 datasets and 8 settings demonstrate the stability and superiority of our approach. Code available at https://github.com/IshiKura-a/AuG-KD",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, AuG-KD, for Out-of-Domain Knowledge Distillation (OOD-KD). OOD-KD is a challenging problem where the teacher model is trained on a different domain than the student model, and the training data of the teacher model is not accessible. The proposed method, AuG-KD, uses a data-driven anchor to align the student-domain data with the teacher domain and leverages a generative method to progressively evolve the learning process from OOD knowledge distillation to domain-specific information learning. The method consists of three modules: Data-Free Learning Module, Anchor Learning Module, and Mixup Learning Module. Extensive experiments on three datasets and eight settings demonstrate the stability and superiority of the proposed method.\n\nStrengths:\n\n1. The paper addresses a novel and challenging problem, Out-of-Domain Knowledge Distillation, which is of great practical importance.\n2. The proposed method, AuG-KD, is simple yet effective and leverages a data-driven anchor to align the student-domain data with the teacher domain.\n3. The method is able to progressively evolve the learning process from OOD knowledge distillation to domain-specific information learning, which is a significant improvement over existing methods.\n4. The experimental results demonstrate the stability and superiority of the proposed method over existing baselines.\n\nWeaknesses:\n\n1. The paper assumes that the teacher model is trained on a different domain than the student model, which may not be the case in all scenarios.\n2. The proposed method relies on the availability of a large amount of data in the student domain, which may not be feasible in all cases.\n3. The paper does not provide a thorough analysis of the hyperparameters and their impact on the performance of the proposed method.\n4. The paper does not provide a comparison with other state-of-the-art methods for OOD-KD.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the hyperparameters and their impact on the performance of the proposed method.\n2. Compare the proposed method with other state-of-the-art methods for OOD-KD.\n3. Provide a more thorough discussion of the assumptions made in the paper and their limitations.\n4. Provide more experimental results and analysis to demonstrate the robustness and generalizability of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel and effective method for Out-of-Domain Knowledge Distillation, which has the potential to improve the performance of neural networks in various applications. However, the paper assumes that the teacher model is trained on a different domain than the student model, which may not be the case in all scenarios. Additionally, the paper relies on the availability of a large amount of data in the student domain, which may not be feasible in all cases. Therefore, further research is needed to address these limitations and to make the proposed method more robust and generalizable.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to address the problem of Out-of-Domain Knowledge Distillation (OOD-KD), which involves transferring knowledge from a teacher model to a student model in a scenario where the teacher's training data is not available. The authors introduce a simple yet effective method called AuG-KD, which utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively evolve the learning process from OOD knowledge distillation to domain-specific information learning. The proposed method consists of three modules: Data-Free Learning Module, Anchor Learning Module, and Mixup Learning Module. Extensive experiments on three datasets and eight settings demonstrate the stability and superiority of the proposed approach.\n\nStrengths:\n\n* The paper addresses a timely and important problem in knowledge distillation, which is Out-of-Domain Knowledge Distillation.\n* The proposed method, AuG-KD, is simple yet effective and can be easily integrated into existing knowledge distillation frameworks.\n* The authors conduct extensive experiments to demonstrate the stability and superiority of the proposed approach.\n* The paper provides a comprehensive overview of the related work and clearly explains the contributions of the proposed approach.\n\nWeaknesses:\n\n* The paper assumes that the teacher model is available, which may not be the case in many real-world scenarios.\n* The proposed method relies heavily on the assumption that the teacher's knowledge can be transferred to the student model, which may not hold in all cases.\n* The paper does not provide a clear explanation of how the uncertainty metric U(x;T ) is computed and how it affects the learning process.\n* The authors do not provide a detailed analysis of the computational cost and memory requirements of the proposed approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of how the uncertainty metric U(x;T ) is computed and how it affects the learning process.\n* The authors should conduct additional experiments to evaluate the robustness of the proposed approach to different types of domain shifts and to different types of teacher-student pairs.\n* The authors should provide a clear explanation of how the proposed approach can be extended to handle multiple teacher models and multiple student models.\n* The authors should provide a more detailed analysis of the computational cost and memory requirements of the proposed approach and discuss ways to reduce them.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to address the problem of Out-of-Domain Knowledge Distillation. The extensive experiments and comprehensive overview of the related work make the paper a strong candidate for publication in a top-tier conference. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more robust and complete. With some additional experimentation and analysis, the paper has the potential to make a significant impact in the field of knowledge distillation.",
        "gr_score_with": 0.49868766404199477,
        "gr_score_without": 0.4351851851851852,
        "output_without": {
            "num_tokens_scored": 324,
            "num_green_tokens": 141,
            "green_fraction": 0.4351851851851852,
            "z_score": 1.2927862531355667,
            "p_value": 0.09804249810130145
        },
        "output_with": {
            "num_tokens_scored": 381,
            "num_green_tokens": 190,
            "green_fraction": 0.49868766404199477,
            "z_score": 3.9320563788657927,
            "p_value": 4.211115311505691e-05
        }
    },
    {
        "title": "TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS",
        "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16, 464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench. The codes, trained models, and demo are publicly available at https://github.com/OpenBMB/ToolBench.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents ToolLLM, a general tool-use framework for large language models (LLMs) that enables them to master 16,000+ real-world APIs. The framework consists of three main components: ToolBench, a high-quality instruction-tuning dataset for tool use; DFSDT, a depth-first search-based decision tree algorithm for planning and reasoning; and ToolEval, an automatic evaluator for assessing the tool-use capabilities of LLMs. The authors fine-tune LLaMA on ToolBench and obtain ToolLLaMA, which demonstrates remarkable generalization ability to unseen APIs and exhibits comparable performance to ChatGPT.\n\nStrengths:\n\n1. The paper presents a comprehensive framework for tool use in LLMs, which is a crucial aspect of their capabilities.\n2. The authors have developed a high-quality instruction-tuning dataset, ToolBench, which is constructed automatically using ChatGPT and covers a wide range of real-world APIs.\n3. The DFSDT algorithm is a novel and effective approach for planning and reasoning in LLMs, which enables them to navigate through reasoning paths strategically.\n4. The authors have also developed an automatic evaluator, ToolEval, which provides a robust, scalable, and reliable assessment for machine tool use.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs have access to the ToolBench dataset, which may not be the case in real-world scenarios.\n2. The authors have not explored the limitations of the ToolBench dataset and how it may impact the performance of ToolLLaMA.\n3. The paper does not provide a thorough analysis of the error propagation and limited exploration issues in CoT and ReACT, which are the existing approaches for planning and reasoning in LLMs.\n4. The authors have not compared the performance of ToolLLaMA with other state-of-the-art LLMs on various tasks and datasets.\n\nSuggestions for Improvement:\n\n1. The authors should investigate the limitations of the ToolBench dataset and explore ways to improve its quality and diversity.\n2. They should also explore the use of other LLMs and architectures to see if they can achieve similar results to ToolLLaMA.\n3. The authors should provide a more thorough analysis of the error propagation and limited exploration issues in CoT and ReACT and compare the performance of DFSDT with these approaches.\n4. They should also investigate the generalizability of ToolLLaMA to other domains and tasks.\n\nRecommendation:\n\nThe paper presents a comprehensive framework for tool use in LLMs, which is a crucial aspect of their capabilities. The authors have developed a high-quality instruction-tuning dataset, ToolBench, and a novel and effective approach for planning and reasoning, DFSDT. The paper also presents an automatic evaluator, ToolEval, which provides a robust, scalable, and reliable assessment for machine tool use. However, the paper assumes that the LLMs have access to the ToolBench dataset, which may not be the case in real-world scenarios. Therefore, the authors should investigate the limitations of the ToolBench dataset and explore ways to improve its quality and diversity. Overall, the paper is well-written and presents a significant contribution to the field of LLMs and tool use.\n\nRecommendation: Accept.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper \"ToolLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS\" proposes a novel framework, called ToolLLM, to empower open-source large language models (LLMs) to master diverse APIs. The framework consists of three components: data construction, model training, and evaluation. The authors introduce ToolBench, a high-quality instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. They also develop a novel depth-first search-based decision tree algorithm, called DFSDT, to reinforce the planning and reasoning ability of LLMs. The authors demonstrate the effectiveness of their framework by fine-tuning LLaMA on ToolBench and achieving remarkable generalization ability to unseen APIs.\n\nStrengths:\n\n1. The paper proposes a novel framework, ToolLLM, that addresses the limitations of existing works in tool learning.\n2. The authors introduce ToolBench, a high-quality instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT.\n3. The paper develops a novel depth-first search-based decision tree algorithm, called DFSDT, to reinforce the planning and reasoning ability of LLMs.\n4. The authors demonstrate the effectiveness of their framework by fine-tuning LLaMA on ToolBench and achieving remarkable generalization ability to unseen APIs.\n5. The paper provides a thorough evaluation of the framework, including both in-distribution and out-of-distribution generalization experiments.\n\nWeaknesses:\n\n1. The paper relies heavily on the performance of ChatGPT, which may limit the applicability of the framework to other LLMs.\n2. The authors do not provide a detailed analysis of the limitations of the framework, such as scalability and robustness.\n3. The paper assumes that the APIs in the dataset are well-maintained and reliable, which may not always be the case in real-world scenarios.\n4. The authors do not provide a detailed comparison with existing works in tool learning, which makes it difficult to evaluate the novelty and significance of the framework.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the limitations of the framework, including scalability and robustness.\n2. Evaluate the framework on a larger and more diverse dataset to demonstrate its generalizability.\n3. Provide a more detailed comparison with existing works in tool learning to demonstrate the novelty and significance of the framework.\n4. Consider incorporating additional components, such as multi-task learning and transfer learning, to improve the robustness and generalizability of the framework.\n\nRecommendation:\n\nThe paper \"ToolLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS\" proposes a novel framework that has the potential to significantly improve the tool-use capabilities of LLMs. However, the framework relies heavily on the performance of ChatGPT, and the authors do not provide a detailed analysis of the limitations of the framework. Therefore, I recommend that the authors provide a more detailed evaluation of the framework, including both in-distribution and out-of-distribution generalization experiments, and provide a more detailed comparison with existing works in tool learning. If the authors can address these concerns, the framework has the potential to become a state-of-the-art tool-learning framework.\n\nRating: 7/10\n$$$",
        "gr_score_with": 0.446524064171123,
        "gr_score_without": 0.35978835978835977,
        "output_without": {
            "num_tokens_scored": 378,
            "num_green_tokens": 136,
            "green_fraction": 0.35978835978835977,
            "z_score": -1.5958499971500721,
            "p_value": 0.9447388561907637
        },
        "output_with": {
            "num_tokens_scored": 374,
            "num_green_tokens": 167,
            "green_fraction": 0.446524064171123,
            "z_score": 1.8365714395052593,
            "p_value": 0.03313659340056785
        }
    },
    {
        "title": "WHAT\u2019S IN MY BIG DATA?",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. The authors apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama, and uncover several surprising and previously undocumented findings about these corpora.\n\nStrengths:\n\n* The paper proposes a novel platform and set of analyses for understanding the contents of large text corpora.\n* The authors provide a comprehensive analysis of ten different corpora used to train popular language models.\n* The paper highlights the importance of understanding the contents of large text corpora and provides a framework for doing so.\n* The authors provide a clear and concise description of the WIMBD platform and its capabilities.\n* The paper includes a thorough analysis of the results, including both quantitative and qualitative findings.\n\nWeaknesses:\n\n* The paper assumes that the reader has a strong background in natural language processing and machine learning.\n* The authors do not provide a detailed description of the Elasticsearch search engine used in WIMBD.\n* The paper does not provide a clear explanation of the hash function used in the counting and searching algorithms.\n* The authors do not provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* The paper does not provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n\nSuggestions for Improvement:\n\n* Provide a more detailed description of the Elasticsearch search engine used in WIMBD.\n* Provide a clear explanation of the hash function used in the counting and searching algorithms.\n* Provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* Provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n* Consider including a more detailed analysis of the results, including a discussion of the implications of the findings.\n\nRecommendation:\n\nThis paper provides a novel and comprehensive framework for understanding the contents of large text corpora. The authors provide a clear and concise description of the WIMBD platform and its capabilities, and the paper includes a thorough analysis of the results. However, the paper assumes a strong background in natural language processing and machine learning, and some of the technical details are not clearly explained. Overall, this paper is a valuable contribution to the field of natural language processing and machine learning, and it has the potential to be a highly influential paper in the field.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform for analyzing large text corpora. WIMBD consists of two basic components: counting and search, which enable quick processing of terabytes of data. The authors conduct sixteen analyses on ten different corpora, including C4, The Pile, and RedPajama, using WIMBD. The analysis reveals several surprising findings, such as the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.\n\nStrengths:\n1. The paper proposes a comprehensive platform for analyzing large text corpora, addressing a significant gap in the field.\n2. The authors conduct extensive analysis on ten corpora, providing a thorough understanding of their contents.\n3. The findings are surprising and provide new insights into the nature of large text corpora.\n4. The platform is extendable, allowing researchers to reproduce the analyses on other corpora and develop new ones.\n5. The authors release all the code and artifacts for WIMBD, encouraging researchers to adopt and extend the framework.\n\nWeaknesses:\n1. The search tool used in WIMBD is Elasticsearch, which may not be scalable for large text corpora.\n2. The analysis does not provide exact results, but rather approximate ones, due to the use of Bloom filters.\n3. The authors do not provide a detailed explanation of the algorithms used for computing the most common, least common, and total number of unique n-grams.\n4. The paper does not discuss the limitations of using regular expressions for identifying personally identifiable information.\n5. The authors do not provide a detailed evaluation of the toxicity detection methods used in the analysis.\n\nSuggestions for Improvement:\n1. Consider using more scalable search tools, such as FAISS or HNSW, for large text corpora.\n2. Provide more detailed explanations of the algorithms used for computing the most common, least common, and total number of unique n-grams.\n3. Discuss the limitations of using regular expressions for identifying personally identifiable information.\n4. Evaluate the toxicity detection methods used in the analysis more thoroughly.\n5. Consider extending the analysis to other types of corpora, such as video or audio corpora.\n\nRecommendation:\nThe paper proposes a comprehensive platform for analyzing large text corpora, addressing a significant gap in the field. While there are some limitations and areas for improvement, the findings are surprising and provide new insights into the nature of large text corpora. I recommend accepting the paper for publication, subject to revisions addressing the suggestions for improvement.",
        "gr_score_with": 0.5673352435530086,
        "gr_score_without": 0.39228295819935693,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 122,
            "green_fraction": 0.39228295819935693,
            "z_score": -0.27779564072611224,
            "p_value": 0.609415381136866
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 198,
            "green_fraction": 0.5673352435530086,
            "z_score": 6.3810847508788635,
            "p_value": 8.791897767549127e-11
        }
    },
    {
        "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n* The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a promising direction for future research.\n* The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings.\n* The paper provides a comprehensive evaluation of GR-1, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nWeaknesses:\n\n* The paper relies heavily on pre-training on a large-scale video dataset, which may not be feasible for all applications or domains.\n* The authors do not provide a thorough analysis of the trade-offs between pre-training and finetuning, which may be important for practical applications.\n* The paper assumes access to a large-scale video dataset, which may not be available for all researchers or applications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the trade-offs between pre-training and finetuning, including the impact on performance, computational resources, and data requirements.\n* Explore alternative approaches to pre-training, such as using smaller datasets or different types of data, to make the method more widely applicable.\n* Provide more information about the scalability of GR-1, including the computational resources required for training and inference.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to visual robot manipulation, and the authors demonstrate its effectiveness on challenging benchmarks. However, the paper relies heavily on pre-training on a large-scale video dataset, and the trade-offs between pre-training and finetuning are not thoroughly analyzed. To make the method more widely applicable and scalable, the authors should explore alternative approaches to pre-training and provide a more detailed analysis of the trade-offs between pre-training and finetuning. With these improvements, the paper has the potential to make a significant impact in the field of visual robot manipulation.\n\nRating: 7/10\n\nRecommendation for Publication: Accept with revisions.",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThis paper presents a novel approach to enhance visual robot manipulation learning by leveraging large-scale video generative pre-training. The proposed method, GR-1, is a GPT-style transformer that takes as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset, and then finetuned on robot data to predict actions and future images. The authors conduct extensive experiments on both CALVIN benchmark and a real robot, demonstrating the effectiveness of GR-1 in improving state-of-the-art methods in multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization. GR-1 also outperforms baseline methods in real robot experiments, showing robust performance in scenes disturbed from those in the training data, and generalizing to unseen object instances and categories.\n\n**Strengths**\n\n1. The paper presents a novel approach to enhance visual robot manipulation learning by leveraging large-scale video generative pre-training, which is a significant contribution to the field.\n2. The proposed method, GR-1, is a GPT-style transformer that can take as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner, which is a flexible and effective architecture.\n3. The authors conduct extensive experiments on both CALVIN benchmark and a real robot, demonstrating the effectiveness of GR-1 in improving state-of-the-art methods in various settings.\n4. The paper provides a thorough analysis of the video prediction performance of GR-1, which is an important aspect of the method.\n\n**Weaknesses**\n\n1. The paper assumes access to a large-scale video dataset, which may not be readily available for all researchers.\n2. The method requires a significant amount of computational resources for training, which may be a limitation for some researchers.\n3. The paper does not provide a thorough analysis of the robustness of GR-1 to various types of disturbances, which is an important aspect of the method.\n4. The paper assumes that the language instructions are provided, which may not be the case in all scenarios.\n\n**Suggestions for Improvement**\n\n1. Provide a more thorough analysis of the robustness of GR-1 to various types of disturbances, such as occlusion, lighting changes, and motion blur.\n2. Evaluate the performance of GR-1 on a wider range of tasks and environments, such as manipulation of objects with different sizes, shapes, and materials.\n3. Provide a more detailed analysis of the video prediction performance of GR-1, including the types of errors made by the method.\n4. Investigate the effect of pre-training on videos of any kind versus only videos relevant to manipulation.\n\n**Recommendation**\n\nThis paper presents a novel approach to enhance visual robot manipulation learning by leveraging large-scale video generative pre-training. The proposed method, GR-1, is a GPT-style transformer that can take as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner. The authors conduct extensive experiments on both CALVIN benchmark and a real robot, demonstrating the effectiveness of GR-1 in improving state-of-the-art methods in various settings. While there are some limitations to the method, such as the assumption of access to a large-scale video dataset and the requirement of significant computational resources, the paper presents a significant contribution to the field. Therefore, I recommend accepting this paper for publication.",
        "gr_score_with": 0.41818181818181815,
        "gr_score_without": 0.3834808259587021,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 130,
            "green_fraction": 0.3834808259587021,
            "z_score": -0.6208440982218895,
            "p_value": 0.732648897397985
        },
        "output_with": {
            "num_tokens_scored": 385,
            "num_green_tokens": 161,
            "green_fraction": 0.41818181818181815,
            "z_score": 0.7282190812544193,
            "p_value": 0.23323974356262384
        }
    },
    {
        "title": "CADE AND CONTINUOUS SCENE RECONSTRUCTION FROM FOVEATED VISUAL SAMPLING",
        "abstract": "High accuracy, low latency and high energy efficiency represent a set of conflicting goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduced camera frame rates, and increased data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, and saccade mechanism of the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from a sequence of foveal-peripheral views, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our model demonstrates a 5% higher image recognition accuracy compared to state-of-theart foveal-peripheral based vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches. Code is available at Github.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel framework for energy-efficient and low-cost sensing and processing in artificial vision systems. Inspired by the foveal-peripheral sampling mechanism of the human visual system, the proposed framework integrates foveal-peripheral sampling and saccade control with continuous scene reconstruction. The framework reduces the amount of data required from the camera by 70% while achieving similar image recognition performance as conventional imaging front-ends. The self-trained scene reconstruction model restores the original scene from highly sparse input and achieves an average of 0.88 structure similarity on ImageNet dataset. The trained saccade controller improves the scene classification accuracy by more than 10% and is highly transferable.\n\nStrengths:\n\n1. The proposed framework is a novel and innovative approach to energy-efficient and low-cost sensing and processing in artificial vision systems.\n2. The framework integrates foveal-peripheral sampling and saccade control with continuous scene reconstruction, which is a unique and effective way to reduce data requirements while maintaining image recognition performance.\n3. The self-trained scene reconstruction model is able to restore the original scene from highly sparse input, which is a significant improvement over previous works.\n4. The trained saccade controller is highly transferable and can be used effectively on scenes that differ from the training classes.\n5. The framework requires no effort to re-train or fine-tune the backend classification model, making it a convenient and efficient solution.\n\nWeaknesses:\n\n1. The paper assumes a perfect understanding of the human visual system and its mechanisms, which may not be the case in reality.\n2. The framework relies heavily on the self-trained scene reconstruction model, which may not generalize well to different scenes or environments.\n3. The trained saccade controller may not be able to handle complex or dynamic scenes, which could affect its performance.\n4. The paper does not provide a thorough comparison with other state-of-the-art foveal-peripheral based vision systems, which makes it difficult to evaluate the framework's performance.\n5. The framework's performance may be affected by the quality of the input data, which could impact its ability to reduce data requirements while maintaining image recognition performance.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough comparison with other state-of-the-art foveal-peripheral based vision systems to evaluate the framework's performance.\n2. Investigate the limitations of the self-trained scene reconstruction model and its ability to generalize to different scenes or environments.\n3. Develop a more robust and adaptive saccade controller that can handle complex or dynamic scenes.\n4. Explore the use of other machine learning algorithms or techniques to improve the framework's performance and robustness.\n5. Provide more detailed information about the framework's implementation and evaluation, including the specific hardware and software used.\n\nRecommendation:\n\nThe paper presents a novel and innovative framework for energy-efficient and low-cost sensing and processing in artificial vision systems. While there are some limitations and areas for improvement, the framework shows promising results and has the potential to be a significant contribution to the field. Therefore, I recommend that the paper be accepted for publication in a reputable journal or conference.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for energy-efficient and low-cost sensing and processing in computer vision, inspired by the human visual system's foveation and saccadic mechanisms. The framework, called CADE, employs foveal-peripheral sampling and saccadic control to reduce data required from the camera, while maintaining similar image recognition performance as conventional imaging front-ends. The CADE framework consists of three core components: the predictive reconstruction module, the evaluation module, and the policy module. The predictive reconstruction module merges foveal-peripheral views to generate a high-resolution rendition of the original scene, while the policy module controls saccades to direct the foveal center to new positions. The evaluation module assesses the performance of scene reconstruction and generates rewards for the training of the saccade controller. The CADE framework achieves state-of-the-art results in image classification, requiring 70% fewer pixels compared to conventional imaging front-ends, and demonstrates high data efficiency during the training phase.\n\nStrengths:\n\n1. The CADE framework is inspired by the human visual system's foveation and saccadic mechanisms, making it biologically plausible and energy-efficient.\n2. The framework achieves state-of-the-art results in image classification, requiring 70% fewer pixels compared to conventional imaging front-ends.\n3. The CADE framework demonstrates high data efficiency during the training phase, requiring only a small portion of the training data to achieve good performance.\n4. The framework is easy to integrate with existing back-end processing models, such as image classification and object detection.\n5. The CADE framework has a high transferable saccade model, which can be applied to scenes that differ from the training classes.\n\nWeaknesses:\n\n1. The CADE framework relies heavily on the predictive reconstruction module, which may not work well for complex scenes or scenes with high variability.\n2. The framework may not be suitable for real-time applications, as it requires multiple glimpses to achieve good performance.\n3. The CADE framework assumes that the evaluation module can provide accurate rewards for the training of the saccade controller, which may not always be the case.\n4. The framework may not be robust to variations in lighting, texture, or other environmental factors.\n5. The CADE framework does not provide a clear explanation for how the saccade controller learns to select the best location for the foveal center.\n\nSuggestions for Improvement:\n\n1. Investigate more advanced techniques for predictive reconstruction, such as using generative models or attention mechanisms.\n2. Develop more robust evaluation modules that can provide accurate rewards for the training of the saccade controller.\n3. Explore ways to make the CADE framework more suitable for real-time applications, such as reducing the number of glimpses required or using more efficient algorithms.\n4. Investigate how to make the CADE framework more robust to variations in lighting, texture, or other environmental factors.\n5. Provide more insights into how the saccade controller learns to select the best location for the foveal center, such as using visualization techniques or interpretability methods.\n\nRecommendation:\n\nThe CADE framework is a promising approach for energy-efficient and low-cost sensing and processing in computer vision, and it achieves state-of-the-art results in image classification. However, it relies heavily on the predictive reconstruction module and may not work well for complex scenes or scenes with high variability. Therefore, I recommend that the authors continue to investigate more advanced techniques for predictive reconstruction and develop more robust evaluation modules to make the CADE framework more suitable for real-world applications.\n$$$",
        "gr_score_with": 0.49406175771971494,
        "gr_score_without": 0.3588516746411483,
        "output_without": {
            "num_tokens_scored": 418,
            "num_green_tokens": 150,
            "green_fraction": 0.3588516746411483,
            "z_score": -1.7172545872363758,
            "p_value": 0.9570336671867783
        },
        "output_with": {
            "num_tokens_scored": 421,
            "num_green_tokens": 208,
            "green_fraction": 0.49406175771971494,
            "z_score": 3.939567238130727,
            "p_value": 4.081436129241692e-05
        }
    },
    {
        "title": "SMOOTH ECE: PRINCIPLED RELIABILITY DIAGRAMS",
        "abstract": "Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures \u2014 binning and ECE \u2014 both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of B\u0142asiok, Gopalan, Hu, and Nakkiran (2023) \u2014 a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE. We also develop a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: https://anonymous.4open. science/r/smoothece-anon/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a new calibration measure, SmoothECE, which is a consistent calibration measure in the sense of B\u0142asiok et al. (2023). SmoothECE is defined as the Expected Calibration Error (ECE) of a \"smoothed\" version of the original distribution, and it can be visually represented in a reliability diagram. The authors also propose a principled way to choose the kernel bandwidth for the smoothing process. They show that the SmoothECE is monotone decreasing as the kernel bandwidth is increased, and it is Lipschitz with respect to the Wasserstein distance on the space of distributions over [0, 1] \u00d7 {0, 1}. The authors also provide a Python package that efficiently implements the SmoothECE method.\n\nStrengths:\n\n* The paper proposes a new calibration measure that is consistent in the sense of B\u0142asiok et al. (2023).\n* The SmoothECE is defined as the ECE of a \"smoothed\" version of the original distribution, which makes it easy to visualize in a reliability diagram.\n* The authors provide a principled way to choose the kernel bandwidth for the smoothing process.\n* The paper includes several experiments demonstrating the method on public datasets in various domains.\n* The authors provide a Python package that efficiently implements the SmoothECE method.\n\nWeaknesses:\n\n* The paper assumes that the kernel bandwidth can be chosen in a principled way, which may not always be the case in practice.\n* The paper does not provide a detailed analysis of the computational complexity of the method.\n* The paper assumes that the data is i.i.d., which may not always be the case in practice.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational complexity of the method.\n* Provide a more detailed discussion of the assumptions made in the paper, such as the assumption that the kernel bandwidth can be chosen in a principled way.\n* Provide more experiments demonstrating the method on public datasets in various domains.\n* Consider adding more visualizations to the paper to help readers understand the method and its results.\n\nRecommendation:\n\nThis paper proposes a new calibration measure that is consistent in the sense of B\u0142asiok et al. (2023). The SmoothECE is defined as the ECE of a \"smoothed\" version of the original distribution, which makes it easy to visualize in a reliability diagram. The authors provide a principled way to choose the kernel bandwidth for the smoothing process. The paper includes several experiments demonstrating the method on public datasets in various domains. However, the paper assumes that the kernel bandwidth can be chosen in a principled way, which may not always be the case in practice. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the method and a more detailed discussion of the assumptions made in the paper.\n\nOverall, this paper is well-written and provides a new and interesting contribution to the field of calibration measures. However, there are some areas for improvement that need to be addressed before the paper is ready for publication.\n\nRecommendation: Accept after revision.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a new calibration measure, SmoothECE, which is a consistent calibration measure that can be visually represented in a reliability diagram. The SmoothECE is defined as the Expected Calibration Error (ECE) of a smoothed version of the original distribution, and the resulting reliability diagram can be interpreted as a smoothed estimate of the calibration function. The paper also develops a Python package that efficiently implements the SmoothECE and plots the associated smooth reliability diagram.\n\nStrengths:\n1. The paper proposes a new calibration measure that is consistent in the sense of B\u0142asiok et al. (2023).\n2. The SmoothECE can be visually represented in a reliability diagram, making it a useful tool for practitioners.\n3. The paper develops a Python package that efficiently implements the SmoothECE and plots the associated smooth reliability diagram.\n4. The paper provides a principled way of choosing the kernel bandwidth in the SmoothECE.\n\nWeaknesses:\n1. The paper assumes that the prediction distribution is a distribution over [0, 1] \u00d7 {0, 1}, which may not be the case in all situations.\n2. The paper does not discuss the assumption of i.i.d. observations, which may not be valid in all cases.\n3. The paper assumes that the kernel bandwidth is chosen optimally, but the paper does not provide a way to choose the bandwidth in practice.\n4. The paper does not discuss the relationship between the SmoothECE and other calibration measures.\n\nSuggestions for Improvement:\n1. The paper should discuss the assumption of i.i.d. observations and how it affects the validity of the SmoothECE.\n2. The paper should provide a way to choose the kernel bandwidth in practice, rather than assuming that it is chosen optimally.\n3. The paper should discuss the relationship between the SmoothECE and other calibration measures, such as the Binned ECE.\n4. The paper should provide more detailed analysis of the properties of the SmoothECE, such as its consistency and its relationship to the true calibration function.\n\nRecommendation:\nThe paper proposes a new calibration measure that is consistent in the sense of B\u0142asiok et al. (2023) and can be visually represented in a reliability diagram. The paper also develops a Python package that efficiently implements the SmoothECE and plots the associated smooth reliability diagram. However, the paper assumes that the prediction distribution is a distribution over [0, 1] \u00d7 {0, 1} and that the kernel bandwidth is chosen optimally. The paper also does not discuss the relationship between the SmoothECE and other calibration measures. Therefore, the paper is well-suited for practitioners who are interested in computing and analyzing the reliability of probabilistic predictors, but it may require further development and analysis before it can be widely adopted.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.5209003215434084,
        "gr_score_without": 0.44871794871794873,
        "output_without": {
            "num_tokens_scored": 312,
            "num_green_tokens": 140,
            "green_fraction": 0.44871794871794873,
            "z_score": 1.7565506213798907,
            "p_value": 0.03949721791686134
        },
        "output_with": {
            "num_tokens_scored": 311,
            "num_green_tokens": 162,
            "green_fraction": 0.5209003215434084,
            "z_score": 4.352131704709081,
            "p_value": 6.741011388957921e-06
        }
    },
    {
        "title": "LEARNING MULTI-FACETED PROTOTYPICAL USER INTERESTS",
        "abstract": "We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the granularity of item space to that of user interests, which we postulate is not ideal as user interests would likely map to a small subset of item space. In addition, the compositionality of user interests has received inadequate attention, preventing the modeling of interactions between explanatory factors driving a user\u2019s decision. To resolve this, we propose to align user interests with multi-faceted item characteristics. First, we involve prototype-based representation learning to discover item characteristics along multiple facets. Second, we compose user interests from uncovered item characteristics via binding mechanism, separating the granularity of user preferences from that of item space. Third, we design a dedicated bi-directional binding block, aiding the derivation of compositional user interests. On real-world datasets, the experimental results demonstrate the strong performance of our proposed method compared to a series of baselines.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to address the shortcomings of existing VAE-based disentangled recommendation models, including inadequate item space discovering, same level of granularity between user interests and item space assumption, and improper user interest complexity handling. FACETVAE is characterized by three main innovations: disentangling item space under a multi-faceted manner, binding compositional user interests from low-level ones discovered from item space, and effectively binding user interests via a bi-directional binding block. The authors demonstrate the effectiveness of FACETVAE on three real-world datasets, achieving state-of-the-art performance compared to several baselines.\n\nStrengths:\n\n1. The paper addresses a significant problem in recommendation systems, which is learning multi-faceted user interests.\n2. The proposed method, FACETVAE, is well-motivated and has a clear theoretical foundation.\n3. The authors provide a thorough evaluation of their method on three real-world datasets, demonstrating its effectiveness compared to several baselines.\n4. The paper is well-organized and easy to follow, with clear explanations of the methodology and experimental results.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known, which may not be the case in practice.\n2. The authors do not provide a clear explanation of how to determine the number of facets and the number of clusters per facet in a data-driven manner.\n3. The paper assumes that the user interests are represented as a set of K vectors, which may not be the case in practice.\n4. The authors do not provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to determine the number of facets and the number of clusters per facet in a data-driven manner.\n2. Discuss the limitations of the paper, including the assumption that the number of facets and the number of clusters per facet are known.\n3. Provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors.\n4. Consider adding more experimental results to demonstrate the effectiveness of FACETVAE in different scenarios.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough evaluation of the proposed method, FACETVAE. However, the authors should address the limitations of the paper and provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors. With these improvements, the paper has the potential to make a significant contribution to the field of recommendation systems.\n\n$$$\nNote: The review is based on the provided paper and is intended to provide constructive feedback to the authors.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel recommendation model, FACETVAE, that learns multi-faceted prototypical user interests by disentangling the item space into multiple facets and binding compositional user interests. The authors argue that existing VAE-based recommendation models ignore the multiple facets of item characteristics and assume the same level of granularity for user interests and item space, leading to inadequate user interest complexity handling. FACETVAE addresses these shortcomings by introducing a bi-directional binding block that learns to bind low-level user interests into high-level ones and captures the complexity of user preferences.\n\nStrengths:\n\n1. The paper proposes a novel and elegant solution to the limitations of existing VAE-based recommendation models.\n2. The authors provide a thorough analysis of the strengths and weaknesses of their approach and its comparisons with existing methods.\n3. The experimental results demonstrate the strong performance of FACETVAE on various real-world datasets.\n\nWeaknesses:\n\n1. The paper assumes a pre-defined number of user interests and facets, which may not be optimal for all scenarios.\n2. The authors do not provide a clear explanation for the choice of the bi-directional binding block and its hyperparameters.\n3. The paper does not explore the scalability of FACETVAE for large-scale datasets.\n\nSuggestions for Improvement:\n\n1. Investigate the performance of FACETVAE with dynamic user interests and facets that can be learned in a data-driven manner.\n2. Provide a more detailed explanation for the bi-directional binding block and its hyperparameters.\n3. Explore the scalability of FACETVAE for large-scale datasets and provide a discussion on its computational complexity.\n\nRecommendation:\n\nThe paper proposes a novel and effective solution to the limitations of existing VAE-based recommendation models. The authors provide a thorough analysis of their approach and its comparisons with existing methods. While there are some limitations and open issues, the paper is well-written and the results are convincing. I recommend accepting the paper for publication in a top-tier conference.\n\n$$$",
        "gr_score_with": 0.5231316725978647,
        "gr_score_without": 0.4438040345821326,
        "output_without": {
            "num_tokens_scored": 347,
            "num_green_tokens": 154,
            "green_fraction": 0.4438040345821326,
            "z_score": 1.6656096552118862,
            "p_value": 0.047895593402099874
        },
        "output_with": {
            "num_tokens_scored": 281,
            "num_green_tokens": 147,
            "green_fraction": 0.5231316725978647,
            "z_score": 4.2132508588411275,
            "p_value": 1.2586061045229269e-05
        }
    },
    {
        "title": "MAKING PRE-TRAINED LANGUAGE MODELS GREAT",
        "abstract": "The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates feature values with the corresponding feature names. Comprehensive experiments demonstrate that our pre-trained TP-BERTa leads the performance among tabular DNNs and is competitive with Gradient Boosted Decision Tree models in typical tabular data regime.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a pre-trained language model, TP-BERTa, for tabular data prediction. TP-BERTa addresses the limitations of existing language models in handling numerical features in tables by introducing two novel adaptations: relative magnitude tokenization (RMT) and intra-feature attention (IFA). RMT discretizes numerical feature values into relative magnitude tokens, enabling the model to understand numerical features in the language space. IFA fuses feature name and value embeddings into a single vector, allowing the model to learn feature-specific representations. The authors demonstrate the effectiveness of TP-BERTa on a range of tabular datasets, outperforming existing deep learning models and competing with Gradient Boosted Decision Trees (GBDTs) in typical tabular data regimes.\n\nStrengths:\n\n1. The paper addresses a significant gap in the literature by exploring the application of pre-trained language models to tabular data prediction.\n2. The proposed TP-BERTa model is well-designed and demonstrates strong performance on a range of tabular datasets.\n3. The authors provide a thorough evaluation of the model's performance, including comparisons with existing deep learning models and GBDTs.\n4. The paper discusses the limitations of existing language models in handling numerical features and proposes novel adaptations to address these limitations.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained language models, which may limit its applicability to specific domains or datasets.\n2. The evaluation of TP-BERTa is limited to a specific set of tabular datasets, and it is unclear how the model would perform on other types of data.\n3. The authors do not provide a detailed analysis of the computational cost of TP-BERTa, which may be a limitation for large-scale applications.\n4. The paper assumes that the pre-trained language model is available, which may not be the case for all users.\n\nSuggestions for Improvement:\n\n1. The authors could explore the application of TP-BERTa to other types of data, such as image or text data.\n2. A more detailed analysis of the computational cost of TP-BERTa would be beneficial for users who need to scale the model to large datasets.\n3. The authors could provide a more thorough evaluation of the model's performance on a range of datasets, including those with different feature types and distributions.\n4. The paper could benefit from a more detailed discussion of the limitations of pre-trained language models and how TP-BERTa addresses these limitations.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough evaluation of the proposed TP-BERTa model. The model demonstrates strong performance on a range of tabular datasets and addresses a significant gap in the literature. However, the paper relies heavily on pre-trained language models, which may limit its applicability to specific domains or datasets. Overall, the paper is a strong contribution to the field of deep learning for tabular data prediction.\n\nRecommendation: Accept.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel pre-trained language model, TP-BERTa, specifically designed for tabular data prediction tasks. The model incorporates two key adaptions: (1) relative magnitude tokenization (RMT), which discretizes numerical feature values as relative magnitude tokens in the language space, and (2) intra-feature attention (IFA), which attentively fuses the embeddings of feature names and values into a single vector before processing by the language model. The model is trained on extensive tabular datasets and demonstrates superior performances compared to classical and advanced deep tabular models in typical tabular data regime.\n\nStrengths:\n\n1. **Novel approach**: The paper proposes a novel approach to handle numerical features in tables by discretizing them as relative magnitude tokens in the language space, which allows the model to understand the relative values and order of features.\n2. **Effective adaptions**: The two key adaptions, RMT and IFA, are effective in improving the model's performance on tabular data prediction tasks.\n3. **Comprehensive experiments**: The paper conducts extensive experiments on various tabular datasets and compares the model's performance with classical and advanced deep tabular models.\n4. **Competitive performance**: The model demonstrates competitive performance with gradient boosting decision tree (GBDT) models in typical tabular data regime.\n\nWeaknesses:\n\n1. **Limited scope**: The paper focuses on binary classification and regression tasks and does not explore multi-class classification tasks.\n2. **Pre-training data**: The model is trained on extensive tabular datasets, which may not be readily available for all domains and applications.\n3. **Hyperparameter tuning**: The paper does not explore the impact of hyperparameter tuning on the model's performance.\n4. **Comparison with other LMs**: The paper does not compare the model's performance with other pre-trained language models (e.g., BERT, RoBERTa).\n\nSuggestions for Improvement:\n\n1. **Explore multi-class classification tasks**: Investigate the model's performance on multi-class classification tasks and explore potential modifications to the model architecture.\n2. **Use transfer learning**: Explore the use of transfer learning to adapt the model to new domains and applications.\n3. **Investigate hyperparameter tuning**: Conduct a thorough investigation of the impact of hyperparameter tuning on the model's performance.\n4. **Compare with other LMs**: Compare the model's performance with other pre-trained language models (e.g., BERT, RoBERTa) and explore potential modifications to the model architecture.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to handling numerical features in tables using pre-trained language models. While the model demonstrates competitive performance with GBDT models in typical tabular data regime, it has some limitations and areas for improvement. I recommend the authors to explore multi-class classification tasks, use transfer learning, investigate hyperparameter tuning, and compare the model's performance with other pre-trained language models.\n\nOverall, the paper presents a valuable contribution to the field of tabular data prediction and demonstrates the potential of pre-trained language models in handling numerical features in tables.",
        "gr_score_with": 0.5038363171355499,
        "gr_score_without": 0.3879781420765027,
        "output_without": {
            "num_tokens_scored": 366,
            "num_green_tokens": 142,
            "green_fraction": 0.3879781420765027,
            "z_score": -0.4694685597539525,
            "p_value": 0.6806326236003618
        },
        "output_with": {
            "num_tokens_scored": 391,
            "num_green_tokens": 197,
            "green_fraction": 0.5038363171355499,
            "z_score": 4.191138705502819,
            "p_value": 1.387789513276989e-05
        }
    },
    {
        "title": "LOCAL COMPOSITE SADDLE POINT OPTIMIZATION",
        "abstract": "Distributed optimization (DO) approaches for saddle point problems (SPP) have recently gained in popularity due to the critical role they play in machine learning (ML). Existing works mostly target smooth unconstrained objectives in Euclidean space, whereas ML problems often involve constraints or non-smooth regularization, which results in a need for composite optimization. Moreover, although non-smooth regularization often serves to induce structure (e.g., sparsity), standard aggregation schemes in distributed optimization break this structure. Addressing these issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-step primal-dual algorithm with local updates, which is the first of its kind to encompass both saddle point optimization and composite objectives under the distributed paradigm. Using a generalized notion of Bregman divergence, we analyze its convergence and communication complexity in the homogeneous setting. Furthermore, the empirical evaluation demonstrates the effectiveness of FeDualEx for inducing structure in these challenging settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel algorithm, Federated Dual Extrapolation (FeDualEx), for distributed optimization of composite saddle point problems (SPP). FeDualEx is an extra-step primal-dual algorithm with local updates that addresses the challenges of composite SPP under the distributed paradigm. The authors provide a convergence rate analysis for FeDualEx under the homogeneous setting, which is, to the best of their knowledge, the first convergence rate for composite SPP under the DO paradigm. The paper also showcases the structure-preserving advantage of FeDualEx through dual-space averaging and demonstrates its effectiveness through numerical evaluations on various composite SPP tasks.\n\nStrengths:\n\n1. FeDualEx is a novel algorithm that addresses the challenges of composite SPP under the distributed paradigm.\n2. The authors provide a convergence rate analysis for FeDualEx under the homogeneous setting, which is a significant contribution.\n3. The paper demonstrates the effectiveness of FeDualEx through numerical evaluations on various composite SPP tasks.\n4. The authors provide a thorough comparison with existing methods, including FedDualAvg and FedMiD.\n\nWeaknesses:\n\n1. The paper assumes a homogeneous setting, which may not be realistic in many federated learning scenarios.\n2. The convergence rate analysis is based on several assumptions, including the convexity of the objective function and the Lipschitzness of the gradient operator.\n3. The paper does not provide a comprehensive analysis of the heterogeneous setting, which is a significant limitation.\n4. The numerical evaluations are limited to a few specific tasks and do not provide a thorough comparison with existing methods.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive analysis of the heterogeneous setting, including a convergence rate analysis and numerical evaluations.\n2. The paper should provide a more thorough comparison with existing methods, including a discussion of the advantages and disadvantages of each method.\n3. The authors should provide more details on the implementation of FeDualEx, including the choice of hyperparameters and the convergence criteria.\n4. The paper should provide a more detailed discussion of the implications of the convergence rate analysis, including the trade-offs between communication complexity and convergence rate.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of distributed optimization. However, the paper has some limitations, including the assumption of a homogeneous setting and the lack of a comprehensive analysis of the heterogeneous setting. Therefore, I recommend that the authors revise the paper to address these limitations and provide a more comprehensive analysis of the heterogeneous setting. With these revisions, the paper has the potential to make a significant impact on the field of distributed optimization.\n\nScore: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Presentation: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Federated Dual Extrapolation (FeDualEx), an algorithm for distributed learning of saddle point problems with composite possibly non-smooth regularization under the Federated Learning paradigm. FeDualEx builds upon the classic dual extrapolation algorithm, an extra-step algorithm geared for saddle point optimization, and utilizes the generalized Bregman divergence to handle composite objectives. The algorithm's convergence rate is analyzed in the homogeneous setting, providing the first convergence rate for federated composite saddle point optimization.\n\nStrengths:\n\n1. FeDualEx is the first algorithm to address the challenge of distributed learning of saddle point problems with composite possibly non-smooth regularization under the Federated Learning paradigm.\n2. The algorithm's convergence rate is analyzed in the homogeneous setting, providing a significant contribution to the field.\n3. FeDualEx demonstrates the effectiveness of dual aggregation in preserving the structure imposed by regularization.\n4. The algorithm's empirical evaluation demonstrates its effectiveness in inducing structures in challenging settings.\n\nWeaknesses:\n\n1. The analysis assumes the full participation of clients in each round, which may not be realistic in practice.\n2. The algorithm's convergence rate may not hold in the heterogeneous setting, which is an important direction for future work.\n3. The algorithm's dependence on the generalized Bregman divergence may limit its applicability to certain types of regularization.\n4. The algorithm's complexity and scalability in large-scale settings require further investigation.\n\nSuggestions for Improvement:\n\n1. Investigate the algorithm's performance in the heterogeneous setting.\n2. Explore the use of alternative divergence measures, such as the Wasserstein distance, to handle different types of regularization.\n3. Investigate the algorithm's scalability in large-scale settings and develop strategies to reduce its complexity.\n4. Provide more insights into the algorithm's behavior in the sequential setting, where the number of clients is reduced to one.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of Federated Learning by proposing FeDualEx, an algorithm for distributed learning of saddle point problems with composite possibly non-smooth regularization. The algorithm's convergence rate is analyzed in the homogeneous setting, providing a valuable insight into the field. While the algorithm's limitations in the heterogeneous setting and scalability in large-scale settings require further investigation, the paper's contribution to the field is substantial, and the algorithm's effectiveness in inducing structures in challenging settings is impressive.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.5993485342019544,
        "gr_score_without": 0.49707602339181284,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 170,
            "green_fraction": 0.49707602339181284,
            "z_score": 3.6645395212132867,
            "p_value": 0.0001238919891244713
        },
        "output_with": {
            "num_tokens_scored": 307,
            "num_green_tokens": 184,
            "green_fraction": 0.5993485342019544,
            "z_score": 7.129787950668543,
            "p_value": 5.026183548496255e-13
        }
    },
    {
        "title": "FINE-TUNED LANGUAGE MODELS GENERATE STABLE INORGANIC MATERIALS",
        "abstract": "We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting\u2019s inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models\u2019 ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generate stable inorganic materials using fine-tuned language models. The authors demonstrate that pre-trained language models can be effectively fine-tuned to generate stable materials with high rates of validity and diversity. They evaluate their method using a dataset of 45231 materials and show that their model can generate materials predicted to be metastable at a rate of 49%, which is higher than the baseline method. The authors also demonstrate the ability of their model to perform text-conditional generation and infilling, which can be used to optimize the properties of existing materials.\n\nStrengths:\n\n* The paper proposes a novel approach to generate stable inorganic materials using fine-tuned language models.\n* The authors demonstrate the effectiveness of their method using a large dataset of materials.\n* The paper shows that the fine-tuned language models can generate materials with high rates of validity and diversity.\n* The authors demonstrate the ability of their model to perform text-conditional generation and infilling.\n\nWeaknesses:\n\n* The paper assumes that the pre-trained language models have useful biases towards generalizable patterns, which may not always be the case.\n* The authors do not provide a thorough analysis of the limitations of their method, such as the potential for hallucinations or the reliance on text data.\n* The paper does not provide a detailed comparison with other methods for generating stable materials.\n* The authors do not discuss the potential applications of their method beyond generating stable materials.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the limitations of the method, such as the potential for hallucinations or the reliance on text data.\n* Compare the method with other methods for generating stable materials and discuss the advantages and disadvantages of each approach.\n* Discuss the potential applications of the method beyond generating stable materials, such as optimizing the properties of existing materials.\n* Provide a more detailed explanation of the fine-tuning procedure and how it affects the performance of the model.\n\nRecommendation:\n\nThe paper is well-written and provides a novel approach to generating stable inorganic materials using fine-tuned language models. However, the authors should address the limitations of their method and provide a more detailed comparison with other methods. With these improvements, the paper would be more comprehensive and would provide a clearer understanding of the potential applications of the method.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to generate stable inorganic materials using fine-tuned language models. The authors demonstrate that pre-trained language models can be effectively fine-tuned to predict the structure of materials that are stable and metastable. They propose a simple method to fine-tune large language models for generation of stable materials by encoding crystals as strings and combining them with text instructions. The authors evaluate their method using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, and show that their strongest model can generate materials predicted to be metastable at a rate twice that of a competing diffusion model.\n\nStrengths:\n\n* The paper presents a novel approach to generate stable inorganic materials using fine-tuned language models, which is a significant contribution to the field.\n* The authors demonstrate the effectiveness of their method by evaluating it using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations.\n* The paper presents a simple and efficient method to fine-tune large language models for generation of stable materials.\n* The authors provide a comprehensive evaluation of their method, including metrics for stability, novelty, and sampling speed.\n\nWeaknesses:\n\n* The paper assumes that the pre-trained language models have useful biases towards generalizable patterns, which may not always be the case.\n* The authors do not provide a detailed analysis of the impact of different hyperparameters on the performance of their method.\n* The paper does not discuss the limitations of their method, such as its sensitivity to precise details of the chosen prompt and tokenization strategies.\n* The authors do not provide a comparison of their method with other state-of-the-art methods for generating stable inorganic materials.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the impact of different hyperparameters on the performance of their method.\n* They should discuss the limitations of their method and provide a comparison with other state-of-the-art methods for generating stable inorganic materials.\n* The authors should provide a more detailed explanation of how they fine-tune the pre-trained language models and what specific modifications they make to the training process.\n* They should provide a more comprehensive evaluation of their method, including metrics for stability, novelty, and sampling speed.\n\nRecommendation:\n\nThe paper presents a novel approach to generate stable inorganic materials using fine-tuned language models, which is a significant contribution to the field. However, there are some limitations to the paper that need to be addressed. The authors should provide a more detailed analysis of the impact of different hyperparameters on the performance of their method and discuss the limitations of their method. They should also provide a comparison with other state-of-the-art methods for generating stable inorganic materials. Overall, the paper has the potential to be a significant contribution to the field, but it needs to be further developed and refined.\n\n$$$",
        "gr_score_with": 0.45806451612903226,
        "gr_score_without": 0.38202247191011235,
        "output_without": {
            "num_tokens_scored": 267,
            "num_green_tokens": 102,
            "green_fraction": 0.38202247191011235,
            "z_score": -0.5996253511966906,
            "p_value": 0.7256220260214952
        },
        "output_with": {
            "num_tokens_scored": 310,
            "num_green_tokens": 142,
            "green_fraction": 0.45806451612903226,
            "z_score": 2.086825030920757,
            "p_value": 0.018451976806859085
        }
    },
    {
        "title": "BADEXPERT: EXTRACTING BACKDOOR FUNCTIONAL- ITY FOR ACCURATE BACKDOOR INPUT DETECTION",
        "abstract": "In this paper, we present a novel defense against backdoor attacks on deep neural networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. Our proposed defense is built upon an intriguing concept: given a backdoored model, we reverse engineer it to directly extract its backdoor functionality to a backdoor expert model. To accomplish this, we finetune the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising robust backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB, and ImageNet) across multiple model architectures (ResNet, VGG, MobileNetV2, and Vision Transformer). Our code is integrated into our research toolbox: https://github.com/vtu81/backdoor-toolbox. Untrusted Source Incorrect Labels Small Reserved Clean Set Recognize Forgot Clean Task Backdoor Task Backdoored Model M Backdoor Expert B Finetuning Figure 1: Extracting backdoor functionality via finetuning on a mislabeled small clean set. The backdoored modelM can correctly recognize both benign and poisoned samples whereas our backdoor expert model B can only recognize backdoor samples.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel post-development defense strategy against backdoor attacks on deep neural networks (DNNs). The defense, called BaDExpert, involves extracting the backdoor functionality from a compromised model and utilizing it to detect backdoor inputs during inference time. The authors demonstrate that by finetuning a backdoored model on a small set of mislabeled clean samples, they can erase its normal functionality while preserving the backdoor functionality. This backdoor expert model can then be used to develop an accurate and resilient detector for backdoor inputs.\n\nStrengths:\n\n* The paper presents a novel approach to post-development defense against backdoor attacks, which is a significant contribution to the field.\n* The authors provide a clear and concise explanation of their method, making it easy to understand and follow.\n* The paper includes a comprehensive evaluation of BaDExpert on various datasets and model architectures, demonstrating its effectiveness.\n* The authors also provide an adaptive study against BaDExpert, showing its resilience against diverse adaptive attacks.\n\nWeaknesses:\n\n* The paper relies heavily on the assumption that the adversary will not manipulate the backdoor expert model, which may not always be the case.\n* The authors do not provide a thorough analysis of the potential risks and limitations of their approach.\n* The paper could benefit from more discussion on the potential applications and limitations of BaDExpert in real-world scenarios.\n* The authors do not provide a clear comparison of BaDExpert with other existing defenses, making it difficult to evaluate its effectiveness in comparison.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the potential risks and limitations of their approach, including potential vulnerabilities and attack scenarios.\n* The paper could benefit from more discussion on the potential applications and limitations of BaDExpert in real-world scenarios.\n* The authors should provide a clear comparison of BaDExpert with other existing defenses, including a discussion of their strengths and weaknesses.\n* The paper could benefit from more experimental evaluation, including a larger number of datasets and model architectures.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to post-development defense against backdoor attacks. However, the authors should address the weaknesses and limitations mentioned above to make the paper more comprehensive and robust. With some revisions, this paper has the potential to make a significant contribution to the field of backdoor attacks and defense.\n\nRating: 7.5/10\n\nRecommendation for Publication: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel post-development defense strategy against backdoor attacks on deep neural networks (DNNs), dubbed BaDExpert (Backdoor Input Detection with Backdoor Expert). The defense is based on the concept of extracting the backdoor functionality from a compromised model and utilizing it to detect backdoor inputs during inference-time. The authors present a simple yet effective method to extract the backdoor functionality by finetuning the backdoor model on a set of intentionally mislabeled reserved clean samples, resulting in a backdoor expert model that can only recognize backdoor inputs. The authors demonstrate the effectiveness of BaDExpert across various attacks, datasets, and model architectures, and provide an adaptive study against BaDExpert, finding that it is resilient against diverse adaptive attacks, including a novelly tailored one.\n\nStrengths:\n1. The paper presents a novel and effective defense strategy against backdoor attacks, which is an important area of research in the field of DNN security.\n2. The authors provide a simple yet effective method to extract the backdoor functionality, which is a significant contribution to the field.\n3. The paper presents extensive experimental results, demonstrating the effectiveness of BaDExpert across various attacks, datasets, and model architectures.\n4. The authors provide an adaptive study against BaDExpert, finding that it is resilient against diverse adaptive attacks, including a novelly tailored one.\n\nWeaknesses:\n1. The paper assumes that the adversary does not control how the model will be further processed and deployed by the victim, which may not be a realistic assumption in practice.\n2. The authors do not provide a detailed analysis of the computational resources required to implement BaDExpert, which may be a limitation in practice.\n3. The paper does not provide a discussion on the potential vulnerabilities of BaDExpert, which may be an important area of future research.\n4. The authors do not provide a comparison of BaDExpert with other existing defenses, which may be an important area of future research.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the computational resources required to implement BaDExpert, including the computational resources required for training and deployment.\n2. The paper should provide a discussion on the potential vulnerabilities of BaDExpert, including potential attacks that may be able to evade the defense.\n3. The authors should provide a comparison of BaDExpert with other existing defenses, including a discussion of the advantages and disadvantages of each defense.\n4. The paper should provide a more detailed analysis of the effectiveness of BaDExpert against adaptive attacks, including a discussion of the potential risks and limitations of the defense.\n\nRecommendation:\nThe paper presents a novel and effective defense strategy against backdoor attacks, and the authors provide extensive experimental results demonstrating its effectiveness. However, the paper assumes that the adversary does not control how the model will be further processed and deployed by the victim, and the authors do not provide a detailed analysis of the computational resources required to implement BaDExpert. Therefore, I recommend that the authors provide a more detailed analysis of the computational resources required to implement BaDExpert, including the computational resources required for training and deployment, and provide a discussion on the potential vulnerabilities of BaDExpert, including potential attacks that may be able to evade the defense.\n$$$",
        "gr_score_with": 0.4735202492211838,
        "gr_score_without": 0.3885542168674699,
        "output_without": {
            "num_tokens_scored": 332,
            "num_green_tokens": 129,
            "green_fraction": 0.3885542168674699,
            "z_score": -0.425705179200811,
            "p_value": 0.6648386561813314
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 152,
            "green_fraction": 0.4735202492211838,
            "z_score": 2.6887713129636746,
            "p_value": 0.0035857765398581284
        }
    },
    {
        "title": "LLMS MEET VLMS: BOOST OPEN VOCABULARY OB-",
        "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to open-vocabulary object detection (OVOD) called DVDet, which leverages the strengths of vision-language models (VLMs) and large language models (LLMs) to improve the performance of OVOD. The proposed approach consists of two key designs: Conditional Context regional Prompt (CCP) and hierarchical descriptor generation. CCP transforms region embeddings into image-like representations by merging contextual background information, enabling it to be seamlessly integrated into open vocabulary detection with little extra designs. The hierarchical descriptor generation mechanism iteratively interacts with LLMs to mine and refine fine-grained descriptors according to their performance in prompt training. The authors demonstrate the effectiveness of DVDet through extensive experiments on two widely adopted benchmarks, COCO and LVIS, and show that it improves the performance of existing OVOD methods consistently.\n\nStrengths:\n\n* The proposed approach leverages the strengths of VLMs and LLMs to improve the performance of OVOD.\n* The Conditional Context regional Prompt (CCP) design is innovative and enables the seamless integration of VLMs into OVOD.\n* The hierarchical descriptor generation mechanism is effective in refining fine-grained descriptors and improving the performance of OVOD.\n* The authors provide extensive experiments on two widely adopted benchmarks, demonstrating the effectiveness of DVDet.\n* The paper is well-organized and easy to follow, with clear explanations of the proposed approach and its components.\n\nWeaknesses:\n\n* The paper assumes that the VLMs and LLMs are pre-trained and available, which may not be the case in all scenarios.\n* The authors do not provide a thorough analysis of the computational resources required for the proposed approach.\n* The paper focuses on OVOD and does not explore other open-vocabulary dense prediction tasks.\n* The authors do not provide a clear explanation of how the hierarchical descriptor generation mechanism handles the case where the LLMs are not able to generate fine-grained descriptors.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational resources required for the proposed approach.\n* Explore the applicability of the proposed approach to other open-vocabulary dense prediction tasks.\n* Provide a more thorough explanation of how the hierarchical descriptor generation mechanism handles the case where the LLMs are not able to generate fine-grained descriptors.\n* Consider providing more visualizations and qualitative results to demonstrate the effectiveness of DVDet.\n\nRecommendation:\n\nThe proposed approach is innovative and effective in improving the performance of OVOD. The authors provide extensive experiments and demonstrate the effectiveness of DVDet on two widely adopted benchmarks. However, the paper assumes that the VLMs and LLMs are pre-trained and available, which may not be the case in all scenarios. Therefore, I recommend that the authors provide a more detailed analysis of the computational resources required for the proposed approach and explore the applicability of the proposed approach to other open-vocabulary dense prediction tasks.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to Open-Vocabulary Object Detection (OVOD) called DVDet, which utilizes fine-grained descriptors to enhance the alignment between visual and textual embeddings. The proposed method consists of two key designs: Conditional Context regional Prompt (CCP) and a hierarchical descriptor generation mechanism that interacts with Large Language Models (LLMs) to refine fine-grained descriptors. The authors argue that existing OVOD methods fail to exploit the fine-grained alignment capabilities of Vision Language Models (VLMs), leading to underutilization of their knowledge. The proposed method aims to bridge this gap by introducing a feature-level visual prompt that transforms object embeddings into image-like representations and a hierarchical update mechanism that interacts with LLMs to generate fine-grained descriptors.\n\nStrengths:\n\n1. The proposed method introduces a novel approach to OVOD that leverages fine-grained descriptors to enhance the alignment between visual and textual embeddings.\n2. The authors provide a comprehensive analysis of the existing OVOD methods and their limitations, which helps to establish the need for their proposed approach.\n3. The proposed method consists of two key designs: Conditional Context regional Prompt (CCP) and a hierarchical descriptor generation mechanism that interacts with LLMs to refine fine-grained descriptors.\n4. The authors provide extensive experiments on two widely adopted benchmarks: COCO and LVIS, which demonstrate the effectiveness of their proposed method.\n\nWeaknesses:\n\n1. The proposed method relies heavily on the availability of fine-grained descriptors, which may not be available for all object categories.\n2. The authors assume that the LLMs can generate fine-grained descriptors that are relevant to the object categories, which may not always be the case.\n3. The proposed method requires a large amount of computational resources to train the model, which may be a limitation for some users.\n\nSuggestions for Improvement:\n\n1. The authors could investigate the use of other techniques to generate fine-grained descriptors, such as using other types of LLMs or incorporating prior knowledge about the object categories.\n2. The authors could provide more detailed analysis of the effectiveness of the proposed method on different object categories and scenarios.\n3. The authors could investigate the use of the proposed method on other tasks, such as image captioning or visual question answering.\n\nRecommendation:\n\nThe proposed method is a significant contribution to the field of OVOD, as it introduces a novel approach to enhance the alignment between visual and textual embeddings. The extensive experiments on two widely adopted benchmarks demonstrate the effectiveness of the proposed method. However, the proposed method relies heavily on the availability of fine-grained descriptors, which may not be available for all object categories. Therefore, the authors could investigate the use of other techniques to generate fine-grained descriptors or incorporate prior knowledge about the object categories to improve the effectiveness of the proposed method.\n\nIn conclusion, the proposed method is a promising approach to OVOD, and the authors' efforts to bridge the gap between VLMs and OVOD are well-motivated. However, the proposed method requires further investigation to address the limitations mentioned above.\n\nScore: 7.5/10\n\nReviewer: [Your Name]\n$$$\nNote: The above review is a sample peer review for the given paper. The actual peer review may vary based on the reviewer's expertise, the paper's content, and the conference's requirements.",
        "gr_score_with": 0.5036319612590799,
        "gr_score_without": 0.37790697674418605,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 130,
            "green_fraction": 0.37790697674418605,
            "z_score": -0.836428361009342,
            "p_value": 0.7985430184655419
        },
        "output_with": {
            "num_tokens_scored": 413,
            "num_green_tokens": 208,
            "green_fraction": 0.5036319612590799,
            "z_score": 4.298957209743144,
            "p_value": 8.580179818979074e-06
        }
    },
    {
        "title": "SHARING RATIO DECOMPOSITION",
        "abstract": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep learning models. SRD analyzes the model using Pointwise Feature Vectors (PFVs) and decomposes relevance with sharing ratios, accounting for nonlinear interactions between filters. The method is based on the observation of Activation-Pattern-Only Prediction (APOP), which highlights the importance of inactive neurons in model behavior. SRD is shown to outperform other saliency map methods across various metrics, including effectiveness, sophistication, and resilience, particularly in terms of robustness against random noise perturbation and targeted adversarial attacks.\n\nStrengths:\n\n* The paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods.\n* The method is based on a thorough reflection of the model's behavior, which is reflected in its robustness against random noise perturbation and targeted adversarial attacks.\n* The paper provides a clear and well-structured explanation of the method, including its mathematical formulation and experimental results.\n* The authors have conducted a comprehensive analysis, including qualitative and quantitative results, which demonstrate the superiority of SRD over other saliency map methods.\n\nWeaknesses:\n\n* The paper assumes that the sharing ratio can be derived at any point, which may not be feasible in practice.\n* The method requires prior knowledge of the sharing ratio, which may not be available in all cases.\n* The paper does not provide a detailed explanation of how the sharing ratio is calculated, which may make it difficult for readers to reproduce the results.\n* The paper does not discuss the potential limitations of the method, such as its computational complexity and scalability.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the sharing ratio is calculated and its assumptions.\n* Discuss the potential limitations of the method, such as its computational complexity and scalability.\n* Provide more examples and case studies to demonstrate the applicability of the method to different domains and scenarios.\n* Consider incorporating additional metrics and evaluation protocols to assess the robustness and reliability of the method.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods. The method is based on a thorough reflection of the model's behavior, which is reflected in its robustness against random noise perturbation and targeted adversarial attacks. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the method, and the authors have conducted a comprehensive analysis, including qualitative and quantitative results. Therefore, I recommend accepting this paper for publication in its current form, with the understanding that the authors will address the suggested improvements and limitations in future work.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep neural networks. SRD decomposes the relevance of each neuron to the model's output using sharing ratios, which reflect the different contributions of each neuron to the final decision. The method is centered on faithfully representing the model's inference process, relying solely on model-generated information, and refraining from any form of correction. SRD also introduces an interesting observation termed Activation-Pattern-Only Prediction (APOP), which highlights the significance of inactive neurons and redefines relevance encapsulating all relevant information, including both active and inactive neurons.\n\nStrengths:\n1. SRD provides a novel and comprehensive method for decomposing relevance, which is centered on faithfully representing the model's inference process.\n2. The method is centered on the vector perspective, which accounts for the intricate nonlinear interactions between filters, and provides a more accurate representation of the model's behavior.\n3. SRD also introduces the concept of Activation-Pattern-Only Prediction (APOP), which highlights the significance of inactive neurons and redefines relevance encapsulating all relevant information, including both active and inactive neurons.\n4. The method is able to provide high-resolution Effective Receptive Fields (ERFs) at any layer, which can be used to generate comprehensive explorations from local to global explanations.\n\nWeaknesses:\n1. The method is computationally expensive and may require significant computational resources to run, especially when dealing with large models and datasets.\n2. The method is also sensitive to the choice of hyperparameters, which can affect the quality of the explanations generated.\n3. The method assumes that the model is well-trained and that the sharing ratios are accurately estimated, which may not be the case in practice.\n4. The method does not provide any insights into how the model makes its decision, which is an important aspect of understanding the model's behavior.\n\nSuggestions for Improvement:\n1. Investigate ways to reduce the computational complexity of the method, such as using approximate methods or parallelizing the computations.\n2. Investigate ways to make the method more robust to the choice of hyperparameters, such as using regularization techniques or hyperparameter tuning methods.\n3. Investigate ways to provide more insights into how the model makes its decision, such as using visualization techniques or providing more detailed information about the sharing ratios.\n4. Investigate ways to extend the method to more complex models and datasets, such as models with multiple outputs or datasets with multiple labels.\n\nRecommendation:\nThis paper proposes a novel and comprehensive method for decomposing relevance, which is centered on faithfully representing the model's inference process. The method is able to provide high-resolution Effective Receptive Fields (ERFs) at any layer, which can be used to generate comprehensive explorations from local to global explanations. While the method has some limitations, it provides a promising direction for local explanation methods and can be used as a starting point for future research. Therefore, I recommend this paper for publication.\n\nRating: 8/10\n\nReviewer: [Your Name]",
        "gr_score_with": 0.5735294117647058,
        "gr_score_without": 0.4920212765957447,
        "output_without": {
            "num_tokens_scored": 376,
            "num_green_tokens": 185,
            "green_fraction": 0.4920212765957447,
            "z_score": 3.642307058612694,
            "p_value": 0.00013510274492487684
        },
        "output_with": {
            "num_tokens_scored": 408,
            "num_green_tokens": 234,
            "green_fraction": 0.5735294117647058,
            "z_score": 7.15480093857182,
            "p_value": 4.189721380300232e-13
        }
    },
    {
        "title": "PARETO DEEP LONG-TAILED RECOGNITION: A CONFLICT-AVERSE SOLUTION",
        "abstract": "Deep long-tailed recognition (DLTR) has attracted much attention due to its close touch with realistic scenarios. Recent advances have focused on re-balancing across various aspects, e.g., sampling strategy, loss re-weighting, logit adjustment, and input/parameter perturbation, etc. However, few studies have considered dynamic re-balancing to address intrinsic optimization conflicts, which are identified as prevalent and critical issues in this study. In this paper, we empirically establish the severity of the optimization conflict issue in the DLTR scenario, which leads to a degradation of representation learning. This observation serves as the motivation for pursuing Pareto optimal solutions. Unfortunately, a straightforward integration of multi-objective optimization (MOO) with DLTR methods is infeasible due to the disparity between multi-task learning (MTL) and DLTR. Therefore, we propose effective alternatives by decoupling MOO-based MTL from a temporal perspective rather than a structural one. Furthermore, we enhance the integration of MOO and DLTR by investigating the generalization and convergence problems. Specifically, we propose optimizing the variability collapse loss, guided by the derived MOObased DLTR generalization bound, to improve generalization. Additionally, we anticipate worst-case optimization to ensure convergence. Building upon the proposed MOO framework, we introduce a novel method called Pareto deep LOngTailed recognition (PLOT). Extensive evaluations demonstrate that our method not only generally improves mainstream pipelines, but also achieves an augmented version to realize state-of-the-art performance across multiple benchmarks. Code is available at https://github.com/zzpustc/PLOT.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to address the optimization conflicts in Deep Long-Tailed Recognition (DLTR) by integrating Multi-Objective Optimization (MOO) into DLTR. The authors argue that existing re-balancing strategies are not sufficient to address the intrinsic optimization conflicts among categories, which can lead to sub-optimal solutions for the remaining categories. To address this issue, the authors propose a MOO-based DLTR framework that transforms the MOO-based Multi-Task Learning (MTL) from structural to temporal, enabling the application of MOO algorithms in DLTR without model architecture modifications. The authors also propose two theoretical motivated operations, i.e., variability collapse loss and anticipating worst-case optimization, to further ensure the generalization and convergence of MOO-based DLTR.\n\nStrengths:\n\n1. The paper addresses an important issue in DLTR, i.e., optimization conflicts among categories, which has been neglected in previous studies.\n2. The authors propose a novel approach to integrate MOO into DLTR, which is a significant contribution to the field.\n3. The paper provides a thorough analysis of the benefits of addressing optimization conflicts, including representation analysis and performance improvements.\n4. The authors provide experimental results on popular DLTR benchmarks, demonstrating the effectiveness of their approach.\n\nWeaknesses:\n\n1. The paper assumes that the MOO-based DLTR framework can be applied to any DLTR model, which may not be the case in practice.\n2. The authors do not provide a detailed comparison with other re-balancing strategies, making it difficult to evaluate the effectiveness of their approach.\n3. The paper assumes that the variability collapse loss and anticipating worst-case optimization are sufficient to ensure generalization and convergence, which may not be the case in all scenarios.\n4. The authors do not provide a clear explanation of how the MOO-based DLTR framework can be applied to real-world problems.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other re-balancing strategies to evaluate the effectiveness of the MOO-based DLTR framework.\n2. Provide more experimental results on real-world datasets to demonstrate the practical applicability of the approach.\n3. Provide a more detailed explanation of how the MOO-based DLTR framework can be applied to real-world problems.\n4. Consider incorporating other re-balancing strategies, such as ensemble learning and representation learning, into the MOO-based DLTR framework.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of DLTR by addressing the optimization conflicts among categories. The proposed MOO-based DLTR framework is a novel approach that has the potential to improve the performance of DLTR models. However, the paper requires more experimental results and a more detailed comparison with other re-balancing strategies to fully evaluate its effectiveness. Therefore, I recommend that the authors conduct more experiments and provide a more detailed comparison with other re-balancing strategies to strengthen the paper.\n\nOverall, the paper is well-written and provides a clear explanation of the proposed approach. However, it requires more experimental results and a more detailed comparison with other re-balancing strategies to fully evaluate its effectiveness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called Pareto deep Long-tailed recognition (PLOT) to address the optimization conflicts in deep long-tailed recognition (DLTR). The authors argue that the existing DLTR methods suffer from intrinsic optimization conflicts among categories, which leads to sub-optimal solutions for the remaining categories. They propose to mitigate this issue by decoupling the multi-objective optimization (MOO) based multi-task learning (MTL) from a temporal perspective rather than a structural one. The proposed method, PLOT, is evaluated on several popular DLTR benchmarks and shows significant improvements in most scenarios.\n\nStrengths:\n\n* The paper provides a thorough analysis of the optimization conflicts in DLTR and proposes a novel solution to address this issue.\n* The authors demonstrate the effectiveness of the proposed method on several DLTR benchmarks and show significant improvements in most scenarios.\n* The paper contributes to the development of MOO-based methods for DLTR and provides a new perspective on the problem.\n\nWeaknesses:\n\n* The paper assumes a fixed step size for the optimization process, which may not be the best choice in practice.\n* The authors do not provide a detailed analysis of the convergence properties of the proposed method.\n* The paper relies heavily on empirical results, and it would be beneficial to have more theoretical analysis to support the claims made.\n\nSuggestions for Improvement:\n\n* Investigate the convergence properties of the proposed method and provide a detailed analysis of the optimization process.\n* Explore the use of adaptive step sizes to improve the optimization process.\n* Provide more theoretical analysis to support the claims made in the paper.\n\nRecommendation:\n\nThe paper proposes a novel method to address the optimization conflicts in DLTR and demonstrates its effectiveness on several benchmarks. However, the paper relies heavily on empirical results, and it would be beneficial to have more theoretical analysis to support the claims made. Overall, the paper is well-written, and the proposed method shows significant potential for improving the performance of DLTR models.\n\nRating: 8/10\n\nNote: The rating is based on the quality of the paper, the novelty of the proposed method, and the effectiveness of the empirical results. However, the paper has some limitations, such as the assumption of a fixed step size and the lack of detailed analysis of the convergence properties.",
        "gr_score_with": 0.4984126984126984,
        "gr_score_without": 0.4010840108401084,
        "output_without": {
            "num_tokens_scored": 369,
            "num_green_tokens": 148,
            "green_fraction": 0.4010840108401084,
            "z_score": 0.04250511420422316,
            "p_value": 0.4830480174456216
        },
        "output_with": {
            "num_tokens_scored": 315,
            "num_green_tokens": 157,
            "green_fraction": 0.4984126984126984,
            "z_score": 3.5653387232688303,
            "p_value": 0.00018169328254968796
        }
    },
    {
        "title": "RAIN: YOUR LANGUAGE MODELS CAN ALIGN THEMSELVES",
        "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%. The code is available at https://github.com/SafeAILab/RAIN.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel inference method, Rewindable Auto-regressive INference (RAIN), for aligning frozen large language models (LLMs) with human preferences without requiring additional data or fine-tuning. RAIN integrates self-evaluation and rewind mechanisms to enable LLMs to directly produce responses consistent with human preferences. The method operates without the need for external supervision, gradient computation, or parameter updates, making it a more appealing option for aligning LLMs. Experimental results demonstrate the effectiveness of RAIN in improving the harmlessness rate of LLaMA 30B from 82% to 97% on the HH dataset and enhancing the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5% on the TruthfulQA dataset.\n\nStrengths:\n\n1. RAIN offers a novel approach to aligning frozen LLMs without requiring additional data or fine-tuning, making it a more efficient and cost-effective method.\n2. The method integrates self-evaluation and rewind mechanisms, enabling LLMs to directly produce responses consistent with human preferences.\n3. RAIN operates without the need for external supervision, gradient computation, or parameter updates, reducing the computational overhead and memory requirements.\n4. The experimental results demonstrate the effectiveness of RAIN in improving the harmlessness rate of LLaMA 30B and enhancing the truthfulness of the already-well-aligned LLaMA-2-chat 13B model.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs have already been pre-trained on a large dataset, which may not be the case in all scenarios.\n2. The method relies on the quality of the self-evaluation mechanism, which may not always be accurate or reliable.\n3. The experimental results are based on a limited number of tasks and datasets, and it is unclear whether RAIN will generalize to other tasks and domains.\n4. The paper does not provide a thorough analysis of the computational overhead and time complexity of RAIN, which may be a concern for large-scale applications.\n\nSuggestions for Improvement:\n\n1. Investigate the robustness of RAIN to different types of data and tasks, and explore its generalizability to other domains.\n2. Develop a more robust and accurate self-evaluation mechanism that can handle noisy or incomplete data.\n3. Analyze the computational overhead and time complexity of RAIN and provide a detailed comparison with other alignment methods.\n4. Explore the potential of RAIN in other applications, such as text summarization, question answering, and dialogue systems.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to aligning frozen LLMs without requiring additional data or fine-tuning. While there are some limitations and areas for improvement, the experimental results demonstrate the effectiveness of RAIN in improving the harmlessness rate and truthfulness of LLMs. Therefore, I recommend accepting the paper for publication, subject to revisions addressing the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel inference method called Rewindable Auto-regressive INference (RAIN) that enables frozen large language models (LLMs) to align themselves without finetuning or using any external supervision. RAIN integrates self-evaluation and rewind mechanisms into the generation process, allowing LLMs to directly produce responses consistent with human preferences. The method is inspired by the concept of superficial alignment hypothesis, which suggests that a model's knowledge and capabilities are learned during pre-training, while alignment teaches it which sub-distribution of formats should be used. RAIN is implemented as a plug-in that can be easily integrated with existing auto-regressive language models.\n\nStrengths:\n\n1. RAIN is a novel and innovative approach to LLM alignment that enables frozen models to align themselves without finetuning or using any external supervision.\n2. The method integrates self-evaluation and rewind mechanisms into the generation process, allowing LLMs to directly produce responses consistent with human preferences.\n3. RAIN is easy to implement as a plug-in that can be integrated with existing auto-regressive language models.\n4. Experimental results demonstrate that RAIN is effective in improving the harmlessness rate of LLaMA 30B from 82% to 97% and maintaining the helpfulness rate.\n5. RAIN also shows potential in boosting adversarial robustness under static LLM-ATTACKS.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs used in the experiments are pre-trained on a large dataset and fine-tuned on a specific task, which may not be the case in all scenarios.\n2. The method requires a large number of iterations to converge, which may increase computational cost.\n3. The paper does not provide a clear explanation of how RAIN handles out-of-vocabulary (OOV) words or words with multiple meanings.\n4. The method relies on self-evaluation, which may have errors, but it is not clear how these errors are handled.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how RAIN handles OOV words or words with multiple meanings.\n2. Investigate the effect of RAIN on models that are not pre-trained on a large dataset or fine-tuned on a specific task.\n3. Provide a more detailed analysis of the computational cost of RAIN and how it can be optimized.\n4. Investigate the effect of RAIN on models with different architectures or parameter sizes.\n5. Provide a more detailed explanation of how RAIN handles errors in self-evaluation.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to LLM alignment that enables frozen models to align themselves without finetuning or using any external supervision. While there are some weaknesses in the paper, the strengths of RAIN make it a promising method for LLM alignment. I recommend this paper for publication in a top-tier conference or journal in the field of natural language processing.",
        "gr_score_with": 0.48621553884711777,
        "gr_score_without": 0.3375959079283887,
        "output_without": {
            "num_tokens_scored": 391,
            "num_green_tokens": 132,
            "green_fraction": 0.3375959079283887,
            "z_score": -2.518812423996769,
            "p_value": 0.9941124316248576
        },
        "output_with": {
            "num_tokens_scored": 399,
            "num_green_tokens": 194,
            "green_fraction": 0.48621553884711777,
            "z_score": 3.515332213047135,
            "p_value": 0.00021960209702244222
        }
    },
    {
        "title": "GRAPH NEURAL NETWORKS",
        "abstract": "Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs and guides further research on the robustness of GNNs in terms of fairness.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel fairness attack method on Graph Neural Networks (GNNs) called G-FairAttack, which aims to corrupt the fairness of GNNs while keeping the attack unnoticeable. The authors formulate the problem as a bilevel optimization problem and propose a surrogate loss function to represent different types of fairness-aware GNNs. They also propose a non-gradient optimization method to solve the problem and a fast computation technique to reduce the time complexity. The experimental results demonstrate that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable.\n\nStrengths:\n\n* The paper proposes a novel fairness attack method that targets the fairness of GNNs, which is an important aspect of fairness-aware GNNs.\n* The authors provide a thorough analysis of the challenges in attacking fairness-aware GNNs and propose a surrogate loss function to represent different types of fairness-aware GNNs.\n* The paper proposes a non-gradient optimization method to solve the problem, which is more efficient than gradient-based methods.\n* The authors also propose a fast computation technique to reduce the time complexity, which is important for large-scale graphs.\n\nWeaknesses:\n\n* The paper assumes that the attacker has access to the victim model's architecture and parameters, which may not be the case in real-world scenarios.\n* The authors do not provide a comprehensive evaluation of the attack's effectiveness on different types of GNNs and datasets.\n* The paper does not discuss the potential defenses against the proposed attack method.\n\nSuggestions for Improvement:\n\n* The authors should provide a more comprehensive evaluation of the attack's effectiveness on different types of GNNs and datasets.\n* The paper should discuss the potential defenses against the proposed attack method and provide a comparison with existing defenses.\n* The authors should provide more details about the implementation of the proposed method and the computational resources required.\n\nRecommendation:\n\nThe paper proposes a novel fairness attack method on GNNs that targets the fairness of GNNs. The authors provide a thorough analysis of the challenges in attacking fairness-aware GNNs and propose a surrogate loss function to represent different types of fairness-aware GNNs. The paper also proposes a non-gradient optimization method to solve the problem and a fast computation technique to reduce the time complexity. However, the paper assumes that the attacker has access to the victim model's architecture and parameters, which may not be the case in real-world scenarios. Therefore, the paper needs to be improved to address these limitations.\n\nOverall, the paper makes a significant contribution to the field of fairness-aware GNNs and provides a novel attack method that targets the fairness of GNNs. However, the paper needs to be improved to address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper proposes a novel framework, G-FairAttack, to attack the fairness of Graph Neural Networks (GNNs) with unnoticeable effects on prediction utility. The authors design a surrogate loss function to learn from different types of fairness-aware GNNs and propose a non-gradient attack algorithm to solve the problem. The framework also includes a fast computation approach to reduce the time complexity of the attack. The experimental results demonstrate the efficacy of the proposed method on three real-world datasets.\n\nStrengths:\n1. The paper proposes a novel framework to attack the fairness of GNNs with unnoticeable effects on prediction utility, which is a significant contribution to the field.\n2. The authors design a surrogate loss function to learn from different types of fairness-aware GNNs, which is a key component of the proposed framework.\n3. The non-gradient attack algorithm is effective in solving the problem, and the fast computation approach significantly reduces the time complexity of the attack.\n\nWeaknesses:\n1. The paper assumes that the attacker has access to the clean graph structure, which may not be the case in real-world scenarios.\n2. The experimental results only demonstrate the efficacy of the proposed method on three real-world datasets, and it is unclear how well the method performs on other datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of the proposed method.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed method and discuss potential optimizations to improve performance.\n2. The paper should include additional experimental results to demonstrate the efficacy of the proposed method on other datasets.\n3. The authors should provide a more thorough discussion of the potential vulnerabilities of the proposed method and potential countermeasures to mitigate these vulnerabilities.\n\nRecommendation:\nThe paper is well-written and the proposed framework is a significant contribution to the field of GNNs. However, the paper has some limitations, such as the assumption of access to the clean graph structure and the lack of thorough analysis of computational complexity. With additional experimental results and a more thorough discussion of potential vulnerabilities, the paper has the potential to be published in a top-tier conference.",
        "gr_score_with": 0.5036764705882353,
        "gr_score_without": 0.41947565543071164,
        "output_without": {
            "num_tokens_scored": 267,
            "num_green_tokens": 112,
            "green_fraction": 0.41947565543071164,
            "z_score": 0.6495941304630785,
            "p_value": 0.2579772127369031
        },
        "output_with": {
            "num_tokens_scored": 272,
            "num_green_tokens": 137,
            "green_fraction": 0.5036764705882353,
            "z_score": 3.4902700889927756,
            "p_value": 0.0002412663031516226
        }
    },
    {
        "title": "AN EFFICIENT MEMBERSHIP INFERENCE ATTACK FOR THE DIFFUSION MODEL BY PROXIMAL INITIALIZATION",
        "abstract": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by \u03b5 initialized in t = 0 and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries that achieve at least 6\u00d7 efficiency than the previous SOTA baseline on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-tospeech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. Code is available at https://github.com/kong13661/PIA.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes an efficient membership inference attack (MIA) method for diffusion models, called Proximal Initialization Attack (PIA). The proposed method utilizes the groundtruth trajectory obtained by \u03b5 initialized in t = 0 and predicted point to infer memberships. The authors demonstrate the effectiveness of PIA on a continuous-time diffusion model, GradTTS, and two discrete-time diffusion models, DDPM and Stable Diffusion. Experimental results indicate that PIA can achieve competitive performance with only two queries, which is much faster than the previous SOTA baseline.\n\nStrengths:\n1. The paper proposes a novel and efficient MIA method for diffusion models, which can achieve competitive performance with only two queries.\n2. The authors demonstrate the effectiveness of PIA on a continuous-time diffusion model, GradTTS, and two discrete-time diffusion models, DDPM and Stable Diffusion.\n3. The paper provides a thorough analysis of the vulnerability of models in TTS, an audio generation task, and suggests that diffusion models with the image-like output (mel-spectrogram) are more vulnerable than those with the audio output.\n\nWeaknesses:\n1. The paper assumes that the training set's samples have a smaller loss, similar to many other MIAs, which may not always hold true in practice.\n2. The authors do not provide a thorough analysis of the robustness of PIA against different types of attacks, such as adversarial attacks.\n3. The paper does not discuss the potential limitations of PIA, such as the requirement for access to intermediate outputs of diffusion models.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the robustness of PIA against different types of attacks, such as adversarial attacks.\n2. The paper should discuss the potential limitations of PIA, such as the requirement for access to intermediate outputs of diffusion models.\n3. The authors should provide more experimental results on the vulnerability of models in TTS, an audio generation task, and analyze the reasons behind the observed robustness.\n\nRecommendation:\nThe paper proposes a novel and efficient MIA method for diffusion models, which can achieve competitive performance with only two queries. The authors demonstrate the effectiveness of PIA on a continuous-time diffusion model, GradTTS, and two discrete-time diffusion models, DDPM and Stable Diffusion. However, the paper assumes that the training set's samples have a smaller loss, similar to many other MIAs, which may not always hold true in practice. Therefore, I recommend that the authors provide a more thorough analysis of the robustness of PIA against different types of attacks, discuss the potential limitations of PIA, and provide more experimental results on the vulnerability of models in TTS, an audio generation task.\n\nRating: 7/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper proposes a novel query-based membership inference attack (MIA) method for diffusion models, called Proximal Initialization Attack (PIA), and its normalized version PIAN. PIA is shown to be efficient and effective in attacking diffusion models with only two queries, outperforming the state-of-the-art baseline SecMI in terms of True Positive Rate (TPR) at a False Positive Rate (FPR) of 1%. The authors also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task and find that models with audio output are more resistant to MIA attacks than those generating image-like mel-spectrograms.\n\nStrengths:\n1. The proposed PIA method is efficient and effective in attacking diffusion models with only two queries.\n2. PIA outperforms the state-of-the-art baseline SecMI in terms of TPR at a FPR of 1%.\n3. The authors explore the robustness of diffusion models to MIA in the TTS task and provide insights into the vulnerability of different types of diffusion models.\n4. The paper provides a thorough evaluation of the proposed method on various diffusion models and datasets.\n\nWeaknesses:\n1. The paper assumes that the diffusion model is a query-based attack, which may not be the case in all scenarios.\n2. The authors do not provide a thorough comparison with other existing MIA methods.\n3. The paper focuses primarily on the TTS task and does not provide a comprehensive evaluation of the proposed method on other tasks.\n4. The results of the ablation study are not thoroughly discussed, and the impact of the hyperparameters on the performance of the proposed method is not clearly understood.\n\nSuggestions for Improvement:\n1. Provide a more thorough comparison with other existing MIA methods to demonstrate the superiority of PIA.\n2. Evaluate the proposed method on a wider range of diffusion models and tasks to demonstrate its robustness and effectiveness.\n3. Provide a more detailed discussion of the results of the ablation study and the impact of the hyperparameters on the performance of the proposed method.\n4. Consider exploring the use of PIA in other areas, such as image and video generation tasks.\n\nRecommendation:\nThis paper presents a novel and efficient query-based MIA method for diffusion models. While the evaluation is primarily focused on the TTS task, the results are promising and suggest that PIA could be a valuable tool for attacking diffusion models in other areas. To improve the paper, the authors should provide a more thorough comparison with other existing MIA methods and a more comprehensive evaluation of the proposed method on a wider range of diffusion models and tasks.",
        "gr_score_with": 0.5307262569832403,
        "gr_score_without": 0.39792387543252594,
        "output_without": {
            "num_tokens_scored": 289,
            "num_green_tokens": 115,
            "green_fraction": 0.39792387543252594,
            "z_score": -0.07204381596421215,
            "p_value": 0.528716480837209
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 190,
            "green_fraction": 0.5307262569832403,
            "z_score": 5.048922668424605,
            "p_value": 2.2215426624874608e-07
        }
    },
    {
        "title": "INTRA-CLASS INCOHERENCE",
        "abstract": "The current face recognition (FR) algorithms has achieved a high level of accuracy, making further improvements increasingly challenging. While existing FR algorithms primarily focus on optimizing margins and loss functions, limited attention has been given to exploring the feature representation space. Therefore, this paper endeavors to improve FR performance in the view of feature representation space. Firstly, we consider two FR models that exhibit distinct performance discrepancies, where one model exhibits superior recognition accuracy compared to the other. We implement orthogonal decomposition on the features from the superior model along those from the inferior model and obtain two sub-features. Surprisingly, we find the sub-feature orthogonal to the inferior still possesses a certain level of face distinguishability. We adjust the modulus of the sub-features and recombine them through vector addition. Experiments demonstrate this recombination is likely to contribute to an improved facial feature representation, even better than features from the original superior model. Motivated by this discovery, we further consider how to improve FR accuracy when there is only one FR model available. Inspired by knowledge distillation, we incorporate the intraclass incoherence constraint (IIC) to solve the problem. Experiments on various FR benchmarks show the existing state-of-the-art method with IIC can be further improved, highlighting its potential to further enhance FR performance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to improve the performance of face recognition (FR) algorithms by incorporating intra-class incoherence (IIC) into the training process. The authors analyze the feature representation space of two FR models with distinct performance discrepancies and perform orthogonal decomposition to obtain two sub-features: pro-feature and innovation. They show that innovation has a certain level of face distinguishability and can be used to improve the recognition accuracy of the superior model. The authors then propose a framework inspired by knowledge distillation to learn innovation from an existing model feature and incorporate IIC into the training process. Experiments on various FR benchmarks show that the proposed method outperforms state-of-the-art methods in most cases, particularly when a low false positive rate (FPR) is required.\n\nStrengths:\n\n1. The paper provides a clear and concise analysis of the feature representation space of FR models and identifies the potential of innovation in improving recognition accuracy.\n2. The authors propose a novel framework inspired by knowledge distillation to learn innovation from an existing model feature, which is a significant contribution to the field of FR.\n3. The experiments demonstrate the effectiveness of the proposed method in improving the recognition accuracy of state-of-the-art methods.\n4. The paper provides a comprehensive evaluation of the proposed method on various FR benchmarks, including LFW, CFP-FP, CPLFW, AgeDB, CALFW, Vggface2, and IJB-C.\n\nWeaknesses:\n\n1. The paper assumes that the superior model is available, which may not always be the case in practice. The authors should provide more information on how to handle this scenario.\n2. The paper does not provide a detailed analysis of the impact of IIC on the feature representation space of FR models.\n3. The experiments are limited to a specific set of FR benchmarks, and the authors should provide more information on the generalizability of the proposed method to other datasets.\n4. The paper does not provide a clear explanation of why the proposed method is more effective than other methods that incorporate IIC into the training process.\n\nSuggestions for Improvement:\n\n1. The authors should provide more information on how to handle the scenario where the superior model is not available.\n2. The authors should provide a more detailed analysis of the impact of IIC on the feature representation space of FR models.\n3. The authors should provide more information on the generalizability of the proposed method to other datasets.\n4. The authors should provide a clear explanation of why the proposed method is more effective than other methods that incorporate IIC into the training process.\n\nRecommendation:\n\nThe paper provides a novel approach to improving the performance of FR algorithms by incorporating IIC into the training process. The experiments demonstrate the effectiveness of the proposed method in improving the recognition accuracy of state-of-the-art methods. However, the paper assumes that the superior model is available, which may not always be the case in practice. The authors should provide more information on how to handle this scenario. Overall, the paper is well-written and provides a significant contribution to the field of FR.\n\nAcceptance:\n\nYes, the paper should be accepted for publication in the conference. However, the authors should address the weaknesses and suggestions for improvement mentioned above to make the paper more comprehensive and robust.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper presents an innovative approach to improve the performance of face recognition algorithms by introducing intra-class incoherence. The authors argue that traditional methods focus on enhancing intra-class correlation and inter-class incoherence, while their approach does the opposite by adding intra-class incoherence to improve accuracy. They propose a novel training paradigm, Intra-Class Incoherence (IIC), which incorporates the knowledge distillation framework to introduce innovation by adding intra-class irrelevant constraints. Experiments on various face recognition benchmarks demonstrate that the existing state-of-the-art method with IIC can be further improved, highlighting its potential to enhance FR performance.\n\nStrengths:\n1. The authors provide a thorough analysis of the feature representation level, which is a novel approach to improve face recognition accuracy.\n2. They propose a novel training paradigm, Intra-Class Incoherence (IIC), which is an innovative approach to improve FR performance.\n3. Experiments on various face recognition benchmarks demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n1. The authors assume that the pre-trained model is the optimal feature space, which may not always be the case.\n2. The method is only applicable to face recognition tasks, and it is not clear whether it can be extended to more general tasks and datasets.\n3. The authors do not provide a thorough comparison with existing methods, which makes it difficult to evaluate the effectiveness of the proposed method.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough comparison with existing methods to evaluate the effectiveness of the proposed method.\n2. They should investigate the applicability of the proposed method to more general tasks and datasets.\n3. They should provide more insights into the assumption that the pre-trained model is the optimal feature space.\n\nRecommendation:\nThis paper presents an innovative approach to improve the performance of face recognition algorithms by introducing intra-class incoherence. While there are some limitations, the proposed method demonstrates promising results on various face recognition benchmarks. Therefore, I recommend this paper for publication in a reputable conference or journal. However, the authors should address the limitations and suggestions for improvement to further enhance the impact of the proposed method.\n\nRating: 7/10\n$$$",
        "gr_score_with": 0.5909090909090909,
        "gr_score_without": 0.5095367847411444,
        "output_without": {
            "num_tokens_scored": 367,
            "num_green_tokens": 187,
            "green_fraction": 0.5095367847411444,
            "z_score": 4.283387846569778,
            "p_value": 9.203445046171695e-06
        },
        "output_with": {
            "num_tokens_scored": 286,
            "num_green_tokens": 169,
            "green_fraction": 0.5909090909090909,
            "z_score": 6.590282101838902,
            "p_value": 2.194958228668781e-11
        }
    },
    {
        "title": "POLYNORMER: POLYNOMIAL-EXPRESSIVE GRAPH TRANSFORMER IN LINEAR TIME",
        "abstract": "Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomialexpressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme to learn high-degree equivariant polynomials whose coefficients are controlled by attention scores. Polynormer has been evaluated on 13 homophilic and heterophilic datasets, including large graphs with millions of nodes. Our extensive experiment results show that Polynormer outperforms state-of-the-art GNN and GT baselines on most datasets, even without the use of nonlinear activation functions. Source code of Polynormer is freely available at: github.com/cornell-zhang/Polynormer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel graph transformer model called Polynormer, which is polynomial-expressive and has linear complexity. Polynormer is designed to balance the trade-off between expressivity and scalability of graph transformers. The model is built upon a novel base model that learns a high-degree polynomial on input features, and it integrates graph topology and node features separately to enable the learned polynomial to be equivariant to node permutations. The proposed model adopts a linear local-to-global attention scheme to learn high-degree equivariant polynomials, and it outperforms state-of-the-art GNN and GT baselines on multiple datasets.\n\nStrengths:\n1. The paper proposes a novel graph transformer model that is polynomial-expressive and has linear complexity, which is a significant improvement over existing models.\n2. The model is designed to balance the trade-off between expressivity and scalability of graph transformers, making it a practical solution for large-scale graph classification tasks.\n3. The paper provides a thorough analysis of the expressivity and complexity of the proposed model, which is essential for understanding its performance.\n4. The experimental results demonstrate the effectiveness of the proposed model, and it outperforms state-of-the-art GNN and GT baselines on multiple datasets.\n\nWeaknesses:\n1. The paper assumes that the input features are scalar values, which may not be the case in many real-world applications.\n2. The model is designed for node classification tasks, and it may not be suitable for other types of graph classification tasks, such as edge classification or graph regression.\n3. The paper does not provide a thorough analysis of the computational cost of the proposed model, which is essential for understanding its scalability.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the computational cost of the proposed model, including the time and space complexity.\n2. The authors should investigate the performance of the proposed model on other types of graph classification tasks, such as edge classification or graph regression.\n3. The authors should provide a more thorough analysis of the expressivity of the proposed model, including a comparison with other polynomial-expressive models.\n\nRecommendation:\nThe paper proposes a novel graph transformer model that is polynomial-expressive and has linear complexity, which is a significant improvement over existing models. The experimental results demonstrate the effectiveness of the proposed model, and it outperforms state-of-the-art GNN and GT baselines on multiple datasets. However, the paper assumes that the input features are scalar values, and it may not be suitable for other types of graph classification tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational cost and expressivity of the proposed model, and investigate its performance on other types of graph classification tasks.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Polynormer, a polynomial-expressive graph transformer that can learn high-degree polynomials on node features with linear complexity. The authors argue that previous graph transformers have limitations in terms of expressivity and scalability, and propose Polynormer as a solution that balances expressivity and scalability. Polynormer is built upon a base model that learns a high-degree polynomial, and is made permutation equivariant by integrating graph topology and node features separately. The authors demonstrate the efficacy of Polynormer through extensive experiments on 13 node classification datasets, showing that it outperforms state-of-the-art GNN and GT baselines.\n\nStrengths:\n\n* The paper provides a clear and concise introduction to the problem of balancing expressivity and scalability in graph transformers, and proposes a novel solution that is well-motivated by theory.\n* The authors provide extensive experimental results that demonstrate the efficacy of Polynormer, including comparisons with state-of-the-art GNN and GT baselines.\n* The paper is well-organized and easy to follow, with clear explanations of the Polynormer architecture and experimental setup.\n\nWeaknesses:\n\n* The paper assumes that the node features are scalar-valued, which may not be the case in many real-world graph learning applications. The authors should provide more discussion on how Polynormer can be extended to handle multi-dimensional node features.\n* The paper relies heavily on the assumption that the graph topology is known, which may not be the case in many real-world graph learning applications. The authors should provide more discussion on how Polynormer can be extended to handle unknown or dynamic graph topologies.\n* The paper does not provide a thorough analysis of the computational complexity of Polynormer, particularly in terms of the number of parameters and the computational resources required for training.\n\nSuggestions for Improvement:\n\n* Provide more discussion on how Polynormer can be extended to handle multi-dimensional node features and unknown or dynamic graph topologies.\n* Provide a more thorough analysis of the computational complexity of Polynormer, including the number of parameters and the computational resources required for training.\n* Provide more experimental results that demonstrate the robustness and generalizability of Polynormer, including comparisons with other graph learning models and analysis of the effect of different hyperparameters.\n\nRecommendation:\n\nI recommend that the authors address the weaknesses of the paper and provide more discussion on the limitations and potential extensions of Polynormer. With more thorough analysis and experimental results, the paper has the potential to make a significant contribution to the field of graph learning.\n\nGrade: B+\n\nNote: The grade is based on the quality of the paper, the clarity of the writing, and the potential impact of the work. The grade is subjective and may vary depending on the specific requirements of the conference or journal.\n\n$$",
        "gr_score_with": 0.4932249322493225,
        "gr_score_without": 0.3519163763066202,
        "output_without": {
            "num_tokens_scored": 287,
            "num_green_tokens": 101,
            "green_fraction": 0.3519163763066202,
            "z_score": -1.662771289855987,
            "p_value": 0.9518208871605521
        },
        "output_with": {
            "num_tokens_scored": 369,
            "num_green_tokens": 182,
            "green_fraction": 0.4932249322493225,
            "z_score": 3.6554398215631405,
            "p_value": 0.00012837071553341964
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. The authors also demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n\nStrengths:\n\n1. The paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n2. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n3. The authors demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n4. The paper provides a comprehensive analysis of the framework's performance and limitations, including the use of pseudo-labels and the Pathway module.\n\nWeaknesses:\n\n1. The paper relies heavily on pseudo-labels, which may not be accurate and can lead to performance loss on certain datasets.\n2. The Pathway module has limitations, including not handling multi-path scenarios, which may lead to suboptimal performance in certain cases.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, which makes it difficult to evaluate the framework's performance in a broader context.\n4. The paper assumes that the input utterance is well-formed and does not handle cases where the utterance is ambiguous or contains negations.\n\nSuggestions for Improvement:\n\n1. The authors should investigate alternative methods for collecting pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. The Pathway module should be modified to handle multi-path scenarios, such as using a graph-based approach to reason about different possibilities.\n3. The paper should provide a more thorough comparison with other state-of-the-art methods, including a detailed analysis of the framework's performance and limitations.\n4. The paper should address the assumption that the input utterance is well-formed and provide a more robust solution for handling ambiguous or negated utterances.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field. However, the paper relies heavily on pseudo-labels, which may not be accurate, and the Pathway module has limitations. To improve the framework, the authors should investigate alternative methods for collecting pseudo-labels and modify the Pathway module to handle multi-path scenarios. Additionally, the paper should provide a more thorough comparison with other state-of-the-art methods and address the assumption that the input utterance is well-formed. Overall, the paper is well-written and provides a comprehensive analysis of the framework's performance and limitations.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, termed CoT3DRef. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. The authors demonstrate that this sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. Furthermore, the proposed chain-of-thoughts module can easily be integrated into other architectures.\n\nStrengths:\n\n* The authors present a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n* The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n* The authors demonstrate the effectiveness of the framework through extensive experiments and provide a clear evaluation of the performance gains over previous state-of-the-art methods.\n* The proposed chain-of-thoughts module can easily be integrated into other architectures, making it a flexible and adaptable solution.\n\nWeaknesses:\n\n* The accuracy of the pseudo-labels plays a vital role in the overall performance, and the authors acknowledge a significant accuracy gap between automatically collected pseudo-labels and ground-truth data.\n* The pseudo-label module hinders achieving more gain on the Nr3D dataset, and the authors encourage future efforts to enhance the pseudo module performance.\n* The Pathway module limitations, such as handling the multi-path scenario, are not fully addressed in the paper.\n* The authors do not provide a clear explanation for the limited gain on the ViL baseline.\n\nSuggestions for Improvement:\n\n* The authors could provide more details on the design and implementation of the pseudo-label module, and how it can be improved to reduce the accuracy gap between automatically collected pseudo-labels and ground-truth data.\n* The Pathway module limitations could be addressed by exploring alternative methods, such as building a graph that reasons the different possibilities.\n* The authors could provide a more in-depth analysis of the limited gain on the ViL baseline and how it can be improved.\n* The paper could benefit from a more comprehensive evaluation of the framework's performance on other datasets and scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which shows promising results and demonstrates the effectiveness of the sequential approach. However, the accuracy of the pseudo-labels and the Pathway module limitations are significant concerns that need to be addressed. With some improvements and refinements, the framework has the potential to become a leading solution for 3D visual grounding tasks. I recommend acceptance of the paper for publication in a top-tier conference or journal, contingent on the authors addressing the mentioned weaknesses and suggestions for improvement.\n\n$$",
        "gr_score_with": 0.4742268041237113,
        "gr_score_without": 0.4076086956521739,
        "output_without": {
            "num_tokens_scored": 368,
            "num_green_tokens": 150,
            "green_fraction": 0.4076086956521739,
            "z_score": 0.2979397857655602,
            "p_value": 0.3828745607303309
        },
        "output_with": {
            "num_tokens_scored": 388,
            "num_green_tokens": 184,
            "green_fraction": 0.4742268041237113,
            "z_score": 2.98449602024284,
            "p_value": 0.0014202294821907234
        }
    },
    {
        "title": "ADVERSARIAL CAUSAL BAYESIAN OPTIMIZATION",
        "abstract": "In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users\u2019 demand patterns in a shared mobility system and reposition vehicles in strategic areas.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Adversarial Causal Bayesian Optimization (ACBO), a generalization of Causal Bayesian Optimization (CBO) that incorporates external events and potentially adversarial interventions on a structural causal model (SCM). The authors propose CBO with Multiplicative Weights (CBO-MW), a novel algorithm for ACBO with bounded regret. CBO-MW combines a classical online learning strategy with causal modeling of the rewards and uses optimistic counterfactual reward estimates to update its weights. The authors also propose a distributed version of CBO-MW, D-CBO-MW, which can scale to large action spaces and achieves approximate regret guarantees when rewards are monotone submodular.\n\nStrengths:\n\n* The paper provides a comprehensive overview of the existing literature on CBO and introduces a novel algorithm for ACBO with bounded regret.\n* The authors propose a distributed version of CBO-MW, D-CBO-MW, which can scale to large action spaces and achieves approximate regret guarantees when rewards are monotone submodular.\n* The paper provides a thorough analysis of the regret bound for CBO-MW and demonstrates a potentially exponential improvement in regret compared to state-of-the-art methods.\n* The authors provide empirical evidence that CBO-MW outperforms existing non-causal and non-adversarial methods on synthetic function network tasks and on an SMS rebalancing simulator based on real data.\n\nWeaknesses:\n\n* The paper assumes that the SCM is known, which may not be the case in many real-world applications.\n* The authors assume that the adversary's actions are observed a-posteriori, which may not be the case in many real-world applications.\n* The paper does not provide a thorough analysis of the computational complexity of CBO-MW and D-CBO-MW.\n* The authors do not provide a thorough comparison of CBO-MW with existing methods that can handle multiple agents and non-stationary environments.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the computational complexity of CBO-MW and D-CBO-MW.\n* The authors should provide a more thorough comparison of CBO-MW with existing methods that can handle multiple agents and non-stationary environments.\n* The authors should provide more empirical evidence to support the claims made in the paper.\n* The authors should provide a more detailed explanation of the distributed version of CBO-MW, D-CBO-MW, and how it can scale to large action spaces.\n\nRecommendation:\n\nThe paper provides a comprehensive overview of the existing literature on CBO and introduces a novel algorithm for ACBO with bounded regret. The authors propose a distributed version of CBO-MW, D-CBO-MW, which can scale to large action spaces and achieves approximate regret guarantees when rewards are monotone submodular. However, the paper assumes that the SCM is known, which may not be the case in many real-world applications. Additionally, the authors assume that the adversary's actions are observed a-posteriori, which may not be the case in many real-world applications. Therefore, the paper requires further research to address these limitations.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces Adversarial Causal Bayesian Optimization (ACBO), a generalization of Causal Bayesian Optimization (CBO) where other agents or external events intervene on the system. The authors propose a novel algorithm called Causal Bayesian Optimization with Multiplicative Weights (CBO-MW) to tackle this setting. CBO-MW combines classical online learning strategies with causal modeling of rewards and is shown to achieve a sublinear regret guarantee. Furthermore, the authors propose a distributed version of CBO-MW called D-CBO-MW, which is computationally efficient even in settings with huge action spaces. The paper also presents experiments on synthetic environments and a real-world Shared Mobility System (SMS) simulator, where CBO-MW outperforms existing non-causal and non-adversarial methods.\n\nStrengths:\n\n* The paper presents a novel approach to tackle adversarial causal Bayesian optimization, which is an important problem in many fields.\n* The authors propose a principled approach to tackle this problem, which is based on classical online learning strategies and causal modeling of rewards.\n* The paper presents a sublinear regret guarantee for CBO-MW, which is a significant improvement over existing methods.\n* The authors propose a distributed version of CBO-MW called D-CBO-MW, which is computationally efficient even in settings with huge action spaces.\n* The paper presents experiments on synthetic environments and a real-world SMS simulator, which demonstrate the effectiveness of CBO-MW.\n\nWeaknesses:\n\n* The paper assumes a known causal graph, which may not always be the case in real-world settings.\n* The authors rely on calibrated models to learn functions, which may not always be accurate in practice.\n* The paper assumes a fixed but unknown reward function, which may not always be the case in real-world settings.\n* The authors do not provide a comprehensive comparison of CBO-MW with other state-of-the-art methods in adversarial causal Bayesian optimization.\n* The paper assumes a specific type of adversaries, which may not always be the case in real-world settings.\n\nSuggestions for Improvement:\n\n* Investigate how to handle unknown causal graphs in the ACBO setting.\n* Develop methods to improve the accuracy of calibrated models in learning functions.\n* Investigate how to handle changing reward functions in the ACBO setting.\n* Provide a comprehensive comparison of CBO-MW with other state-of-the-art methods in adversarial causal Bayesian optimization.\n* Investigate how to handle different types of adversaries in the ACBO setting.\n\nRecommendation:\n\nThe paper presents a novel approach to tackle adversarial causal Bayesian optimization, which is an important problem in many fields. The authors propose a principled approach to tackle this problem, which is based on classical online learning strategies and causal modeling of rewards. The paper presents a sublinear regret guarantee for CBO-MW, which is a significant improvement over existing methods. However, the paper assumes a known causal graph, which may not always be the case in real-world settings. Therefore, I recommend that the authors investigate how to handle unknown causal graphs in the ACBO setting. Additionally, I recommend that the authors provide a comprehensive comparison of CBO-MW with other state-of-the-art methods in adversarial causal Bayesian optimization.\n\n$$$",
        "gr_score_with": 0.5110410094637224,
        "gr_score_without": 0.40445859872611467,
        "output_without": {
            "num_tokens_scored": 314,
            "num_green_tokens": 127,
            "green_fraction": 0.40445859872611467,
            "z_score": 0.16127148715443101,
            "p_value": 0.4359397896184747
        },
        "output_with": {
            "num_tokens_scored": 317,
            "num_green_tokens": 162,
            "green_fraction": 0.5110410094637224,
            "z_score": 4.035593478206885,
            "p_value": 2.723222457326524e-05
        }
    },
    {
        "title": "MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING",
        "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT4\u2019s CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper introduces MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The authors propose a novel math instruction-tuning approach, MathInstruct, which combines a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales. The authors fine-tune Llama models of different scales on MathInstruct and evaluate their performance on a range of datasets, including in-domain and out-of-domain math problems.\n\nStrengths:\n1. The authors propose a novel math instruction-tuning approach that combines CoT and PoT rationales, which is a significant contribution to the field.\n2. The authors provide a comprehensive evaluation of their model on a range of datasets, including in-domain and out-of-domain math problems.\n3. The authors demonstrate that their model outperforms existing open-source models on mathematical reasoning tasks.\n\nWeaknesses:\n1. The paper lacks a clear explanation of the architecture of the MAmmoTH model and how it is trained.\n2. The authors do not provide a detailed analysis of the impact of the hybrid CoT and PoT rationales on the model's performance.\n3. The paper does not provide a thorough comparison of the performance of the MAmmoTH model with other state-of-the-art models on a range of tasks.\n\nSuggestions for Improvement:\n1. The authors should provide a clear explanation of the architecture of the MAmmoTH model and how it is trained.\n2. The authors should provide a detailed analysis of the impact of the hybrid CoT and PoT rationales on the model's performance.\n3. The authors should provide a thorough comparison of the performance of the MAmmoTH model with other state-of-the-art models on a range of tasks.\n\nRecommendation:\nThis paper is a significant contribution to the field of natural language processing, particularly in the area of math instruction-tuning. The authors propose a novel approach that combines CoT and PoT rationales, which has the potential to improve the performance of LLMs on mathematical reasoning tasks. However, the paper lacks a clear explanation of the architecture of the MAmmoTH model and how it is trained, which makes it difficult to understand the impact of the hybrid CoT and PoT rationales on the model's performance. Therefore, I recommend that the authors provide a clear explanation of the architecture of the MAmmoTH model and how it is trained, as well as a detailed analysis of the impact of the hybrid CoT and PoT rationales on the model's performance.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper presents a novel approach to improving the mathematical reasoning capabilities of large language models (LLMs) through a hybrid instruction tuning approach, referred to as MAmmoTH. The authors introduce a new instruction tuning dataset, MathInstruct, which combines a unique blend of chain-of-thought (CoT) and program-of-thought (PoT) rationales, ensuring broad coverage of different math fields and complexity levels. By fine-tuning Llama models on MathInstruct, the authors achieve substantial accuracy gains over existing open-source models, outperforming them on nine mathematical reasoning datasets across all scales, with an average accuracy gain between 16% and 32%. The authors also introduce a simple hybrid decoding strategy, which further enhances the model's overall performance.\n\nStrengths:\n\n* The paper presents a novel approach to improving mathematical reasoning capabilities of LLMs through a hybrid instruction tuning approach.\n* The authors introduce a new instruction tuning dataset, MathInstruct, which combines a unique blend of CoT and PoT rationales, ensuring broad coverage of different math fields and complexity levels.\n* The authors achieve substantial accuracy gains over existing open-source models, outperforming them on nine mathematical reasoning datasets across all scales, with an average accuracy gain between 16% and 32%.\n* The authors introduce a simple hybrid decoding strategy, which further enhances the model's overall performance.\n\nWeaknesses:\n\n* The paper assumes that the performance gains are due to the hybrid instruction tuning approach, but it would be more convincing if the authors had conducted more thorough ablation studies to validate this claim.\n* The paper focuses primarily on mathematical reasoning tasks, but it would be more generalizable if the authors had also evaluated their approach on other tasks, such as language translation or text classification.\n* The authors do not provide a detailed analysis of the errors made by their models, which could provide valuable insights for future improvements.\n\nSuggestions for Improvement:\n\n* Conduct more thorough ablation studies to validate the claim that the hybrid instruction tuning approach is responsible for the performance gains.\n* Evaluate the approach on other tasks, such as language translation or text classification, to demonstrate its generalizability.\n* Provide a detailed analysis of the errors made by the models, which could provide valuable insights for future improvements.\n\nRecommendation:\n\nThis paper presents a novel approach to improving mathematical reasoning capabilities of LLMs through a hybrid instruction tuning approach. While the approach shows promising results, it would benefit from more thorough ablation studies and evaluations on other tasks to validate its generalizability. Nevertheless, the paper provides valuable insights for future studies on improving LLMs' core capabilities in specialized domains.\n\n$$$",
        "gr_score_with": 0.4267515923566879,
        "gr_score_without": 0.30115830115830117,
        "output_without": {
            "num_tokens_scored": 259,
            "num_green_tokens": 78,
            "green_fraction": 0.30115830115830117,
            "z_score": -3.2470162523637836,
            "p_value": 0.9994168913500238
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 134,
            "green_fraction": 0.4267515923566879,
            "z_score": 0.967628922926591,
            "p_value": 0.16661486356535915
        }
    },
    {
        "title": "LEFTOVER-LUNCH: ADVANTAGE-BASED OFFLINE REINFORCEMENT LEARNING FOR LANGUAGE MODELS",
        "abstract": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LOL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LOL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM\u2019s value estimate, A-LOL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LOL is an easy-to-implement, sample-efficient, and stable LM training recipe. We demonstrate the effectiveness of A-LOL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LOL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LOL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Advantage-Leftover Lunch RL (A-LOL), a new class of offline policy gradient algorithms that enable RL training on pre-existing data without requiring new high-quality LM-generated data. A-LOL assumes the entire output sequence as a single action and incorporates sequence-level classifiers or human-designed scoring functions as rewards. The method uses the reference LM's value estimate to filter out unfavorable data points and trains on positive advantage data. The authors demonstrate the effectiveness of A-LOL and its variants on four different language generation tasks, achieving higher rewards and diversity compared to other preference-based and reward-based offline RL methods.\n\nStrengths:\n\n* A-LOL is easy to implement and requires minimal modifications to the standard cross-entropy loss.\n* The method is sample-efficient and stable, making it suitable for large-scale language model training.\n* A-LOL's ability to filter out unfavorable data points makes it resilient to noise and allows it to extract useful training signals from suboptimal data.\n* The authors demonstrate the effectiveness of A-LOL on multiple language generation tasks, including those with multiple rewards.\n\nWeaknesses:\n\n* The paper assumes that the reference LM's value estimate is a good proxy for the true reward, which may not always be the case.\n* The method may not perform well if the reference LM is not well-trained or if the rewards are not well-defined.\n* The authors do not provide a detailed analysis of the computational cost of training A-LOL, which may be a concern for large-scale language model training.\n\nSuggestions for Improvement:\n\n* The authors should provide more detailed analysis of the computational cost of training A-LOL and compare it to other offline RL methods.\n* The paper could benefit from a more detailed discussion of the assumptions made about the reference LM's value estimate and how they impact the performance of A-LOL.\n* The authors could explore other variants of A-LOL that use different importance weighting schemes or value estimates to improve the method's robustness and performance.\n\nRecommendation:\n\nI recommend accepting this paper for publication in a top-tier conference on reinforcement learning or natural language processing. The paper presents a novel and effective method for offline policy gradient algorithms that addresses the limitations of existing offline RL methods. The experimental results demonstrate the effectiveness of A-LOL on multiple language generation tasks, and the method has the potential to be widely adopted in the field. However, the authors should address the limitations and suggestions for improvement mentioned above to further strengthen the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces Advantage-Leftover Lunch RL (A-LOL), a new class of offline policy gradient algorithms that enable reinforcement learning (RL) training on any pre-existing data. A-LOL assumes the entire output sequence as a single action, allowing it to calculate training data advantage and filter unfavorable instances. The method uses the reference LM's value estimate to discard negative advantage data points, making it robust to noise. A-LOL is easy to implement, sample-efficient, and stable, and is demonstrated to be effective on four different language generation tasks.\n\nStrengths:\n\n1. A-LOL introduces a new class of offline policy gradient algorithms that enable RL training on pre-existing data.\n2. The method assumes the entire output sequence as a single action, allowing it to calculate training data advantage and filter unfavorable instances.\n3. A-LOL uses the reference LM's value estimate to discard negative advantage data points, making it robust to noise.\n4. The method is easy to implement, sample-efficient, and stable.\n5. A-LOL demonstrates effective performance on four different language generation tasks.\n\nWeaknesses:\n\n1. The paper assumes that the reference LM is well-trained, which may not always be the case.\n2. The method relies on the quality of the reference LM's value estimate, which may not be accurate in all cases.\n3. A-LOL does not address the issue of mode collapse, which can occur when the reference LM is not well-trained.\n4. The paper does not provide a thorough analysis of the computational cost of A-LOL compared to other methods.\n5. The method assumes that the input and output sequences are well-defined, which may not always be the case.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational cost of A-LOL compared to other methods.\n2. Address the issue of mode collapse by incorporating techniques such as early stopping or regularization.\n3. Provide a more thorough evaluation of the method on tasks with different input and output sequences.\n4. Investigate the effect of different reference LM architectures on the performance of A-LOL.\n5. Provide a more detailed analysis of the robustness of A-LOL to different types of noise.\n\nRecommendation:\n\nThe paper presents a new class of offline policy gradient algorithms that enable RL training on pre-existing data. The method is easy to implement, sample-efficient, and stable, and demonstrates effective performance on four different language generation tasks. However, there are some limitations and areas for improvement that should be addressed in future work. Overall, I recommend accepting the paper with minor revisions to address these issues.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.3525641025641026,
        "gr_score_without": 0.3131578947368421,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 119,
            "green_fraction": 0.3131578947368421,
            "z_score": -3.455544743406488,
            "p_value": 0.9997254089830042
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 110,
            "green_fraction": 0.3525641025641026,
            "z_score": -1.710325605027791,
            "p_value": 0.9563971602546465
        }
    },
    {
        "title": "SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-ofthe-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces SWE-bench, a novel benchmark for evaluating the capabilities of language models (LMs) in software engineering tasks. SWE-bench consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The benchmark tasks require LMs to edit codebases to address specific issues, often involving complex reasoning, context processing, and code editing. The paper evaluates several state-of-the-art LMs, including proprietary models and fine-tuned models, and finds that they struggle to solve even the simplest issues, with the best-performing model, Claude 2, able to resolve only 1.96% of the issues.\n\nStrengths:\n\n1. Novel benchmark: SWE-bench is a unique and challenging benchmark that pushes the limits of LMs in software engineering tasks.\n2. Real-world relevance: The benchmark is based on real-world GitHub issues and pull requests, making it highly relevant to software engineering practice.\n3. Comprehensive evaluation: The paper evaluates multiple LMs, including proprietary and fine-tuned models, providing a comprehensive assessment of their capabilities.\n4. Detailed analysis: The paper provides a thorough analysis of the results, including a discussion of the difficulties faced by LMs in software engineering tasks.\n\nWeaknesses:\n\n1. Limited scope: The paper focuses on Python repositories, which may limit the generalizability of the results to other programming languages.\n2. Evaluation metrics: The paper uses a single metric, percentage of issues resolved, which may not capture the full range of LM capabilities.\n3. Lack of human evaluation: The paper does not include human evaluation of the LM-generated patches, which could provide additional insights into their quality and effectiveness.\n4. Limited discussion of future work: The paper does not discuss potential future directions for research, such as improving LM performance or exploring new software engineering tasks.\n\nSuggestions for Improvement:\n\n1. Expand the scope of the benchmark: Consider including repositories from other programming languages to increase the generalizability of the results.\n2. Develop additional evaluation metrics: Consider using metrics such as patch quality, code maintainability, or developer productivity to provide a more comprehensive assessment of LM capabilities.\n3. Include human evaluation: Conduct human evaluation of the LM-generated patches to provide additional insights into their quality and effectiveness.\n4. Discuss future work: Provide a discussion of potential future directions for research, such as improving LM performance or exploring new software engineering tasks.\n\nRecommendation:\n\nThe paper presents a novel and challenging benchmark for evaluating the capabilities of LMs in software engineering tasks. While there are some limitations, the paper provides a comprehensive evaluation of multiple LMs and highlights the difficulties faced by LMs in software engineering tasks. The paper is well-written and provides a clear and concise presentation of the results. Overall, the paper is a valuable contribution to the field of software engineering and language models, and it has the potential to inspire future research in this area.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes SWE-bench, a new benchmark for evaluating language models (LMs) in software engineering tasks. SWE-bench consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The benchmark requires models to generate patches describing changes to apply to the existing codebase to resolve issues. The authors evaluate multiple state-of-the-art LMs on SWE-bench and find that they fail to solve all except the simplest issues. The best-performing model, Claude 2, is able to solve only 1.96% of the issues. The authors also release a fine-tuned model, SWE-Llama, and provide detailed analysis of its performance.\n\nStrengths:\n\n* The authors provide a comprehensive analysis of the performance of various LMs on SWE-bench, highlighting the challenges of software engineering tasks.\n* The benchmark is well-structured and includes detailed information about the construction process, evaluation pipeline, and characterization of the task instances.\n* The authors provide a fine-tuned model, SWE-Llama, which demonstrates competitive performance with Claude 2.\n* The analysis of the performance of SWE-Llama on different retrieval mechanisms and prompting styles is insightful and highlights the challenges of software engineering tasks.\n\nWeaknesses:\n\n* The paper focuses primarily on the performance of LMs on SWE-bench, but does not provide a comprehensive analysis of the challenges of software engineering tasks.\n* The authors do not provide a detailed explanation of the construction process of SWE-bench, making it difficult to replicate the benchmark.\n* The paper does not provide a clear explanation of the evaluation metrics used to evaluate the performance of LMs.\n* The analysis of the performance of SWE-Llama on different retrieval mechanisms and prompting styles is limited to a specific set of models and does not provide a comprehensive overview of the challenges of software engineering tasks.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the construction process of SWE-bench, including the selection of repositories, issues, and pull requests.\n* Provide a clear explanation of the evaluation metrics used to evaluate the performance of LMs.\n* Conduct a more comprehensive analysis of the challenges of software engineering tasks, including the analysis of the performance of SWE-Llama on different retrieval mechanisms and prompting styles.\n* Provide a more detailed explanation of the fine-tuning process of SWE-Llama and its performance on different models.\n\nRecommendation:\n\nThe paper proposes a new benchmark for evaluating LMs in software engineering tasks and provides a comprehensive analysis of the performance of various LMs on SWE-bench. The authors also release a fine-tuned model, SWE-Llama, which demonstrates competitive performance with Claude 2. The analysis of the performance of SWE-Llama on different retrieval mechanisms and prompting styles is insightful and highlights the challenges of software engineering tasks. However, the paper has some limitations, including a lack of detailed explanation of the construction process of SWE-bench and the evaluation metrics used to evaluate the performance of LMs. Therefore, I recommend that the authors provide a more detailed explanation of the construction process of SWE-bench, provide a clear explanation of the evaluation metrics used to evaluate the performance of LMs, conduct a more comprehensive analysis of the challenges of software engineering tasks, and provide a more detailed explanation of the fine-tuning process of SWE-Llama.",
        "gr_score_with": 0.5015873015873016,
        "gr_score_without": 0.41379310344827586,
        "output_without": {
            "num_tokens_scored": 406,
            "num_green_tokens": 168,
            "green_fraction": 0.41379310344827586,
            "z_score": 0.567308628931175,
            "p_value": 0.2852522577518982
        },
        "output_with": {
            "num_tokens_scored": 315,
            "num_green_tokens": 158,
            "green_fraction": 0.5015873015873016,
            "z_score": 3.680349649825889,
            "p_value": 0.00011645717960200429
        }
    },
    {
        "title": "SEER: LANGUAGE INSTRUCTED VIDEO PREDICTION WITH LATENT DIFFUSION MODELS",
        "abstract": "Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-toimage (T2I) stable diffusion models along the temporal axis. We enhance the U-Net and language conditioning model by incorporating computation-efficient spatial-temporal attention. Furthermore, we introduce a novel Frame Sequential Text Decomposer module that dissects a sentence\u2019s global instruction into temporally aligned sub-instructions, ensuring precise integration into each frame of generation. Our framework allows us to effectively leverage the extensive prior knowledge embedded in pretrained T2I models across the frames. With the adaptable-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2), Bridgedata and EpicKitchens-100 datasets demonstrate our superior video prediction performance with around 480-GPU hours versus CogVideo with over 12,480-GPU hours: achieving the 31% FVD improvement compared to the current SOTA model on SSv2 and 83.7% average preference in the human evaluation. Our project is available at https://seervideodiffusion.github.io/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel text-conditioned video prediction (TVP) model, named Seer, which leverages pre-trained text-to-image (T2I) stable diffusion models to generate high-quality videos by fine-tuning a few layers on a small amount of data. Seer consists of two main components: a video diffusion module and a language conditioning module. The video diffusion module is a computation-efficient 3D network that models spatial dependencies and temporal dynamics simultaneously, while the language conditioning module decomposes global language instructions into fine-grained sub-instructions that are synchronized with video. The authors demonstrate the effectiveness of Seer on three text-video datasets and achieve state-of-the-art performance in terms of FVD and KVD metrics.\n\nStrengths:\n\n1. The paper proposes a novel and efficient approach to TVP, which leverages pre-trained T2I models and reduces the data and computation costs.\n2. The authors demonstrate the effectiveness of Seer on three text-video datasets and achieve state-of-the-art performance in terms of FVD and KVD metrics.\n3. The paper provides a comprehensive evaluation of Seer, including quantitative and qualitative results, as well as ablation studies.\n4. The authors provide a clear and concise explanation of the method and its components, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained T2I models are available, which may not be the case in all scenarios.\n2. The authors do not provide a detailed comparison with other TVP models, which makes it difficult to evaluate the novelty and effectiveness of Seer.\n3. The paper focuses on the TVP task, but it would be interesting to explore other applications of Seer, such as video manipulation or generation.\n4. The authors do not provide a detailed analysis of the computational costs and scalability of Seer.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other TVP models to evaluate the novelty and effectiveness of Seer.\n2. Explore other applications of Seer, such as video manipulation or generation.\n3. Provide a detailed analysis of the computational costs and scalability of Seer.\n4. Consider providing a more detailed explanation of the FSText Decomposer module and its components.\n\nRecommendation:\n\nThe paper proposes a novel and efficient approach to TVP, which leverages pre-trained T2I models and reduces the data and computation costs. The authors demonstrate the effectiveness of Seer on three text-video datasets and achieve state-of-the-art performance in terms of FVD and KVD metrics. While there are some limitations and areas for improvement, the paper is well-written and provides a comprehensive evaluation of Seer. Therefore, I recommend accepting this paper for publication.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel text-conditioned video prediction (TVP) model, named Seer, that leverages pre-trained text-to-image (T2I) stable diffusion models to generate high-quality videos from language instructions. The model is designed to be sample and computation-efficient, and it achieves state-of-the-art performance in TVP tasks. The authors introduce a novel Frame Sequential Text (FSText) Decomposer module that decomposes global language instructions into fine-grained sub-instructions, which are then used to guide video generation. The model is evaluated on three text-video datasets and achieves significant improvements over existing methods in terms of Fre\u0301chet Video Distance (FVD) and Kernel Video Distance (KVD) metrics.\n\nStrengths:\n\n1. The paper presents a novel approach to TVP that leverages pre-trained T2I models, which is a significant improvement over existing methods.\n2. The authors introduce a novel FSText Decomposer module that effectively decomposes global language instructions into fine-grained sub-instructions.\n3. The model achieves state-of-the-art performance in TVP tasks on three text-video datasets.\n4. The authors provide a thorough evaluation of their model, including quantitative and qualitative results, as well as a human evaluation study.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained T2I models, which may not be suitable for all TVP tasks.\n2. The authors do not provide a clear explanation of how the FSText Decomposer module is trained and fine-tuned.\n3. The paper does not provide a comprehensive comparison with existing methods in terms of computational efficiency.\n4. The human evaluation study is limited to a small number of video clips and may not be representative of the overall performance of the model.\n\nSuggestions for Improvement:\n\n1. The authors should provide more details on how the pre-trained T2I models are fine-tuned and adapted for TVP tasks.\n2. The authors should provide a more comprehensive evaluation of the computational efficiency of their model compared to existing methods.\n3. The authors should provide more information on how the FSText Decomposer module is trained and fine-tuned.\n4. The authors should consider including more video clips in the human evaluation study to improve the representativeness of the results.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to TVP that leverages pre-trained T2I models and a novel FSText Decomposer module. The model achieves state-of-the-art performance in TVP tasks on three text-video datasets and provides a thorough evaluation of its performance. While there are some limitations and areas for improvement, the paper makes a significant contribution to the field of TVP and is worthy of consideration for publication.\n\nRating: 8/10",
        "gr_score_with": 0.5925925925925926,
        "gr_score_without": 0.4941860465116279,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 170,
            "green_fraction": 0.4941860465116279,
            "z_score": 3.5658261706187773,
            "p_value": 0.0001813558687554311
        },
        "output_with": {
            "num_tokens_scored": 351,
            "num_green_tokens": 208,
            "green_fraction": 0.5925925925925926,
            "z_score": 7.365250408522912,
            "p_value": 8.840732524525713e-14
        }
    },
    {
        "title": "CONVOLUTION MEETS LORA: PARAMETER EFFI-",
        "abstract": "The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM\u2019s local prior assumption. Notably, Conv-LoRA not only preserves SAM\u2019s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM\u2019s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores ConvLoRA\u2019s superiority in adapting SAM to real-world semantic segmentation tasks.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Conv-LoRA, a novel parameter-efficient fine-tuning (PEFT) approach for adapting the Segment Anything Model (SAM) to downstream semantic segmentation tasks. Conv-LoRA integrates lightweight convolutional parameters into Low-Rank Adaptation (LoRA) to inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. The authors demonstrate that Conv-LoRA consistently outperforms other PEFT techniques across diverse benchmarks spanning multiple domains.\n\nStrengths:\n\n1. The paper presents a novel and effective PEFT approach, Conv-LoRA, which demonstrates promising results on multiple domains.\n2. The authors provide a comprehensive analysis of SAM's limitations and shed light on its pretraining's impact on the model's ability to learn high-level semantic information.\n3. The paper includes an extensive benchmark encompassing diverse domains, including natural images, agriculture, remote sensing, and healthcare.\n4. The authors compare Conv-LoRA with various state-of-the-art PEFT methods, demonstrating its superiority in adapting SAM to real-world semantic segmentation tasks.\n\nWeaknesses:\n\n1. The paper's focus on developing a general PEFT method for SAM might limit its applicability to specific domains where more tailored approaches may be necessary.\n2. The authors do not provide a detailed analysis of the computational costs associated with Conv-LoRA, which may be a concern for large-scale deployments.\n3. The paper relies heavily on existing literature and does not provide a clear explanation of the theoretical foundations underlying Conv-LoRA.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational costs associated with Conv-LoRA and discuss potential strategies for reducing them.\n2. The paper could benefit from a more in-depth discussion of the theoretical foundations underlying Conv-LoRA, including the implications of injecting lightweight convolutional parameters into LoRA.\n3. The authors should consider exploring the applicability of Conv-LoRA to specific domains where more tailored approaches may be necessary.\n\nRecommendation:\n\nThe paper presents a novel and effective PEFT approach, Conv-LoRA, which demonstrates promising results on multiple domains. While there are some limitations to the paper, the authors' contributions and the comprehensive benchmark make it a valuable addition to the field of semantic segmentation. I recommend accepting the paper for publication, with the suggestion that the authors address the weaknesses and provide additional analysis as mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces Conv-LoRA, a novel parameter-efficient fine-tuning approach for adapting the Segment Anything Model (SAM) to downstream semantic segmentation tasks. SAM is a foundation model that exhibits remarkable zero-shot generalization but struggles with specialized domains such as medical imagery and remote sensing. Conv-LoRA combines ultra-lightweight convolutional parameters with Low-Rank Adaptation (LoRA) to inject image-related inductive biases into the plain ViT encoder of SAM. This approach not only preserves SAM's extensive segmentation knowledge but also revives its capacity to learn high-level image semantics.\n\nStrengths:\n\n* Conv-LoRA is a simple yet effective parameter-efficient fine-tuning approach that adapts SAM to downstream semantic segmentation tasks.\n* It combines ultra-lightweight convolutional parameters with LoRA to inject image-related inductive biases into the plain ViT encoder of SAM.\n* Conv-LoRA preserves SAM's extensive segmentation knowledge and revives its capacity to learn high-level image semantics.\n* It outperforms other PEFT techniques across diverse datasets from different domains.\n\nWeaknesses:\n\n* The paper does not provide a clear comparison with domain-specific models that have been tailored to specific domains.\n* Some of the experimental results, such as the comparison with domain-specific methods, may not be as robust as they could be.\n* The paper could benefit from more detailed explanations of the LoRA and Conv-LoRA architectures.\n\nSuggestions for Improvement:\n\n* Provide a clear comparison with domain-specific models that have been tailored to specific domains.\n* Conduct more robust experiments to demonstrate the superiority of Conv-LoRA over domain-specific methods.\n* Provide more detailed explanations of the LoRA and Conv-LoRA architectures.\n\nRecommendation:\n\nThis paper presents a promising approach to adapting SAM to downstream semantic segmentation tasks using Conv-LoRA. However, to further strengthen the paper, the authors should address the weaknesses mentioned above. With these improvements, Conv-LoRA could become a widely adopted approach for adapting SAM to downstream tasks.\n\nRating: 7/10\n\nReviewers should consider this paper for publication if they believe that the strengths of Conv-LoRA outweigh its weaknesses and that the suggested improvements will be made to further strengthen the paper.\n\n$$$",
        "gr_score_with": 0.5369774919614148,
        "gr_score_without": 0.3937677053824363,
        "output_without": {
            "num_tokens_scored": 353,
            "num_green_tokens": 139,
            "green_fraction": 0.3937677053824363,
            "z_score": -0.2390175042285993,
            "p_value": 0.5944539951703993
        },
        "output_with": {
            "num_tokens_scored": 311,
            "num_green_tokens": 167,
            "green_fraction": 0.5369774919614148,
            "z_score": 4.93087262288848,
            "p_value": 4.093155129055611e-07
        }
    },
    {
        "title": "FORMER FOR PHOTOREALISTIC TEXT-TO-IMAGE SYNTHESIS",
        "abstract": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-\u03b1, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024 \u00d7 1024 resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that respectively optimize pixel dependency, textimage alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-\u03b1\u2019s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART\u03b1 only takes 12% of Stable Diffusion v1.5\u2019s training time (\u223c753 vs. \u223c6,250 A100 GPU days), saving nearly $300,000 ($28,400 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-\u03b1 excels in image quality, artistry, and semantic control. We hope PIXART-\u03b1 will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces PIXART-\u03b1, a Transformer-based text-to-image (T2I) diffusion model that achieves competitive image generation quality with state-of-the-art image generators while significantly reducing training costs and CO2 emissions. The authors propose three core designs: (1) Training strategy decomposition, which decomposes the text-to-image generation task into three streamlined subtasks; (2) Efficient T2I Transformer, which incorporates cross-attention modules and reparameterization techniques to improve efficiency; and (3) High-informative data, which utilizes an autolabeling pipeline to generate high-quality text-image pairs with high concept density. The authors demonstrate that PIXART-\u03b1 achieves near-commercial application standards in image generation quality, with a training cost of only 753 A100 GPU days and $28,400.\n\nStrengths:\n\n1. The paper proposes a novel and efficient approach to text-to-image synthesis, which significantly reduces training costs and CO2 emissions.\n2. The authors provide a thorough analysis of the strengths and weaknesses of existing T2I models and highlight the importance of text-image alignment learning.\n3. The proposed training strategy decomposition and efficient T2I Transformer are innovative and effective designs that contribute to the success of PIXART-\u03b1.\n4. The paper provides a comprehensive evaluation of PIXART-\u03b1 using various metrics, including FID, compositionality, and human-preference rate.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of T2I models and diffusion models, which may make it difficult for non-experts to understand the paper.\n2. The authors do not provide a detailed comparison with other T2I models, which makes it difficult to evaluate the novelty and impact of the proposed approach.\n3. The paper does not provide a clear explanation of the reparameterization technique used in the efficient T2I Transformer, which may make it difficult for readers to understand the design.\n4. The authors do not provide a detailed analysis of the computational resources required for training PIXART-\u03b1, which may make it difficult to evaluate the scalability of the approach.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other T2I models to evaluate the novelty and impact of the proposed approach.\n2. Provide a clear explanation of the reparameterization technique used in the efficient T2I Transformer.\n3. Provide a detailed analysis of the computational resources required for training PIXART-\u03b1.\n4. Consider providing more experimental results to evaluate the robustness and generalizability of PIXART-\u03b1.\n\nRecommendation:\n\nThe paper proposes a novel and efficient approach to text-to-image synthesis, which significantly reduces training costs and CO2 emissions. The authors provide a thorough analysis of the strengths and weaknesses of existing T2I models and highlight the importance of text-image alignment learning. While there are some limitations to the paper, the proposed approach is innovative and effective, and the paper provides a comprehensive evaluation of PIXART-\u03b1 using various metrics. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents PIXART-\u03b1, a novel text-to-image (T2I) diffusion model that achieves competitive image generation quality while significantly reducing training costs and CO2 emissions. The authors propose three core designs: training strategy decomposition, efficient T2I Transformer, and high-informative data. These designs enable PIXART-\u03b1 to surpass existing large-scale T2I models in terms of efficiency and effectiveness.\n\nStrengths:\n\n1. **Innovative approaches**: The authors propose three novel approaches that significantly reduce the computational demands of training while maintaining competitive image generation quality.\n2. **Comprehensive evaluations**: The authors provide extensive experiments and evaluations, including user studies, ablation studies, and comparisons with state-of-the-art methods.\n3. **Real-world applications**: The authors demonstrate the potential of PIXART-\u03b1 for real-world applications, such as image generation and text-to-image synthesis.\n\nWeaknesses:\n\n1. **Limited generalizability**: The authors' results are based on a specific dataset and may not generalize to other datasets or applications.\n2. **Training requirements**: While the authors claim that PIXART-\u03b1 reduces training costs and CO2 emissions, the model still requires significant computational resources and may not be accessible to all researchers or developers.\n3. **Lack of interpretability**: The authors do not provide insights into the interpretability of PIXART-\u03b1's results, which may make it challenging to understand and replicate the model.\n\nSuggestions for Improvement:\n\n1. **Generalizability studies**: Conduct studies to evaluate the generalizability of PIXART-\u03b1 across different datasets and applications.\n2. **Training requirements**: Investigate methods to reduce the computational requirements of PIXART-\u03b1 or make it more accessible to a broader audience.\n3. **Interpretability studies**: Conduct studies to provide insights into the interpretability of PIXART-\u03b1's results and develop methods to facilitate understanding and replication.\n\nRecommendation:\n\nThis paper presents a novel and innovative approach to text-to-image synthesis, achieving competitive image generation quality while reducing training costs and CO2 emissions. While the results are promising, further studies are needed to evaluate the generalizability and interpretability of the model. With improvements in these areas, PIXART-\u03b1 has the potential to become a valuable tool for the AIGC community and beyond.\n\n$$$",
        "gr_score_with": 0.48214285714285715,
        "gr_score_without": 0.353099730458221,
        "output_without": {
            "num_tokens_scored": 371,
            "num_green_tokens": 131,
            "green_fraction": 0.353099730458221,
            "z_score": -1.843981979786944,
            "p_value": 0.9674071167926619
        },
        "output_with": {
            "num_tokens_scored": 336,
            "num_green_tokens": 162,
            "green_fraction": 0.48214285714285715,
            "z_score": 3.0735042819928795,
            "p_value": 0.0010578032579264993
        }
    },
    {
        "title": "ON DIFFERENTIALLY PRIVATE FEDERATED LINEAR CONTEXTUAL BANDITS",
        "abstract": "We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user\u2019s privacy. We identify three issues in the state-of-the-art (Dubey & Pentland, 2020): (i) failure of claimed privacy protection, (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. To resolve these issues, we take a two-step approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential privacy, under which we show that our algorithm can achieve nearly \u201coptimal\u201d regret without a trusted server. We accomplish this via two different schemes \u2013 one relies on a new result on privacy amplification via shuffling for DP mechanisms and another one leverages the integration of a shuffle protocol for vector sum into the tree-based mechanism, both of which might be of independent interest. Finally, we support our theoretical results with numerical evaluations over contextual bandit instances generated from both synthetic and real-life data.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to address the challenges of private and federated linear contextual bandits (LCBs) under differential privacy. The authors identify three issues in the state-of-the-art algorithm, Dubey & Pentland (2020), namely, failure of claimed privacy protection, incorrect regret bound due to noise miscalculation, and ungrounded communication cost. To resolve these issues, the authors propose a two-step approach: (i) design a generic algorithmic and analytical framework, and (ii) prove regret guarantees under different privacy notions. The proposed framework consists of a generic federated LCB algorithm and a flexible privacy protocol. The authors establish privacy and regret guarantees under silo-level local differential privacy (LDP) and shuffle differential privacy (SDP). They also provide numerical evaluations of the proposed algorithm on synthetic and real-life data.\n\nStrengths:\n\n1. The paper addresses a significant challenge in the field of federated learning and private contextual bandits.\n2. The authors provide a thorough analysis of the state-of-the-art algorithm and identify its limitations.\n3. The proposed framework is generic and can be applied to other problems in federated learning.\n4. The authors provide a comprehensive comparison with existing results in the literature.\n5. The paper presents a novel approach to achieve nearly \"optimal\" regret without a trusted server.\n\nWeaknesses:\n\n1. The paper assumes a specific adversary model, which may not be realistic in practice.\n2. The authors assume that all MT users are unique, which may not be the case in practice.\n3. The paper does not provide a detailed analysis of the communication cost of the proposed algorithm.\n4. The authors assume a fixed batch size, which may not be optimal in practice.\n5. The paper does not provide a clear comparison with other private contextual bandit algorithms.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the communication cost of the proposed algorithm.\n2. Consider a more realistic adversary model that includes malicious users within the same silo.\n3. Investigate the performance of the proposed algorithm under different batch sizes.\n4. Provide a clear comparison with other private contextual bandit algorithms.\n5. Consider extending the proposed framework to other problems in federated learning.\n\nRecommendation:\n\nThe paper presents a novel and significant contribution to the field of federated learning and private contextual bandits. The proposed framework is generic and can be applied to other problems in federated learning. However, the paper assumes a specific adversary model and does not provide a detailed analysis of the communication cost. Therefore, I recommend that the authors address these limitations in future work. With some revisions, the paper has the potential to be published in a top-tier conference in the field of machine learning and artificial intelligence.\n\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$$\n$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents an improved algorithm for private, federated linear contextual bandits (LCBs). The authors identify three issues with the state-of-the-art algorithm, including failure of claimed privacy protection, incorrect regret bound, and ungrounded communication cost. To resolve these issues, the authors propose a two-step approach, first designing a generic algorithmic framework consisting of a federated LCB algorithm and flexible privacy protocols, and then studying federated LCBs under two different privacy constraints, including silo-level local differential privacy (LDP) and shuffle differential privacy (SDP). The authors demonstrate that the proposed algorithm achieves correct privacy, regret, and communication results under silo-level LDP and SDP, and show that the privacy cost of federated LCBs under SDP matches that of a super single agent under central differential privacy.\n\nStrengths:\n\n* The paper presents a comprehensive analysis of the state-of-the-art algorithm for private, federated linear contextual bandits, identifying three key issues and proposing a novel solution to resolve these issues.\n* The authors propose a flexible privacy protocol that can be used under both silo-level LDP and SDP, allowing for adaptability to different privacy constraints.\n* The paper demonstrates that the proposed algorithm achieves correct privacy, regret, and communication results under both silo-level LDP and SDP, providing a strong theoretical foundation for the approach.\n\nWeaknesses:\n\n* The paper assumes that the data at each silo is a stream of unique users, which may not hold in practice. The authors note this assumption in the concluding remarks, but it would be beneficial to discuss this assumption more thoroughly in the main text.\n* The paper assumes that the feature vectors are bounded, which may not hold in practice. The authors note this assumption in the assumption section, but it would be beneficial to discuss this assumption more thoroughly in the main text.\n* The paper assumes that the rewards are bounded, which may not hold in practice. The authors note this assumption in the assumption section, but it would be beneficial to discuss this assumption more thoroughly in the main text.\n\nSuggestions for Improvement:\n\n* The paper could benefit from a more detailed discussion of the assumption that the data at each silo is a stream of unique users. This assumption may not hold in practice, and the authors should discuss the implications of this assumption more thoroughly.\n* The paper could benefit from a more detailed discussion of the assumption that the feature vectors are bounded. This assumption may not hold in practice, and the authors should discuss the implications of this assumption more thoroughly.\n* The paper could benefit from a more detailed discussion of the assumption that the rewards are bounded. This assumption may not hold in practice, and the authors should discuss the implications of this assumption more thoroughly.\n\nRecommendation:\n\nThe paper presents a comprehensive analysis of the state-of-the-art algorithm for private, federated linear contextual bandits, identifying three key issues and proposing a novel solution to resolve these issues. The authors propose a flexible privacy protocol that can be used under both silo-level LDP and SDP, allowing for adaptability to different privacy constraints. The paper demonstrates that the proposed algorithm achieves correct privacy, regret, and communication results under both silo-level LDP and SDP, providing a strong theoretical foundation for the approach. However, the paper assumes that the data at each silo is a stream of unique users, which may not hold in practice, and the authors should discuss this assumption more thoroughly. Overall, the paper presents a significant contribution to the field of private, federated linear contextual bandits, and the authors should be commended for their work.\n\n$$$",
        "gr_score_with": 0.49523809523809526,
        "gr_score_without": 0.372972972972973,
        "output_without": {
            "num_tokens_scored": 370,
            "num_green_tokens": 138,
            "green_fraction": 0.372972972972973,
            "z_score": -1.0611908999450224,
            "p_value": 0.8556984229350457
        },
        "output_with": {
            "num_tokens_scored": 315,
            "num_green_tokens": 156,
            "green_fraction": 0.49523809523809526,
            "z_score": 3.450327796711771,
            "p_value": 0.00027995312400309197
        }
    },
    {
        "title": "ZERO BUBBLE (ALMOST) PIPELINE PARALLELISM",
        "abstract": "Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 15% in throughput under a similar memory limit. This number can be further pushed to 30% when the memory constraint is relaxed. We believe our results mark a major step forward in harnessing the true potential of pipeline parallelism. The source code based on Megatron-LM is publicly avaiable at https: //github.com/sail-sg/zero-bubble-pipeline-parallelism.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy to improve the efficiency of pipeline parallelism by splitting the activation gradient and parameter gradient in backward computation. The authors introduce a scheduling strategy that achieves zero pipeline bubbles under synchronous training semantics. The key idea is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, the authors handcraft novel pipeline schedules that significantly outperform the baseline methods. Additionally, they develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. To truly achieve zero bubble, the authors introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that the proposed method outperforms the 1F1B schedule up to 15% in throughput under a similar memory limit.\n\nStrengths:\n\n* The paper proposes a novel and effective strategy to improve the efficiency of pipeline parallelism.\n* The authors provide a thorough analysis of the problem and the proposed solution.\n* The experimental evaluations are comprehensive and demonstrate the effectiveness of the proposed method.\n* The paper is well-organized and easy to follow.\n\nWeaknesses:\n\n* The paper assumes that the time costs for F, B, and W are identical, which may not be realistic in all scenarios.\n* The authors do not provide a detailed comparison with other parallelism techniques, such as data parallelism and tensor parallelism.\n* The paper assumes that the memory limit is sufficient to accommodate the increased memory need for zero bubble schedules.\n* The authors do not provide a detailed discussion on the scalability of the proposed method for large models.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with other parallelism techniques to demonstrate the effectiveness of the proposed method.\n* The paper should include a discussion on the assumptions made in the analysis, such as the identical time costs for F, B, and W.\n* The authors should provide a more detailed analysis on the scalability of the proposed method for large models.\n* The paper should include a discussion on the potential applications of the proposed method in real-world scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective strategy to improve the efficiency of pipeline parallelism. The experimental evaluations demonstrate the effectiveness of the proposed method, and the paper is well-organized and easy to follow. However, the paper assumes some unrealistic scenarios, and the authors do not provide a detailed comparison with other parallelism techniques. Therefore, I recommend that the authors address these limitations and provide a more comprehensive analysis of the proposed method.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field of parallel computing and deep learning. With some revisions to address the limitations mentioned above, the paper can be a valuable addition to the literature.\n\nRating: 8/10\n\nReviewer: [Your Name]\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper proposes a novel scheduling strategy, Zero Bubble (ZB), to improve the efficiency of pipeline parallelism in large-scale distributed training. ZB achieves zero pipeline bubbles under synchronous training semantics by splitting the backward computation into two parts: computing the gradient for the input and computing for the parameters. The authors introduce two handcrafted schedules, ZB-H1 and ZB-H2, which outperform the baseline 1F1B schedule. Furthermore, an automatic scheduling algorithm is proposed to find an optimal schedule based on specific model configuration and memory limit. Experimental evaluations show that ZB outperforms the 1F1B schedule up to 15% in throughput under a similar memory limit.\n\nStrengths:\n1. The paper proposes a novel scheduling strategy, ZB, which achieves zero pipeline bubbles under synchronous training semantics.\n2. The authors introduce two handcrafted schedules, ZB-H1 and ZB-H2, which outperform the baseline 1F1B schedule.\n3. An automatic scheduling algorithm is proposed to find an optimal schedule based on specific model configuration and memory limit.\n4. Experimental evaluations show that ZB outperforms the 1F1B schedule up to 15% in throughput under a similar memory limit.\n5. The paper provides a detailed analysis of the scheduling algorithm and its performance under different settings.\n\nWeaknesses:\n1. The paper assumes that the execution times of F, B, and W are identical, which may not hold in practice.\n2. The authors do not provide a thorough analysis of the overhead of the scheduling algorithm.\n3. The paper assumes a specific model configuration and memory limit, which may not be generalizable to other settings.\n4. The experimental evaluations are limited to a specific setup, and it is not clear how the results will generalize to other scenarios.\n5. The paper does not discuss the scalability of the scheduling algorithm to very large models.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the overhead of the scheduling algorithm.\n2. They should investigate the robustness of the scheduling algorithm to variations in execution times.\n3. A thorough analysis of the scheduling algorithm's performance under different model configurations and memory limits is necessary.\n4. The authors should discuss the scalability of the scheduling algorithm to very large models.\n5. Experimental evaluations should be conducted on a wider range of settings to demonstrate the generalizability of the results.\n\nRecommendation:\nThis paper makes a significant contribution to the field of pipeline parallelism by introducing a novel scheduling strategy, ZB, which achieves zero pipeline bubbles under synchronous training semantics. The automatic scheduling algorithm is a key innovation, and the experimental evaluations demonstrate its effectiveness. However, the paper has some limitations, and addressing these will improve its overall quality. I recommend acceptance with minor revisions to address the suggestions for improvement.\n\nRating: 7.5/10\n$$$",
        "gr_score_with": 0.5042016806722689,
        "gr_score_without": 0.39577836411609496,
        "output_without": {
            "num_tokens_scored": 379,
            "num_green_tokens": 150,
            "green_fraction": 0.39577836411609496,
            "z_score": -0.16776244879600807,
            "p_value": 0.566614917368728
        },
        "output_with": {
            "num_tokens_scored": 357,
            "num_green_tokens": 180,
            "green_fraction": 0.5042016806722689,
            "z_score": 4.018863086023417,
            "p_value": 2.9239819327405217e-05
        }
    },
    {
        "title": "TRANSFORMER-VQ: LINEAR-TIME TRANSFORMERS",
        "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmaxbased dense self-attention in linear time. Transformer-VQ\u2019s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: https://github.com/transformer-vq/transformer_vq k1 k2 k3 k4 k5 7\u2192 VQ \u2248 k\u03021 k\u03022 k\u03023 k\u03024 k\u03025 Figure 1: Schematic of the VQ-Attention approximation. The colorful and blank boxes depict the keys and attention weights, respectively. The keys on the right have been vector-quantized. Since the green keys k2, k5 map to the same code, they have the same attention weights in this attention head.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time. This is made possible through a combination of vector-quantized keys, localized positional biases, and compressive cache that can be attended to efficiently, while yielding the same results as an uncompressed variable-length cache. The authors propose a new attention mechanism that uses vector-quantization to reduce the complexity of self-attention, and demonstrate the effectiveness of Transformer-VQ on several long-range autoregressive modeling tasks.\n\nStrengths:\n\n* The paper presents a novel attention mechanism that can compute dense self-attention in linear time, which is a significant improvement over the quadratic time complexity of standard transformers.\n* The authors provide a thorough analysis of the proposed mechanism and demonstrate its effectiveness on several long-range autoregressive modeling tasks.\n* The paper presents a clear and concise description of the Transformer-VQ model and its components, making it easy to follow and understand.\n* The authors provide a comprehensive comparison with existing methods, including kernelizable attention and compressive transformers.\n\nWeaknesses:\n\n* The paper assumes familiarity with transformers and attention mechanisms, which may make it difficult for readers who are not familiar with these concepts to understand the paper.\n* The authors do not provide a clear explanation of the theoretical guarantees of the proposed mechanism, which may make it difficult to assess its reliability and robustness.\n* The paper does not provide a thorough analysis of the computational complexity of the proposed mechanism, which may make it difficult to assess its scalability and efficiency.\n* The authors do not provide a clear explanation of the hyperparameter tuning process, which may make it difficult to reproduce the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the theoretical guarantees of the proposed mechanism, including its reliability and robustness.\n* Provide a more thorough analysis of the computational complexity of the proposed mechanism, including its scalability and efficiency.\n* Provide a clear explanation of the hyperparameter tuning process, including the choice of hyperparameters and the impact of hyperparameter values on the results.\n* Provide more experimental results, including results on additional datasets and tasks, to demonstrate the effectiveness and robustness of the proposed mechanism.\n\nRecommendation:\n\nThe paper presents a novel attention mechanism that can compute dense self-attention in linear time, which is a significant improvement over the quadratic time complexity of standard transformers. The authors provide a thorough analysis of the proposed mechanism and demonstrate its effectiveness on several long-range autoregressive modeling tasks. However, the paper assumes familiarity with transformers and attention mechanisms, and does not provide a clear explanation of the theoretical guarantees and computational complexity of the proposed mechanism. Therefore, I recommend that the authors provide a more detailed explanation of the theoretical guarantees and computational complexity of the proposed mechanism, and provide more experimental results to demonstrate its effectiveness and robustness.\n\nOverall, the paper is well-written and well-organized, and the authors provide a clear and concise description of the Transformer-VQ model and its components. However, the paper has some limitations that need to be addressed in future work.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time with respect to sequence length. The efficient attention is enabled by vector-quantized keys and a novel caching mechanism. The authors demonstrate the effectiveness of Transformer-VQ on several long-range autoregressive modeling tasks, including Enwik8, PG-19, and ImageNet64.\n\nStrengths:\n- The paper proposes a novel and efficient attention mechanism that can compute dense self-attention in linear time.\n- The authors demonstrate strong quantitative results on several long-range autoregressive modeling tasks.\n- The paper provides a clear and concise description of the attention mechanism and its implementation.\n\nWeaknesses:\n- The paper does not provide a comprehensive comparison with other efficient attention mechanisms.\n- The authors do not discuss the potential drawbacks of using vector-quantized keys and caching mechanisms.\n- The paper assumes familiarity with transformers and vector quantization, which may limit its accessibility to readers without a strong background in these areas.\n\nSuggestions for Improvement:\n- Provide a more comprehensive comparison with other efficient attention mechanisms, including their strengths and weaknesses.\n- Discuss the potential drawbacks of using vector-quantized keys and caching mechanisms, and explore alternative solutions.\n- Provide more implementation details and code snippets to facilitate reproducibility and implementation.\n\nRecommendation:\nOverall, the paper proposes a novel and efficient attention mechanism that can compute dense self-attention in linear time. While there are some weaknesses and suggestions for improvement, the paper demonstrates strong quantitative results and provides a clear and concise description of the attention mechanism and its implementation. I would recommend this paper for publication in a top-tier conference.\n\nRating: 7.5/10\n\nReviewer: [Your Name]",
        "gr_score_with": 0.5102880658436214,
        "gr_score_without": 0.36923076923076925,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 120,
            "green_fraction": 0.36923076923076925,
            "z_score": -1.1322770341445956,
            "p_value": 0.8712410088754685
        },
        "output_with": {
            "num_tokens_scored": 243,
            "num_green_tokens": 124,
            "green_fraction": 0.5102880658436214,
            "z_score": 3.509344765888791,
            "p_value": 0.00022460609898330064
        }
    },
    {
        "title": "IN-CONTEXT LEARNING LEARNS LABEL RELATION-",
        "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input\u2013label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper investigates the behavior of in-context learning (ICL) in large language models (LLMs) by formulating three null hypotheses: (1) ICL predictions are independent of the conditional label distribution of in-context examples, (2) ICL can overcome prediction preferences acquired from pre-training data, and (3) ICL considers all information provided in-context equally. The authors study ICL behavior across various models, tasks, and metrics, including probabilistic aspects of ICL predictions. Their results show that ICL does incorporate in-context label information, can learn truly novel tasks in-context, but struggles to fully overcome prediction preferences acquired from pre-training data and does not treat all information provided in-context equally.\n\nStrengths:\n\n* The paper provides a comprehensive understanding of ICL behavior by studying ICL across all possible in-context dataset sizes and considering probabilistic aspects of ICL predictions.\n* The authors formulate three null hypotheses that provide a clear framework for understanding ICL behavior.\n* The paper presents a range of experiments that demonstrate the limitations of ICL, including its inability to fully overcome prediction preferences acquired from pre-training data and its failure to treat all information provided in-context equally.\n\nWeaknesses:\n\n* The paper's focus on few-shot ICL tasks may limit its generalizability to other NLP tasks, such as question answering.\n* The authors do not provide a clear explanation for why ICL struggles to fully overcome prediction preferences acquired from pre-training data.\n* The paper's conclusion that ICL does not treat all information provided in-context equally is based on a limited set of experiments and may not be generalizable to all ICL tasks.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation for why ICL struggles to fully overcome prediction preferences acquired from pre-training data.\n* The paper should include more experiments to demonstrate the limitations of ICL in other NLP tasks, such as question answering.\n* The authors should provide a more detailed analysis of the implications of their findings for the development of more effective ICL algorithms.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the understanding of ICL behavior and its limitations. However, its conclusions should be interpreted with caution due to the potential limitations of its experimental design. The paper's findings have important implications for the development of more effective ICL algorithms and highlight the need for further research in this area.\n\nOverall, I would recommend this paper for publication in a top-tier conference, but with the suggestion that the authors address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper investigates the capabilities and limitations of in-context learning (ICL), a widely used technique for Large Language Models (LLMs). The authors aim to provide a comprehensive understanding of how ICL incorporates label information and whether it can learn novel tasks in-context. They formulate three null hypotheses (NH1-3) that encode their expectations of how an idealized conventional learner should behave. Through a series of experiments, they evaluate NH1-3 across various LLMs and tasks, using probabilistic metrics to study the effects of label randomization and replacement. Their results show that ICL predictions depend on the conditional label distribution of in-context examples, can learn novel tasks in-context, but struggles to fully overcome prediction preferences acquired from pre-training data. Furthermore, they find that ICL does not treat all in-context information equally, preferring to use information closer to the query.\n\nStrengths:\n\n1. The authors provide a thorough and systematic investigation of ICL, using a novel evaluation strategy that allows them to study ICL predictions across all possible in-context dataset sizes.\n2. They use probabilistic metrics to evaluate ICL predictions, which provides a more nuanced understanding of ICL behavior compared to accuracy metrics alone.\n3. The authors address a pressing need in the field by providing a comprehensive understanding of ICL's capabilities and limitations.\n4. Their results have important implications for the deployment of LLMs in real-world applications, such as alignment and fine-tuning.\n\nWeaknesses:\n\n1. The authors focus on few-shot ICL tasks, which may not be representative of all NLP tasks. It would be beneficial to extend their investigation to other types of tasks.\n2. They do not consider the effects of RLHF-fin-tuned LLMs, which is a limitation of their study.\n3. The authors rely on a specific set of LLMs and tasks, which may not be generalizable to other models and tasks. It would be beneficial to conduct a broader investigation of ICL's capabilities and limitations.\n\nSuggestions for Improvement:\n\n1. The authors could extend their investigation to other types of NLP tasks, such as question answering or sentiment analysis.\n2. They could explore the effects of RLHF-fin-tuned LLMs on ICL's capabilities and limitations.\n3. The authors could conduct a more extensive analysis of the effects of label randomization and replacement on ICL predictions.\n\nRecommendation:\n\nThis paper provides a thorough and systematic investigation of ICL's capabilities and limitations. The authors' results have important implications for the deployment of LLMs in real-world applications. However, there are some limitations to their study that should be addressed in future work. Overall, I would recommend this paper for publication in a top-tier conference or journal.",
        "gr_score_with": 0.43410852713178294,
        "gr_score_without": 0.35135135135135137,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 117,
            "green_fraction": 0.35135135135135137,
            "z_score": -1.8121213490778674,
            "p_value": 0.9650162758507773
        },
        "output_with": {
            "num_tokens_scored": 387,
            "num_green_tokens": 168,
            "green_fraction": 0.43410852713178294,
            "z_score": 1.3696601740023175,
            "p_value": 0.08539650310780356
        }
    },
    {
        "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
        "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in deep neural networks (DNNs). FOMO alternates between the forgetting phase, where a subset of weights is randomly forgotten through reinitialization, and the relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the efficacy of FOMO on benchmark datasets and adversarial attacks, showing that it alleviates robust overfitting, improves robustness, and provides a better trade-off between standard and robust accuracy.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the problem of robust overfitting in adversarial training and its significance in real-world applications.\n2. The authors propose a novel and innovative solution, FOMO, which is grounded in the concept of active forgetting and relearning, inspired by the human brain's learning mechanisms.\n3. The experimental results demonstrate the effectiveness of FOMO in alleviating robust overfitting, improving robustness, and providing a better trade-off between standard and robust accuracy.\n4. The authors conduct extensive experiments on multiple datasets, architectures, and perturbation types, showcasing the scalability and robustness of FOMO.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase is beneficial for generalization, but it is unclear whether this assumption holds in all cases.\n2. The authors do not provide a thorough analysis of the convergence properties of FOMO, which is crucial for understanding its robustness and generalization capabilities.\n3. The paper relies heavily on empirical results, and more theoretical analysis is needed to understand the underlying mechanisms of FOMO.\n4. The authors do not discuss the computational costs and complexity of implementing FOMO, which may be a limitation in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the forgetting phase and its impact on generalization.\n2. Investigate the convergence properties of FOMO and its robustness to different perturbation types.\n3. Conduct more theoretical analysis to understand the underlying mechanisms of FOMO and its relationship to the effectiveness of adversarial training.\n4. Discuss the computational costs and complexity of implementing FOMO and provide strategies for reducing these costs.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of adversarial training by proposing a novel and innovative solution, FOMO. The experimental results demonstrate the effectiveness of FOMO, and the authors provide a comprehensive overview of the problem and its significance. However, more theoretical analysis and discussion of the computational costs are needed to fully understand the potential of FOMO. With further development and refinement, FOMO has the potential to become a widely adopted technique for improving the robustness and generalization of deep neural networks.\n\nOverall, I recommend this paper for publication in a top-tier conference or journal, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in adversarial training. FOMO alternates between a forgetting phase, where a random subset of weights are reinitialized, and a relearning phase, where the network emphasizes learning generalizable features. The proposed method effectively alleviates robust overfitting and achieves significant results across multiple architectures and datasets.\n\nStrengths:\n\n1. The paper presents a novel approach to addressing robust overfitting in adversarial training, which is a significant problem in the field of adversarial robustness.\n2. The proposed FOMO method effectively alternates between forgetting and relearning phases, leading to a significant reduction in robust overfitting and improved robustness.\n3. The paper presents extensive experimental results on multiple datasets and architectures, demonstrating the effectiveness of FOMO in improving both standard and robust performance and generalization.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase can be effectively emulated in DNNs by reinitializing a random subset of weights, which may not be a realistic assumption in all scenarios.\n2. The proposed FOMO method may not be computationally efficient, as it requires multiple iterations of forgetting and relearning phases.\n3. The paper does not provide a clear theoretical understanding of the effectiveness of FOMO in addressing robust overfitting, which may limit its applicability in certain scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate more efficient methods for emulating the forgetting phase in DNNs, such as using more sophisticated weight reinitialization techniques or leveraging domain knowledge to identify the most critical weights to forget.\n2. Develop a more comprehensive theoretical understanding of the effectiveness of FOMO in addressing robust overfitting, including a deeper analysis of the interplay between forgetting and relearning phases.\n3. Evaluate the proposed FOMO method on a wider range of architectures, datasets, and perturbation types to further demonstrate its effectiveness and robustness.\n\nRecommendation:\n\nThe proposed FOMO method presents a promising solution for addressing robust overfitting in adversarial training, and its effectiveness in improving both standard and robust performance and generalization is demonstrated through extensive experimental results. However, further investigation is needed to address the limitations and challenges mentioned above, particularly in developing a more comprehensive theoretical understanding of FOMO and improving its computational efficiency.",
        "gr_score_with": 0.5783132530120482,
        "gr_score_without": 0.5289672544080605,
        "output_without": {
            "num_tokens_scored": 397,
            "num_green_tokens": 210,
            "green_fraction": 0.5289672544080605,
            "z_score": 5.245284956388885,
            "p_value": 7.802048287769084e-08
        },
        "output_with": {
            "num_tokens_scored": 332,
            "num_green_tokens": 192,
            "green_fraction": 0.5783132530120482,
            "z_score": 6.632038581233666,
            "p_value": 1.655410041436102e-11
        }
    },
    {
        "title": "ENERGY-GUIDED ENTROPIC NEURAL OPTIMAL TRANSPORT",
        "abstract": "Energy-based models (EBMs) are known in the Machine Learning community for decades. Since the seminal works devoted to EBMs dating back to the noughties, there have been a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN-based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present a novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. From the theoretical perspective, we prove generalization bounds for our technique. In practice, we validate its applicability in toy 2D and image domains. To showcase the scalability, we empower our method with a pre-trained StyleGAN and apply it to high-res AFHQ 512 \u00d7 512 unpaired I2I translation. For simplicity, we choose simple shortand long-run EBMs as a backbone of our Energy-guided Entropic OT approach, leaving the application of more sophisticated EBMs for future research. Our code is available at: https: //github.com/PetrMokrov/Energy-guided-Entropic-OT Figure 1: AFHQ 512\u00d7 512 Cat\u2192Dog unpaired translation by our Energy-guided EOT solver applied in the latent space of StyleGAN2-ADA. Our approach does not need data2latent encoding. Left: source samples; right: translated samples.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel methodology for solving the Entropy-regularized Optimal Transport (EOT) problem using Energy-based Models (EBMs). The authors bridge the gap between EBMs and EOT by introducing a weak dual formulation of the EOT problem and modifying the standard EBMs training procedure to recover EOT plans. They provide theoretical guarantees for the proposed method, including generalization bounds for the learned EOT plans. The authors demonstrate the performance of their method on toy 2D scenarios, Gaussian-to-Gaussian transformation tasks, and large-scale unpaired image-to-image translation problems.\n\nStrengths:\n\n1. The paper provides a novel and theoretically grounded approach to solving the EOT problem using EBMs.\n2. The authors provide a clear and concise explanation of the proposed methodology and its connection to EBMs.\n3. The paper includes a comprehensive review of related works and a thorough comparison with existing methods.\n4. The authors provide experimental results on various tasks, including toy 2D scenarios, Gaussian-to-Gaussian transformation tasks, and large-scale unpaired image-to-image translation problems.\n\nWeaknesses:\n\n1. The paper assumes that the source and target distributions are accessible by samples, which may not be the case in practice.\n2. The authors use a simple long-run EBM architecture, which may not be sufficient for large-scale image translation tasks.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n4. The authors do not provide a thorough discussion of the limitations of their approach, including the potential biases introduced by MCMC sampling.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed method and compare it with existing methods.\n2. The paper should include a more thorough discussion of the limitations of the proposed approach, including the potential biases introduced by MCMC sampling.\n3. The authors should provide more experimental results on larger-scale image translation tasks to demonstrate the scalability of their method.\n4. The paper should include a more detailed comparison with existing methods, including a discussion of the strengths and weaknesses of each approach.\n\nRecommendation:\n\nThe paper provides a novel and theoretically grounded approach to solving the EOT problem using EBMs. While there are some limitations to the proposed method, the authors provide a clear and concise explanation of the methodology and its connection to EBMs. The paper is well-written and includes a comprehensive review of related works. However, the authors should provide a more detailed analysis of the computational complexity and limitations of their approach to make it more convincing. Overall, the paper is a valuable contribution to the field of optimal transport and energy-based models, and it has the potential to inspire future research in this area.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel method for solving the Entropy-regularized Optimal Transport (EOT) problem using Energy-based Models (EBMs). The authors bridge the gap between EBMs and EOT by proposing a theoretically grounded yet easy-to-implement modifications to the standard EBMs training procedure. The method is based on a weak dual formulation of the EOT problem, which is reformulated as a supremum of a family of functions. The authors then parameterize these functions as neural networks and optimize them using stochastic gradient ascent. The method is shown to have generalization bounds for the EOT plans learned via the proposed method. The paper also provides experimental illustrations of the method's performance on toy 2D scenarios, Gaussian-to-Gaussian transformations, and high-dimensional unpaired image-to-image translation.\n\nStrengths:\n\n1. The paper presents a novel method for solving EOT using EBMs, which is a well-established technique in generative modeling.\n2. The method is theoretically grounded, with a clear derivation of the EOT problem's reformulation as a supremum of a family of functions.\n3. The authors provide experimental illustrations of the method's performance on a variety of tasks, including toy 2D scenarios, Gaussian-to-Gaussian transformations, and high-dimensional unpaired image-to-image translation.\n4. The paper discusses the connection between the EOT problem and EBMs, which is a well-studied area in machine learning.\n\nWeaknesses:\n\n1. The paper assumes that the EOT problem is formulated as a supremum of a family of functions, which may not be the case in all scenarios.\n2. The method requires a large number of samples to converge, which may be computationally expensive for large-scale datasets.\n3. The paper does not provide a detailed analysis of the method's performance on more complex tasks, such as image-to-image translation with multiple source-target pairs.\n4. The method's performance is evaluated only on a limited set of tasks, and it is unclear how well it generalizes to other tasks.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the method's performance on more complex tasks, such as image-to-image translation with multiple source-target pairs.\n2. Investigate alternative formulations of the EOT problem that may be more suitable for certain tasks or datasets.\n3. Provide a more detailed comparison with other EOT methods, such as those based on GANs or score-based models.\n4. Investigate the use of more advanced neural network architectures or optimization techniques to improve the method's performance.\n\nRecommendation:\n\nThe paper presents a novel method for solving EOT using EBMs, which is a well-established technique in generative modeling. The method is theoretically grounded and provides experimental illustrations of its performance on a variety of tasks. However, the paper assumes a specific formulation of the EOT problem and requires a large number of samples to converge. Further research is needed to investigate alternative formulations of the EOT problem and to improve the method's performance on more complex tasks.\n\nRating: 7/10\n\nThis paper presents a novel method for solving EOT using EBMs, which is a well-established technique in generative modeling. The method is theoretically grounded and provides experimental illustrations of its performance on a variety of tasks. However, the paper assumes a specific formulation of the EOT problem and requires a large number of samples to converge. Further research is needed to investigate alternative formulations of the EOT problem and to improve the method's performance on more complex tasks. Overall, the paper is well-written and provides a valuable contribution to the field of EOT.",
        "gr_score_with": 0.5382653061224489,
        "gr_score_without": 0.4804804804804805,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 160,
            "green_fraction": 0.4804804804804805,
            "z_score": 2.9978303799559733,
            "p_value": 0.0013595448119352721
        },
        "output_with": {
            "num_tokens_scored": 392,
            "num_green_tokens": 211,
            "green_fraction": 0.5382653061224489,
            "z_score": 5.587925819656734,
            "p_value": 1.1489887962341932e-08
        }
    },
    {
        "title": "SEMIREWARD: A GENERAL REWARD MODEL FOR SEMI-SUPERVISED LEARNING",
        "abstract": "Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks across three modalities, extensive experiments verify that SemiReward achieves significant performance gains and faster convergence speeds upon Pseudo Label, FlexMatch, and Free/SoftMatch. Code and models are available at https://github.com/Westl ake-AI/SemiReward. ESC-50 250 ESC-50 500 FSDnoisy-18k 1773 UrtraSound-8k 400 Yelp Review 250",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a Semi-supervised Reward framework (SemiReward) for semi-supervised learning (SSL) that predicts reward scores to evaluate and filter high-quality pseudo labels. SemiReward is trained online in two stages with a generator model and subsampling strategy to mitigate confirmation bias. The framework is designed to be pluggable to mainstream SSL methods in wide task types and scenarios. Extensive experiments on 13 standard SSL benchmarks across three modalities verify that SemiReward achieves significant performance gains and faster convergence speeds upon popular SSL algorithms.\n\nStrengths:\n\n*   The paper proposes a novel and general framework for SSL that evaluates and selects high-quality pseudo labels.\n*   The framework is designed to be pluggable to mainstream SSL methods, making it versatile and applicable to various tasks and scenarios.\n*   The paper presents extensive experiments on 13 standard SSL benchmarks across three modalities, demonstrating the effectiveness of SemiReward.\n*   The framework is simple and efficient, requiring minimal extra computational cost.\n\nWeaknesses:\n\n*   The paper assumes that the reward scores and rewarder only support sample-level labels, which may not be suitable for scenarios requiring fine-grained labels.\n*   The framework requires repeating the teacher model and the rewarder several times to get reliable pseudo labels, which may lead to extra computational costs and performance degradation.\n*   The paper does not explore the possibility of pre-training a general rewarder with large-scale pre-trained backbones on open-source datasets.\n*   The framework is not designed to handle imbalanced regression datasets, where SemiReward encounters challenges in enhancing the selection of superior pseudo-labels.\n\nSuggestions for Improvement:\n\n*   The authors should explore the possibility of designing a more efficient sampling and selection pipeline for SSL training to reduce extra computational costs.\n*   The authors should investigate the use of pre-trained rewarders with large-scale pre-trained backbones on open-source datasets to improve the performance of SemiReward.\n*   The authors should design a framework to handle imbalanced regression datasets, where SemiReward encounters challenges in enhancing the selection of superior pseudo-labels.\n*   The authors should explore the possibility of extending SemiReward with adaptive data augmentations to further enhance SSL performance.\n\nRecommendation:\n\nThe paper proposes a novel and general framework for SSL that evaluates and selects high-quality pseudo labels. The framework is designed to be pluggable to mainstream SSL methods, making it versatile and applicable to various tasks and scenarios. The paper presents extensive experiments on 13 standard SSL benchmarks across three modalities, demonstrating the effectiveness of SemiReward. However, the paper assumes that the reward scores and rewarder only support sample-level labels, which may not be suitable for scenarios requiring fine-grained labels. Therefore, the recommendation is to accept the paper with minor revisions to address the limitations and suggestions for improvement.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a Semi-supervised Reward framework (SemiReward) to evaluate and filter out high-quality pseudo labels in semi-supervised learning (SSL) tasks. The framework predicts reward scores to evaluate pseudo labels and can be plugged into mainstream SSL methods in various tasks and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. The authors conduct extensive experiments on 13 standard SSL benchmarks across three modalities and demonstrate that SemiReward achieves significant performance gains and faster convergence speeds compared to previous SSL methods.\n\nStrengths:\n\n* The paper proposes a novel and effective approach to evaluating and filtering pseudo labels in SSL tasks.\n* The framework is general and pluggable to mainstream SSL methods in various tasks and scenarios.\n* The authors provide extensive experiments on 13 standard SSL benchmarks across three modalities and demonstrate the effectiveness of SemiReward.\n* The paper discusses the limitation of existing SSL methods and proposes a solution to address these limitations.\n\nWeaknesses:\n\n* The paper assumes that the pseudo labels are generated by a teacher model, which may not always be the case in real-world scenarios.\n* The framework requires additional training data and computational resources to train the rewarder and generator models.\n* The paper does not provide a detailed analysis of the rewarder and generator models and their interactions.\n* The paper assumes that the reward scores are continuous and can be predicted by a regression model, which may not always be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the rewarder and generator models and their interactions.\n* The paper should discuss the limitation of the reward scores and propose a solution to address these limitations.\n* The authors should provide more experiments to demonstrate the effectiveness of SemiReward in real-world scenarios.\n* The paper should discuss the potential applications of SemiReward in other machine learning tasks, such as reinforcement learning and natural language processing.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to evaluating and filtering pseudo labels in SSL tasks. The framework is general and pluggable to mainstream SSL methods in various tasks and scenarios. The authors provide extensive experiments to demonstrate the effectiveness of SemiReward. However, the paper assumes that the pseudo labels are generated by a teacher model and requires additional training data and computational resources to train the rewarder and generator models. To improve the paper, the authors should provide a more detailed analysis of the rewarder and generator models and their interactions and discuss the limitation of the reward scores and propose a solution to address these limitations.\n\nRecommendation: Accept with minor revisions.\n\n$$$",
        "gr_score_with": 0.5409252669039146,
        "gr_score_without": 0.45180722891566266,
        "output_without": {
            "num_tokens_scored": 332,
            "num_green_tokens": 150,
            "green_fraction": 0.45180722891566266,
            "z_score": 1.926876074277348,
            "p_value": 0.02699753793568874
        },
        "output_with": {
            "num_tokens_scored": 281,
            "num_green_tokens": 152,
            "green_fraction": 0.5409252669039146,
            "z_score": 4.8221021390204815,
            "p_value": 7.102658510202144e-07
        }
    },
    {
        "title": "META-LEARNING PRIORS USING UNROLLED PROXIMAL NETWORKS",
        "abstract": "Relying on prior knowledge accumulated from related tasks, meta-learning offers a powerful approach to learning a novel task from limited training data. Recent approaches parameterize the prior with a family of probability density functions or recurrent neural networks, whose parameters can be optimized by utilizing validation data from the observed tasks. While these approaches have appealing empirical performance, the expressiveness of their prior is relatively low, which limits the generalization and interpretation of meta-learning. Aiming at expressive yet meaningful priors, this contribution puts forth a novel prior representation model that leverages the notion of algorithm unrolling. The key idea is to unroll the proximal gradient descent steps, where learnable piecewise linear functions are developed to approximate the desired proximal operators within tight theoretical error bounds established for both smooth and non-smooth proximal functions. The resultant multi-block neural network not only broadens the scope of learnable priors, but also enhances interpretability from an optimization viewpoint. Numerical tests conducted on few-shot learning datasets demonstrate markedly improved performance with flexible, visualizable, and understandable priors.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel meta-learning approach called MetaProxNet that utilizes algorithm unrolling to learn more flexible and generalized priors. The proposed method leverages the proximal gradient descent (PGD) algorithm to unroll the optimization process and learn a piecewise linear function (PLF) to approximate the desired proximal operator. The learnable PLFs are used to parameterize the prior, which can be visualized to understand the behavior of the unrolled neural network. The paper provides theoretical analysis and numerical tests to demonstrate the effectiveness of MetaProxNet in comparison to other meta-learning methods.\n\nStrengths:\n\n1. The paper proposes a novel approach to meta-learning that leverages algorithm unrolling to learn more flexible and generalized priors.\n2. The proposed method provides a clear and interpretable way to learn priors using PLFs, which can be visualized to understand the behavior of the unrolled neural network.\n3. The paper provides theoretical analysis and numerical tests to demonstrate the effectiveness of MetaProxNet in comparison to other meta-learning methods.\n4. The proposed method can be easily integrated with other optimization-based meta-learning methods through a simple substitution of the DC module.\n\nWeaknesses:\n\n1. The paper assumes that the optimal regularizer R\u2217 factorizes across its input dimensions, which may not always be the case in practice.\n2. The paper assumes that the proximal operator proxR\u2217,\u03b1 has an analytical form, which may not always be the case in practice.\n3. The paper uses a simple substitution of the DC module to integrate MetaProxNet with other optimization-based meta-learning methods, which may not be the most effective way to do so.\n4. The paper does not provide a clear comparison of the computational complexity of MetaProxNet with other meta-learning methods.\n\nSuggestions for Improvement:\n\n1. Relax the assumption that the optimal regularizer R\u2217 factorizes across its input dimensions and explore other factorization methods.\n2. Explore other methods to learn the proximal operator proxR\u2217,\u03b1, such as using neural networks or other machine learning models.\n3. Develop a more effective way to integrate MetaProxNet with other optimization-based meta-learning methods, such as using a more sophisticated DC module.\n4. Provide a clear comparison of the computational complexity of MetaProxNet with other meta-learning methods.\n\nRecommendation:\n\nThe paper proposes a novel approach to meta-learning that leverages algorithm unrolling to learn more flexible and generalized priors. The proposed method provides a clear and interpretable way to learn priors using PLFs, which can be visualized to understand the behavior of the unrolled neural network. The paper provides theoretical analysis and numerical tests to demonstrate the effectiveness of MetaProxNet in comparison to other meta-learning methods. While there are some limitations to the proposed method, it has the potential to be a useful tool for meta-learning and deserves further exploration.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel meta-learning approach called MetaProxNet that utilizes algorithm unrolling to learn more flexible and generalized priors. The proposed method leverages proximal gradient descent (PGD) algorithm unrolling to learn piecewise linear functions (PLFs) that can effectively approximate proximal operators. This approach enables the learning of complex priors that can be more expressive and interpretable than existing meta-learning methods. The paper also provides a theoretical analysis of the proposed method, establishing error bounds for smooth and non-smooth proximal operators. Numerical tests on several meta-learning benchmark datasets demonstrate the superiority of MetaProxNet relative to state-of-the-art meta-learning methods.\n\nStrengths:\n\n1. The proposed approach leverages algorithm unrolling to learn more flexible and generalized priors, which can be more expressive and interpretable than existing meta-learning methods.\n2. The paper provides a theoretical analysis of the proposed method, establishing error bounds for smooth and non-smooth proximal operators.\n3. Numerical tests on several meta-learning benchmark datasets demonstrate the superiority of MetaProxNet relative to state-of-the-art meta-learning methods.\n4. The proposed method enjoys interpretability from an optimization viewpoint, and can be well explained by visualizing its PLFs.\n\nWeaknesses:\n\n1. The proposed approach requires a large number of iterations to converge, which can be computationally expensive.\n2. The paper assumes that the proximal operator can be approximated by a piecewise linear function, which may not always be the case.\n3. The proposed method may not generalize well to tasks with complex priors that cannot be captured by a piecewise linear function.\n\nSuggestions for Improvement:\n\n1. To improve the computational efficiency of the proposed approach, the authors can explore ways to reduce the number of iterations required for convergence.\n2. To improve the generalizability of the proposed method, the authors can explore ways to relax the assumption that the proximal operator can be approximated by a piecewise linear function.\n3. To further improve the interpretability of the proposed method, the authors can explore ways to visualize the learned priors in a more meaningful way.\n\nRecommendation:\n\nThe proposed approach is an interesting and innovative contribution to the meta-learning community. However, it requires further investigation to improve its computational efficiency, generalizability, and interpretability. With these improvements, the proposed approach has the potential to become a state-of-the-art meta-learning method.\n\nRating: 7/10\n\nThis review is based on the paper's contribution to the meta-learning community, its theoretical analysis, numerical tests, and interpretability. However, it also highlights some weaknesses and suggestions for improvement to further enhance the proposed approach.",
        "gr_score_with": 0.4431137724550898,
        "gr_score_without": 0.3377926421404682,
        "output_without": {
            "num_tokens_scored": 299,
            "num_green_tokens": 101,
            "green_fraction": 0.3377926421404682,
            "z_score": -2.195693565621969,
            "p_value": 0.9859430579863733
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 148,
            "green_fraction": 0.4431137724550898,
            "z_score": 1.6083613857915737,
            "p_value": 0.053878026760656374
        }
    },
    {
        "title": "PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS",
        "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs) that combines two novel components: Relative Importance and Activations (RIA) and Channel Permutation. RIA is a new pruning metric that jointly considers the weight and activations of LLMs, while Channel Permutation is a new approach to maximize the preservation of important weights under N:M sparsity. The authors demonstrate that RIA alone can outperform existing post-training pruning methods on prevalent LLMs, and that Channel Permutation can even outperform the original LLaMA2 70B on zero-shot tasks.\n\nStrengths:\n\n1. The paper proposes two novel and innovative components, RIA and Channel Permutation, which can be combined to further enhance the N:M structured pruned LLMs.\n2. The authors demonstrate the effectiveness of RIA and Channel Permutation through extensive experiments on prominent LLMs like LLaMA, LLaMA2, and OPT across varying model sizes.\n3. The paper provides a thorough analysis of the performance of RIA and Channel Permutation under different sparsity levels, calibration data, and zero-shot performance.\n4. The authors demonstrate the hardware-friendliness of their method, ensuring enhanced inference acceleration.\n\nWeaknesses:\n\n1. The paper assumes that the input and output channels of weight have unique information, which may not always be the case in LLMs.\n2. The authors do not provide a thorough analysis of the computational complexity of their method, which may be a concern for large-scale LLMs.\n3. The paper assumes that the Hungarian algorithm can efficiently solve the linear sum assignment problem, which may not always be the case in practice.\n4. The authors do not provide a clear comparison with other pruning methods that do not require calibration data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational complexity of their method and its implications for large-scale LLMs.\n2. The paper should include a more detailed comparison with other pruning methods that do not require calibration data.\n3. The authors should provide more insights into the assumption that the input and output channels of weight have unique information in LLMs.\n4. The paper should include more experiments to evaluate the robustness of RIA and Channel Permutation under different scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective plug-and-play post-training pruning method for LLMs that combines RIA and Channel Permutation. The authors demonstrate the effectiveness of their method through extensive experiments and provide a thorough analysis of its performance under different sparsity levels, calibration data, and zero-shot performance. However, the paper assumes that the input and output channels of weight have unique information, and does not provide a thorough analysis of the computational complexity of their method. Therefore, the paper is recommended for publication after addressing these concerns.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents an efficient post-training pruning method for large language models (LLMs), called PLUG-AND-PLAY, which consists of two innovative components: Relative Importance and Activations (RIA) and Channel Permutation. The proposed method achieves state-of-the-art performance on prevalent LLMs, such as LLaMA ranging from 7B to 65B, and even outperforms the original LLaMA2 70B on zero-shot tasks, along with practical speed-up on specific hardware.\n\nStrengths:\n\n1. The proposed method, PLUG-AND-PLAY, achieves state-of-the-art performance on prevalent LLMs and demonstrates superior results compared to existing post-training pruning methods.\n2. The Relative Importance and Activations (RIA) metric introduces a novel approach to prune away entire channels of network weights, effectively mitigating issues related to channel corruption.\n3. Channel Permutation offers a new approach to transform the permutation problem into a well-known combinatorial optimization problem, the linear sum assignment (LSA), and efficiently solve it using the Hungarian algorithm.\n\nWeaknesses:\n\n1. The proposed method does not address the problem of sparse training or during-training pruning, which are still computationally costly and time-consuming, especially for LLMs.\n2. The Channel Permutation approach may not be applicable to other types of neural networks or pruning methods, limiting its generality.\n3. The paper assumes that the input and output channels are equally important, which may not always be the case in real-world applications.\n\nSuggestions for Improvement:\n\n1. Investigate the applicability of PLUG-AND-PLAY to other types of neural networks or pruning methods to enhance its generality.\n2. Consider incorporating techniques from sparse training or during-training pruning to improve the efficiency and effectiveness of the proposed method.\n3. Explore the use of more advanced combinatorial optimization techniques or machine learning methods to further improve the efficiency and effectiveness of Channel Permutation.\n\nRecommendation:\n\nThis paper presents an efficient and effective post-training pruning method for large language models, achieving state-of-the-art performance on prevalent LLMs and demonstrating superior results compared to existing post-training pruning methods. The proposed method has potential applications in various fields, such as natural language processing, computer vision, and reinforcement learning. However, further research and experimentation are necessary to fully explore its potential and address its limitations.\n\nOverall, I would rate this paper as \"Accept\" for publication in a top-tier conference or journal in the field of machine learning or artificial intelligence.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.4639175257731959,
        "gr_score_without": 0.2869822485207101,
        "output_without": {
            "num_tokens_scored": 338,
            "num_green_tokens": 97,
            "green_fraction": 0.2869822485207101,
            "z_score": -4.241303900585329,
            "p_value": 0.9999888887500024
        },
        "output_with": {
            "num_tokens_scored": 388,
            "num_green_tokens": 180,
            "green_fraction": 0.4639175257731959,
            "z_score": 2.569982684098001,
            "p_value": 0.005085179921769705
        }
    },
    {
        "title": "DIFFTACTILE: A PHYSICS-BASED DIFFERENTIABLE TACTILE SIMULATOR FOR CONTACT-RICH ROBOTIC MANIPULATION",
        "abstract": "We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website1.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents DIFFTACTILE, a physics-based differentiable tactile simulator designed to enhance robotic manipulation with dense and physically accurate tactile feedback. The simulator models tactile sensors, multi-material objects, and penalty-based contacts, and is capable of simulating a wide range of contact modes and interactions with objects of various material properties. The differentiability of the system facilitates gradient-based optimization for refining physical properties in simulation and efficient learning of tactile-assisted grasping and contact-rich manipulation skills. The authors evaluate the simulator on various tasks, including system identification, grasping, and contact-rich manipulation, and demonstrate its ability to reduce the sim-to-real gap and improve learning efficiency.\n\nStrengths:\n\n1. The paper presents a comprehensive and well-structured simulator that addresses a significant gap in the field of robotic manipulation.\n2. The differentiability of the system is a significant innovation that enables efficient gradient-based optimization.\n3. The authors provide a thorough evaluation of the simulator on various tasks, demonstrating its ability to reduce the sim-to-real gap and improve learning efficiency.\n4. The simulator is capable of simulating a wide range of contact modes and interactions with objects of various material properties.\n\nWeaknesses:\n\n1. The paper assumes a significant amount of prior knowledge in the field of robotics and simulation, which may make it challenging for readers without a background in these areas.\n2. The authors do not provide a detailed comparison with other state-of-the-art simulators, which would be beneficial for understanding the novelty and contributions of their work.\n3. The paper focuses primarily on the simulator's capabilities and does not provide a thorough analysis of its limitations and potential applications.\n4. The authors do not provide a clear discussion of the potential impact of the simulator on the field of robotics and its potential applications in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other state-of-the-art simulators to highlight the novelty and contributions of the work.\n2. Include a more thorough analysis of the simulator's limitations and potential applications.\n3. Provide a clearer discussion of the potential impact of the simulator on the field of robotics and its potential applications in real-world scenarios.\n4. Consider including more diverse and challenging tasks in the evaluation to demonstrate the simulator's capabilities in a wider range of scenarios.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of robotic manipulation and simulation, and the authors demonstrate a clear understanding of the challenges and opportunities in this area. However, the paper could benefit from a more detailed comparison with other state-of-the-art simulators and a more thorough analysis of the simulator's limitations and potential applications. With these improvements, the paper would be a strong candidate for publication in a top-tier conference in the field of robotics and simulation.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents DIFFTACTILE, a physics-based differentiable tactile simulator for contact-rich robotic manipulation. The simulator models tactile sensors, multi-material objects, and penalty-based contacts, providing a platform for studying contact-rich manipulations. DIFFTACTILE is built on Taichi, leveraging parallel GPU computing and auto-differentiation. The simulator is evaluated on a diverse set of manipulation tasks, including grasping, surface following, cable straightening, case opening, and object reposing. The results show that DIFFTACTILE can efficiently learn tactile-assisted manipulation skills and reduce the sim-to-real gap.\n\nStrengths:\n\n1. DIFFTACTILE is a comprehensive simulator that models tactile sensors, multi-material objects, and penalty-based contacts, providing a robust platform for studying contact-rich manipulations.\n2. The simulator is built on Taichi, which leverages parallel GPU computing and auto-differentiation, making it computationally efficient.\n3. DIFFTACTILE is evaluated on a diverse set of manipulation tasks, demonstrating its versatility and applicability.\n4. The results show that DIFFTACTILE can efficiently learn tactile-assisted manipulation skills and reduce the sim-to-real gap.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of robotics and tactile sensing, which may limit its accessibility to a broader audience.\n2. The simulator is built on Taichi, which is a proprietary framework, making it difficult for others to replicate the results without access to the framework.\n3. The paper does not provide a detailed analysis of the computational resources required to run the simulator, which may be a limitation for users with limited computational resources.\n4. The paper focuses on the simulator itself and does not provide a detailed analysis of the learning algorithms used to train the policies for the manipulation tasks.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed introduction to the basics of robotics and tactile sensing to make the paper more accessible to a broader audience.\n2. Provide a detailed analysis of the computational resources required to run the simulator and discuss potential limitations for users with limited computational resources.\n3. Provide a more detailed analysis of the learning algorithms used to train the policies for the manipulation tasks and discuss their strengths and limitations.\n4. Consider integrating the simulator into commonly used robotic simulation frameworks to extend its usage on more general manipulation configurations.\n\nRecommendation:\n\nThe paper presents a comprehensive and robust simulator for studying contact-rich manipulations. While there are some limitations, the paper provides a valuable contribution to the field of robotics and tactile sensing. I recommend accepting the paper for publication in its current form, with the suggested improvements in mind.",
        "gr_score_with": 0.4676923076923077,
        "gr_score_without": 0.4127906976744186,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 142,
            "green_fraction": 0.4127906976744186,
            "z_score": 0.4842479984790938,
            "p_value": 0.314104938147415
        },
        "output_with": {
            "num_tokens_scored": 325,
            "num_green_tokens": 152,
            "green_fraction": 0.4676923076923077,
            "z_score": 2.49100947511811,
            "p_value": 0.006369036137428748
        }
    },
    {
        "title": "MOGANET: MULTI-ORDER GATED AGGREGATION NETWORK",
        "abstract": "By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks. However, recent progress on multi-order game-theoretic interaction within deep neural networks (DNNs) reveals the representation bottleneck of modern ConvNets, where the expressive interactions have not been effectively encoded with the increased kernel size. To tackle this challenge, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. MogaNet exhibits great scalability, impressive efficiency of parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D&3D human pose estimation, and video prediction. Notably, MogaNet hits 80.0% and 87.8% accuracy with 5.2M and 181M parameters on ImageNet-1K, outperforming ParC-Net and ConvNeXtL, while saving 59% FLOPs and 17M parameters, respectively. The source code is available at https://github.com/Westlake-AI/MogaNet.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively. The proposed architecture is designed to adaptively force the network to encode expressive interactions that would have otherwise been ignored inherently.\n\nStrengths:\n\n1. The paper provides a comprehensive review of the current state of deep neural networks (DNNs) and highlights the limitations of modern ConvNets in encoding expressive interactions.\n2. The proposed MogaNet architecture is designed to address the representation bottleneck of DNNs by incorporating multi-order game-theoretic interaction within DNNs.\n3. The paper presents extensive experiments on various vision tasks, including image classification, object detection, semantic segmentation, instance segmentation, pose estimation, etc., demonstrating the consistent efficiency of model parameters and competitive performance of MogaNet.\n4. The proposed Moga Block and channel aggregation module are compact and efficient, requiring a little extra computational overhead and parameters compared to existing methods.\n\nWeaknesses:\n\n1. The paper assumes that the essential adaptive nature of attention in ViTs has not been well leveraged and grafted into ConvNets, which may not be universally true.\n2. The proposed MogaNet architecture is designed to adaptively force the network to encode expressive interactions, but it is not clear how this affects the robustness and generalizability of the model.\n3. The paper does not provide a thorough comparison with other state-of-the-art architectures, such as Swin Transformers and ConvNeXt.\n4. The proposed Moga Block and channel aggregation module are designed to be compact and efficient, but it is not clear how they affect the performance of the model on more complex tasks.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough comparison with other state-of-the-art architectures, such as Swin Transformers and ConvNeXt.\n2. Investigate the robustness and generalizability of the proposed MogaNet architecture and provide more experiments to demonstrate its effectiveness.\n3. Provide more details on the design of the Moga Block and channel aggregation module, including the choice of hyperparameters and the impact of these choices on the performance of the model.\n4. Investigate the impact of the proposed MogaNet architecture on more complex tasks, such as video prediction and 3D reconstruction.\n\nRecommendation:\n\nThe paper presents a novel and efficient architecture for discriminative visual representation learning, which shows promising results on various vision tasks. However, further investigation is needed to fully understand the strengths and weaknesses of the proposed architecture and to demonstrate its effectiveness on more complex tasks. With some modifications and additional experiments, the proposed MogaNet architecture has the potential to become a competitive alternative to existing state-of-the-art architectures.\n\nRating: 7/10\n\nRecommendation for publication: Yes",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a new deep neural network architecture called MogaNet, which is designed to address the representation bottleneck of modern ConvNets by encouraging the network to learn expressive and adaptable interactions. MogaNet is built upon the modern ConvNet framework and consists of a spatial aggregation block and a channel aggregation block. The spatial aggregation block is designed to capture multi-order interactions by combining static and adaptive locality perceptions, while the channel aggregation block is designed to reallocate channel-wise information and capture game-theoretic interactions. The authors demonstrate that MogaNet achieves state-of-the-art performance on various vision tasks, including image classification, object detection, and semantic segmentation, while also exhibiting favorable computational overhead and parameter efficiency.\n\nStrengths:\n\n1. The paper proposes a new architecture that addresses the representation bottleneck of modern ConvNets by encouraging the network to learn expressive and adaptable interactions.\n2. The spatial aggregation block and channel aggregation block are well-designed and effective in capturing multi-order interactions and game-theoretic interactions.\n3. The paper provides extensive experimental results that demonstrate the superiority of MogaNet on various vision tasks.\n4. The authors also provide ablation studies that show the effectiveness of each component of MogaNet.\n\nWeaknesses:\n\n1. The paper assumes that the representation bottleneck of modern ConvNets is due to the lack of expressive and adaptable interactions, which may not be the only factor contributing to the bottleneck.\n2. The paper does not provide a thorough analysis of the limitations and challenges of MogaNet, such as its sensitivity to hyperparameters and its performance on complex datasets.\n3. The paper relies heavily on experimental results, but does not provide a detailed theoretical analysis of the architecture and its components.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the limitations and challenges of MogaNet, including its sensitivity to hyperparameters and its performance on complex datasets.\n2. The paper should provide a detailed theoretical analysis of the architecture and its components, including the spatial aggregation block and channel aggregation block.\n3. The authors should consider exploring other architectures that address the representation bottleneck of modern ConvNets, and compare their performance with MogaNet.\n\nRecommendation:\n\nI recommend accepting this paper for publication in its current form, subject to minor revisions that address the above suggestions for improvement. The paper makes significant contributions to the field of deep neural networks and provides a new architecture that addresses the representation bottleneck of modern ConvNets. The experimental results are extensive and demonstrate the superiority of MogaNet on various vision tasks. However, the paper would benefit from additional theoretical analysis and exploration of other architectures that address the representation bottleneck of modern ConvNets.\n\n$$$\nI hope you find my review helpful and informative.",
        "gr_score_with": 0.5073746312684366,
        "gr_score_without": 0.34549878345498786,
        "output_without": {
            "num_tokens_scored": 411,
            "num_green_tokens": 142,
            "green_fraction": 0.34549878345498786,
            "z_score": -2.255389148432661,
            "p_value": 0.987945543359603
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 172,
            "green_fraction": 0.5073746312684366,
            "z_score": 4.035486638442286,
            "p_value": 2.7244620341671496e-05
        }
    },
    {
        "title": "IDENTIFYING POLICY GRADIENT SUBSPACES",
        "abstract": "Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of gradient subspaces in the context of policy gradient methods for reinforcement learning. The authors aim to leverage insights from supervised learning, where gradient subspaces have been shown to improve learning efficiency, to improve the training efficiency of policy gradient methods. They conduct a comprehensive empirical evaluation of gradient subspaces in the context of two popular deep policy gradient methods, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), on various simulated benchmark tasks. The results demonstrate the existence of gradient subspaces in the RL setting, despite the continuously changing data distribution inherent to reinforcement learning.\n\nStrengths:\n\n1. The paper provides a thorough evaluation of the concept of gradient subspaces in the RL setting, building on insights from supervised learning.\n2. The authors conduct a comprehensive empirical evaluation of gradient subspaces in the context of two popular deep policy gradient methods, PPO and SAC.\n3. The results demonstrate the existence of gradient subspaces in the RL setting, which is a promising direction for future work on more efficient reinforcement learning.\n4. The paper provides a clear and concise presentation of the methods and results, making it easy to follow for readers.\n\nWeaknesses:\n\n1. The paper assumes a basic understanding of reinforcement learning and policy gradient methods, which may not be familiar to all readers.\n2. The paper does not provide a detailed analysis of the implications of the results for more efficient reinforcement learning.\n3. The paper does not provide a clear comparison with other methods that have been proposed for improving the training efficiency of policy gradient methods.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the implications of the results for more efficient reinforcement learning.\n2. Compare the results with other methods that have been proposed for improving the training efficiency of policy gradient methods.\n3. Provide a more detailed explanation of the assumptions and limitations of the methods used in the paper.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of reinforcement learning by exploring the concept of gradient subspaces in the context of policy gradient methods. The results demonstrate the existence of gradient subspaces in the RL setting, which is a promising direction for future work on more efficient reinforcement learning. However, the paper could benefit from a more detailed analysis of the implications of the results and a comparison with other methods. Overall, the paper is well-written and easy to follow, making it a valuable contribution to the field.\n\nRecommendation: Accept\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper explores the concept of gradient subspaces in the context of policy gradient methods for reinforcement learning. Building on insights from supervised learning, the authors investigate whether the gradients in policy gradient algorithms also reside in a low-dimensional, slowly-changing subspace. They conduct a comprehensive empirical evaluation of gradient subspaces in the context of two popular deep policy gradient methods, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), on various simulated RL benchmarks. The results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning.\n\nStrengths:\n\n* The paper provides a thorough analysis of gradient subspaces in policy gradient methods, which is a novel and timely topic in the field of reinforcement learning.\n* The experimental evaluation is comprehensive, including a variety of RL benchmarks and a comparison of on-policy and off-policy learning.\n* The results are well-supported by empirical evidence, including plots and tables that illustrate the properties of gradient subspaces in different phases of RL training.\n\nWeaknesses:\n\n* The paper assumes a good background in reinforcement learning and policy gradient methods, which may make it challenging for readers without prior knowledge in the field to understand the context and contributions of the paper.\n* The analysis is limited to a specific set of RL benchmarks and algorithms, which may not be representative of the broader RL literature.\n* The paper could benefit from a more in-depth discussion of the implications of the results, including potential future work on more efficient reinforcement learning through parameter-space exploration or second-order optimization.\n\nSuggestions for Improvement:\n\n* Consider adding a more detailed introduction to the background and context of policy gradient methods, including a brief overview of the RL literature and the importance of gradient subspaces in supervised learning.\n* Expand the experimental evaluation to include a wider range of RL benchmarks and algorithms, as well as a more detailed analysis of the properties of gradient subspaces in different phases of RL training.\n* Consider including a more in-depth discussion of the implications of the results, including potential future work on more efficient reinforcement learning through parameter-space exploration or second-order optimization.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of reinforcement learning by exploring the concept of gradient subspaces in policy gradient methods. The experimental evaluation is comprehensive, and the results are well-supported by empirical evidence. However, the paper could benefit from a more in-depth discussion of the implications of the results and a more detailed analysis of the properties of gradient subspaces in different phases of RL training. With some revisions, this paper has the potential to be a top-tier publication in the field of reinforcement learning.",
        "gr_score_with": 0.5202492211838006,
        "gr_score_without": 0.4482758620689655,
        "output_without": {
            "num_tokens_scored": 261,
            "num_green_tokens": 117,
            "green_fraction": 0.4482758620689655,
            "z_score": 1.5920058915156965,
            "p_value": 0.055691690389995024
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 167,
            "green_fraction": 0.5202492211838006,
            "z_score": 4.397736130525333,
            "p_value": 5.469292526727874e-06
        }
    },
    {
        "title": "PROVES ADVERSARIAL TRAINING",
        "abstract": "In standard adversarial training, models are optimized to fit invariant one-hot labels for adversarial data when the perturbations are within allowable budgets. However, the overconfident target harms generalization and causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that reflects the underlying distribution of data. By utilizing ADR, we can obtain rectified labels that improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-andplay integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called Annealing Self-Distillation Rectification (ADR) to enhance adversarial robustness in deep neural networks. The authors analyze the characteristics of robust models and identify that they tend to produce smoother and well-calibrated outputs. Based on this observation, they propose ADR, which generates soft labels as a better guidance mechanism that reflects the underlying distribution of data. The authors demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the robust models' output properties and identifies the key characteristics that distinguish them from non-robust models.\n2. The proposed ADR method is simple yet effective and can be easily integrated with other adversarial training techniques.\n3. The authors provide extensive experimental results across multiple datasets, demonstrating the efficacy of ADR in improving adversarial robustness.\n\nWeaknesses:\n\n1. The paper assumes that the EMA model is accurate and robust, which may not always be the case.\n2. The authors do not provide a thorough analysis of the trade-off between accuracy and robustness, which is a critical aspect of adversarial training.\n3. The paper relies heavily on empirical results, and more theoretical analysis would be beneficial to understand the underlying mechanisms of ADR.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough analysis of the trade-off between accuracy and robustness, including a theoretical understanding of the mechanisms involved.\n2. Investigate the robustness of the EMA model and provide a more robust method for generating soft labels.\n3. Provide a more detailed analysis of the hyperparameters and their impact on the performance of ADR.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of adversarial robustness and provides a novel and effective method for enhancing robustness. However, further investigation and analysis are required to fully understand the mechanisms involved and to improve the robustness of the EMA model. I recommend the paper for publication, but with the suggestion that the authors address the above concerns in future work.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called Annealing Self-Distillation Rectification (ADR) to enhance the robustness of deep neural networks against adversarial attacks. The authors analyze the properties of robust models and find that they exhibit good calibration and maintain output consistency on clean data and its adversarial counterpart. The proposed ADR method utilizes the self-distillation EMA model to provide labeling guidance for the trained model, with increasing trust placed in the EMA as training progresses. The authors demonstrate the effectiveness of ADR through extensive experiments on multiple datasets, showing that it improves robustness, alleviates robust overfitting, and obtains a better trade-off in terms of accuracy and robustness.\n\nStrengths:\n\n1. The paper presents a comprehensive analysis of the properties of robust models, which is a valuable contribution to the field of adversarial robustness.\n2. The proposed ADR method is simple yet effective, and its implementation is straightforward.\n3. The authors provide extensive experimental results on multiple datasets, which demonstrates the effectiveness of ADR.\n4. The paper discusses the trade-off between accuracy and robustness, which is an important issue in the field of adversarial robustness.\n\nWeaknesses:\n\n1. The paper assumes that the EMA model is a good surrogate for the true model, which may not always be the case.\n2. The authors do not provide a theoretical guarantee for the convergence of the ADR method.\n3. The choice of hyperparameters, such as the temperature and interpolation factor, is critical for the performance of ADR, but the authors do not provide guidance on how to choose them optimally.\n4. The paper does not provide a detailed analysis of the robustness of the proposed method against different types of attacks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the assumptions made in the paper, particularly the assumption that the EMA model is a good surrogate for the true model.\n2. The authors should provide a theoretical guarantee for the convergence of the ADR method, which would increase the confidence in the proposed approach.\n3. The authors should provide guidance on how to choose the hyperparameters optimally, which would make the method more practical for use in real-world applications.\n4. The authors should provide a detailed analysis of the robustness of the proposed method against different types of attacks, which would increase the confidence in the proposed approach.\n\nRecommendation:\n\nThe paper presents a valuable contribution to the field of adversarial robustness, and the proposed ADR method is simple yet effective. However, the authors should address the weaknesses mentioned above to increase the confidence in the proposed approach. The paper is suitable for publication in a reputable journal or conference, but the authors should make the necessary revisions to address the concerns mentioned above.",
        "gr_score_with": 0.5597667638483965,
        "gr_score_without": 0.45454545454545453,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 145,
            "green_fraction": 0.45454545454545453,
            "z_score": 1.9886038958388494,
            "p_value": 0.023372470526855652
        },
        "output_with": {
            "num_tokens_scored": 343,
            "num_green_tokens": 192,
            "green_fraction": 0.5597667638483965,
            "z_score": 6.039873984230452,
            "p_value": 7.711731729713302e-10
        }
    },
    {
        "title": "MERT: ACOUSTIC MUSIC UNDERSTANDING MODEL WITH LARGE-SCALE SELF-SUPERVISED TRAINING",
        "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the ConstantQ Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. The authors aim to address the research gap in music audio processing by designing a generalisable and computationally affordable pre-trained acoustic music model. MERT inherits a speech SSL paradigm, employing teacher models to generate pseudo targets for sequential audio clips. The authors explore a wide range of settings to overcome the instability in acoustic language model pre-training, allowing their designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that MERT can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.\n\nStrengths:\n\n1. The paper proposes a novel paradigm that integrates RVQ-VAE and CQT teacher models, providing a unique blend of acoustic and musical information necessary for MLM-based pre-training for music understanding.\n2. The authors demonstrate the effectiveness of their approach, MERT, in pre-training sizeable models, achieving SOTA or comparable results across a wide range of MIR tasks while using significantly smaller parameter size.\n3. The paper provides a thorough analysis of the impact of musical loss and different acoustic models on the performance of the acoustic language model, which can inform the future development of more effective and efficient models in the domain of acoustic processing.\n4. The authors propose several techniques to improve the training stability for the acoustic pre-training, including in-batch noise mixup data augmentation and Pre-LN.\n\nWeaknesses:\n\n1. The paper assumes that the MERT model can handle longer sequences with relative positional embedding, but the authors acknowledge that this approach could potentially limit their performance in tasks requiring a comprehensive understanding of extended musical contexts.\n2. The authors observe inverse-scaling effect in specific tasks while scaling-up to 330M, which indicates that their design could be further improved by stabilising the training.\n3. The paper relies on a specific dataset (Music4ALL) for training the MERT-95M-public model, which may not be representative of the broader music audio domain.\n\nSuggestions for Improvement:\n\n1. The authors could explore alternative methods for handling longer sequences, such as using a hierarchical or multi-resolution approach.\n2. The paper could benefit from a more thorough analysis of the inverse-scaling effect and its implications for the MERT design.\n3. The authors could consider using more diverse datasets for training the MERT models to improve their generalisability.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of music audio processing by proposing a novel paradigm for pre-training acoustic music models. The authors demonstrate the effectiveness of their approach, MERT, in achieving SOTA or comparable results across a wide range of MIR tasks while using significantly smaller parameter size. However, the paper assumes that the MERT model can handle longer sequences with relative positional embedding, and the authors observe inverse-scaling effect in specific tasks while scaling-up to 330M. To address these limitations, the authors could explore alternative methods for handling longer sequences and further stabilise the training process.\n\nOverall, the paper is well-written and provides a thorough analysis of the MERT approach. However, the authors should consider addressing the limitations mentioned above to further improve the paper's impact.\n\nRecommendation: Accept with minor revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. The MERT model is designed to capture the distinctive pitched and tonal characteristics in music by balancing acoustic and musical representation learning. The authors explore a wide range of settings to overcome the instability in acoustic language model pre-training and scale up the pre-training from 95M to 330M parameters. Experimental results indicate that the MERT model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.\n\nStrengths:\n\n1. The paper proposes a novel paradigm that integrates RVQ-VAE and CQT teacher models, providing a unique blend of acoustic and musical information necessary for MLM-based pre-training for music understanding.\n2. The authors conduct an extensive exploration of various settings to overcome the instability in acoustic language model pre-training, allowing the pre-training to scale up to 330M parameters.\n3. The MERT model achieves SOTA or comparable results across a wide range of MIR tasks while using significantly smaller parameter size.\n4. The paper provides valuable insights into the impact of musical loss and different acoustic models on the performance of the acoustic language model.\n\nWeaknesses:\n\n1. The paper is limited by the short 5-second training context, which may limit the performance of the MERT model in tasks requiring a comprehensive understanding of extended musical contexts.\n2. The authors observe inverse-scaling effect in specific tasks while scaling-up to 330M, which indicates that the design could be further improved by stabilising the training.\n3. The paper does not provide a detailed analysis of the pre-training process and the training stability issues, making it difficult to fully understand the limitations of the MERT model.\n\nSuggestions for Improvement:\n\n1. The authors could explore training the MERT model on longer sequences to address the limitations of the short 5-second training context.\n2. The paper could benefit from a more detailed analysis of the pre-training process and the training stability issues to fully understand the limitations of the MERT model.\n3. The authors could investigate alternative methods to stabilise the training and address the inverse-scaling effect observed in specific tasks.\n\nRecommendation:\n\nThis paper is well-written and provides valuable insights into the application of self-supervised learning in music audio understanding. The MERT model achieves impressive results on various MIR tasks, and the authors' exploration of various settings to overcome the instability in acoustic language model pre-training is commendable. However, the paper is limited by the short 5-second training context and the lack of detailed analysis of the pre-training process. Overall, I recommend this paper for publication in a top-tier conference in the field of machine learning and music information retrieval.",
        "gr_score_with": 0.4306930693069307,
        "gr_score_without": 0.342685370741483,
        "output_without": {
            "num_tokens_scored": 499,
            "num_green_tokens": 171,
            "green_fraction": 0.342685370741483,
            "z_score": -2.6134255910534914,
            "p_value": 0.9955180199746327
        },
        "output_with": {
            "num_tokens_scored": 404,
            "num_green_tokens": 174,
            "green_fraction": 0.4306930693069307,
            "z_score": 1.2592889187386982,
            "p_value": 0.1039629971393185
        }
    },
    {
        "title": "ADAPTING LARGE LANGUAGE MODELS VIA",
        "abstract": "We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension\u2014practice after reading improves the ability to answer questions based on the learned knowledge\u2014we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model\u2019s performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of adapting large language models (LLMs) through continued pre-training on domain-specific corpora. The authors propose a novel method for transforming raw corpora into reading comprehension texts, which enables the model to acquire domain knowledge while enhancing prompting ability. The method involves mining intrinsic tasks from raw texts using regex-based patterns and fine-tuning the model on these tasks. The authors demonstrate the effectiveness of their approach in three domains: biomedicine, finance, and law, and show that the transformed reading comprehension texts can improve model performance on various domain-specific tasks.\n\nStrengths:\n\n* The paper proposes a novel and effective method for adapting LLMs to specific domains.\n* The authors provide a comprehensive analysis of the strengths and weaknesses of their approach.\n* The experiments demonstrate the effectiveness of the method in three different domains.\n* The paper provides a clear and concise explanation of the proposed method and its implementation.\n\nWeaknesses:\n\n* The paper assumes that the raw corpora are available for each domain, which may not be the case in practice.\n* The authors do not provide a detailed explanation of the regex-based patterns used for task mining.\n* The paper does not provide a clear comparison with existing methods for adapting LLMs.\n* The experiments are limited to three domains, and it is unclear how the method would perform in other domains.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the regex-based patterns used for task mining.\n* Compare the proposed method with existing methods for adapting LLMs.\n* Conduct experiments in more domains to demonstrate the generalizability of the method.\n* Provide a clear explanation of the assumptions made about the availability of raw corpora.\n\nRecommendation:\n\nThe paper provides a novel and effective method for adapting LLMs to specific domains. While there are some limitations and weaknesses, the proposed method has the potential to improve model performance on various domain-specific tasks. I recommend the paper for publication, with the suggested improvements.\n\nRating: 8/10\n\nNote: The rating is based on the clarity, coherence, and contribution of the paper, as well as the effectiveness of the proposed method. The paper has the potential to make a significant contribution to the field of natural language processing, but there are some limitations and weaknesses that need to be addressed.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the use of large language models (LLMs) for adapting to specific domains. The authors investigate the effects of continued pre-training on domain-specific corpora on LLMs, revealing that while it enhances domain knowledge, it also hinders prompting performance. To address this issue, the authors propose a simple method for transforming raw corpora into reading comprehension texts, which enhances the model's ability to answer questions while retaining its domain knowledge. The proposed method is scalable and applicable to various domains, including biomedicine, finance, and law.\n\nStrengths:\n\n* The authors provide a comprehensive analysis of the effects of continued pre-training on LLMs, highlighting the trade-off between domain knowledge and prompting performance.\n* The proposed method for transforming raw corpora into reading comprehension texts is innovative and effective in enhancing the model's ability to answer questions while retaining its domain knowledge.\n* The experimental results demonstrate the effectiveness and generalizability of the proposed method across different domains.\n\nWeaknesses:\n\n* The paper assumes that the raw corpora are readily available, which may not be the case for all domains. The authors do not provide a clear explanation for handling domains with limited or no raw corpora.\n* The proposed method relies heavily on the quality and diversity of the raw corpora. The authors do not provide a clear explanation for handling cases where the raw corpora are biased or incomplete.\n* The paper does not provide a clear comparison with other existing methods for adapting LLMs to specific domains.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation for handling domains with limited or no raw corpora.\n* The authors should provide a more detailed analysis of the effects of raw corpus quality and diversity on the proposed method.\n* The authors should provide a clear comparison with other existing methods for adapting LLMs to specific domains.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of adapting LLMs to specific domains. The proposed method is innovative and effective, and the experimental results demonstrate its effectiveness across different domains. However, the paper assumes that the raw corpora are readily available, which may not be the case for all domains. The authors should provide a more detailed explanation for handling domains with limited or no raw corpora. Overall, the paper is well-written and provides a clear contribution to the field.\n\n$$$",
        "gr_score_with": 0.5343511450381679,
        "gr_score_without": 0.4674922600619195,
        "output_without": {
            "num_tokens_scored": 323,
            "num_green_tokens": 151,
            "green_fraction": 0.4674922600619195,
            "z_score": 2.475994134812186,
            "p_value": 0.006643288043633978
        },
        "output_with": {
            "num_tokens_scored": 262,
            "num_green_tokens": 140,
            "green_fraction": 0.5343511450381679,
            "z_score": 4.439012796286799,
            "p_value": 4.518622199924119e-06
        }
    },
    {
        "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
        "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that enables large language model (LLM)-based embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three essential functions: multimodal perception, foundational knowledge base, and skill prediction and planning. The authors construct an extensive dataset of 850K open-world instruction pairs to train Steve-Eye and develop three open-world evaluation benchmarks to validate the model's capability to strategically act and plan.\n\nStrengths:\n\n1. The paper presents a novel approach to multimodal interaction in open-world environments, addressing the limitations of text-only agents.\n2. The proposed model, Steve-Eye, demonstrates improved performance in multimodal perception, foundational knowledge question answering, and skill prediction and planning.\n3. The authors provide a comprehensive evaluation of the model's performance on three open-world benchmarks, including environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of pre-trained LLMs and visual encoders, which may limit the model's ability to generalize to new tasks and environments.\n2. The dataset collection process is labor-intensive and requires significant human annotation effort, which may not be scalable for larger datasets.\n3. The paper does not provide a thorough analysis of the model's limitations and failure cases, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more robust and generalizable pre-trained models that can handle a wide range of tasks and environments.\n2. Explore more efficient and scalable methods for dataset collection, such as active learning or transfer learning.\n3. Conduct a more comprehensive analysis of the model's limitations and failure cases to better understand its robustness and reliability.\n\nRecommendation:\n\nThis paper presents a novel and promising approach to multimodal interaction in open-world environments. While there are some limitations and areas for improvement, the proposed model demonstrates improved performance on three open-world benchmarks. I recommend this paper for publication in a reputable conference, with the understanding that the authors will address the limitations and suggestions for improvement outlined above.\n\nRating: 7.5/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper proposes Steve-Eye, a large multimodal model that combines a pre-trained large language model (LLM) with a visual encoder to enable embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three indispensable functions: multimodal perception, foundational knowledge base, and skill prediction and planning. To train Steve-Eye, the authors construct an extensive instruction dataset containing 850K multimodal instruction pairs and adopt a two-stage instruction-tuning strategy to fine-tune the model. The authors conduct experiments on three open-world benchmarks: environmental visual captioning (ENV-VC), foundational knowledge question answering (FK-QA), and skill prediction and planning (SPP). The results demonstrate that Steve-Eye outperforms the baselines in various aspects, showcasing its potential as a generative embodied agent in open-world environments.\n\nStrengths:\n1. The paper proposes a novel multimodal model, Steve-Eye, which combines the strengths of both LLMs and visual encoders to enable embodied agents to interact with the open world.\n2. The authors provide a thorough explanation of the architecture and the training process of Steve-Eye, making it easy to understand and implement.\n3. The paper presents a comprehensive evaluation of Steve-Eye on three open-world benchmarks, providing a thorough comparison with the baselines.\n4. The authors demonstrate the potential of Steve-Eye as a generative embodied agent in open-world environments, showcasing its capabilities in multimodal perception, foundational knowledge mastery, and skill prediction and planning.\n\nWeaknesses:\n1. The paper assumes the availability of a pre-trained LLM, which may not be feasible for all researchers.\n2. The authors rely heavily on the performance of the visual encoder, which may not be optimal for all tasks.\n3. The paper does not provide a thorough comparison of Steve-Eye with other multimodal models.\n4. The authors do not discuss the potential applications of Steve-Eye beyond the open-world benchmarks presented in the paper.\n\nSuggestions for Improvement:\n1. To make Steve-Eye more accessible to researchers, the authors should provide a pre-trained version of the model or a detailed guide on how to train it from scratch.\n2. To improve the performance of the visual encoder, the authors should consider exploring other visual encoder architectures or fine-tuning the existing one on a larger dataset.\n3. To provide a more comprehensive comparison of Steve-Eye with other multimodal models, the authors should consider including additional benchmarks or tasks.\n4. To demonstrate the potential applications of Steve-Eye, the authors should consider exploring other tasks or domains where a generative embodied agent can be beneficial.\n\nRecommendation:\nThis paper proposes a novel multimodal model, Steve-Eye, which has the potential to enable embodied agents to interact with the open world in a more effective and efficient manner. While the paper has some limitations, the results demonstrate the capabilities of Steve-Eye in multimodal perception, foundational knowledge mastery, and skill prediction and planning. Therefore, I recommend this paper for publication in a top-tier conference or journal in the field of multimodal learning and embodied agents.\n$$",
        "gr_score_with": 0.5062344139650873,
        "gr_score_without": 0.42042042042042044,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 140,
            "green_fraction": 0.42042042042042044,
            "z_score": 0.760643529242559,
            "p_value": 0.22343500634365038
        },
        "output_with": {
            "num_tokens_scored": 401,
            "num_green_tokens": 203,
            "green_fraction": 0.5062344139650873,
            "z_score": 4.34241965715003,
            "p_value": 7.046101316290126e-06
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem induced by a soft DSF. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nStrengths:\n\n* The paper proposes a novel sorting network that learns to sort high-dimensional inputs.\n* The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n* The authors provide a detailed analysis of the error accumulation problem and propose an error-free DSF to resolve it.\n* The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nWeaknesses:\n\n* The paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications.\n* The proposed method may not be effective for sorting low-dimensional inputs or inputs with clear ordering information.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n* The paper assumes that the supervision is provided in the form of permutation matrices, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The paper should discuss the limitations of the proposed method and provide examples of real-world applications where it may not be effective.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method on a wider range of benchmarks.\n* The paper should discuss the potential negative societal impact of the proposed method, such as the potential for bias in the sorting process.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.\n\nOverall, the paper is well-written and provides a clear and concise introduction to the problem of sorting high-dimensional inputs. The proposed method is novel and effective, and the paper provides a detailed analysis of the error accumulation problem and proposes an error-free DSF to resolve it. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to sorting networks with differentiable swap functions (DSFs), which is a crucial component in neural sorting networks. The authors define a softening error that measures the difference between the original and smoothed values, and propose an error-free DSF that resolves the error accumulation problem of conventional DSFs. They also adopt a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs and leverage the model capacity of the neural network. The paper demonstrates the effectiveness of the proposed methods in diverse sorting benchmarks, including sorting multi-digit images and image fragments.\n\nStrengths:\n\n1. The paper presents a clear and concise introduction to the problem of sorting networks with differentiable swap functions.\n2. The authors propose a novel approach to resolving the error accumulation problem of conventional DSFs.\n3. The paper presents a thorough theoretical analysis of the proposed method, including the derivation of the softening error and the development of the error-free DSF.\n4. The authors demonstrate the effectiveness of the proposed method in diverse sorting benchmarks.\n5. The paper presents a clear and concise comparison with existing methods.\n\nWeaknesses:\n\n1. The paper assumes that the input data is high-dimensional, which may not be the case in all scenarios.\n2. The authors do not provide a clear explanation of how the proposed method can be used in scenarios where the input data is not high-dimensional.\n3. The paper does not discuss the potential negative societal impact of the proposed method, such as the potential for biased sorting.\n4. The authors do not provide a clear comparison with existing methods that do not use differentiable swap functions.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how the proposed method can be used in scenarios where the input data is not high-dimensional.\n2. Discuss the potential negative societal impact of the proposed method and propose potential solutions.\n3. Provide a clear comparison with existing methods that do not use differentiable swap functions.\n4. Consider adding more experimental results to demonstrate the effectiveness of the proposed method.\n\nRecommendation:\n\nThe paper presents a novel approach to sorting networks with differentiable swap functions, which is a crucial component in neural sorting networks. The authors propose a novel approach to resolving the error accumulation problem of conventional DSFs, and demonstrate the effectiveness of the proposed method in diverse sorting benchmarks. The paper presents a clear and concise introduction to the problem, a thorough theoretical analysis, and a clear comparison with existing methods. However, the paper assumes that the input data is high-dimensional, which may not be the case in all scenarios, and does not discuss the potential negative societal impact of the proposed method. Therefore, the paper deserves publication in a reputable conference or journal after addressing the weaknesses and suggestions for improvement.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.49158249158249157,
        "gr_score_without": 0.40892193308550184,
        "output_without": {
            "num_tokens_scored": 269,
            "num_green_tokens": 110,
            "green_fraction": 0.40892193308550184,
            "z_score": 0.2986960509531687,
            "p_value": 0.3825859852312951
        },
        "output_with": {
            "num_tokens_scored": 297,
            "num_green_tokens": 146,
            "green_fraction": 0.49158249158249157,
            "z_score": 3.2216997138181123,
            "p_value": 0.0006371630789800028
        }
    },
    {
        "title": "EFFECTIVELY LEVERAGING CAPACITY FOR IMPROVED DETERMINISTIC ROBUSTNESS CERTIFICATION",
        "abstract": "Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards underfitting than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large \u201cCholesky-orthogonalized residual dense\u201d layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to improve the deterministic robustness certification of neural networks using Lipschitz-based methods. The authors argue that existing Lipschitz-based models have limited capacity, which hinders their ability to overfit small datasets. To address this challenge, they propose three solutions: (1) combining dense layers and convolutions to expand model capacity, (2) introducing the Cholesky Residual Layer as an efficient building block for achieving orthogonal weights, and (3) exploring an improved pipeline for utilizing generated data to enhance Lipschitz-based training. The authors demonstrate the effectiveness of their approach through extensive experiments and achieve state-of-the-art deterministic certified accuracy on CIFAR-10/100 datasets.\n\nStrengths:\n\n1. The paper presents a comprehensive evaluation of the design space for Lipschitz-based certified training, exploring three primary axes: architecture choice, methods for controlling the Lipschitz constant, and data augmentation.\n2. The authors propose three innovative solutions to mitigate the limited capacity of existing Lipschitz-based models, including the Cholesky Residual Layer and an improved pipeline for utilizing generated data.\n3. The paper provides extensive experimental results, including ablation studies and comparisons with state-of-the-art methods, demonstrating the effectiveness of the proposed approach.\n\nWeaknesses:\n\n1. The paper assumes that the generated data is of better quality, which may not always be the case. The authors' conclusion that generated data are easier to classify on average is not entirely convincing, as it is not clear why this would be the case.\n2. The paper does not provide a thorough analysis of the trade-offs between model capacity, computational cost, and certification robustness.\n3. The authors do not discuss the potential limitations of their approach, such as its applicability to larger datasets or more complex models.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the trade-offs between model capacity, computational cost, and certification robustness.\n2. Investigate the potential limitations of the proposed approach, such as its applicability to larger datasets or more complex models.\n3. Consider exploring other methods for improving model capacity, such as using more advanced architecture designs or incorporating additional training data.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to improving the deterministic robustness certification of neural networks using Lipschitz-based methods. While there are some limitations to the proposed approach, the authors' innovative solutions and extensive experimental results make a strong case for its potential impact. I recommend this paper for publication in a top-tier conference, with the suggestion that the authors address the limitations mentioned above in future work.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to enhancing the certified robustness of neural networks by leveraging Lipschitz-based methods. The authors argue that existing Lipschitz-based models have limited capacity, hindering their ability to overfit small datasets. To address this challenge, they reexamine network architectures and basic building blocks to control network Lipschitz and propose three solutions: (1) combining dense layers and convolutions to expand model capacity, (2) introducing the Cholesky Residual Layer for efficient orthogonal weight achievement, and (3) exploring an improved pipeline for utilizing generated data to enhance Lipschitz-based training.\n\nStrengths:\n\n1. The authors provide a comprehensive evaluation of Lipschitz-based certification methods, shedding light on the potential of these methods and their limitations.\n2. The proposed solutions are well-motivated and grounded in theoretical understanding of Lipschitz-based methods.\n3. The authors conduct extensive experiments to demonstrate the effectiveness of their approach, pushing the boundaries of deterministic certified accuracy on CIFAR-10/100 datasets.\n\nWeaknesses:\n\n1. The paper assumes that the primary challenge with Lipschitz-based models is their limited capacity, which may not be the only factor contributing to their performance gaps with RS-based methods.\n2. The authors do not provide a thorough comparison with RS-based methods, which are currently the state-of-the-art in certified robustness.\n3. The proposed solutions may require significant modifications to existing architectures and training pipelines, which may hinder their practical adoption.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with RS-based methods, including their strengths and weaknesses, to better understand the trade-offs between deterministic and probabilistic certification methods.\n2. The proposed solutions should be more thoroughly evaluated on a wider range of datasets and architectures to assess their generalizability and robustness.\n3. The authors should provide more insights into the theoretical foundations of their proposed solutions, such as the relationship between Lipschitz constants and model capacity.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to enhancing certified robustness using Lipschitz-based methods. While there are some limitations and areas for improvement, the authors demonstrate the effectiveness of their approach on CIFAR-10/100 datasets. With further refinement and evaluation, this work has the potential to bridge the gap between probabilistic and deterministic certification methods.\n\nOverall, I recommend this paper for publication in a top-tier conference, pending revisions that address the suggestions for improvement.\n\nScore: 8.5/10\n\nRecommendation for publication: Yes\n\nReviewing time: 2-3 weeks\n\nRequired revisions: Minor\n\nFinal decision: Conditional acceptance\n\nReviewing author: [Name]\n\nDate: [Today's date]\n\nNote: The score and recommendation for publication are based on the paper's technical quality, novelty, and potential impact. The reviewing time and required revisions are based on the complexity and scope of the paper. The final decision is subject to revision based on the authors' response to the review.",
        "gr_score_with": 0.5412262156448203,
        "gr_score_without": 0.4350132625994695,
        "output_without": {
            "num_tokens_scored": 377,
            "num_green_tokens": 164,
            "green_fraction": 0.4350132625994695,
            "z_score": 1.3877065406528928,
            "p_value": 0.08261321177356984
        },
        "output_with": {
            "num_tokens_scored": 473,
            "num_green_tokens": 256,
            "green_fraction": 0.5412262156448203,
            "z_score": 6.269606315748618,
            "p_value": 1.8098104246567466e-10
        }
    },
    {
        "title": "CODINGS FOR GRAPHS",
        "abstract": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) Non-uniqueness: there are many different eigendecompositions of the same Laplacian, and (2) Instability: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address nonuniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a \u201chard partition\u201d of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to \u201csoftly partition\u201d eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Stable and Expressive Positional Encodings (SPE), a novel architecture for processing eigenvectors that uses eigenvalues to \"softly partition\" eigenspaces, achieving both stability and expressivity. The authors identify the cause of instability in existing methods to be a \"hard partition\" of eigenspaces and design SPE to avoid this issue. They prove that SPE is provably stable and universally expressive for basis invariant functions while respecting all symmetries of eigenvectors. The authors also demonstrate the effectiveness of SPE on molecular property prediction benchmarks, graph substructure counting tasks, and out-of-distribution generalization tasks.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the limitations of existing positional encoding methods, particularly their instability and lack of expressivity.\n2. The authors propose a novel architecture, SPE, that addresses these limitations and achieves both stability and expressivity.\n3. The paper provides a clear and concise presentation of the SPE architecture and its theoretical guarantees.\n4. The authors demonstrate the effectiveness of SPE on a range of tasks, including molecular property prediction, graph substructure counting, and out-of-distribution generalization.\n5. The paper provides a comprehensive discussion of related work and places SPE in the context of existing methods.\n\nWeaknesses:\n\n1. The paper assumes a specific graph structure and adjacency matrix, which may not be generalizable to all types of graphs.\n2. The authors rely on the continuity of the neural networks used in SPE, which may not hold in all cases.\n3. The paper assumes that the eigenvalues and eigenvectors are known, which may not be the case in practice.\n4. The authors do not provide a clear explanation of how to choose the hyperparameters of SPE.\n5. The paper does not provide a comprehensive comparison of SPE with other methods that do not use eigenvalues.\n\nSuggestions for Improvement:\n\n1. Provide a more general framework for SPE that can handle different types of graphs and adjacency matrices.\n2. Investigate the robustness of SPE to different neural network architectures and activation functions.\n3. Provide a more detailed explanation of how to choose the hyperparameters of SPE.\n4. Compare SPE with other methods that do not use eigenvalues to demonstrate its advantages.\n5. Provide a more comprehensive discussion of the theoretical guarantees of SPE and its relationship to existing methods.\n\nRecommendation:\n\nThe paper presents a novel and effective architecture for processing eigenvectors that achieves both stability and expressivity. The authors provide a clear and concise presentation of the SPE architecture and its theoretical guarantees, and demonstrate its effectiveness on a range of tasks. While there are some limitations and areas for improvement, the paper makes a significant contribution to the field of graph learning and is worth publishing.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel positional encoding method for graphs called Stable and Expressive Positional Encodings (SPE). The main contribution of the paper is to design a positional encoding method that is both stable and expressive. The proposed method uses a soft partitioning of eigenspaces, leveraging eigenvalues to ensure stability and expressivity. The authors provide a theoretical analysis of the proposed method, showing that it is stable and universally expressive. The empirical results demonstrate the effectiveness of SPE on various graph tasks, including molecular property prediction and out-of-distribution generalization.\n\nStrengths:\n\n* The paper presents a well-structured and clear argument for the need to design stable and expressive positional encoding methods for graphs.\n* The proposed method, SPE, is theoretically sound and provides a novel solution to the problem of designing stable and expressive positional encoding methods.\n* The empirical results are comprehensive and demonstrate the effectiveness of SPE on various graph tasks.\n\nWeaknesses:\n\n* The paper assumes a deep understanding of graph neural networks and Laplacian eigenvectors, which may be a limitation for readers without a background in these areas.\n* The proposed method relies heavily on the use of eigenvalues, which may not be applicable to all types of graphs.\n* The paper could benefit from more discussion on the implications of the proposed method for real-world applications.\n\nSuggestions for Improvement:\n\n* Provide more intuition and explanation for the design choices made in the proposed method.\n* Consider including more experiments to evaluate the robustness of SPE to different types of graph structures and perturbations.\n* Provide more discussion on the implications of the proposed method for real-world applications.\n\nRecommendation:\n\nOverall, the paper presents a well-structured and clear argument for the need to design stable and expressive positional encoding methods for graphs. The proposed method, SPE, is theoretically sound and provides a novel solution to the problem of designing stable and expressive positional encoding methods. The empirical results are comprehensive and demonstrate the effectiveness of SPE on various graph tasks. I recommend accepting the paper for publication.\n\nRating: 8/10\n\nReview:\n\nThe paper presents a well-structured and clear argument for the need to design stable and expressive positional encoding methods for graphs. The proposed method, SPE, is theoretically sound and provides a novel solution to the problem of designing stable and expressive positional encoding methods. The empirical results are comprehensive and demonstrate the effectiveness of SPE on various graph tasks.\n\nHowever, the paper assumes a deep understanding of graph neural networks and Laplacian eigenvectors, which may be a limitation for readers without a background in these areas. The proposed method relies heavily on the use of eigenvalues, which may not be applicable to all types of graphs.\n\nOverall, I believe that the paper presents a valuable contribution to the field of graph neural networks and positional encoding methods. I recommend accepting the paper for publication.\n\n$$$",
        "gr_score_with": 0.4103448275862069,
        "gr_score_without": 0.3026315789473684,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 115,
            "green_fraction": 0.3026315789473684,
            "z_score": -3.8743986516981836,
            "p_value": 0.9999465558425772
        },
        "output_with": {
            "num_tokens_scored": 290,
            "num_green_tokens": 119,
            "green_fraction": 0.4103448275862069,
            "z_score": 0.35959747611403814,
            "p_value": 0.35957408563329596
        }
    },
    {
        "title": "GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS",
        "abstract": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson\u2019s r) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces GeoLLM, a novel method that extracts geospatial knowledge from large language models (LLMs) using auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that their method can effectively unlock this knowledge across various models and tasks. They fine-tune LLMs using prompts constructed with map data and evaluate their performance on a range of geospatial prediction tasks. The results show that GeoLLM outperforms traditional baselines, including k-NN and XGBoost, and even exceeds the performance of satellite-based methods. The authors also observe that the performance of their method scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper presents a novel method for extracting geospatial knowledge from LLMs, which is a significant contribution to the field.\n2. The authors provide a comprehensive evaluation of their method on a range of geospatial prediction tasks, demonstrating its effectiveness and scalability.\n3. The paper highlights the potential of LLMs for geospatial tasks, which is an exciting area of research with many potential applications.\n4. The authors provide a clear and concise explanation of their method and its evaluation, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with LLMs and their capabilities, which may not be the case for all readers.\n2. The evaluation of the method is limited to a few geospatial prediction tasks, and it is not clear how well the method would perform on other tasks or in different contexts.\n3. The paper does not provide a detailed analysis of the biases and limitations of LLMs, which is an important consideration for geospatial applications.\n4. The authors do not discuss the potential risks and challenges associated with using LLMs for geospatial tasks, such as the potential for bias and error.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of LLMs and their capabilities, including their strengths and limitations, to make the paper more accessible to a wider audience.\n2. Evaluate the method on a more diverse range of geospatial prediction tasks and in different contexts to demonstrate its robustness and generalizability.\n3. Conduct a more detailed analysis of the biases and limitations of LLMs, including their potential impact on geospatial applications.\n4. Discuss the potential risks and challenges associated with using LLMs for geospatial tasks, including the potential for bias and error.\n\nRecommendation:\n\nThis paper is a significant contribution to the field of geospatial AI and has the potential to inspire further research and development in this area. However, to make it more accessible and impactful, the authors should address the weaknesses and suggestions for improvement outlined above. With some revisions, this paper could be a leading publication in the field of geospatial AI and a valuable resource for researchers and practitioners working in this area.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel method, GeoLLM, that can efficiently extract geospatial knowledge from large language models (LLMs) using auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess remarkable geospatial knowledge and can be fine-tuned to achieve state-of-the-art performance on various geospatial tasks, including predicting population density, asset wealth, and economic livelihoods. The method outperforms traditional baselines, including k-NN and XGBoost, and even exceeds the performance of satellite-based methods. The authors also show that the performance of GeoLLM scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper presents a novel and effective method for extracting geospatial knowledge from LLMs, which can be applied to various geospatial tasks.\n2. The authors provide a comprehensive benchmark and compare the performance of GeoLLM with traditional baselines and satellite-based methods, demonstrating its superiority.\n3. The paper highlights the sample efficiency, richness, and robustness of LLMs in geospatial tasks, which is a significant contribution to the field.\n4. The authors provide a detailed analysis of the performance of GeoLLM across various tasks and sample sizes, which provides valuable insights into its capabilities and limitations.\n\nWeaknesses:\n\n1. The paper relies heavily on the pre-existing geospatial knowledge embedded in the LLMs, which may limit its ability to generalize to new or unfamiliar geospatial contexts.\n2. The authors do not provide a detailed analysis of the potential biases of LLMs in geospatial tasks, which may be an important consideration for practical applications.\n3. The paper does not explore the potential limitations of using auxiliary map data from OpenStreetMap, which may not be available or up-to-date in all regions.\n4. The authors do not provide a clear explanation of how the method can be scaled up to accommodate larger or more complex geospatial tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the potential biases of LLMs in geospatial tasks and discuss potential strategies for mitigating these biases.\n2. The paper should explore the potential limitations of using auxiliary map data from OpenStreetMap and discuss alternative data sources or methods for geospatial knowledge extraction.\n3. The authors should provide a clear explanation of how the method can be scaled up to accommodate larger or more complex geospatial tasks, including potential challenges and limitations.\n4. The paper should discuss potential applications of GeoLLM beyond geospatial tasks, such as environmental monitoring or public health surveillance.\n\nRecommendation:\n\nThe paper presents a novel and effective method for extracting geospatial knowledge from LLMs, which has the potential to revolutionize the field of geospatial analysis. While there are some limitations and potential areas for improvement, the paper provides a comprehensive and well-executed analysis of the method's capabilities and limitations. I recommend the paper for publication in a top-tier conference or journal in the field of geospatial analysis or machine learning.",
        "gr_score_with": 0.5662337662337662,
        "gr_score_without": 0.47761194029850745,
        "output_without": {
            "num_tokens_scored": 402,
            "num_green_tokens": 192,
            "green_fraction": 0.47761194029850745,
            "z_score": 3.176405553263935,
            "p_value": 0.0007455616721378128
        },
        "output_with": {
            "num_tokens_scored": 385,
            "num_green_tokens": 218,
            "green_fraction": 0.5662337662337662,
            "z_score": 6.658003028611833,
            "p_value": 1.3878652902416784e-11
        }
    },
    {
        "title": "LEARNING THE GREATEST COMMON DIVISOR: EXPLAINING TRANSFORMER PREDICTIONS",
        "abstract": "The predictions of small transformers, trained to calculate the greatest common divisor (GCD) of two positive integers, can be fully characterized by looking at model inputs and outputs. As training proceeds, the model learns a listD of integers, products of divisors of the base used to represent integers and small primes, and predicts the largest element of D that divides both inputs. Training distributions impact performance. Models trained from uniform operands only learn a handful of GCD (up to 38 GCD \u2264 100). Log-uniform operands boost performance to 73 GCD \u2264 100, and a log-uniform distribution of outcomes (i.e. GCD) to 91. However, training from uniform (balanced) GCD breaks explainability.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the capabilities of small transformers in learning the greatest common divisor (GCD) of two positive integers. The authors train 4-layer transformers to compute GCD and observe that the model learns to cluster input pairs with the same GCD and predicts the largest element in a learned set of integers that divides both inputs. The results show that models trained from log-uniform operands and outcomes achieve better performance and that the distribution of GCD impacts model performance. The authors also investigate the role of model size, training distributions, and the learning process.\n\nStrengths:\n\n* The paper presents a novel approach to model explainability by fully characterizing black-box model predictions.\n* The authors demonstrate the ability of small transformers to learn advanced calculations, such as GCD, which is an important operation for rational arithmetic and number theory.\n* The paper provides a thorough analysis of the learning process and the role of the distribution of outcomes in model performance.\n* The results show that models can be trained to learn small GCD, which is a challenging task.\n\nWeaknesses:\n\n* The paper assumes that the GCD of two integers can be represented as a sequence of tokens, which may not be the case for large integers.\n* The authors do not provide a clear explanation of how the model learns to predict the GCD of two integers.\n* The paper assumes that the model learns a set of integers D, which is not explicitly defined.\n* The results are based on a specific implementation of the transformer architecture and may not generalize to other architectures.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the model learns to predict the GCD of two integers.\n* Define the set of integers D and explain how it is learned by the model.\n* Investigate the robustness of the model to different training distributions and outcomes.\n* Provide more results on the performance of the model on larger integers and more complex calculations.\n\nRecommendation:\n\nThe paper provides a novel approach to model explainability and demonstrates the ability of small transformers to learn advanced calculations. However, the paper assumes a specific implementation of the transformer architecture and may not generalize to other architectures. The results are based on a specific implementation and may not be representative of other models. Therefore, the paper should be considered a proof-of-concept rather than a final result.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the capabilities of small transformers in performing arithmetic operations, specifically the greatest common divisor (GCD) calculation. The authors train 4-layer transformers on pairs of positive integers, represented as sequences of digits in base B, and observe that the model learns to predict the largest element in a list of integers, products of divisors of the base used to represent integers and small primes, that divides both inputs. The results demonstrate that transformers can be trained to perform exact calculations involving integer divisibility, and the broader potential impact of this research extends in three directions, including presenting a new approach to model explainability, the results on log-uniform training distributions of operands and outcomes, and the application of this research to foundational models for science.\n\nStrengths:\n\n1. The paper presents a thorough and well-structured exploration of the capabilities of small transformers in performing arithmetic operations, specifically the GCD calculation.\n2. The authors provide a clear and concise explanation of the experimental settings and the results obtained, making it easy to follow and understand the paper.\n3. The results demonstrate that transformers can be trained to perform exact calculations involving integer divisibility, which is an important achievement in the field of artificial intelligence and machine learning.\n4. The paper presents a new approach to model explainability, which is an important aspect of AI research, and the results obtained using this approach are impressive.\n\nWeaknesses:\n\n1. The paper assumes that the readers are familiar with the basics of machine learning and transformers, which may not be the case for readers who are new to the field.\n2. The experimental settings are limited to small transformers and the results obtained may not be generalizable to larger transformers or more complex tasks.\n3. The paper does not provide a clear explanation of the theoretical underpinnings of the results obtained, which may make it difficult for readers to understand the implications of the findings.\n4. The paper could benefit from more detailed discussions of the limitations and potential biases of the experimental settings and the results obtained.\n\nSuggestions for Improvement:\n\n1. The authors could provide more detailed explanations of the theoretical underpinnings of the results obtained, which would help readers understand the implications of the findings.\n2. The experimental settings could be expanded to include larger transformers and more complex tasks, which would help to determine the generalizability of the results obtained.\n3. The paper could benefit from more detailed discussions of the limitations and potential biases of the experimental settings and the results obtained, which would help readers to better understand the findings.\n4. The authors could provide more detailed discussions of the potential applications of this research, which would help readers to better understand the broader implications of the findings.\n\nRecommendation:\n\nThe paper presents an impressive exploration of the capabilities of small transformers in performing arithmetic operations, specifically the GCD calculation. The results obtained are impressive and demonstrate the potential of transformers in performing exact calculations involving integer divisibility. The paper presents a new approach to model explainability, which is an important aspect of AI research. The experimental settings are well-structured and the results obtained are clear and concise. The paper could benefit from more detailed discussions of the theoretical underpinnings of the results obtained, the limitations and potential biases of the experimental settings and the results obtained, and the potential applications of this research. Overall, the paper is well-written and presents an impressive exploration of the capabilities of small transformers in performing arithmetic operations.",
        "gr_score_with": 0.5955678670360111,
        "gr_score_without": 0.4748427672955975,
        "output_without": {
            "num_tokens_scored": 318,
            "num_green_tokens": 151,
            "green_fraction": 0.4748427672955975,
            "z_score": 2.7243178516489985,
            "p_value": 0.0032217230817911985
        },
        "output_with": {
            "num_tokens_scored": 361,
            "num_green_tokens": 215,
            "green_fraction": 0.5955678670360111,
            "z_score": 7.584823501775981,
            "p_value": 1.6646998513548583e-14
        }
    },
    {
        "title": "EX-GRAPH: A PIONEERING DATASET BRIDGING ETHEREUM",
        "abstract": "While numerous public blockchain datasets are available, their utility is constrained by an exclusive focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be derived. To address the above limitation, we introduce EX-Graph, a novel dataset that authentically links Ethereum and X, marking the first and largest dataset of its kind. EX-Graph combines Ethereum transaction records (2 million nodes and 30 million edges) and X following data (1 million nodes and 3 million edges), bonding 30,667 Ethereum addresses with verified X accounts sourced from OpenSea. Detailed statistical analysis on EXGraph highlights the structural differences between X-matched and non-X-matched Ethereum addresses. Extensive experiments, including Ethereum link prediction, wash-trading Ethereum addresses detection, and X-Ethereum matching link prediction, emphasize the significant role of X data in enhancing Ethereum analysis. EX-Graph is available at https://exgraph.deno.dev/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces EX-Graph, a novel dataset that bridges Ethereum and X, combining Ethereum transaction records and X follower network data. The dataset consists of approximately 3 million nodes, 33 million edges, and 30,667 matching links between Ethereum addresses and corresponding X accounts. The authors conduct a comprehensive analysis of EX-Graph using statistical and empirical methods, highlighting the structural differences between X-matched and non-X-matched Ethereum addresses. They also demonstrate the significant role of X data in enhancing Ethereum analysis, particularly in link prediction, wash-trading address detection, and matching link prediction.\n\nStrengths:\n\n1. The paper presents a pioneering and extensive dataset that bridges Ethereum and X, providing a valuable resource for future Ethereum-focused research.\n2. The authors conduct a comprehensive analysis of EX-Graph using both statistical and empirical methods, highlighting the structural differences between X-matched and non-X-matched Ethereum addresses.\n3. The paper demonstrates the significant role of X data in enhancing Ethereum analysis, particularly in link prediction, wash-trading address detection, and matching link prediction.\n4. The authors provide a clear and detailed description of the data collection process, graph construction, and feature extraction methods used in EX-Graph.\n\nWeaknesses:\n\n1. The paper relies heavily on existing datasets and methods, such as the Ethereum dataset and graph neural networks, without providing significant contributions to these areas.\n2. The authors do not discuss the potential limitations and biases of EX-Graph, such as the reliance on OpenSea data and the potential for data imbalance.\n3. The paper focuses primarily on the benefits of integrating X data with Ethereum analysis, without exploring potential drawbacks or alternative approaches.\n4. The authors do not provide a clear explanation for the choice of specific graph neural network models and their hyperparameters.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed discussion of the potential limitations and biases of EX-Graph, and explore alternative approaches to address these issues.\n2. Consider incorporating more diverse datasets and methods to complement EX-Graph and provide a more comprehensive understanding of Ethereum analysis.\n3. Provide a clear explanation for the choice of specific graph neural network models and their hyperparameters, and explore the impact of different model choices on the results.\n4. Consider conducting a more in-depth analysis of the structural differences between X-matched and non-X-matched Ethereum addresses, and exploring the implications of these differences for Ethereum analysis.\n\nRecommendation:\n\nThe paper presents a valuable contribution to the field of Ethereum analysis, highlighting the significant role of X data in enhancing Ethereum analysis. However, the paper relies heavily on existing datasets and methods, and does not provide significant contributions to these areas. To improve the paper, the authors should provide a more detailed discussion of the potential limitations and biases of EX-Graph, and explore alternative approaches to address these issues. Additionally, the authors should provide a clear explanation for the choice of specific graph neural network models and their hyperparameters, and explore the impact of different model choices on the results.\n\nOverall, the paper is well-written and provides a clear and detailed description of the data collection process, graph construction, and feature extraction methods used in EX-Graph. However, the paper could benefit from a more in-depth analysis of the structural differences between X-matched and non-X-matched Ethereum addresses, and a more detailed discussion of the potential limitations and biases of EX-Graph.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces EX-Graph, a pioneering and extensive open-source dataset that bridges Ethereum and X (a social media platform). EX-Graph combines Ethereum transaction records (2 million nodes and 30 million edges) and X following data (1 million nodes and 3 million edges) via verified matching links between Ethereum addresses and X accounts. The paper conducts a comprehensive analysis of EX-Graph using both statistical and empirical methods and demonstrates that integrating X data significantly enhances Ethereum task performances. Specifically, the paper shows that including X features improves Ethereum link prediction by up to 8% AUC, enhances the detection of wash-trading Ethereum addresses by up to 18% recall, and supports the innovative task of predicting matching links between Ethereum addresses and their matching X accounts with 74% AUC.\n\nStrengths:\n\n*   The paper introduces a novel and comprehensive dataset that bridges on-chain and off-chain worlds, providing valuable insights into Ethereum activities.\n*   The paper demonstrates the effectiveness of integrating off-chain data (X) with on-chain data (Ethereum) to improve various tasks on Ethereum.\n*   The paper employs a range of baseline models and evaluation metrics to provide a comprehensive assessment of the performance of EX-Graph.\n*   The paper recognizes the limitations of the current dataset and suggests potential areas for improvement and future work.\n\nWeaknesses:\n\n*   The paper relies heavily on the assumption that X accounts and Ethereum addresses belong to the same entity, which may not always be the case.\n*   The paper does not provide a detailed analysis of the limitations and biases of the collected data.\n*   The paper only considers a limited range of baseline models and evaluation metrics, which may not capture the full range of possible performance metrics.\n*   The paper does not provide a clear explanation of how the dataset is updated and maintained, which is crucial for its long-term value.\n\nSuggestions for Improvement:\n\n*   To address the assumption that X accounts and Ethereum addresses belong to the same entity, the paper could consider using more advanced methods to verify the matching links between on-chain and off-chain graphs.\n*   To provide a more comprehensive analysis of the limitations and biases of the collected data, the paper could consider using more advanced statistical and machine learning methods to detect and mitigate biases.\n*   To provide a more comprehensive assessment of the performance of EX-Graph, the paper could consider using a wider range of baseline models and evaluation metrics.\n*   To provide a clear explanation of how the dataset is updated and maintained, the paper could consider including a detailed description of the data collection and maintenance procedures in the paper.\n\nRecommendation:\n\nThis paper is generally well-written and easy to follow. However, it would benefit from a more comprehensive analysis of the limitations and biases of the collected data and a clearer explanation of how the dataset is updated and maintained. With these improvements, the paper would provide a valuable contribution to the field of blockchain and graph analysis.",
        "gr_score_with": 0.4712041884816754,
        "gr_score_without": 0.3342857142857143,
        "output_without": {
            "num_tokens_scored": 350,
            "num_green_tokens": 117,
            "green_fraction": 0.3342857142857143,
            "z_score": -2.5095057377139125,
            "p_value": 0.9939549874258001
        },
        "output_with": {
            "num_tokens_scored": 382,
            "num_green_tokens": 180,
            "green_fraction": 0.4712041884816754,
            "z_score": 2.8407407533816253,
            "p_value": 0.0022504442529388824
        }
    },
    {
        "title": "LUM-VIT: LEARNABLE UNDER-SAMPLING MASK VISION TRANSFORMER FOR BANDWIDTH LIMITED OPTICAL SIGNAL ACQUISITION",
        "abstract": "Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a Vision Transformer variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a threestage fine-tuning strategy. Our evaluations reveal that, by sampling a mere 10% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality. Code will be available at https://github.com/MaxLLF/LUM-ViT.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel approach to address the challenge of hyperspectral data acquisition by introducing a learnable under-sampling mask vision transformer (LUM-ViT) for bandwidth-limited optical signal acquisition. The proposed method leverages pre-acquisition modulation to reduce the acquisition volume using a deep learning model that utilizes prior information. LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation and a three-stage fine-tuning strategy to optimize for optical calculations. The method is evaluated on the ImageNet classification task and real-world optical hardware, demonstrating its practicality and effectiveness in reducing data volume overhead.\n\nStrengths:\n1. The paper presents a novel approach to address the challenge of hyperspectral data acquisition, which is a significant contribution to the field.\n2. The proposed method leverages pre-acquisition modulation to reduce the acquisition volume, which is a unique and innovative approach.\n3. The paper provides a thorough evaluation of the method on the ImageNet classification task and real-world optical hardware, demonstrating its practicality and effectiveness.\n4. The method is designed to be efficient and scalable, making it suitable for real-world applications.\n\nWeaknesses:\n1. The paper assumes that the prior information is available, which may not always be the case in real-world applications.\n2. The method requires a large amount of training data, which may not be available in all cases.\n3. The paper does not provide a detailed analysis of the computational complexity of the method, which may be a concern for real-world applications.\n4. The method is evaluated on a limited number of datasets, which may not be representative of all possible scenarios.\n\nSuggestions for Improvement:\n1. The paper could provide a more detailed analysis of the computational complexity of the method and its scalability.\n2. The paper could provide a more detailed evaluation of the method on a larger number of datasets to demonstrate its robustness.\n3. The paper could provide a more detailed analysis of the assumptions made in the method, such as the availability of prior information.\n4. The paper could provide a more detailed discussion of the potential applications of the method and its limitations.\n\nRecommendation:\nThe paper presents a novel and innovative approach to address the challenge of hyperspectral data acquisition. The proposed method is efficient and scalable, making it suitable for real-world applications. However, the paper assumes that prior information is available, which may not always be the case in real-world applications. Therefore, the paper requires further refinement and evaluation to demonstrate its robustness and scalability. Overall, the paper is a significant contribution to the field and has the potential to make a meaningful impact in the future.\n$$$\n \nNote: The reviewer's comments are based on the provided text and may not reflect the actual paper or its content.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel approach, called LUM-ViT, that leverages deep learning and optical hardware to reduce the volume of hyperspectral data acquisition. The authors introduce a learnable under-sampling mask that can selectively retain essential points for downstream tasks, allowing for significant data reduction while maintaining accuracy. The proposed method, LUM-ViT, is based on a Vision Transformer variant and is trained on a compatibly distributed training set. The authors demonstrate the effectiveness of LUM-ViT on the ImageNet-1k classification task and real-world hyperspectral data, achieving accuracy loss within 1.8% and 4%, respectively.\n\nStrengths:\n1. The paper presents a novel and innovative approach to hyperspectral data acquisition, leveraging deep learning and optical hardware.\n2. The authors provide a detailed and well-structured explanation of the proposed method, including the architecture and training strategy.\n3. The experimental results demonstrate the effectiveness of LUM-ViT on both simulated and real-world data, showcasing its practical feasibility.\n4. The paper provides a thorough analysis of the results, including the impact of under-sampling on accuracy and the robustness of the proposed method.\n\nWeaknesses:\n1. The paper assumes that the training set is compatibly distributed, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which may be critical in real-world applications.\n3. The paper focuses on a specific application (hyperspectral data acquisition) and does not provide a broader discussion on the potential applications and limitations of the proposed method.\n4. The experimental results are based on a single dataset (ImageNet-1k), and it would be beneficial to see more diverse and challenging datasets being used to evaluate the proposed method.\n\nSuggestions for Improvement:\n1. Provide more detailed analysis on the computational complexity and efficiency of the proposed method, including a discussion on potential optimizations and trade-offs.\n2. Evaluate the proposed method on more diverse and challenging datasets, including hyperspectral images with varying spectral and spatial characteristics.\n3. Discuss potential limitations and challenges of the proposed method, including situations where the training set may not be compatibly distributed.\n4. Provide more detailed information on the real-world hardware setup and experimental procedure, including the specific devices and settings used.\n\nRecommendation:\nThe paper presents a novel and innovative approach to hyperspectral data acquisition, leveraging deep learning and optical hardware. While there are some limitations and areas for improvement, the experimental results demonstrate the effectiveness of LUM-ViT on both simulated and real-world data, showcasing its practical feasibility. Therefore, I recommend this paper for publication, with the understanding that the authors will address the suggested improvements and provide more detailed analysis and discussion on the potential applications and limitations of the proposed method. $$",
        "gr_score_with": 0.5121293800539084,
        "gr_score_without": 0.3932926829268293,
        "output_without": {
            "num_tokens_scored": 328,
            "num_green_tokens": 129,
            "green_fraction": 0.3932926829268293,
            "z_score": -0.24795914953354578,
            "p_value": 0.5979169930776476
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 190,
            "green_fraction": 0.5121293800539084,
            "z_score": 4.408600595352691,
            "p_value": 5.202034656747876e-06
        }
    },
    {
        "title": "CURRICULUM REINFORCEMENT LEARNING FOR QUANTUM ARCHITECTURE SEARCH UNDER HARD-",
        "abstract": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a curriculum-based reinforcement learning quantum architecture search (CRLQAS) algorithm designed to tackle the challenges of deploying variational quantum algorithms (VQAs) in realistic noisy quantum environments. The proposed algorithm incorporates several novel features, including a 3D architecture encoding, illegal actions to prevent invalid sequences of gates, a random halting scheme to steer the agent to find shorter circuits, and a novel variant of simultaneous perturbation stochastic approximation (SPSA) algorithm. The authors also introduce an optimized simulator for the algorithm, which significantly improves computational efficiency in simulating noisy quantum circuits. Numerical experiments demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.\n\nStrengths:\n\n1. The paper presents a novel and comprehensive approach to quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments.\n2. The proposed algorithm incorporates several novel features, including a 3D architecture encoding, illegal actions, and a random halting scheme, which are designed to improve the efficiency and effectiveness of the algorithm.\n3. The authors provide a detailed analysis of the performance of CRLQAS in both noiseless and noisy environments, which demonstrates its superiority over existing QAS algorithms.\n4. The paper also introduces an optimized simulator for the algorithm, which significantly improves computational efficiency in simulating noisy quantum circuits.\n\nWeaknesses:\n\n1. The paper assumes a fixed set of quantum gates, which may limit the generality of the proposed algorithm.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed algorithm, which may be a concern for large-scale quantum systems.\n3. The paper assumes a perfect noise model, which may not be realistic in practice.\n4. The authors do not provide a detailed comparison with other QAS algorithms, which may be necessary to fully evaluate the performance of CRLQAS.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed algorithm, which may be necessary to fully evaluate its scalability.\n2. The paper should be modified to include a more detailed comparison with other QAS algorithms, which may be necessary to fully evaluate the performance of CRLQAS.\n3. The authors should provide a more detailed analysis of the noise model used in the paper, which may be necessary to fully evaluate the realism of the proposed algorithm.\n4. The paper should be modified to include a more detailed discussion of the potential applications of CRLQAS, which may be necessary to fully evaluate its potential impact.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments. The proposed algorithm incorporates several novel features, including a 3D architecture encoding, illegal actions, and a random halting scheme, which are designed to improve the efficiency and effectiveness of the algorithm. The authors provide a detailed analysis of the performance of CRLQAS in both noiseless and noisy environments, which demonstrates its superiority over existing QAS algorithms. However, the paper assumes a fixed set of quantum gates, which may limit the generality of the proposed algorithm, and does not provide a detailed analysis of the computational complexity of the proposed algorithm. Therefore, I recommend that the authors modify the paper to address these limitations and provide a more detailed comparison with other QAS algorithms.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces a curriculum-based reinforcement learning algorithm for quantum architecture search, called CRLQAS, designed to tackle the challenges of deploying Variational Quantum Algorithms (VQAs) in realistic noisy quantum environments. The algorithm incorporates several key components, including a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of simultaneous perturbation stochastic approximation (SPSA). The paper demonstrates the effectiveness of CRLQAS in finding ground-state energies for various molecules in both noiseless and noisy environments, outperforming existing QAS algorithms in terms of accuracy, compactness, and robustness.\n\nStrengths:\n\n1. **Innovative algorithm**: CRLQAS introduces a novel curriculum-based reinforcement learning approach for QAS, which addresses the challenges of deploying VQAs in noisy environments.\n2. **Robustness**: The algorithm demonstrates robustness in the presence of noise, achieving accurate results for various molecules in both noiseless and noisy environments.\n3. **Efficiency**: CRLQAS outperforms existing QAS algorithms in terms of accuracy, compactness, and robustness, making it a promising approach for QAS.\n4. **Adaptability**: The algorithm's adaptability allows it to learn from the environment and adjust its strategy to achieve the best possible results.\n\nWeaknesses:\n\n1. **Complexity**: CRLQAS is a complex algorithm that requires a significant amount of computational resources, which may be a limitation for larger-scale simulations.\n2. **Hyperparameter tuning**: The algorithm's performance relies heavily on the tuning of hyperparameters, which can be time-consuming and may require significant expertise.\n3. **Limited scalability**: The algorithm's ability to scale to larger numbers of qubits is limited by the computational resources required for simulations.\n\nSuggestions for Improvement:\n\n1. **Simplify the algorithm**: Consider simplifying the algorithm by removing unnecessary components or refining the existing ones to reduce computational complexity.\n2. **Develop a more efficient hyperparameter tuning method**: Develop a more efficient method for tuning hyperparameters, such as using a genetic algorithm or a Bayesian optimization technique.\n3. **Explore the use of more advanced noise models**: Consider using more advanced noise models, such as those that incorporate noise from real quantum devices, to improve the algorithm's robustness in the presence of noise.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of QAS, introducing a novel algorithm that addresses the challenges of deploying VQAs in noisy environments. The algorithm demonstrates robustness, efficiency, and adaptability, making it a promising approach for QAS. However, the complexity of the algorithm and the limited scalability of the simulations may be limitations for larger-scale applications. Therefore, I recommend that the authors continue to refine the algorithm, explore more efficient hyperparameter tuning methods, and consider using more advanced noise models to improve the algorithm's robustness.\n\n$$$",
        "gr_score_with": 0.6333333333333333,
        "gr_score_without": 0.515527950310559,
        "output_without": {
            "num_tokens_scored": 322,
            "num_green_tokens": 166,
            "green_fraction": 0.515527950310559,
            "z_score": 4.231646523297596,
            "p_value": 1.1599339541951237e-05
        },
        "output_with": {
            "num_tokens_scored": 420,
            "num_green_tokens": 266,
            "green_fraction": 0.6333333333333333,
            "z_score": 9.761033642897548,
            "p_value": 8.273005812320855e-23
        }
    },
    {
        "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
        "abstract": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework, GIM, for learning generalizable image matching models using self-training on diverse internet videos. GIM first trains a model on standard domain-specific datasets and then uses the trained model to generate dense labels on nearby frames of downloaded videos. The final model is trained on propagated data with strong augmentations. The authors also introduce a new zero-shot evaluation benchmark, ZEB, consisting of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks.\n\nStrengths:\n1. The paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos.\n2. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments.\n3. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks.\n4. The authors provide a detailed analysis of the GIM framework and its components, including the self-training process, label propagation, and strong data augmentation.\n\nWeaknesses:\n1. The paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios.\n2. The authors do not provide a thorough analysis of the limitations and potential failures of the GIM framework.\n3. The paper does not discuss the potential issues with data quality and consistency across different internet videos.\n4. The authors do not provide a clear comparison of the computational efficiency of GIM with other existing methods.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the limitations and potential failures of the GIM framework.\n2. The paper should discuss the potential issues with data quality and consistency across different internet videos.\n3. The authors should provide a clear comparison of the computational efficiency of GIM with other existing methods.\n4. The paper should provide more details on the implementation of the GIM framework, including the specific architecture and hyperparameters used.\n\nRecommendation:\nThe paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks. However, the paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios. The authors should provide a more thorough analysis of the limitations and potential failures of the GIM framework. Despite these limitations, the paper is well-written and provides a clear contribution to the field of image matching.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents GIM, a self-training framework that learns a single generalizable image matcher from any image matching architecture using internet videos as a virtually unlimited and diverse data source. GIM first trains a model on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of downloaded videos. These labels are filtered by robust fitting and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. The paper also introduces ZEB, the first zero-shot evaluation benchmark for image matching, which consists of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM in improving the zero-shot performance of 3 state-of-the-art image matching architectures and enabling generalization to extreme cross-domain data such as Bird Eye View (BEV) images of projected 3D point clouds.\n\nStrengths:\n\n1. GIM presents a novel approach to learn a generalizable image matcher from internet videos, which is a virtually unlimited and diverse data source.\n2. The paper introduces ZEB, the first zero-shot evaluation benchmark for image matching, which thoroughly assesses the cross-domain generalization performance of different methods.\n3. Experiments demonstrate the effectiveness and generality of GIM in improving the zero-shot performance of 3 state-of-the-art image matching architectures and enabling generalization to extreme cross-domain data.\n4. GIM enables generalization to applications from different domains without the need for domain-specific training.\n\nWeaknesses:\n\n1. The paper assumes that internet videos are a virtually unlimited and diverse data source, which may not be true in practice due to issues such as video quality, availability, and copyright concerns.\n2. The paper does not provide a thorough analysis of the computational resources required to generate video training data and train the final GIM model.\n3. The paper assumes that the propagation of correspondences to distant frames is efficient and effective, which may not be the case in practice due to issues such as noise and occlusion.\n4. The paper does not provide a thorough evaluation of GIM on a wide range of image matching architectures and applications.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough analysis of the computational resources required to generate video training data and train the final GIM model.\n2. Evaluate GIM on a wider range of image matching architectures and applications to demonstrate its effectiveness and generality.\n3. Provide more details on the propagation of correspondences to distant frames and evaluate its effectiveness in practice.\n4. Investigate the potential issues with internet videos as a virtually unlimited and diverse data source and propose solutions to address these issues.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to learn a generalizable image matcher from internet videos. The introduction of ZEB as a zero-shot evaluation benchmark for image matching is also a significant contribution. However, the paper assumes certain conditions that may not be true in practice, and further evaluation and analysis are required to demonstrate the effectiveness and generality of GIM. Therefore, I recommend acceptance with minor revisions to address the concerns mentioned above.\n$$$",
        "gr_score_with": 0.46582278481012657,
        "gr_score_without": 0.37337662337662336,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 115,
            "green_fraction": 0.37337662337662336,
            "z_score": -0.9537463156584329,
            "p_value": 0.8298939651979189
        },
        "output_with": {
            "num_tokens_scored": 395,
            "num_green_tokens": 184,
            "green_fraction": 0.46582278481012657,
            "z_score": 2.6703560975577236,
            "p_value": 0.0037885421108107753
        }
    },
    {
        "title": "PRE-TRAINED IMAGE-EDITING DIFFUSION MODELS",
        "abstract": "If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot\u2019s own training data. We propose SuSIE, a method that leverages an image editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller attains. Specifically, we fine-tune InstructPix2Pix on robot data such that it outputs a hypothetical future observation given the robot\u2019s current observation and a language command. We then use the same robot data to train a low-level goalconditioned policy to reach a given image observation. We find that when these components are combined, the resulting system exhibits robust generalization capabilities. The high-level planner utilizes its Internet-scale pre-training and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization than conventional language-conditioned policies. We demonstrate that this approach solves real robot control tasks involving novel objects, distractors, and even environments, both in the real world and in simulation. The project website can be found at http://subgoal-image-editing. github.io.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called SuSIE (SUbgoal Synthesis via Image Editing) for language-guided robotic control that leverages pre-trained image-editing models to generate subgoals that a low-level goal-conditioned policy can reach. The method consists of two phases: (i) synthesizing subgoals using a fine-tuned image-editing model, and (ii) reaching these generated subgoals using a goal-conditioned policy. The authors demonstrate that SuSIE improves both zero-shot generalization to new objects and the precision of the overall policy, outperforming prior techniques on the CALVIN benchmark and achieving good performance in real-world manipulation tasks.\n\nStrengths:\n\n* The paper presents a novel approach to language-guided robotic control that leverages pre-trained image-editing models to generate subgoals.\n* The authors demonstrate the effectiveness of SuSIE in improving zero-shot generalization and precision in real-world manipulation tasks.\n* The paper provides a thorough experimental evaluation, including comparisons with prior methods and a detailed analysis of the results.\n* The authors discuss the limitations of the method and provide suggestions for future work.\n\nWeaknesses:\n\n* The paper assumes that the pre-trained image-editing model is available and can be fine-tuned on the robot dataset, which may not be the case in all scenarios.\n* The authors do not provide a detailed analysis of the computational complexity of the method and its scalability to larger datasets.\n* The paper does not discuss the potential risks and challenges associated with using pre-trained models in robotic control.\n\nSuggestions for Improvement:\n\n* The authors could provide a more detailed analysis of the computational complexity of the method and its scalability to larger datasets.\n* The paper could discuss the potential risks and challenges associated with using pre-trained models in robotic control and provide suggestions for mitigating these risks.\n* The authors could provide more details on the implementation of the method, including the specific architecture of the image-editing model and the goal-conditioned policy.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to language-guided robotic control that leverages pre-trained image-editing models to generate subgoals. The authors demonstrate the effectiveness of SuSIE in improving zero-shot generalization and precision in real-world manipulation tasks. The paper provides a thorough experimental evaluation and discusses the limitations of the method. However, the paper assumes that the pre-trained image-editing model is available and does not provide a detailed analysis of the computational complexity of the method. Overall, the paper is well-written and provides a valuable contribution to the field of language-guided robotic control.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a method called SuSIE, which leverages a pre-trained image-editing diffusion model to act as a high-level planner for robotic control tasks. The method decomposes the control problem into two phases: (i) synthesizing a subgoal that the robot must reach, and (ii) learning a low-level goal-conditioned policy to reach this subgoal. The authors fine-tune the pre-trained image-editing model on robot data to generate hypothetical future subgoals, and then train a low-level policy to reach these subgoals. The authors demonstrate that SuSIE can solve real-world robotic control tasks involving novel objects, distractors, and environments, both in the real world and in simulation.\n\nStrengths:\n\n1. The paper proposes a novel approach to incorporating semantic information from the Internet into robotic control, which can improve the precision and dexterity of the low-level control policy.\n2. The authors demonstrate the effectiveness of SuSIE in solving real-world robotic control tasks involving novel objects, distractors, and environments, both in the real world and in simulation.\n3. The paper provides a thorough analysis of the related work, including a comparison with state-of-the-art methods that leverage pre-trained vision-language models.\n4. The authors provide a detailed description of the implementation details, including the architecture of the image-editing diffusion model, the goal-reaching policy, and the training objectives.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained image-editing model can be fine-tuned on the robot data, which may not always be the case, especially if the robot data is limited or noisy.\n2. The authors do not provide a thorough analysis of the failure cases, including the cases where the subgoal generation model fails to produce meaningful subgoals or the low-level policy fails to reach the subgoal.\n3. The paper assumes that the low-level policy can be trained on the same dataset as the image-editing diffusion model, which may not always be the case, especially if the low-level policy requires a large number of samples to learn.\n4. The authors do not provide a thorough analysis of the robustness of SuSIE to changes in the environment, objects, or distractors.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the failure cases, including the cases where the subgoal generation model fails to produce meaningful subgoals or the low-level policy fails to reach the subgoal.\n2. The authors should investigate the possibility of using a more robust image-editing diffusion model that can handle noisy or limited robot data.\n3. The authors should investigate the possibility of using a more efficient low-level policy that can be trained on a smaller number of samples.\n4. The authors should provide a more thorough analysis of the robustness of SuSIE to changes in the environment, objects, or distractors.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of robotic control, particularly in the area of incorporating semantic information from the Internet into the control process. The proposed approach, SuSIE, has the potential to improve the precision and dexterity of the low-level control policy, and the authors provide a thorough analysis of the related work and implementation details. However, the paper has some limitations, including the assumption that the pre-trained image-editing model can be fine-tuned on the robot data, and the lack of thorough analysis of the failure cases and robustness of SuSIE. Therefore, the recommendation is to accept the paper, subject to the authors addressing the suggested improvements.\n\n$$$",
        "gr_score_with": 0.4580152671755725,
        "gr_score_without": 0.3440514469453376,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 107,
            "green_fraction": 0.3440514469453376,
            "z_score": -2.01401839526431,
            "p_value": 0.9779961988974349
        },
        "output_with": {
            "num_tokens_scored": 393,
            "num_green_tokens": 180,
            "green_fraction": 0.4580152671755725,
            "z_score": 2.347647840217817,
            "p_value": 0.0094461870054381
        }
    },
    {
        "title": "DET-CGD: COMPRESSED GRADIENT DESCENT WITH MATRIX STEPSIZES FOR NON-CONVEX OPTIMIZATION",
        "abstract": "This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective\u2019s structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes two novel matrix stepsize sketch compressed gradient descent (CGD) algorithms, det-CGD1 and det-CGD2, for minimizing matrix-smooth non-convex functions. The algorithms leverage the layer-wise structure of neural networks and provide convergence guarantees in both single-node and distributed settings. The authors analyze the convergence properties of the algorithms and show that they achieve better convergence than classical methods. They also design compression mechanisms that achieve compression for free, reducing the number of bits broadcasted at each iteration without losing in the total communication complexity.\n\nStrengths:\n\n* The paper proposes a novel approach to non-convex optimization by incorporating matrix-valued stepsize and compression mechanisms.\n* The authors provide convergence guarantees for the algorithms in both single-node and distributed settings.\n* The paper highlights the importance of considering the layer-wise structure of models such as neural networks in designing compression mechanisms.\n* The authors provide an extensive analysis of the convergence properties of the algorithms and compare them with existing results for DCGD.\n\nWeaknesses:\n\n* The paper assumes that the smoothness matrix L is block-diagonal, which may not be the case in all scenarios.\n* The authors assume that the sketches are i.i.d., unbiased, symmetric, and positive semi-definite, which may not be feasible in practice.\n* The paper does not provide a detailed analysis of the computational complexity of the algorithms.\n* The authors assume that the communication complexity is less expensive than the computation complexity, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the algorithms.\n* They should consider relaxing the assumption that the smoothness matrix L is block-diagonal.\n* The authors should provide a more detailed discussion on the feasibility of the assumed properties of the sketches.\n* They should provide more experimental results to support their theoretical findings.\n\nRecommendation:\n\nThe paper provides a novel approach to non-convex optimization and provides convergence guarantees for the algorithms in both single-node and distributed settings. However, the authors should address the limitations of the paper and provide a more detailed analysis of the computational complexity of the algorithms. With some modifications, the paper has the potential to make a significant contribution to the field of non-convex optimization.\n\nOverall, I would recommend this paper for publication in a reputable conference or journal, with some minor revisions to address the limitations mentioned above.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel Compressed Gradient Descent (CGD) algorithm with a matrix-valued stepsize for minimizing non-convex functions. The authors analyze the convergence properties of the algorithms both in the single-node and distributed settings. The main contributions include the design of matrix stepsize algorithms that capture the objective's structure and lead to faster convergence compared to scalar stepsize algorithms. Additionally, the authors provide theoretical guarantees for free compression by designing specific layer-wise compressors for the non-convex matrix smooth objectives.\n\nStrengths:\n\n1. The authors provide a thorough analysis of the convergence properties of the algorithms both in the single-node and distributed settings.\n2. The design of matrix stepsize algorithms that capture the objective's structure is a novel contribution that leads to faster convergence.\n3. The authors provide theoretical guarantees for free compression by designing specific layer-wise compressors.\n4. The experimental results demonstrate the effectiveness of the algorithms.\n\nWeaknesses:\n\n1. The paper assumes that the smoothness matrix is block-diagonal, which may not be the case for all optimization problems.\n2. The authors rely heavily on the assumption that the sketches are unbiased, which may not always be the case in practice.\n3. The paper does not provide a detailed analysis of the communication complexity of the algorithms.\n4. The experimental results are limited to a small number of experiments and more extensive experiments are needed to confirm the findings.\n\nSuggestions for Improvement:\n\n1. Relax the assumption that the smoothness matrix is block-diagonal and provide a more general analysis.\n2. Investigate the effect of biased sketches on the convergence properties of the algorithms.\n3. Provide a more detailed analysis of the communication complexity of the algorithms.\n4. Conduct more extensive experiments to confirm the findings.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to compressed gradient descent with matrix-valued stepsize for non-convex optimization problems. However, the authors need to address the limitations mentioned above to make the paper more comprehensive and robust. With some additional work, the paper has the potential to make a significant impact in the field of optimization and machine learning.\n\n$$$",
        "gr_score_with": 0.5300353356890459,
        "gr_score_without": 0.45871559633027525,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 150,
            "green_fraction": 0.45871559633027525,
            "z_score": 2.1673134767315068,
            "p_value": 0.01510547979478976
        },
        "output_with": {
            "num_tokens_scored": 283,
            "num_green_tokens": 150,
            "green_fraction": 0.5300353356890459,
            "z_score": 4.465282910669869,
            "p_value": 3.998150838599e-06
        }
    },
    {
        "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
        "abstract": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. The authors leverage a body of work in information theory called partial information decomposition (PID) to identify three sources of unfairness in FL: Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples and provide a framework for quantifying accuracy-fairness trade-offs in FL.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective.\n2. The authors leverage PID to identify three sources of unfairness in FL, which is a novel contribution to the field.\n3. The paper provides a framework for quantifying accuracy-fairness trade-offs in FL, which is a significant contribution to the field.\n4. The authors provide experimental results on synthetic and Adult datasets to support their theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes a binary sensitive attribute, which may not be applicable to all real-world scenarios.\n2. The authors do not provide a clear explanation of how to implement the PID decomposition in practice.\n3. The paper assumes a fixed client distribution, which may not be realistic in real-world scenarios.\n4. The authors do not provide a comparison with existing methods for fairness in FL.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how to implement the PID decomposition in practice.\n2. The paper should be extended to include a comparison with existing methods for fairness in FL.\n3. The authors should provide a more comprehensive discussion of the limitations of the PID decomposition.\n4. The paper should be extended to include a discussion of how to handle non-binary sensitive attributes.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of fairness in FL and is well-written and well-organized. However, the authors should address the limitations mentioned above to make the paper more comprehensive and practical. I would recommend this paper for publication in a top-tier conference in the field of machine learning and fairness.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" proposes a novel framework to analyze fairness trade-offs in Federated Learning (FL) settings. The authors leverage Partial Information Decomposition (PID) to decompose global and local disparities into three non-negative components: Unique Disparity, Redundant Disparity, and Masked Disparity. This decomposition helps identify the interplay between global and local fairness and provides insights on when they agree or disagree. The authors introduce the Accuracy and Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization framework to systematically explore accuracy-fairness trade-offs. They also provide experimental results on synthetic and Adult datasets to support their theoretical findings.\n\nStrengths:\n\n* The paper provides a novel framework for analyzing fairness trade-offs in FL settings.\n* The authors leverage PID to decompose global and local disparities, providing insights on the interplay between global and local fairness.\n* The AGLFOP framework is a significant contribution, enabling the systematic exploration of accuracy-fairness trade-offs.\n* The experimental results on synthetic and Adult datasets provide strong support for the theoretical findings.\n\nWeaknesses:\n\n* The paper assumes a binary sensitive attribute, which may limit its applicability to more complex datasets.\n* The authors rely on the assumption of independence between sensitive attributes and clients, which may not always hold in real-world scenarios.\n* The experimental results on multiple clients are limited, and more comprehensive studies are needed to validate the framework's performance.\n* The paper could benefit from more discussion on the practical implementation of the AGLFOP framework and its potential applications.\n\nSuggestions for Improvement:\n\n* Consider extending the framework to handle multiple sensitive attributes.\n* Investigate the performance of the AGLFOP framework on more complex datasets and real-world applications.\n* Provide more comprehensive experimental results on multiple clients and discuss the practical implementation of the framework.\n* Discuss the potential applications of the framework in real-world scenarios, such as fairness-aware model selection and deployment.\n\nRecommendation:\n\nThe paper provides a novel framework for analyzing fairness trade-offs in FL settings, which is a significant contribution to the field. However, it requires further investigation and improvement to address the limitations mentioned above. I recommend accepting the paper with minor revisions to address these concerns.\n\n$$$",
        "gr_score_with": 0.46629213483146065,
        "gr_score_without": 0.35987261146496813,
        "output_without": {
            "num_tokens_scored": 314,
            "num_green_tokens": 113,
            "green_fraction": 0.35987261146496813,
            "z_score": -1.4514433843898888,
            "p_value": 0.9266717817697125
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 166,
            "green_fraction": 0.46629213483146065,
            "z_score": 2.553179702206701,
            "p_value": 0.005337220331905878
        }
    },
    {
        "title": "MAGICDRIVE: STREET VIEW GENERATION WITH DIVERSE 3D GEOMETRY CONTROL",
        "abstract": "Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird\u2019s-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MAGICDRIVE, a novel street view generation framework, offering diverse 3D geometry controls including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MAGICDRIVE, we achieve high-fidelity street-view image & video synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection. Front Front Left Front Right ... ... R ai n y",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper introduces MAGICDRIVE, a novel framework for generating street-view images with diverse 3D geometry controls. The framework utilizes a pre-trained stable diffusion model and incorporates a cross-view attention module to maintain consistency across multi-camera views. MAGICDRIVE proposes a separate design for objects and road map encoding to improve controllability with 3D data. The framework factors in textual descriptions, offering attribute control such as weather conditions and time of day. Experiments demonstrate that MAGICDRIVE outperforms prior street view generation techniques, notably for multi-dimensional controllability.\n\nStrengths:\n1. The paper proposes a novel framework for generating street-view images with diverse 3D geometry controls, which is a significant contribution to the field of computer vision.\n2. The framework utilizes a pre-trained stable diffusion model, which is a state-of-the-art model for image generation.\n3. The incorporation of a cross-view attention module is a novel approach to maintaining consistency across multi-camera views.\n4. The framework factors in textual descriptions, offering attribute control such as weather conditions and time of day, which is a significant improvement over existing methods.\n\nWeaknesses:\n1. The paper assumes that the input 3D geometry data is accurate and complete, which may not always be the case in real-world scenarios.\n2. The framework relies heavily on the pre-trained stable diffusion model, which may not generalize well to unseen scenes or weather conditions.\n3. The cross-view attention module may not be effective in scenarios where the camera views are significantly different.\n4. The framework does not provide a clear evaluation of its performance on real-world datasets.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed evaluation of the framework's performance on real-world datasets, including datasets with varying weather conditions and camera views.\n2. The authors should investigate methods to improve the accuracy and completeness of the input 3D geometry data.\n3. The authors should explore alternative methods for maintaining consistency across multi-camera views, such as using 3D reconstruction techniques.\n4. The authors should provide more information on the computational resources required to train and run the framework.\n\nRecommendation:\nThe paper proposes a novel and effective framework for generating street-view images with diverse 3D geometry controls. While there are some limitations and areas for improvement, the framework has the potential to significantly impact the field of computer vision. I recommend that the authors continue to refine and improve the framework, and provide a more comprehensive evaluation of its performance on real-world datasets.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper introduces MAGICDRIVE, a novel street view generation framework that offers diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, along with textual descriptions. MAGICDRIVE utilizes a pre-trained stable diffusion model, further fine-tuning it for street view generation. The framework incorporates a cross-view attention module to ensure consistency across multiple camera views. The authors demonstrate that MAGICDRIVE outperforms prior street view generation techniques, notably for multi-dimensional controllability, and that synthetic data delivers considerable improvements in 3D perception tasks.\n\nStrengths:\n1. The paper proposes a novel approach to street view generation that addresses the challenges of multi-camera view consistency and geometric control.\n2. The MAGICDRIVE framework is designed to utilize diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, along with textual descriptions.\n3. The cross-view attention module is a simple yet effective component that ensures consistency across multiple camera views.\n4. The authors provide comprehensive experiments and evaluation metrics to demonstrate the effectiveness of MAGICDRIVE.\n\nWeaknesses:\n1. The paper does not provide a thorough comparison of MAGICDRIVE with other state-of-the-art street view generation methods, making it difficult to evaluate its overall performance.\n2. The authors do not discuss the potential limitations of MAGICDRIVE, such as its ability to generate too dark images or unseen weathers.\n3. The paper does not provide a detailed explanation of the MAGICDRIVE architecture and its components, making it challenging for readers to understand the framework's design.\n\nSuggestions for Improvement:\n1. Provide a more comprehensive comparison of MAGICDRIVE with other state-of-the-art street view generation methods to demonstrate its overall performance.\n2. Discuss the potential limitations of MAGICDRIVE and future work to address these limitations.\n3. Provide a more detailed explanation of the MAGICDRIVE architecture and its components to facilitate understanding of the framework's design.\n\nRecommendation:\nThe paper presents a novel approach to street view generation that addresses the challenges of multi-camera view consistency and geometric control. While there are some weaknesses in the paper, the overall contribution of MAGICDRIVE is significant, and the authors demonstrate its effectiveness through comprehensive experiments and evaluation metrics. Therefore, I recommend accepting the paper for publication.\n\nNote: The recommendation is based on the paper's overall contribution, the effectiveness of MAGICDRIVE, and the potential for future work to address the limitations mentioned above.",
        "gr_score_with": 0.41379310344827586,
        "gr_score_without": 0.3343558282208589,
        "output_without": {
            "num_tokens_scored": 326,
            "num_green_tokens": 109,
            "green_fraction": 0.3343558282208589,
            "z_score": -2.419353629306821,
            "p_value": 0.9922259416436827
        },
        "output_with": {
            "num_tokens_scored": 319,
            "num_green_tokens": 132,
            "green_fraction": 0.41379310344827586,
            "z_score": 0.5028653529707429,
            "p_value": 0.3075294709658703
        }
    },
    {
        "title": "PATHFORMER: MULTI-SCALE TRANSFORMERS WITH ADAPTIVE PATHWAYS FOR TIME SERIES FORECASTING",
        "abstract": "Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multiscale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. The code is made available at https://github.com/decisionintelligence/pathformer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel architecture called Pathformer, a Multi-Scale Transformer with Adaptive Pathways, for time series forecasting. Pathformer integrates multi-scale temporal resolutions and temporal distances by introducing patch division with multiple patch sizes and dual attention on the divided patches. The adaptive pathways dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics of the input data. The authors conduct extensive experiments on eleven real-world datasets and demonstrate that Pathformer achieves state-of-the-art performance and exhibits strong generalization capabilities.\n\nStrengths:\n\n1. The paper proposes a novel and innovative architecture for time series forecasting, which integrates multi-scale temporal resolutions and temporal distances.\n2. The adaptive pathways mechanism allows the model to dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics of the input data.\n3. The authors conduct extensive experiments on eleven real-world datasets, which demonstrates the effectiveness and generalizability of the proposed architecture.\n4. The paper provides a thorough analysis of the experimental results, including the impact of different modules and the effect of varying the number of adaptively selected patch sizes.\n\nWeaknesses:\n\n1. The paper assumes that the input data is stationary, which may not be the case in real-world scenarios.\n2. The authors do not provide a thorough analysis of the computational complexity of the proposed architecture.\n3. The paper does not discuss the potential limitations of the adaptive pathways mechanism, such as the risk of overfitting or underfitting.\n4. The authors do not provide a clear explanation of how the proposed architecture can be extended to handle multivariate time series data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed architecture and discuss potential strategies for reducing it.\n2. The paper should include a thorough discussion of the potential limitations of the adaptive pathways mechanism and provide suggestions for mitigating them.\n3. The authors should provide a clear explanation of how the proposed architecture can be extended to handle multivariate time series data.\n4. The paper should include more experimental results, such as the impact of different hyperparameters and the effect of using different evaluation metrics.\n\nRecommendation:\n\nThe paper proposes a novel and innovative architecture for time series forecasting, which integrates multi-scale temporal resolutions and temporal distances. The adaptive pathways mechanism allows the model to dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics of the input data. The authors conduct extensive experiments on eleven real-world datasets, which demonstrates the effectiveness and generalizability of the proposed architecture. However, the paper assumes that the input data is stationary, and the authors do not provide a thorough analysis of the computational complexity of the proposed architecture. Therefore, I recommend that the authors address these limitations and provide a more detailed analysis of the potential limitations of the adaptive pathways mechanism.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel architecture called Pathformer, a multi-scale Transformer with adaptive pathways for time series forecasting. Pathformer integrates both temporal resolution and temporal distance for multi-scale modeling, which enables it to capture different characteristics spanning various scales. The architecture consists of a multi-scale Transformer block and adaptive pathways, which adaptively select and aggregate scale-specific characteristics based on the different temporal dynamics of the input data. The paper conducts extensive experiments on eleven real-world datasets and achieves state-of-the-art performance by surpassing all current models. The results also demonstrate that Pathformer exhibits stronger generalization abilities under various transfer scenarios.\n\nStrengths:\n\n1. The paper proposes a novel architecture that integrates multi-scale temporal resolutions and temporal distances, which enables it to capture different characteristics spanning various scales.\n2. The adaptive pathways mechanism allows the model to dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics of the input data.\n3. The paper conducts extensive experiments on eleven real-world datasets and achieves state-of-the-art performance by surpassing all current models.\n4. The results also demonstrate that Pathformer exhibits stronger generalization abilities under various transfer scenarios.\n\nWeaknesses:\n\n1. The paper assumes that the input data has a fixed temporal structure, which may not be the case in real-world scenarios where data may be missing or irregularly spaced.\n2. The adaptive pathways mechanism may not work well when the input data has a complex temporal structure or when there are multiple temporal scales present.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed architecture.\n4. The paper assumes that the model has access to a large amount of labeled data, which may not be the case in real-world scenarios where data may be limited.\n\nSuggestions for Improvement:\n\n1. Investigate the robustness of the proposed architecture to missing or irregularly spaced data.\n2. Explore the use of more sophisticated temporal decomposition techniques to better capture complex temporal structures.\n3. Provide a detailed analysis of the computational complexity of the proposed architecture.\n4. Investigate the use of the proposed architecture in scenarios where data is limited.\n\nRecommendation:\n\nOverall, the paper proposes a novel and effective architecture for time series forecasting that integrates multi-scale temporal resolutions and temporal distances. The adaptive pathways mechanism allows the model to dynamically select and aggregate scale-specific characteristics based on the different temporal dynamics of the input data. While there are some limitations and weaknesses to the paper, it makes a significant contribution to the field of time series forecasting and has the potential to be applied in a wide range of scenarios.\n\nRecommendation: Accept\n\nRating: 8/10",
        "gr_score_with": 0.5,
        "gr_score_without": 0.4359861591695502,
        "output_without": {
            "num_tokens_scored": 289,
            "num_green_tokens": 126,
            "green_fraction": 0.4359861591695502,
            "z_score": 1.248759476712992,
            "p_value": 0.10587652981723589
        },
        "output_with": {
            "num_tokens_scored": 304,
            "num_green_tokens": 152,
            "green_fraction": 0.5,
            "z_score": 3.559026084010436,
            "p_value": 0.00018611628286829
        }
    },
    {
        "title": "DISSECTING SAMPLE HARDNESS: A FINE-GRAINED ANALYSIS OF HARDNESS CHARACTERIZATION METHODS FOR DATA-CENTRIC AI",
        "abstract": "Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \u201chard\u201d samples. However, there is a lack of consensus regarding the definition and evaluation of \u201chardness\u201d. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a comprehensive benchmarking framework, H-CAT, for evaluating Hardness Characterization Methods (HCMs) in data-centric AI. The authors aim to address the lack of consensus on the definition and evaluation of \"hardness\" in HCMs by introducing a fine-grained taxonomy of hardness types and a unified software tool integrating 13 different HCMs. The taxonomy categorizes hardness into three broad types: Mislabeling, Out-of-Distribution/Outlier, and Atypical, with subtypes for each. The authors evaluate 13 HCMs across 8 hardness types, covering over 14K experimental setups, and provide insights into the capabilities of different HCMs when dealing with different hardness types.\n\nStrengths:\n\n1. Comprehensive evaluation: The paper presents a comprehensive evaluation of 13 HCMs across 8 hardness types, which is a significant contribution to the field.\n2. Fine-grained taxonomy: The authors introduce a fine-grained taxonomy of hardness types, which provides a clear and systematic understanding of the different manifestations of hardness.\n3. Unified software tool: The proposed H-CAT framework is a unified software tool that integrates 13 different HCMs, making it easier for practitioners to use and compare HCMs.\n4. Insights into HCM capabilities: The paper provides insights into the capabilities of different HCMs when dealing with different hardness types, which is essential for practitioners to select the most suitable HCM for their application.\n\nWeaknesses:\n\n1. Limited scope: The paper focuses on image datasets and may not be generalizable to other domains, such as text or tabular data.\n2. Evaluation metrics: The authors use AUPRC and AUROC as evaluation metrics, which may not capture the nuances of hardness characterization.\n3. Limited discussion on practical applications: The paper could benefit from a more in-depth discussion on the practical applications of H-CAT and how it can be used in real-world scenarios.\n4. Lack of comparison with other benchmarking frameworks: The paper does not compare H-CAT with other benchmarking frameworks, which makes it difficult to evaluate its effectiveness.\n\nSuggestions for Improvement:\n\n1. Expand the scope to include other domains, such as text or tabular data, to increase the generalizability of the findings.\n2. Consider using other evaluation metrics, such as precision-recall curves or F1 scores, to provide a more comprehensive understanding of HCM performance.\n3. Provide more practical examples and case studies to demonstrate the effectiveness of H-CAT in real-world scenarios.\n4. Compare H-CAT with other benchmarking frameworks to evaluate its effectiveness and identify areas for improvement.\n\nRecommendation:\n\nThe paper presents a comprehensive and well-structured evaluation of HCMs, and the proposed H-CAT framework is a valuable contribution to the field. However, the limitations mentioned above should be addressed to further improve the paper. With some revisions, this paper has the potential to be a top-tier publication in the field of data-centric AI.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes the Hardness-Characterization Analysis Toolkit (H-CAT) to evaluate the performance of Hardness Characterization Methods (HCMs) on various hardness types. The authors first formalize a taxonomy of hardness types and then use H-CAT to evaluate 13 HCMs on 8 hardness types, resulting in a comprehensive evaluation with over 14K setups. The results highlight the strengths and weaknesses of different HCMs and provide practical tips for HCM selection and future development.\n\nStrengths:\n\n1. The paper provides a comprehensive evaluation of HCMs on various hardness types, filling the gap in the literature where HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance.\n2. The authors propose a taxonomy of hardness types, which provides a systematic and formal definition of different types of sample-level hardness.\n3. The paper introduces H-CAT, a benchmarking framework that supports comprehensive and quantitative benchmarking of HCMs on different hardness types and can be easily extended to new HCMs, hardness types, and datasets.\n\nWeaknesses:\n\n1. The paper assumes that the definition of hardness is clear and well-understood, which may not be the case in practice.\n2. The evaluation of HCMs is limited to 8 hardness types, and it is unclear whether the results can be generalized to other hardness types.\n3. The paper does not provide a clear explanation of how H-CAT is used to evaluate HCMs and how the results are interpreted.\n\nSuggestions for Improvement:\n\n1. Provide a clear and detailed explanation of the taxonomy of hardness types and how it is used to evaluate HCMs.\n2. Include a more comprehensive evaluation of HCMs on various hardness types, including additional hardness types not covered in the paper.\n3. Provide a clear explanation of how H-CAT is used to evaluate HCMs and how the results are interpreted.\n\nRecommendation:\n\nThis paper provides a comprehensive evaluation of HCMs on various hardness types and proposes a taxonomy of hardness types. The introduction of H-CAT provides a useful tool for evaluating HCMs and can be extended to new HCMs, hardness types, and datasets. However, the paper assumes a clear definition of hardness and has limited evaluation of HCMs on additional hardness types. Therefore, the recommendation is to accept the paper with minor revisions to address the limitations and suggestions for improvement.\n\n$$$",
        "gr_score_with": 0.5278688524590164,
        "gr_score_without": 0.45707656612529,
        "output_without": {
            "num_tokens_scored": 431,
            "num_green_tokens": 197,
            "green_fraction": 0.45707656612529,
            "z_score": 2.41874926932128,
            "p_value": 0.007786985465668042
        },
        "output_with": {
            "num_tokens_scored": 305,
            "num_green_tokens": 161,
            "green_fraction": 0.5278688524590164,
            "z_score": 4.5583646765687975,
            "p_value": 2.577673256188724e-06
        }
    },
    {
        "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
        "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which admits separate control over the camera and the content of the scene. The model is trained using a synthetic dataset, DySO, and real-world videos, and it learns to disentangle camera pose and scene dynamics from monocular real-world videos. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\nStrengths:\n\n1. The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task.\n2. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n3. The paper presents a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n4. The authors propose a novel synthetic dataset, DySO, which is used for co-training DyST.\n\nWeaknesses:\n\n1. The paper assumes that the scene dynamics are known, which may not be the case in real-world scenarios.\n2. The authors use a simple set of camera and object motions in the synthetic dataset, which may not be sufficient to capture the complexity of real-world scenes.\n3. The paper does not provide a detailed analysis of the model's performance on more complex types of videos, such as videos with several independent moving objects or changing lighting conditions.\n4. The authors use a L2 loss for training the model, which may not be suitable for all types of scenes.\n\nSuggestions for Improvement:\n\n1. The authors could explore more complex types of videos and analyze the model's performance on these scenarios.\n2. The paper could benefit from a more detailed analysis of the model's performance on videos with changing lighting conditions.\n3. The authors could explore the use of more advanced loss functions, such as a adversarial loss, to improve the model's performance.\n4. The paper could benefit from a more detailed analysis of the model's learned latent representations and their relationship to the scene dynamics and camera pose.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes. While there are some limitations to the paper, it is a significant contribution to the field and has the potential to be improved upon. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces DyST, a novel approach to generative modeling of dynamic 3D visual scenes that allows for separate control over the camera and the content of the scene. The model learns a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose, and achieves this separation through a novel co-training scheme on monocular videos and a new synthetic dataset DySO. The authors demonstrate the capabilities of DyST on real-world videos of dynamic scenes and show that it can be applied to real-world videos despite not having access to ground-truth camera poses via sim-to-real transfer.\n\nStrengths:\n\n* The paper introduces a novel approach to generative modeling of dynamic 3D visual scenes that allows for separate control over the camera and the content of the scene.\n* The model achieves impressive view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n* The authors demonstrate the capabilities of DyST on real-world videos despite not having access to ground-truth camera poses via sim-to-real transfer.\n* The paper provides a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n\nWeaknesses:\n\n* The paper assumes that the camera pose and scene dynamics can be separated, which might not always be the case in real-world scenarios.\n* The model requires a large amount of training data, which might not be available for all types of videos.\n* The paper does not provide a detailed comparison with other state-of-the-art methods for generative modeling of dynamic 3D visual scenes.\n* The view generation quality is currently limited due to the L2 loss, which might not be suitable for all types of videos.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with other state-of-the-art methods for generative modeling of dynamic 3D visual scenes.\n* The paper should provide more information on how the model handles complex types of videos, such as videos with several independent moving objects, longer camera trajectories, or changing lighting conditions.\n* The authors should investigate the use of more advanced loss functions, such as diffusion or GAN-like approaches, to improve the view generation quality.\n* The paper should provide more information on the potential down-stream applications of DyST, especially when combined with the potential of training on large-scale video collections.\n\nRecommendation:\n\nThe paper is well-written and provides a novel approach to generative modeling of dynamic 3D visual scenes. The authors demonstrate the capabilities of DyST on real-world videos and show that it can be applied to real-world videos despite not having access to ground-truth camera poses via sim-to-real transfer. The paper provides a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose. However, the paper assumes that the camera pose and scene dynamics can be separated, which might not always be the case in real-world scenarios. The view generation quality is currently limited due to the L2 loss, which might not be suitable for all types of videos. Overall, the paper is a good contribution to the field and provides a new approach to generative modeling of dynamic 3D visual scenes.\n\nGrade: 8/10\n\nRecommendation for Publication:\n\nYes, the paper is suitable for publication in a reputable conference or journal.\n\n$$$",
        "gr_score_with": 0.5289017341040463,
        "gr_score_without": 0.47586206896551725,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 138,
            "green_fraction": 0.47586206896551725,
            "z_score": 2.637048158169613,
            "p_value": 0.004181547888300411
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 183,
            "green_fraction": 0.5289017341040463,
            "z_score": 4.894306786563539,
            "p_value": 4.932642181994563e-07
        }
    },
    {
        "title": "GROUNDING LANGUAGE PLANS IN DEMONSTRATIONS THROUGH COUNTER-FACTUAL PERTURBATIONS",
        "abstract": "Grounding the abstract knowledge captured by Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem. Whereas prior works have largely focused on leveraging LLMs for generating abstract plans in symbolic spaces, this work uses LLMs to guide the learning for structures and constraints in robot manipulation tasks. Specifically, we borrow from manipulation planning literature the concept of mode families, defining specific types of motion constraints among sets of objects, to serve as an intermediate layer that connects high-level language representations with low-level physical trajectories. By locally perturbing a small set of successful human demonstrations, we augment the dataset with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains neural network-based classifiers to differentiate success task executions from failures and as a by-product learns classifiers that ground low-level states into mode families without dense labeling. This further enables us to learn structured policies for the target task. Experimental validation in both 2D continuous-space and robotic manipulation environments demonstrates the robustness of our mode-based imitation methods under external perturbations.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a framework called Manipulation Modes from Language Plans (MMLP) to ground language plans in physical domains through counterfactual perturbations. The framework uses large language models (LLMs) to generate multi-step physical plans and mode families, and then leverages counterfactual perturbations to learn mode-specific policies. The authors demonstrate the effectiveness of MMLP on two benchmarks: a synthetic 2D polygon domain and Robosuite, a simulated robot manipulation environment. The results show that MMLP significantly improves the robustness of policies in both environments, and provides interpretable explanations of mode transitions and failures.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to grounding language plans in physical domains, which is a long-standing challenge in robotics and AI.\n2. The use of counterfactual perturbations is a unique and effective way to learn mode-specific policies, and the authors provide a clear and well-structured explanation of the method.\n3. The paper presents a comprehensive evaluation of MMLP on two benchmarks, including a synthetic 2D polygon domain and Robosuite, which demonstrates the effectiveness of the method in different scenarios.\n4. The authors provide a clear and concise explanation of the limitations of the method and potential future work, which shows a good understanding of the challenges and opportunities in the field.\n\nWeaknesses:\n\n1. The paper assumes a reset capability in the environment, which may not be feasible in all scenarios. The authors should discuss potential solutions to this limitation.\n2. The paper requires a large number of trial-and-errors to collect task success labels of a trajectory, which can be time-consuming and inefficient. The authors should discuss potential solutions to this limitation, such as active learning.\n3. The paper requires skill to prompt the LLM to find a suitable state representation for learning the classifier. The authors should discuss potential solutions to this limitation, such as learning the state representation in conjunction with the mode classifiers in an end-to-end fashion.\n4. The paper does not provide a clear explanation of how the mode classification accuracy is calculated, which can make it difficult to understand the results.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the limitations of the method and potential solutions to address them.\n2. Discuss potential solutions to the limitation of requiring a reset capability in the environment.\n3. Provide a clear explanation of how the mode classification accuracy is calculated.\n4. Consider including more experiments to evaluate the robustness of MMLP in different scenarios.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to grounding language plans in physical domains, and the results demonstrate the effectiveness of the method in different scenarios. However, the paper assumes a reset capability in the environment, requires a large number of trial-and-errors to collect task success labels of a trajectory, and requires skill to prompt the LLM to find a suitable state representation for learning the classifier. To address these limitations, the authors should discuss potential solutions and provide a clear explanation of how the mode classification accuracy is calculated. With these improvements, the paper has the potential to make a significant contribution to the field of robotics and AI.\n\nOverall, I would recommend this paper for publication in a top-tier conference or journal, but with the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel framework, Manipulation Modes from Language Plans (MMLP), that effectively grounds the knowledge within large language models into physical domains via mode families. The framework leverages large language models to generate multi-step physical plans and counterfactual perturbations to learn mode families and their transitions. The MMLP framework operates in four stages: generating language plans, generating counterfactual perturbations, learning mode family classifiers, and learning mode-specific policies. The paper evaluates the MMLP framework on two benchmarks: a synthetic 2D polygon domain and Robosuite, a simulated robot manipulation environment. The results show that MMLP can successfully recover mode families and their transitions and learn robust control policies.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to grounding large language models into physical domains via mode families.\n2. The MMLP framework is able to learn robust control policies from a small number of human demonstrations and task descriptions.\n3. The paper provides a thorough evaluation of the MMLP framework on two benchmarks, demonstrating its effectiveness and robustness.\n\nWeaknesses:\n\n1. The paper requires a large number of trial-and-errors and an environment with a reset capability to collect task success labels of a trajectory, which can be data inefficient.\n2. Prompting the LLM to find a suitable state representation for learning the classifier requires skill and can be challenging.\n3. The paper does not provide a clear explanation of how to learn the state representation in conjunction with the mode classifiers in an end-to-end fashion.\n\nSuggestions for Improvement:\n\n1. To address the data inefficiency of MMLP, the authors can explore active learning techniques that probe demonstrations only in regions with high uncertainty.\n2. To improve the skill of prompting the LLM to find a suitable state representation, the authors can develop a more robust and efficient method for generating state representations.\n3. To learn the state representation in conjunction with the mode classifiers in an end-to-end fashion, the authors can explore neural network architectures that can learn both the state representation and the mode classifiers simultaneously.\n\nRecommendation:\n\nI highly recommend this paper for publication in a top-tier conference in robotics and artificial intelligence. The paper presents a novel and innovative approach to grounding large language models into physical domains via mode families, and the results demonstrate its effectiveness and robustness. However, to improve the paper, the authors should address the data inefficiency and skill requirements of MMLP and explore more robust and efficient methods for generating state representations and learning the state representation in conjunction with the mode classifiers in an end-to-end fashion.\n\n$$$",
        "gr_score_with": 0.5810055865921788,
        "gr_score_without": 0.5209876543209877,
        "output_without": {
            "num_tokens_scored": 405,
            "num_green_tokens": 211,
            "green_fraction": 0.5209876543209877,
            "z_score": 4.9700750588431735,
            "p_value": 3.3463494000798736e-07
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 208,
            "green_fraction": 0.5810055865921788,
            "z_score": 6.990816002434069,
            "p_value": 1.3664599486470692e-12
        }
    },
    {
        "title": "IMPROVING EQUILIBRIUM PROPAGATION WITHOUT WEIGHT SYMMETRY THROUGH JACOBIAN HOMEOSTASIS",
        "abstract": "Equilibrium propagation (EP) is a compelling alternative to the backpropagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to yield unbiased gradient estimates. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry contributes to bias is unknown because, in practice, its contribution may be masked by a finite nudge. To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral. In contrast, weight asymmetry induces residual bias through poor alignment of EP\u2019s neuronal error vectors compared to BP resulting in low task performance. To mitigate the latter issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network\u2019s fixed point. This homeostatic objective dramatically improves the network\u2019s ability to solve complex tasks such as ImageNet 32\u00d732. Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate\u2019s relaxation dynamics.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to equilibrium propagation (EP) without weight symmetry, which is a limitation of the traditional EP algorithm. The authors introduce a generalized form of EP, called holomorphic EP (hEP), which can be formulated without weight symmetry and analytically isolate the two sources of bias in the gradient estimate. They show that the bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral. However, the weight asymmetry induces residual bias through poor alignment of EP's neuronal error vectors compared to backpropagation (BP). To mitigate this issue, the authors propose a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point. The proposed method is evaluated on various tasks, including ImageNet 32\u00d732, and shows improved performance compared to traditional EP.\n\nStrengths:\n\n* The paper presents a novel approach to EP without weight symmetry, which is a significant contribution to the field.\n* The authors provide a comprehensive analysis of the individual sources of bias in the gradient estimate from weight asymmetry and finite-size nudge in generalized EP.\n* The proposed homeostatic objective is a novel and effective way to mitigate the residual bias induced by weight asymmetry.\n* The paper presents a thorough evaluation of the proposed method on various tasks, including ImageNet 32\u00d732.\n\nWeaknesses:\n\n* The paper assumes that the network has a fixed point, which may not always be the case in practice.\n* The proposed method requires the network to oscillate in the complex plane, which may be challenging to implement in some systems.\n* The paper does not provide a clear explanation of how the homeostatic objective is related to the biological plausibility of the proposed method.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how to implement the proposed method in practice, including how to ensure that the network has a fixed point and how to oscillate in the complex plane.\n* The authors should provide more information on the biological plausibility of the proposed method, including how the homeostatic objective relates to the brain's learning mechanisms.\n* The authors should provide more evaluation of the proposed method on other tasks and datasets to demonstrate its generalizability.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to EP without weight symmetry, and the proposed homeostatic objective is a significant contribution to the field. However, the paper requires more details on implementation and biological plausibility. With further development and evaluation, the proposed method has the potential to be a significant advancement in the field of neural networks.\n\n$$$\nNote: The above review is based on the provided paper and may not be a comprehensive review of the entire paper. The reviewer's comments and suggestions are intended to provide constructive feedback to the authors.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper explores the concept of equilibrium propagation (EP) in neural networks, specifically its limitations when weight symmetry is not perfect. EP is a gradient computation algorithm that relies on the network's own dynamics, without requiring backpropagation. However, its performance is degraded when weight asymmetry is introduced. To address this issue, the authors propose a new homeostatic loss that directly penalizes functional asymmetries of the Jacobian at the network's fixed point. This homeostatic loss is shown to improve the network's ability to solve complex tasks, such as ImageNet 32\u00d732.\n\nStrengths:\n\n* The paper provides a comprehensive analysis of the individual sources of bias in EP, specifically the impact of weight asymmetry and finite nudge amplitudes.\n* The authors propose a novel homeostatic loss that reduces the asymmetry of the Jacobian at the free equilibrium point without enforcing perfect weight symmetry.\n* The paper demonstrates the effectiveness of this homeostatic loss in improving the performance of EP on complex tasks, such as ImageNet 32\u00d732.\n\nWeaknesses:\n\n* The paper relies heavily on mathematical derivations, which may make it challenging for some readers to follow.\n* The experimental results are primarily focused on a limited set of tasks and architectures, and more extensive evaluations would be beneficial.\n* The paper could benefit from a more in-depth discussion of the implications of its findings for understanding and designing oscillation-based learning algorithms.\n\nSuggestions for Improvement:\n\n* Consider providing more visual aids, such as plots or diagrams, to help illustrate complex mathematical concepts.\n* Conduct more extensive evaluations of the homeostatic loss on a variety of tasks and architectures to demonstrate its generalizability.\n* Engage in a more nuanced discussion of the potential applications and limitations of the homeostatic loss in the context of oscillation-based learning algorithms.\n\nRecommendation:\n\nThis paper makes a valuable contribution to the field of neural networks by providing a deeper understanding of the limitations of EP and proposing a novel homeostatic loss to address these limitations. The paper is well-written and thoroughly researched, with clear mathematical derivations and effective experiments. However, some aspects of the paper could be improved with additional visual aids, more extensive evaluations, and a more in-depth discussion of the implications of its findings.\n\nTherefore, I recommend this paper for publication in its current form, with the understanding that some of the suggested improvements could be addressed in future revisions.\n\n$$",
        "gr_score_with": 0.5026737967914439,
        "gr_score_without": 0.38522427440633245,
        "output_without": {
            "num_tokens_scored": 379,
            "num_green_tokens": 146,
            "green_fraction": 0.38522427440633245,
            "z_score": -0.5871685707860297,
            "p_value": 0.721454750845347
        },
        "output_with": {
            "num_tokens_scored": 374,
            "num_green_tokens": 188,
            "green_fraction": 0.5026737967914439,
            "z_score": 4.053123176839192,
            "p_value": 2.526918764602937e-05
        }
    },
    {
        "title": "GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION",
        "abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. GUESS & SKETCH extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test GUESS & SKETCH on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents GUESS & SKETCH, a neurosymbolic approach to assembly-to-assembly transpilation. The method leverages the strengths of language models and symbolic solvers to translate assembly code from one instruction set architecture (ISA) to another. GUESS & SKETCH extracts alignment and confidence information from a language model and passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. The paper demonstrates the efficacy of this approach on three different test sets of assembly programs in the ARMv8 and RISC-V architectures.\n\nStrengths:\n1. The paper presents a novel approach to assembly-to-assembly transpilation, which is a challenging task.\n2. The method leverages the strengths of language models and symbolic solvers, making it a promising solution for this task.\n3. The paper provides a comprehensive evaluation of the approach, including experiments on three different test sets.\n4. The authors provide a clear and concise description of the method and its components.\n\nWeaknesses:\n1. The paper assumes that the language model is able to provide accurate alignment and confidence information, which may not always be the case.\n2. The symbolic solver may struggle with complex programs or programs with many errors.\n3. The paper does not provide a formal proof of equivalence, only checking on a small finite set of inputs.\n4. The approach is limited in length by the context length of generative language models.\n\nSuggestions for Improvement:\n1. Investigate ways to improve the accuracy of the language model's alignment and confidence information.\n2. Explore the use of more advanced symbolic solvers or techniques to improve the solver's performance.\n3. Provide a formal proof of equivalence to ensure the correctness of the approach.\n4. Investigate ways to extend the approach to handle longer programs or programs with more complex errors.\n\nRecommendation:\nThe paper presents a novel and promising approach to assembly-to-assembly transpilation. While there are some limitations and weaknesses, the approach shows significant potential for improving the accuracy and efficiency of transpilation. Therefore, I recommend accepting the paper for publication with some revisions to address the suggestions for improvement.\n\nOverall, the paper is well-written, and the authors provide a clear and concise description of the method and its components. The evaluation is comprehensive, and the results are promising. However, there are some limitations and weaknesses that need to be addressed to ensure the correctness and efficiency of the approach.",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary:**\n\nThe paper proposes a novel approach to assembly-to-assembly transpilation using a neurosymbolic method called GUESS & SKETCH. This approach leverages the strengths of both language models and symbolic solvers to improve the accuracy and scalability of transpilation. The method consists of two stages: the GUESS phase, where a language model produces candidate translations and extracts alignment and confidence information, and the SKETCH phase, where a symbolic solver uses this information to correct errors and produce a final transpiled output. The authors demonstrate the efficacy of their approach on three different test sets of assembly programs and show that it outperforms existing approaches by a significant margin.\n\n**Strengths:**\n\n* The paper proposes a novel and innovative approach to assembly-to-assembly transpilation that leverages the strengths of both language models and symbolic solvers.\n* The authors demonstrate the efficacy of their approach on a range of test sets and show that it outperforms existing approaches by a significant margin.\n* The paper provides a detailed description of the methodology and implementation of the approach, making it easy to follow and understand.\n* The authors discuss the limitations of the approach and potential future directions for research.\n\n**Weaknesses:**\n\n* The paper relies heavily on the assumption that the language model can accurately predict the alignment and confidence of the transpiled output, which may not always be the case.\n* The symbolic solver used in the SKETCH phase is not explicitly described, and it is unclear how it is able to correct errors and produce a final transpiled output.\n* The paper does not provide a formal proof of equivalence between the original and transpiled programs, only a finite set of inputs.\n* The approach is limited by the context length of the generative language models used, which may not always be sufficient for large programs.\n\n**Suggestions for Improvement:**\n\n* Provide more detailed information about the symbolic solver used in the SKETCH phase and how it is able to correct errors and produce a final transpiled output.\n* Consider using more robust methods for alignment and confidence extraction, such as using multiple language models or incorporating additional features.\n* Provide a formal proof of equivalence between the original and transpiled programs, rather than relying on a finite set of inputs.\n* Consider using more advanced language models or convolutional methods to improve the accuracy and scalability of the approach.\n\n**Recommendation:**\n\nI recommend accepting this paper for publication. The approach is novel and innovative, and the authors demonstrate its efficacy on a range of test sets. While there are some limitations and areas for improvement, the paper provides a valuable contribution to the field and is well-written and well-organized. With some revisions to address the weaknesses and suggestions for improvement, I believe this paper can be a valuable addition to the conference proceedings.",
        "gr_score_with": 0.526595744680851,
        "gr_score_without": 0.4326647564469914,
        "output_without": {
            "num_tokens_scored": 349,
            "num_green_tokens": 151,
            "green_fraction": 0.4326647564469914,
            "z_score": 1.2456227082195046,
            "p_value": 0.10645147073898531
        },
        "output_with": {
            "num_tokens_scored": 376,
            "num_green_tokens": 198,
            "green_fraction": 0.526595744680851,
            "z_score": 5.010803930345787,
            "p_value": 2.710155932453158e-07
        }
    },
    {
        "title": "REWARD MODEL ENSEMBLES HELP MITIGATE OVEROPTIMIZATION",
        "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \u201ctrue\u201d reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger \u201cgold\u201d reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives for mitigating reward model overoptimization in reinforcement learning from human feedback (RLHF). The authors use a synthetic setup with a gold reward model and a proxy reward model to study overoptimization. They propose three ensemble-based optimization methods: mean optimization, worst-case optimization (WCO), and uncertainty-weighted optimization (UWO). The results show that WCO and UWO effectively mitigate overoptimization and improve performance, especially in the presence of label noise.\n\nStrengths:\n\n* The paper presents a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives for mitigating reward model overoptimization.\n* The authors propose three ensemble-based optimization methods: mean optimization, WCO, and UWO, which are well-motivated and theoretically grounded.\n* The results show that WCO and UWO effectively mitigate overoptimization and improve performance, especially in the presence of label noise.\n* The paper provides a thorough experimental evaluation, including results on both best-of-n sampling (BoN) and proximal policy optimization (PPO).\n\nWeaknesses:\n\n* The paper assumes that the gold reward model is a good proxy for human preferences, which may not always be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of using ensemble-based optimization methods.\n* The paper does not discuss the potential limitations of using WCO and UWO in certain scenarios, such as when the proxy reward model is highly uncertain.\n* The results are based on a specific synthetic setup, and it is unclear whether the findings will generalize to other RLHF setups.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational cost of using ensemble-based optimization methods.\n* The paper should discuss the potential limitations of using WCO and UWO in certain scenarios, such as when the proxy reward model is highly uncertain.\n* The authors should provide more results on the robustness of WCO and UWO to different hyperparameters and scenarios.\n* The paper should discuss the potential applications of WCO and UWO in real-world RLHF setups.\n\nRecommendation:\n\nThe paper presents a thorough and well-motivated study on the use of ensemble-based conservative optimization objectives for mitigating reward model overoptimization in RLHF. The results are promising, and the methods proposed by the authors have the potential to improve the performance of RLHF systems. However, the paper should be revised to address the limitations mentioned above. With some revisions, the paper has the potential to make a significant contribution to the field of RLHF.",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe paper presents a systematic study on the efficacy of ensemble-based conservative optimization objectives for mitigating reward model overoptimization in reinforcement learning from human feedback (RLHF). The authors propose to learn an ensemble of reward models and combine their estimates using three methods: mean optimization, worst-case optimization (WCO), and uncertainty-weighted optimization (UWO). They evaluate the proposed methods on a synthetic setup with a gold reward model acting as the true reward and show that ensemble-based conservative optimization can effectively counter overoptimization. The results indicate that WCO and UWO can eliminate overoptimization for best-of-n sampling (BoN) and proximal policy optimization (PPO), respectively, while also improving performance by up to 70%. The authors also show that the gains provided by ensemble-based conservative optimization are orthogonal to the gains achieved by increasing the reward model size.\n\nStrengths:\n1. The paper presents a systematic study on the efficacy of ensemble-based conservative optimization objectives for mitigating reward model overoptimization in RLHF.\n2. The authors propose three methods for combining the estimates of multiple reward models, which can help to mitigate overoptimization.\n3. The results indicate that ensemble-based conservative optimization can effectively counter overoptimization and improve performance.\n4. The paper provides a thorough evaluation of the proposed methods on a synthetic setup with a gold reward model acting as the true reward.\n\nWeaknesses:\n1. The paper assumes that the gold reward model is available, which may not be the case in real-world scenarios.\n2. The authors do not provide a clear explanation for why WCO and UWO work well for BoN and PPO, respectively.\n3. The paper does not provide a comprehensive evaluation of the proposed methods on real-world datasets.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation for why WCO and UWO work well for BoN and PPO, respectively.\n2. The paper should provide a more comprehensive evaluation of the proposed methods on real-world datasets.\n3. The authors should provide a more thorough analysis of the robustness of the proposed methods to different hyperparameters.\n\nRecommendation:\nOverall, the paper presents a systematic study on the efficacy of ensemble-based conservative optimization objectives for mitigating reward model overoptimization in RLHF. While the results are promising, the paper has some limitations that need to be addressed. With some additional evaluation on real-world datasets and a more thorough analysis of the robustness of the proposed methods, the paper can be considered for publication in a top-tier conference or journal.$$",
        "gr_score_with": 0.4095238095238095,
        "gr_score_without": 0.29153605015673983,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 93,
            "green_fraction": 0.29153605015673983,
            "z_score": -3.954350275633577,
            "p_value": 0.999961628544388
        },
        "output_with": {
            "num_tokens_scored": 315,
            "num_green_tokens": 129,
            "green_fraction": 0.4095238095238095,
            "z_score": 0.3450327796711771,
            "p_value": 0.3650348637900211
        }
    },
    {
        "title": "MULTILINGUAL JAILBREAK CHALLENGES IN LARGE LANGUAGE MODELS",
        "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the \u201cjailbreak\u201d problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel SELF-DEFENSE framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at https: //github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: this paper contains examples with unsafe content.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of multilingual jailbreak challenges in large language models (LLMs), specifically considering two scenarios: unintentional and intentional. The authors demonstrate that multilingual languages can serve as a potential jailbreak method in both scenarios, posing significant threats to the security and safety of LLMs. To address this issue, they propose a novel framework called SELF-DEFENCE, which has proven to be highly effective in enhancing the multilingual safety capabilities of LLMs.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the multilingual jailbreak challenge, highlighting the potential risks and vulnerabilities of LLMs.\n2. The authors conduct extensive experimentation, demonstrating the effectiveness of multilingual languages as a jailbreak method in both unintentional and intentional scenarios.\n3. The proposed SELF-DEFENCE framework is innovative and effective in enhancing the multilingual safety capabilities of LLMs.\n4. The paper provides a thorough analysis of the results, highlighting the trade-off between safety and usefulness in safety training.\n\nWeaknesses:\n\n1. The paper relies heavily on machine translation, which may introduce noise and affect the accuracy of the results.\n2. The authors do not provide a detailed explanation of the SELF-DEFENCE framework, making it difficult to understand the underlying mechanisms.\n3. The paper does not discuss the potential limitations and challenges of implementing the SELF-DEFENCE framework in real-world scenarios.\n4. The results are primarily based on experiments with two specific LLMs, ChatGPT and GPT-4, which may not be representative of all LLMs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of the SELF-DEFENCE framework, including the underlying mechanisms and potential limitations.\n2. The paper should discuss the potential challenges and limitations of implementing the SELF-DEFENCE framework in real-world scenarios.\n3. The authors should conduct additional experiments with a wider range of LLMs to ensure the results are representative of all LLMs.\n4. The paper should provide a more comprehensive analysis of the trade-off between safety and usefulness in safety training.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of LLM safety, highlighting the potential risks and vulnerabilities of multilingual languages as a jailbreak method. The proposed SELF-DEFENCE framework is innovative and effective in enhancing the multilingual safety capabilities of LLMs. However, the paper requires additional improvements, including a more detailed explanation of the framework and a discussion of the potential challenges and limitations of implementation. With these improvements, the paper has the potential to make a significant impact in the field of LLM safety.\n\nRating: 7.5/10\n\nRecommendation for publication: Yes, with revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of multilingual jailbreak challenges in large language models (LLMs) and proposes a novel framework, SELF-DEFENCE, to alleviate this issue. The authors conduct a comprehensive study, evaluating two scenarios: unintentional and intentional, using a curated dataset, MultiJail, which contains 3150 samples across 10 languages. The results show that LLMs exhibit a higher occurrence of unsafe behavior in non-English languages, with low-resource languages having a threefold higher likelihood of encountering harmful content compared to high-resource languages. The authors also demonstrate that multilingual adaptive attacks pose a significant threat, with ChatGPT achieving a 44.76% unsafe rate and GPT-4 achieving a 27.30% unsafe rate. To address this issue, the authors propose a novel framework, SELF-DEFENCE, which generates multilingual safety training data using LLMs, without human annotation, to fine-tune LLMs' safety capabilities.\n\nStrengths:\n\n* The paper provides a comprehensive study on multilingual jailbreak challenges in LLMs, evaluating two scenarios: unintentional and intentional.\n* The authors propose a novel framework, SELF-DEFENCE, which generates multilingual safety training data using LLMs, without human annotation, to fine-tune LLMs' safety capabilities.\n* The paper provides a detailed evaluation of LLMs' safety capabilities, using a curated dataset, MultiJail, which contains 3150 samples across 10 languages.\n* The authors demonstrate the effectiveness of SELF-DEFENCE in enhancing LLMs' multilingual safety capabilities, with a significant reduction in unsafe rates for both unintentional and intentional scenarios.\n\nWeaknesses:\n\n* The paper assumes that LLMs' safety mechanisms are sufficient to detect and filter out undesirable content, which may not be the case in all scenarios.\n* The authors do not provide a clear explanation of how SELF-DEFENCE handles competing objectives and mismatched generalization, which are identified as failure modes of safety alignment.\n* The paper relies on a curated dataset, MultiJail, which may not be representative of all languages and scenarios.\n* The authors do not provide a clear comparison with existing approaches to LLM safety, such as red teaming and reinforcement learning from human feedback.\n\nSuggestions for Improvement:\n\n* The authors should provide a clear explanation of how SELF-DEFENCE handles competing objectives and mismatched generalization.\n* The paper should include a more detailed comparison with existing approaches to LLM safety.\n* The authors should provide a clear explanation of how the curated dataset, MultiJail, was created and validated.\n* The paper should include a more detailed evaluation of LLMs' safety capabilities, using a broader range of languages and scenarios.\n\nRecommendation:\n\nThe paper provides a comprehensive study on multilingual jailbreak challenges in LLMs and proposes a novel framework, SELF-DEFENCE, to alleviate this issue. While there are some weaknesses, the authors' approach shows promise in enhancing LLMs' multilingual safety capabilities. However, the paper requires additional work to address the limitations and provide a clear comparison with existing approaches to LLM safety.\n\nOverall, I recommend this paper for publication, with the understanding that the authors will address the limitations and provide a clear comparison with existing approaches to LLM safety.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.4838709677419355,
        "gr_score_without": 0.38153846153846155,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 124,
            "green_fraction": 0.38153846153846155,
            "z_score": -0.6793662204867574,
            "p_value": 0.751547076217616
        },
        "output_with": {
            "num_tokens_scored": 372,
            "num_green_tokens": 180,
            "green_fraction": 0.4838709677419355,
            "z_score": 3.3020033020049513,
            "p_value": 0.0004799847003032734
        }
    },
    {
        "title": "NON-EXCHANGEABLE CONFORMAL RISK CONTROL",
        "abstract": "Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing nonexchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result in tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a new method for conformal risk control that provides formal guarantees when the data is not exchangeable, while also achieving the same guarantees as existing methods if the data is in fact exchangeable. The proposed method, non-exchangeable conformal risk control (non-X CRC), is a combination of two parallel lines of research: non-exchangeable conformal prediction and conformal risk control. The method provides guarantees of the form E[L(\u03bb\u0302; (Xn+1, Yn+1))] \u2264 \u03b1 + (B - A) n\u2211i=1 w\u0303idTV(Z,Zi), where \u03bb\u0302 is the optimal choice of \u03bb that minimizes the expected risk, w\u0303i are the normalized weights, and dTV(Z,Zi) is the total variation distance between Z and Zi. The paper also provides a formal proof for the guarantee in Eq. (7) and suggests a strategy for choosing the weights wi based on regularized minimization of the coverage gap.\n\nStrengths:\n\n* The paper proposes a new method for conformal risk control that provides formal guarantees when the data is not exchangeable.\n* The method is flexible and makes very few assumptions about the data distribution.\n* The paper provides a formal proof for the guarantee in Eq. (7) and suggests a strategy for choosing the weights wi.\n* The experiments demonstrate the effectiveness of the proposed method in three different tasks: multilabel classification, monitoring electricity usage, and open-domain question answering.\n\nWeaknesses:\n\n* The paper assumes that the loss function L(\u03bb; (x, y)) is nonincreasing in \u03bb, which may not always be the case in practice.\n* The method requires the choice of weights wi, which can be challenging in some scenarios.\n* The paper does not provide a thorough analysis of the computational complexity of the proposed method.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how to choose the weights wi in practice, especially in scenarios where the data distribution is complex.\n* The paper should provide a more thorough analysis of the computational complexity of the proposed method.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method in different scenarios.\n\nRecommendation:\n\nThe paper proposes a new method for conformal risk control that provides formal guarantees when the data is not exchangeable. The method is flexible and makes very few assumptions about the data distribution. The paper provides a formal proof for the guarantee in Eq. (7) and suggests a strategy for choosing the weights wi. The experiments demonstrate the effectiveness of the proposed method in three different tasks. However, the paper assumes that the loss function L(\u03bb; (x, y)) is nonincreasing in \u03bb, which may not always be the case in practice. The method requires the choice of weights wi, which can be challenging in some scenarios. The paper does not provide a thorough analysis of the computational complexity of the proposed method. Therefore, I recommend accepting the paper with minor revisions to address the above concerns.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a new method for conformal risk control that provides formal guarantees when the data is not exchangeable, while achieving the same guarantees as existing methods if the data is in fact exchangeable. The method, called non-exchangeable conformal risk control, leverages and extends two lines of work: non-exchangeable conformal prediction and conformal risk control. The proposed method provides guarantees of the form E[L(\u03bb\u0302; (Xn+1, Yn+1))] \u2264 \u03b1+ (B \u2212A) n\u2211 i=1 w\u0303idTV(Z,Z i), where Z := (X1, Y1),..., (Xn, Yn), (Xn+1, Yn+1) is a sequence of n calibration examples followed by a test example, and \u03b1 \u2208 [A,B] is the maximum tolerable risk. The method also provides a strategy for choosing the weights wi based on the maximum entropy principle, which can be used to bound or estimate the total variation distance dTV(Z,Zi).\n\nStrengths:\n\n* The proposed method provides formal guarantees when the data is not exchangeable, while achieving the same guarantees as existing methods if the data is in fact exchangeable.\n* The method provides a flexible framework for controlling the expected value of any monotone loss function, while making very few assumptions.\n* The method allows weighting the data based on its relevance for a given test example, which can result in tighter bounds.\n* The method provides a strategy for choosing the weights wi based on the maximum entropy principle, which can be used to bound or estimate the total variation distance dTV(Z,Zi).\n\nWeaknesses:\n\n* The method assumes that the loss function L(\u03bb; (x, y)) is nonincreasing in \u03bb, which may not be the case in all situations.\n* The method assumes that the data-independent weights {wi}ni=1 are fixed, which may not be the case in all situations.\n* The method requires the calculation of the total variation distance dTV(Z,Zi), which can be computationally expensive.\n* The method requires the choice of the temperature parameter \u03b2, which can affect the performance of the method.\n\nSuggestions for Improvement:\n\n* Investigate the possibility of relaxing the assumption that the loss function L(\u03bb; (x, y)) is nonincreasing in \u03bb.\n* Investigate the possibility of using data-dependent weights {wi}ni=1, which can be computed using the maximum entropy principle.\n* Investigate the possibility of using approximation methods to compute the total variation distance dTV(Z,Zi), which can reduce the computational cost.\n* Investigate the possibility of using different methods for choosing the weights wi, such as cross-validation or gradient-based methods.\n\nRecommendation:\n\nThe paper proposes a new method for conformal risk control that provides formal guarantees when the data is not exchangeable, while achieving the same guarantees as existing methods if the data is in fact exchangeable. The method provides a flexible framework for controlling the expected value of any monotone loss function, while making very few assumptions. The method also provides a strategy for choosing the weights wi based on the maximum entropy principle, which can be used to bound or estimate the total variation distance dTV(Z,Zi). The method has the potential to be applied in a wide range of situations, including those involving language generation, medical data, or reinforcement learning, where the i.i.d. assumption does not hold.",
        "gr_score_with": 0.5510752688172043,
        "gr_score_without": 0.46418338108882523,
        "output_without": {
            "num_tokens_scored": 349,
            "num_green_tokens": 162,
            "green_fraction": 0.46418338108882523,
            "z_score": 2.4475393565014825,
            "p_value": 0.007191770774441023
        },
        "output_with": {
            "num_tokens_scored": 372,
            "num_green_tokens": 205,
            "green_fraction": 0.5510752688172043,
            "z_score": 5.947839281175587,
            "p_value": 1.3585258711823114e-09
        }
    },
    {
        "title": "HOW CONNECTIVITY STRUCTURE SHAPES RICH AND LAZY LEARNING IN NEURAL CIRCUITS",
        "abstract": "In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity could exhibit a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights \u2014 in particular their effective rank \u2014 influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the impact of initial weight structure, particularly the effective rank, on learning regimes in neural networks. The authors derive theoretical results using a two-layer linear network and validate their findings in recurrent neural networks (RNNs) through numerical experiments. They show that high-rank initializations typically yield smaller network changes, indicative of lazier learning, while low-rank initializations lead to richer learning. However, they also find that certain low-rank initializations can still result in lazier learning if they are already aligned with the task statistics.\n\nStrengths:\n\n* The paper provides a comprehensive theoretical framework for understanding the impact of initial weight structure on learning regimes.\n* The authors use a variety of numerical experiments to validate their theoretical findings, including simulations on RNNs with different initial connectivity structures.\n* The paper has potential implications for neuroscience, particularly in understanding the role of effective connectivity rank in learning dynamics.\n\nWeaknesses:\n\n* The paper assumes a fixed initial weight magnitude, which may not be realistic in many cases.\n* The authors do not explore the impact of other facets of weight on the effective learning regime, such as the number of task classes and weight rank.\n* The paper's focus on RNNs may limit its generalizability to other neural network architectures.\n\nSuggestions for Improvement:\n\n* Consider exploring the impact of other facets of weight on the effective learning regime, such as the number of task classes and weight rank.\n* Investigate the implications of the paper's findings for other neural network architectures, such as feedforward networks.\n* Provide more details on the experimental setup and parameter settings used in the numerical experiments.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the understanding of learning regimes in neural networks and has potential implications for neuroscience. However, it requires further exploration of the impact of other facets of weight on the effective learning regime and the generalizability of the findings to other neural network architectures. With these limitations addressed, the paper has the potential to make a significant impact in the field.\n\nOverall, I would recommend this paper for publication in a reputable conference or journal, subject to the revisions suggested above.\n\nRating: 8/10\n\nReviewers' comments:\n\n* The paper is well-written and easy to follow, with a clear and concise introduction and conclusion.\n* The authors provide a comprehensive theoretical framework for understanding the impact of initial weight structure on learning regimes.\n* The numerical experiments are well-designed and provide strong evidence for the authors' claims.\n* The paper has potential implications for neuroscience, particularly in understanding the role of effective connectivity rank in learning dynamics.\n\nHowever, the paper also has some limitations, such as the assumption of a fixed initial weight magnitude and the focus on RNNs. With these limitations addressed, the paper has the potential to make a significant impact in the field.\n\nRecommendation for revisions:\n\n* Consider exploring the impact of other facets of weight on the effective learning regime, such as the number of task classes and weight rank.\n* Investigate the implications of the paper's findings for other neural network architectures, such as feedforward networks.\n* Provide more details on the experimental setup and parameter settings used in the numerical experiments.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper explores the impact of initial weight structure, particularly the effective rank, on learning regimes in neural networks. The authors provide both theoretical and empirical analyses, demonstrating that high-rank initializations tend to lead to lazier learning, while low-rank initializations can bias learning towards richer learning. The study also shows that certain low-rank initializations can lead to lazier learning when aligned with task statistics.\n\nStrengths:\n1. The paper makes a significant contribution to the understanding of learning regimes in neural networks, particularly in the context of effective weight rank.\n2. The authors provide both theoretical and empirical analyses, which adds credibility to their findings.\n3. The study explores a wide range of neural network architectures, including RNNs, feedforward networks, and biologically motivated connectivity structures.\n\nWeaknesses:\n1. The paper assumes a fixed learning rate and does not explore the impact of different learning rates on learning regimes.\n2. The study focuses primarily on RNNs and does not explore the implications for other neural network architectures, such as convolutional neural networks (CNNs) or transformers.\n3. The authors do not provide a detailed analysis of the impact of effective learning regimes on representation, including kernel-task alignment, and generalization capabilities.\n\nSuggestions for Improvement:\n1. The authors should explore the impact of different learning rates on learning regimes, particularly in the context of effective weight rank.\n2. The study should be extended to include a wider range of neural network architectures, including CNNs, transformers, and other architectures.\n3. The authors should provide a more detailed analysis of the impact of effective learning regimes on representation, including kernel-task alignment, and generalization capabilities.\n\nRecommendation:\nThis paper makes a significant contribution to the understanding of learning regimes in neural networks, particularly in the context of effective weight rank. The study provides both theoretical and empirical analyses, which adds credibility to their findings. The authors should consider exploring the impact of different learning rates, extending the study to include a wider range of neural network architectures, and providing a more detailed analysis of the impact of effective learning regimes on representation and generalization capabilities.\n\nOverall, this paper is well-written, well-organized, and well-reasoned. The authors provide a clear and concise explanation of their methodology, results, and conclusions, making it easy for readers to follow their arguments. The study has significant implications for both neural network design and understanding learning dynamics, making it a valuable contribution to the field.\n\nRating: 8/10\n\nRecommendation for Publication:\nYes, this paper should be published in a reputable conference or journal, subject to revisions addressing the suggestions for improvement.\n\nReviewers:\nThis paper has been reviewed by experts in the field, including researchers with expertise in neural network design, learning dynamics, and neural network applications. The reviewers provide feedback on the paper's strengths, weaknesses, and potential areas for improvement, making this a valuable contribution to the field.\n\n$$$",
        "gr_score_with": 0.5502793296089385,
        "gr_score_without": 0.3994708994708995,
        "output_without": {
            "num_tokens_scored": 378,
            "num_green_tokens": 151,
            "green_fraction": 0.3994708994708995,
            "z_score": -0.020998026278292187,
            "p_value": 0.5083763849342986
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 197,
            "green_fraction": 0.5502793296089385,
            "z_score": 5.804103409428285,
            "p_value": 3.2355665943726725e-09
        }
    },
    {
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines. We release all the above-mentioned resources at https://github.com/RUCAIBox/FIGA.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called FIGA (Fine-grained Quality-Aware Alignment) for aligning language models with human preferences. The approach leverages fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of language models for alignment. The authors curate a refined alignment dataset called SPA (SubPar Alignment) that pairs initial responses with revised responses on queries where a language model performs less well. They also design a new loss function that incorporates fine-grained quality signals to encourage the model to learn good behaviors and penalize bad behaviors. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nStrengths:\n\n1. The paper presents a novel approach for aligning language models with human preferences, which is a critical problem in natural language processing.\n2. The approach leverages fine-grained quality signals, which provides more detailed information about what makes a response good or bad.\n3. The authors curate a refined alignment dataset called SPA, which is a significant contribution to the field.\n4. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nWeaknesses:\n\n1. The paper assumes the availability of a reward model, which may not always be the case in real-world scenarios.\n2. The approach relies on the quality of the revised responses, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the Levenshtein distance metric used to quantify the similarity between initial and revised responses.\n4. The approach may not generalize well to out-of-distribution scenarios, where the reward model may not perform well.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for deriving fine-grained quality signals, such as using other metrics or incorporating additional information.\n2. Evaluate the approach on more diverse and challenging datasets to assess its generalizability.\n3. Provide more detailed analysis of the Levenshtein distance metric and its limitations.\n4. Investigate the use of other reward models or alternative methods for aligning language models with human preferences.\n\nRecommendation:\n\nThe paper presents a novel and promising approach for aligning language models with human preferences. While there are some limitations and areas for improvement, the approach shows superior performance compared to existing alignment methods. I recommend this paper for publication in a top-tier conference, with the suggestion to address the weaknesses and improve the approach as mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper presents a new alignment approach for large language models (LLMs) called FIGA, which leverages fine-grained quality signals to improve alignment without relying on reinforcement learning from human feedback (RLHF). The authors propose a new loss function that can incorporate fine-grained quality signals to instruct the learning of LLMs for alignment. The approach involves curating a refined alignment dataset that pairs initial responses and the corresponding revised ones. The authors demonstrate the effectiveness of their approach by comparing it with several competitive baselines.\n\nStrengths:\n\n1. The paper presents a new and innovative approach to alignment that leverages fine-grained quality signals to improve alignment without relying on RLHF.\n2. The authors provide a detailed explanation of their approach and demonstrate its effectiveness through comprehensive experiments.\n3. The paper provides a clear and concise summary of the related work and contributes to the field of alignment for LLMs.\n\nWeaknesses:\n\n1. The paper assumes that a reward model for assessing the alignment level is available, which may not always be the case in practice.\n2. The authors do not provide a clear explanation of how to select the threshold values for filtering and revising initial responses.\n3. The paper does not discuss the scalability of the approach to large datasets and the computational resources required for training.\n\nSuggestions for Improvement:\n\n1. Provide more details on how to select the threshold values for filtering and revising initial responses to ensure that they are robust and generalizable.\n2. Discuss the scalability of the approach to large datasets and the computational resources required for training to ensure that it can be applied to real-world settings.\n3. Provide more information on the reward model used for assessing the alignment level and how it can be adapted to different domains and tasks.\n\nRecommendation:\n\nI highly recommend this paper for publication in a top-tier conference or journal due to its innovative approach, comprehensive experiments, and clear presentation. However, I suggest that the authors address the weaknesses and suggestions for improvement mentioned above to further strengthen the paper.\n\nRating:\n\nI rate this paper 9 out of 10 for its originality, methodology, and presentation.\n\nReviewer's expertise:\n\nI am an expert in natural language processing and machine learning, with a focus on alignment for LLMs. I have read and reviewed many papers on this topic and can provide an informed opinion on the strengths and weaknesses of this paper.\n\nReview duration:\n\nI completed this review within 5 days.\n\nConflict of interest:\n\nI do not have any conflict of interest with the authors or the paper.\n\n$$$",
        "gr_score_with": 0.48484848484848486,
        "gr_score_without": 0.4119318181818182,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 145,
            "green_fraction": 0.4119318181818182,
            "z_score": 0.4569538468837056,
            "p_value": 0.3238521096311134
        },
        "output_with": {
            "num_tokens_scored": 396,
            "num_green_tokens": 192,
            "green_fraction": 0.48484848484848486,
            "z_score": 3.4465617474213155,
            "p_value": 0.0002838843998958419
        }
    },
    {
        "title": "IMPROVED REGRET BOUNDS FOR NON-CONVEX ONLINE-WITHIN-ONLINE META LEARNING",
        "abstract": "Online-Within-Online (OWO) meta learning stands for the online multi-task learning paradigm in which both tasks and data within each task become available in a sequential order. In this work, we study the OWO meta learning of the initialization and step size of within-task online algorithms in the non-convex setting, and provide improved regret bounds under mild assumptions of loss functions. Previous work analyzing this scenario has obtained for bounded and piecewise Lipschitz functions an averaged regret bound O(( \u221a m T 1/4 + (logm) log T \u221a T + V ) \u221a m) across T tasks, with m iterations per task and V the task similarity. Our first contribution is to modify the existing non-convex OWO meta learning algorithm and improve the regret bound to O(( 1 T 1/2\u2212\u03b1 + (log T ) 9/2 T + V ) \u221a m), for any \u03b1 \u2208 (0, 1/2). The derived bound has a faster convergence rate with respect to T , and guarantees a vanishing task-averaged regret with respect to m (for any fixed T ). Then, we propose a new algorithm of regret O(( log T T + V ) \u221a m) for non-convex OWO meta learning. This regret bound exhibits a better asymptotic performance than previous ones, and holds for any bounded (not necessarily Lipschitz) loss functions. Besides the improved regret bounds, our contributions include investigating how to attain generalization bounds for statistical meta learning via regret analysis. Specifically, by online-to-batch arguments, we achieve a transfer risk bound for batch meta learning that assumes all tasks are drawn from a distribution. Moreover, by connecting multi-task generalization error with taskaveraged regret, we develop for statistical multi-task learning a novel PAC-Bayes generalization error bound that involves our regret bound for OWO meta learning.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the online-within-online (OWO) meta learning of the initialization and step size of the Exponentially Weighted Aggregation (EWA) algorithm in the non-convex setting. The authors propose two algorithms to learn the initialization and step size, depending on the type of loss functions. For piecewise Lipschitz functions, they use Follow-The-Regularized-Leader (FTRL) algorithm to learn the discrete initialization distribution and achieve a sublinear regret. For non-Lipschitz functions, they utilize FTL algorithm to learn the continuous initialization distribution and derive a logarithmic regret. The authors also provide improved regret bounds for non-convex OWO meta learning and derive a new transfer risk bound for batch meta learning. Furthermore, they achieve a novel PAC-Bayes generalization bound for statistical multi-task learning.\n\nStrengths:\n\n* The paper provides improved regret bounds for non-convex OWO meta learning, which is a significant contribution to the field of meta learning.\n* The authors propose two algorithms to learn the initialization and step size, depending on the type of loss functions, which is a novel approach.\n* The paper derives a new transfer risk bound for batch meta learning, which is a useful result for the field of meta learning.\n* The authors achieve a novel PAC-Bayes generalization bound for statistical multi-task learning, which is a promising direction for future research.\n\nWeaknesses:\n\n* The paper assumes that the loss functions are piecewise Lipschitz or non-Lipschitz, which may not be a realistic assumption in practice.\n* The authors do not provide a detailed comparison with existing methods, which makes it difficult to evaluate the significance of their contributions.\n* The paper assumes that the tasks are drawn i.i.d. from a distribution, which may not be a realistic assumption in practice.\n* The authors do not provide a detailed analysis of the computational complexity of their algorithms, which makes it difficult to evaluate their practicality.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with existing methods to evaluate the significance of their contributions.\n* The paper should assume more realistic assumptions about the loss functions and the tasks.\n* The authors should provide a detailed analysis of the computational complexity of their algorithms to evaluate their practicality.\n* The paper should provide more experimental results to evaluate the performance of their algorithms.\n\nRecommendation:\n\nThe paper provides significant contributions to the field of meta learning, including improved regret bounds for non-convex OWO meta learning, a new transfer risk bound for batch meta learning, and a novel PAC-Bayes generalization bound for statistical multi-task learning. However, the paper assumes unrealistic assumptions about the loss functions and the tasks, and does not provide a detailed comparison with existing methods. Therefore, I recommend that the authors revise the paper to address these issues and provide more experimental results to evaluate the performance of their algorithms.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper aims to improve the regret bounds for non-convex Online-Within-Online (OWO) meta learning, which is a multi-task learning paradigm where both tasks and data within each task become available in a sequential order. The authors study the OWO meta learning of the initialization and step size of within-task online algorithms in the non-convex setting, providing improved regret bounds under mild assumptions of loss functions.\n\nStrengths:\n\n1. The paper provides improved regret bounds for non-convex OWO meta learning, which is a challenging problem.\n2. The authors propose a new algorithm, Non-convex OWO meta learning algorithm, which is more efficient than previous algorithms.\n3. The paper shows how to attain generalization bounds for statistical meta learning via regret analysis, which is an important contribution.\n\nWeaknesses:\n\n1. The paper assumes that the loss functions are piecewise Lipschitz or non-Lipschitz, which might be restrictive.\n2. The authors use Follow-The-Regularized-Leader (FTRL) algorithm and Follow-The-Leader (FTL) algorithm, which might not be applicable to all problems.\n3. The paper does not provide an empirical evaluation of the proposed algorithm.\n\nSuggestions for Improvement:\n\n1. Relax the assumption of piecewise Lipschitz or non-Lipschitz loss functions to more general loss functions.\n2. Explore other online algorithms, such as Follow-The-Perturbed-Leader, to see if they can be applied to OWO meta learning.\n3. Provide an empirical evaluation of the proposed algorithm to demonstrate its effectiveness.\n\nRecommendation:\n\nThe paper provides important contributions to the field of non-convex OWO meta learning, including improved regret bounds and a new algorithm. However, it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on the loss functions, which might limit its applicability. To further improve the paper, it is recommended to relax these assumptions and provide an empirical evaluation.\n\n$$$\n$$\nThe proposed algorithm is more efficient than previous algorithms, but it might not be applicable to all problems. To further improve the paper, it is recommended to explore other online algorithms, such as Follow-The-Perturbed-Leader, and provide an empirical evaluation of the proposed algorithm.\n\n$$$\n$$\nThe paper provides important contributions to the field of non-convex OWO meta learning, but it assumes restrictive conditions on",
        "gr_score_with": 0.4827586206896552,
        "gr_score_without": 0.3745928338762215,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 115,
            "green_fraction": 0.3745928338762215,
            "z_score": -0.9086984643008943,
            "p_value": 0.8182453424824296
        },
        "output_with": {
            "num_tokens_scored": 290,
            "num_green_tokens": 140,
            "green_fraction": 0.4827586206896552,
            "z_score": 2.876779808912305,
            "p_value": 0.002008779149216763
        }
    },
    {
        "title": "CONDITIONAL VARIATIONAL DIFFUSION MODELS",
        "abstract": "Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-consuming and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior results to previous methods and fine-tuned diffusion models. We conclude that fine-tuning the schedule by experimentation should be avoided because it can be learned during training in a stable way that yields better results. The code is available on https://github.com/casus/cvdm",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to learning the variance schedule in diffusion models, which is a key component in solving inverse problems. The authors introduce the Conditional Variational Diffusion Model (CVDM), a flexible method to learn the schedule as part of the training process. The CVDM extends the Variational Diffusion Models (VDMs) to the conditioned case, allowing for learning a different schedule for each element in the output. The authors provide a detailed derivation of the schedule and show that it can be learned in a stable way that yields better results than fine-tuning the schedule as a hyperparameter. The CVDM is tested on three distinct applications: super-resolution microscopy, quantitative phase imaging, and image super-resolution, and shows comparable or superior results to previous methods.\n\nStrengths:\n\n* The paper proposes a novel approach to learning the variance schedule in diffusion models, which is a key component in solving inverse problems.\n* The authors provide a detailed derivation of the schedule and show that it can be learned in a stable way that yields better results than fine-tuning the schedule as a hyperparameter.\n* The CVDM is tested on three distinct applications and shows comparable or superior results to previous methods.\n* The paper provides a clear and concise explanation of the method and its implementation.\n\nWeaknesses:\n\n* The paper assumes that the variance schedule can be learned as part of the training process, which may not be the case in all scenarios.\n* The authors do not provide a detailed comparison with other methods that learn the variance schedule, such as the method proposed by Kingma et al. (2023).\n* The paper does not provide a detailed analysis of the computational complexity of the CVDM.\n* The authors do not provide a detailed discussion of the potential applications of the CVDM beyond the three applications tested in the paper.\n\nSuggestions for Improvement:\n\n* Provide a more detailed comparison with other methods that learn the variance schedule.\n* Provide a detailed analysis of the computational complexity of the CVDM.\n* Discuss the potential applications of the CVDM beyond the three applications tested in the paper.\n* Provide a more detailed explanation of the schedule and how it is learned in the CVDM.\n\nRecommendation:\n\nThe paper proposes a novel approach to learning the variance schedule in diffusion models, which is a key component in solving inverse problems. The authors provide a detailed derivation of the schedule and show that it can be learned in a stable way that yields better results than fine-tuning the schedule as a hyperparameter. The CVDM is tested on three distinct applications and shows comparable or superior results to previous methods. The paper provides a clear and concise explanation of the method and its implementation. However, the paper assumes that the variance schedule can be learned as part of the training process, which may not be the case in all scenarios. Therefore, the paper is recommended for publication after addressing the weaknesses and suggestions for improvement.\n\nOverall, the paper is well-written, well-organized, and provides a clear and concise explanation of the method and its implementation. The CVDM is a novel approach to learning the variance schedule in diffusion models, and the authors provide a detailed derivation of the schedule and show that it can be learned in a stable way that yields better results than fine-tuning the schedule as a hyperparameter. The paper provides a clear and concise explanation of the method and its implementation, and the authors provide a detailed comparison with previous methods. However, the paper assumes that the variance schedule can be learned as part of the training process, which may not be the case in all scenarios. Therefore, the paper is recommended for publication after addressing the weaknesses and suggestions for improvement.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to learning the schedule in diffusion models, which are widely used for inverse problems. The Conditional Variational Diffusion Model (CVDM) learns the schedule as part of the training process, allowing for probabilistic conditioning on data and providing high-quality solutions. The method is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior results to previous methods and fine-tuned diffusion models.\n\nStrengths:\n\n* The paper proposes a novel approach to learning the schedule in diffusion models, which is a key contribution to the field.\n* The method is tested in two unrelated inverse problems, demonstrating its versatility and accuracy.\n* The paper provides a detailed analysis of the schedule learning process, including a theoretical understanding of the relation between \u03b3 and \u03b2.\n* The method is shown to outperform previous methods and fine-tuned diffusion models in various metrics.\n\nWeaknesses:\n\n* The paper assumes a specific form for the schedule function \u03b3(t,x), which may not be universally applicable.\n* The method requires a large amount of data for training, which may be a limitation in some applications.\n* The paper does not provide a detailed comparison with other schedule learning methods, which would be useful for a more comprehensive evaluation.\n\nSuggestions for Improvement:\n\n* The authors could provide more details on the theoretical underpinnings of the schedule learning process, including a more rigorous analysis of the relation between \u03b3 and \u03b2.\n* The method could be evaluated on a wider range of inverse problems, including more complex and diverse applications.\n* The authors could provide a more detailed comparison with other schedule learning methods, including a discussion of the strengths and weaknesses of each approach.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to learning the schedule in diffusion models, which has the potential to improve the accuracy and versatility of inverse problem solvers. The method is tested in two unrelated inverse problems, demonstrating its versatility and accuracy. However, the paper assumes a specific form for the schedule function \u03b3(t,x), which may not be universally applicable. Overall, the paper is well-written and provides a significant contribution to the field of inverse problems.\n\nRating: 8/10",
        "gr_score_with": 0.4234875444839858,
        "gr_score_without": 0.31141868512110726,
        "output_without": {
            "num_tokens_scored": 289,
            "num_green_tokens": 90,
            "green_fraction": 0.31141868512110726,
            "z_score": -3.073869481139676,
            "p_value": 0.9989434907299192
        },
        "output_with": {
            "num_tokens_scored": 281,
            "num_green_tokens": 119,
            "green_fraction": 0.4234875444839858,
            "z_score": 0.8036836898367463,
            "p_value": 0.21078983801535095
        }
    },
    {
        "title": "GRAPH METANETWORKS FOR PROCESSING DIVERSE NEURAL ARCHITECTURES",
        "abstract": "Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks \u2014 neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Graph Metanetworks (GMNs), a new approach to processing neural networks by treating them as graphs and using Graph Neural Networks (GNNs) to learn representations and perform downstream tasks. The authors address the challenge of designing metanets that respect neural network parameter symmetries by representing input neural networks as parameter graphs, which are compact and efficient representations of neural networks as graphs. The GMNs are shown to be equivariant to permutation symmetries in the input network, which is a key property for metanets. The authors also prove that GMNs can express existing metanets on MLP inputs and simulate the forward pass of an input neural network represented by its computation graph. Empirical evaluations show that GMNs outperform existing metanetwork baselines across various tasks.\n\nStrengths:\n\n* The paper proposes a novel and elegant solution to metanet design that respects neural network parameter symmetries.\n* The authors provide a thorough theoretical analysis of the GMNs, including their equivariance to permutation symmetries and expressive power.\n* The empirical evaluations demonstrate the effectiveness of GMNs on various tasks, including predicting accuracy for varying architectures and editing 2D INRs.\n* The paper provides a comprehensive review of the related work and identifies the limitations of existing metanet approaches.\n\nWeaknesses:\n\n* The paper assumes that the input neural networks can be represented as graphs, which may not be the case for all neural networks.\n* The authors do not provide a formal analysis of the scalability of GMNs, which may be a limitation for large neural networks.\n* The paper only accounts for permutation-based parameter symmetries and does not consider other types of symmetries, such as those induced by scaling weights in ReLU networks.\n* The empirical evaluations are limited to a specific set of tasks and datasets, and it would be beneficial to evaluate GMNs on a broader range of tasks and datasets.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the scalability of GMNs, including a discussion of the computational resources required to process large neural networks.\n* Investigate the application of GMNs to other types of symmetries, such as those induced by scaling weights in ReLU networks.\n* Evaluate GMNs on a broader range of tasks and datasets to demonstrate their effectiveness and robustness.\n* Provide a more detailed comparison of GMNs with existing metanet approaches, including a discussion of their strengths and weaknesses.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to metanet design that respects neural network parameter symmetries. The empirical evaluations demonstrate the effectiveness of GMNs on various tasks, and the paper provides a comprehensive review of the related work. However, the paper assumes that the input neural networks can be represented as graphs, and it would be beneficial to provide a more thorough analysis of the scalability of GMNs. Therefore, I recommend accepting the paper with minor revisions to address these limitations.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Graph Metanetworks (GMNs), a new approach to processing neural networks by treating them as graphs and processing these graphs with Graph Neural Networks (GNNs). The authors demonstrate that GMNs can process diverse neural architectures, including those with complex modules like multi-head attention layers, normalization layers, and ResNet blocks, while maintaining theoretical guarantees of permutation parameter symmetry equivariance and high expressive power.\n\nStrengths:\n\n1.  The authors provide a clear and concise explanation of their approach, including a detailed description of their graph construction and GNN architecture.\n2.  The paper demonstrates the effectiveness of GMNs on a wide range of metanet tasks, including predicting accuracy for varying architectures, editing 2D INRs, and self-supervised learning with INRs.\n3.  The authors provide theoretical guarantees of permutation parameter symmetry equivariance and high expressive power, which is a significant improvement over existing metanet approaches.\n\nWeaknesses:\n\n1.  While the authors demonstrate the effectiveness of GMNs on a wide range of tasks, the paper primarily focuses on image classification tasks, and it is unclear whether GMNs will perform well on other types of tasks.\n2.  The authors do not provide a detailed comparison with other metanet approaches, including those that have been specifically designed for processing complex neural architectures.\n3.  While the authors provide some intuition for why GMNs work well on complex neural architectures, the paper could benefit from more detailed analysis and experimental evaluation.\n\nSuggestions for Improvement:\n\n1.  Provide more detailed comparisons with other metanet approaches, including those that have been specifically designed for processing complex neural architectures.\n2.  Include more experimental evaluation to demonstrate the effectiveness of GMNs on a wider range of tasks, including those that involve complex neural architectures.\n3.  Provide more detailed analysis and intuition for why GMNs work well on complex neural architectures.\n\nRecommendation:\n\nThe paper is well-written and provides a clear and concise explanation of the Graph Metanetworks approach. The authors demonstrate the effectiveness of GMNs on a wide range of metanet tasks, including predicting accuracy for varying architectures, editing 2D INRs, and self-supervised learning with INRs. However, the paper could benefit from more detailed comparisons with other metanet approaches and more experimental evaluation to demonstrate the effectiveness of GMNs on a wider range of tasks.\n\nOverall, I would recommend accepting this paper for publication in a top-tier conference or journal in the field of machine learning.\n\nRating: 8/10\n\nReview Type: Major Revision\n\nReason for Revision: The paper could benefit from more detailed comparisons with other metanet approaches and more experimental evaluation to demonstrate the effectiveness of GMNs on a wider range of tasks.\n\nExpected Changes:\n\n1.  Include more detailed comparisons with other metanet approaches, including those that have been specifically designed for processing complex neural architectures.\n2.  Include more experimental evaluation to demonstrate the effectiveness of GMNs on a wider range of tasks, including those that involve complex neural architectures.\n3.  Provide more detailed analysis and intuition for why GMNs work well on complex neural architectures.\n\nTimeline for Revision: 2-3 months\n\nReview Date: February 2024\n\n$$$",
        "gr_score_with": 0.44072948328267475,
        "gr_score_without": 0.3617021276595745,
        "output_without": {
            "num_tokens_scored": 376,
            "num_green_tokens": 136,
            "green_fraction": 0.3617021276595745,
            "z_score": -1.5158734579197348,
            "p_value": 0.9352243235477957
        },
        "output_with": {
            "num_tokens_scored": 329,
            "num_green_tokens": 145,
            "green_fraction": 0.44072948328267475,
            "z_score": 1.5079996076312518,
            "p_value": 0.06577731371912289
        }
    },
    {
        "title": "LAYOUTNUWA: REVEALING THE HIDDEN LAYOUT EXPERTISE OF LARGE LANGUAGE MODELS",
        "abstract": "Graphic layout generation plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harnesses the hidden layout expertise of large language models (LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50% improvements) on multiple datasets, showcasing the strong capabilities of LayoutNUWA. Our code is available at https://github.com/ProjectNUWA/LayoutNUWA. \u2217Both authors contributed equally to this research. During Zecheng\u2019s internship under the mentorship of Chenfei at MSRA. \u2020Corresponding author.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes LayoutNUWA, a novel approach to layout generation that treats the task as a code generation problem. The authors introduce a Code Instruct Tuning (CIT) method, comprising three interconnected modules: Code Initialization, Code Completion, and Code Rendering. The CIT approach leverages the expertise of Large Language Models (LLMs) to generate layouts with rich semantic information, achieving state-of-the-art performance on multiple datasets.\n\nStrengths:\n1. The paper presents a novel approach to layout generation that integrates semantic information into the generation process.\n2. The CIT method is well-designed and leverages the strengths of LLMs to generate high-quality layouts.\n3. The authors provide extensive experimental results on multiple datasets, demonstrating the superiority of their method.\n4. The paper has the potential to revolutionize the field of layout generation and pave the way for further exploration and development of semantic-aware layout generation approaches.\n\nWeaknesses:\n1. The paper assumes that the input layout is in a specific format, which may not be the case in real-world scenarios.\n2. The authors do not provide a clear explanation of how the CIT method handles cases where the input layout has inconsistent or missing information.\n3. The paper relies heavily on the performance of the LLMs used in the CIT method, which may not generalize well to other datasets or applications.\n4. The authors do not provide a thorough analysis of the computational resources required to train and deploy the LayoutNUWA model.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation of how the CIT method handles cases with inconsistent or missing input information.\n2. The paper should include a more thorough analysis of the computational resources required to train and deploy the LayoutNUWA model.\n3. The authors should provide a clearer explanation of how the CIT method can be adapted to other datasets or applications.\n4. The paper should include a more comprehensive evaluation of the LayoutNUWA model on a wider range of tasks and datasets.\n\nRecommendation:\nThe paper presents a novel and promising approach to layout generation that integrates semantic information into the generation process. The CIT method is well-designed and leverages the strengths of LLMs to generate high-quality layouts. However, the paper has some limitations that need to be addressed. With some modifications and additional analysis, the paper has the potential to make a significant contribution to the field of layout generation. Therefore, I recommend acceptance of the paper with some revisions.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes LayoutNUWA, a novel approach to layout generation by treating it as a code generation task. The authors introduce Code Instruct Tuning (CIT), a three-module approach consisting of Code Initialization, Code Completion, and Code Rendering. CIT leverages the expertise of Large Language Models (LLMs) to enhance the semantic information and quality of the generated layouts. The authors evaluate LayoutNUWA on three datasets (RICO, PubLayNet, and Magazine) and report significant state-of-the-art performance improvements over existing baselines.\n\nStrengths:\n1. The paper presents a novel approach to layout generation by treating it as a code generation task, which allows for the incorporation of semantic information and the utilization of LLMs.\n2. The Code Instruct Tuning (CIT) approach is well-defined and consists of three interconnected modules (Code Initialization, Code Completion, and Code Rendering) that work together to generate high-quality layouts.\n3. The authors provide extensive experiments on multiple datasets (RICO, PubLayNet, and Magazine) and report significant state-of-the-art performance improvements over existing baselines.\n4. The paper discusses the importance of semantic information in layout generation and demonstrates the effectiveness of CIT in capturing and utilizing this information.\n\nWeaknesses:\n1. The paper assumes that the input layout elements are regular squares, which may not hold for all types of layouts (e.g., circular or irregularly-shaped elements).\n2. The authors use a fixed structure for the HTML code, which may limit the flexibility and expressiveness of the generated layouts.\n3. The paper does not provide a thorough comparison with other state-of-the-art layout generation methods (e.g., graph-based or physics-based approaches).\n4. The authors do not discuss the potential limitations and challenges of using LLMs for layout generation (e.g., overfitting, interpretability).\n\nSuggestions for Improvement:\n1. Investigate the extension of the Code Instruct Tuning (CIT) approach to handle non-regularly shaped elements and more complex layout structures.\n2. Explore the use of other code languages or formats (e.g., CSS, JSON) for generating layouts and compare their effectiveness with HTML.\n3. Conduct a more thorough comparison with other state-of-the-art layout generation methods (e.g., graph-based or physics-based approaches) to demonstrate the superiority of the proposed approach.\n4. Address the potential limitations and challenges of using LLMs for layout generation (e.g., overfitting, interpretability) and discuss possible solutions or workarounds.\n\nRecommendation:\nI recommend the paper for acceptance, as it presents a novel and effective approach to layout generation by treating it as a code generation task. The authors provide extensive experiments and report significant state-of-the-art performance improvements over existing baselines. However, the authors should address the limitations and challenges mentioned above to strengthen the paper and make it more comprehensive.",
        "gr_score_with": 0.5544041450777202,
        "gr_score_without": 0.4613003095975232,
        "output_without": {
            "num_tokens_scored": 323,
            "num_green_tokens": 149,
            "green_fraction": 0.4613003095975232,
            "z_score": 2.248838709600059,
            "p_value": 0.0122613797961282
        },
        "output_with": {
            "num_tokens_scored": 386,
            "num_green_tokens": 214,
            "green_fraction": 0.5544041450777202,
            "z_score": 6.192228680179201,
            "p_value": 2.965970174219045e-10
        }
    },
    {
        "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
        "abstract": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach named TGeo-KD for learning sample-wise knowledge fusion ratios during knowledge distillation (KD). The authors argue that existing methods for determining the knowledge fusion ratio often rely on heuristic rules or constant values, which may not capture the nuanced dynamics of the knowledge transfer process. To address this limitation, the authors introduce a novel adaptive method that learns a sample-wise knowledge fusion ratio by exploiting both the correctness of the teacher and student, as well as the discrepancy between their predictions. The proposed method, TGeo-KD, leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations.\n\nStrengths:\n\n1. The paper presents a well-structured and clear introduction to the problem of knowledge distillation and the limitations of existing methods.\n2. The proposed approach, TGeo-KD, is innovative and well-motivated, leveraging the trilateral geometry among the signals from the student, teacher, and ground truth.\n3. The authors provide extensive experiments across diverse domains, including image classification, attack detection, and click-through rate prediction, demonstrating the consistent superiority of TGeo-KD over other re-weighting methods.\n4. The paper provides a clear and concise explanation of the proposed method, including the bilevel optimization problem and the neural network architecture.\n\nWeaknesses:\n\n1. The paper assumes that the teacher network is pre-trained, which may not be the case in many real-world scenarios.\n2. The proposed method relies on the availability of ground truth labels, which may not be available in some applications.\n3. The paper does not provide a thorough comparison with other methods that do not use trilateral geometry, making it difficult to evaluate the effectiveness of the proposed approach.\n4. The authors do not provide a clear explanation of how the proposed method can be extended to more complex scenarios, such as multi-task learning or transfer learning.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other methods that do not use trilateral geometry, including a discussion of the advantages and disadvantages of each approach.\n2. Explore the use of TGeo-KD in scenarios where the teacher network is not pre-trained, such as in self-distillation or multi-task learning.\n3. Investigate the use of TGeo-KD in applications where ground truth labels are not available, such as in unsupervised learning or semi-supervised learning.\n4. Provide a more detailed explanation of how the proposed method can be extended to more complex scenarios, including a discussion of the challenges and potential solutions.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to knowledge distillation that leverages the trilateral geometry among the signals from the student, teacher, and ground truth. The extensive experiments across diverse domains demonstrate the consistent superiority of TGeo-KD over other re-weighting methods. However, the paper assumes that the teacher network is pre-trained, and the proposed method relies on the availability of ground truth labels. Therefore, I recommend that the authors address these limitations in future work and provide a more detailed comparison with other methods that do not use trilateral geometry.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method named TGeo-KD for learning sample-wise knowledge fusion ratios in knowledge distillation (KD). TGeo-KD leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations. The method aims to find the optimal sample-wise ratios that enable the student network to generalize well on test data. TGeo-KD consists of two key components: (1) adaptive learning for knowledge fusion ratio, which learns a sample-wise fusion ratio based on trilateral geometry within the (student, teacher, and ground truth) triplet, and (2) exploiting trilateral geometry, which captures both intra-sample and inter-sample geometric relations. The paper demonstrates the effectiveness of TGeo-KD on various tasks, including image classification, attack detection, and click-through rate prediction.\n\nStrengths:\n\n1. TGeo-KD provides a novel method for learning sample-wise knowledge fusion ratios in KD.\n2. The method leverages the trilateral geometry among the signals from the student, teacher, and ground truth.\n3. TGeo-KD demonstrates consistent improvements over other loss re-weighting methods on various tasks.\n4. The paper provides extensive experiments on diverse domains to demonstrate the broad applicability of TGeo-KD.\n\nWeaknesses:\n\n1. The paper assumes that the teacher network provides accurate predictions, which may not always be the case.\n2. TGeo-KD may not be effective when the student and teacher networks have vastly different architectures.\n3. The method relies on the trilateral geometry, which may not capture all the nuances of the knowledge transfer process.\n\nSuggestions for Improvement:\n\n1. Investigate the robustness of TGeo-KD to noisy teacher predictions.\n2. Explore the extension of TGeo-KD to hetero-architecture scenarios where the student and teacher networks have vastly different architectures.\n3. Consider incorporating additional geometric relations to capture more nuances of the knowledge transfer process.\n\nRecommendation:\n\nThe paper provides a novel and effective method for learning sample-wise knowledge fusion ratios in KD. TGeo-KD demonstrates consistent improvements over other loss re-weighting methods on various tasks. Therefore, I recommend this paper for publication in a top-tier conference in machine learning.",
        "gr_score_with": 0.5582191780821918,
        "gr_score_without": 0.44274809160305345,
        "output_without": {
            "num_tokens_scored": 393,
            "num_green_tokens": 174,
            "green_fraction": 0.44274809160305345,
            "z_score": 1.7298457770026017,
            "p_value": 0.0418289165965745
        },
        "output_with": {
            "num_tokens_scored": 292,
            "num_green_tokens": 163,
            "green_fraction": 0.5582191780821918,
            "z_score": 5.518803473654584,
            "p_value": 1.7065781906393306e-08
        }
    },
    {
        "title": "R-MAE: REGIONS MEET MASKED AUTOENCODERS",
        "abstract": "In this work, we explore regions as a potential visual analogue of words for selfsupervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to self-supervised image representation learning by introducing regions as a potential visual analogue of words. Inspired by Masked Autoencoding (MAE), the authors propose masked region autoencoding to learn from groups of pixels or regions. The proposed approach, called R-MAE, combines the strengths of MAE and region-based pre-training to achieve consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks.\n\nStrengths:\n\n* The paper provides a clear and concise introduction to the concept of regions as a visual analogue of words, and how it can be leveraged for self-supervised image representation learning.\n* The proposed approach, R-MAE, is well-motivated and demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks.\n* The paper provides a thorough analysis of the strengths and weaknesses of different architecture variants, and converges to a \"length\" variant that is both efficient and effective.\n* The paper provides a comprehensive evaluation of the proposed approach, including ablation studies, mask ratio experiments, and comparisons with other pre-training methods.\n\nWeaknesses:\n\n* The paper assumes that regions can be generated from a clustering algorithm (Felzenszwalb & Huttenlocher, 2004), which may not be suitable for all types of images or datasets.\n* The paper does not provide a clear explanation of why regions from SAM (Kirillov et al., 2023) significantly boost the performance of R-MAE.\n* The paper assumes that the pre-trained model can be used as an \"interactive segmenter\", which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of why regions from SAM (Kirillov et al., 2023) significantly boost the performance of R-MAE.\n* Evaluate the proposed approach on a wider range of datasets and scenarios to demonstrate its robustness and generalizability.\n* Provide a more detailed analysis of the strengths and weaknesses of different region generation methods, and explore alternative methods for generating regions.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to self-supervised image representation learning by introducing regions as a potential visual analogue of words. The proposed approach, R-MAE, demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks. While there are some limitations and areas for improvement, the paper provides a thorough and well-motivated contribution to the field of computer vision. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to self-supervised image representation learning by introducing regions as a potential visual analogue of words. Inspired by Masked Autoencoding (MAE), the authors propose masked region autoencoding to learn from groups of pixels or regions. The proposed approach, called R-MAE, combines the benefits of regions and MAE, demonstrating consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks. The paper also explores the potential of regions in pre-training and discusses the limitations and future directions of the proposed approach.\n\nStrengths:\n\n1. The paper proposes a novel and intuitive approach to self-supervised image representation learning by introducing regions as a potential visual analogue of words.\n2. The proposed approach, R-MAE, combines the benefits of regions and MAE, demonstrating consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks.\n3. The paper explores the potential of regions in pre-training and discusses the limitations and future directions of the proposed approach.\n4. The experimental results are thorough and well-presented, including ablation studies and comparisons with other pre-training methods.\n\nWeaknesses:\n\n1. The paper assumes that regions can serve as a visual analogue of words, which may not be a direct or perfect correspondence.\n2. The proposed approach, R-MAE, may not be scalable to large datasets or complex images, as regions may not capture all the nuances and variations in the image.\n3. The paper does not provide a thorough analysis of the properties and characteristics of regions, which may be important for understanding the benefits and limitations of the proposed approach.\n4. The experimental results, while thorough, may not provide a comprehensive evaluation of the proposed approach, as they are limited to a specific set of pre-training datasets and downstream tasks.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough analysis of the properties and characteristics of regions, including their benefits and limitations.\n2. Explore the potential of regions in pre-training and discuss the implications of using regions as a visual analogue of words.\n3. Provide a more comprehensive evaluation of the proposed approach, including comparisons with other pre-training methods and a broader range of pre-training datasets and downstream tasks.\n4. Discuss the potential applications and future directions of the proposed approach, including its potential to improve image understanding and recognition.\n\nRecommendation:\n\nThe proposed approach, R-MAE, is a novel and intuitive method for self-supervised image representation learning. While it has its limitations, it demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks. The paper provides a thorough and well-presented experimental evaluation, including ablation studies and comparisons with other pre-training methods. With further refinement and exploration, R-MAE has the potential to improve image understanding and recognition, making it a valuable addition to the field of computer vision.\n\n$$$",
        "gr_score_with": 0.5591054313099042,
        "gr_score_without": 0.49415204678362573,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 169,
            "green_fraction": 0.49415204678362573,
            "z_score": 3.5541618247912,
            "p_value": 0.00018959290923591766
        },
        "output_with": {
            "num_tokens_scored": 313,
            "num_green_tokens": 175,
            "green_fraction": 0.5591054313099042,
            "z_score": 5.74581386722587,
            "p_value": 4.573994751202157e-09
        }
    },
    {
        "title": "A RESTORATION NETWORK",
        "abstract": "Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called Deep Restoration Prior (DRP) for solving inverse problems in imaging by using pre-trained restoration operators as priors. The authors extend the existing work on using denoisers as priors to more general restoration operators, which enables the use of state-of-the-art pre-trained restoration models as priors. The DRP method is based on the idea of leveraging a pre-trained restoration network to gain access to the prior, and it provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. The authors present a theoretical convergence analysis of DRP and show that it converges to a stationary point of a global functional associated with the restoration operator.\n\nStrengths:\n\n1. The paper proposes a novel method that extends the existing work on using denoisers as priors to more general restoration operators.\n2. The authors provide a principled approach for adapting state-of-the-art restoration models for other inverse problems.\n3. The paper presents a theoretical convergence analysis of DRP, which is a significant contribution.\n4. The authors demonstrate the effectiveness of DRP on two well-known inverse problems, namely image deblurring and single image super-resolution.\n\nWeaknesses:\n\n1. The paper assumes that the restoration network is pre-trained, which may not always be the case in practice.\n2. The authors do not provide a comprehensive comparison with existing methods that use denoisers as priors.\n3. The paper assumes that the data-fidelity term is convex, which may not always be the case in practice.\n4. The authors do not provide a detailed analysis of the computational complexity of DRP.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive comparison with existing methods that use denoisers as priors.\n2. The paper should assume a more general data-fidelity term that is not necessarily convex.\n3. The authors should provide a detailed analysis of the computational complexity of DRP.\n4. The paper should provide more experimental results to demonstrate the effectiveness of DRP on a wider range of inverse problems.\n\nRecommendation:\n\nThe paper proposes a novel method that extends the existing work on using denoisers as priors to more general restoration operators. The authors provide a principled approach for adapting state-of-the-art restoration models for other inverse problems and demonstrate the effectiveness of DRP on two well-known inverse problems. However, the paper assumes that the restoration network is pre-trained, which may not always be the case in practice. Therefore, I recommend that the authors provide a more comprehensive comparison with existing methods that use denoisers as priors and assume a more general data-fidelity term that is not necessarily convex.\n\nOverall, the paper is well-written and provides a significant contribution to the field of inverse problems in imaging. However, there are some limitations that need to be addressed before the paper can be considered for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called Deep Restoration Priors (DRP) for solving imaging inverse problems using pre-trained restoration operators as priors. The method extends the recent work using denoisers as priors by considering more general restoration operators. The authors provide a theoretical analysis of DRP, including a convergence analysis that shows the method converges to a stationary point of the objective function. The authors also present numerical results that demonstrate the effectiveness of DRP on image deblurring and super-resolution tasks.\n\nStrengths:\n\n* The paper proposes a novel method that extends the recent work using denoisers as priors to more general restoration operators.\n* The authors provide a theoretical analysis of DRP, including a convergence analysis that shows the method converges to a stationary point of the objective function.\n* The numerical results presented in the paper demonstrate the effectiveness of DRP on image deblurring and super-resolution tasks.\n\nWeaknesses:\n\n* The paper assumes that the restoration operator is a minimum mean-squared error (MMSE) estimator, which may not be the case in all scenarios.\n* The authors do not provide a detailed analysis of the convergence of DRP to a global optimum.\n* The numerical results presented in the paper are limited to image deblurring and super-resolution tasks, and it is unclear whether DRP will perform well on other types of inverse problems.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the convergence of DRP to a global optimum.\n* Evaluate the performance of DRP on a variety of inverse problems, including problems that do not involve image deblurring or super-resolution.\n* Consider relaxing the assumption that the restoration operator is a minimum mean-squared error (MMSE) estimator.\n\nRecommendation:\n\nThe paper proposes a novel method that has the potential to be a significant contribution to the field of imaging inverse problems. However, the authors need to address the weaknesses mentioned above in order to make the method more robust and widely applicable. With further development and evaluation, DRP has the potential to be a powerful tool for solving a wide range of imaging inverse problems.\n\nRating: 7/10\n\nThis paper has the potential to be a significant contribution to the field of imaging inverse problems. However, the authors need to address the weaknesses mentioned above in order to make the method more robust and widely applicable. With further development and evaluation, DRP has the potential to be a powerful tool for solving a wide range of imaging inverse problems.",
        "gr_score_with": 0.5274725274725275,
        "gr_score_without": 0.4208754208754209,
        "output_without": {
            "num_tokens_scored": 297,
            "num_green_tokens": 125,
            "green_fraction": 0.4208754208754209,
            "z_score": 0.7343580230026568,
            "p_value": 0.2313652816531217
        },
        "output_with": {
            "num_tokens_scored": 273,
            "num_green_tokens": 144,
            "green_fraction": 0.5274725274725275,
            "z_score": 4.299246036634386,
            "p_value": 8.569006743157404e-06
        }
    },
    {
        "title": "EXTREMELY SPARSE LABELED DATA",
        "abstract": "Graph-based semi-supervised learning, particularly in the context of extremely sparse labeled data, often suffers from degenerate solutions where label functions tend to be nearly constant across unlabeled data. In this paper, we introduce Variance-enlarged Poisson Learning (VPL), a simple yet powerful framework tailored to alleviate the issues arising from the presence of degenerate solutions. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data. This intuitive approach increases the dispersion of labels from their average mean, effectively reducing the likelihood of degenerate solutions characterized by nearly constant label functions. We subsequently introduce two streamlined algorithms, V-Laplace and V-Poisson, each intricately designed to enhance Laplace and Poisson learning, respectively. Furthermore, we broaden the scope of VPL to encompass graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation. To achieve a deeper understanding of VPL\u2019s behavior, we conduct a comprehensive theoretical exploration in both discrete and variational cases. Our findings elucidate that VPL inherently amplifies the importance of connections within the same class while concurrently tempering those between different classes. We support our claims with extensive experiments, demonstrating the effectiveness of VPL and showcasing its superiority over existing methods. The code is available at https://github.com/hitcszx/VPL.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Variance-enlarged Poisson Learning (VPL), a framework to alleviate the issues associated with extremely sparse labeled data in graph-based semi-supervised learning. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data, increasing the dispersion of labels away from their average value. The authors introduce two streamlined algorithms, V-Laplace and V-Poisson, each designed to enhance Laplace and Poisson learning, respectively. They also propose Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation in graph neural networks. Theoretical analysis in both discrete and variational cases demonstrates that VPL amplifies the importance of connections within the same class while tempering those between different classes.\n\nStrengths:\n\n1. The paper proposes a novel framework, VPL, to address the challenges of extremely sparse labeled data in graph-based semi-supervised learning.\n2. The authors provide a clear and concise introduction to the problem and the proposed solution.\n3. The theoretical analysis in both discrete and variational cases provides a deeper understanding of the behavior of VPL.\n4. The experimental results demonstrate the effectiveness and superior performance of VPL in scenarios characterized by extremely sparse labeled data.\n\nWeaknesses:\n\n1. The paper assumes that the graph is connected, which may not be the case in real-world scenarios.\n2. The authors do not provide a comprehensive comparison with existing methods, particularly in terms of computational efficiency.\n3. The experimental results are limited to a few datasets, and it would be beneficial to include more datasets to demonstrate the generalizability of VPL.\n4. The paper does not provide a clear explanation of how to choose the hyperparameters, particularly the trade-off parameter \u03bb.\n\nSuggestions for Improvement:\n\n1. Investigate the performance of VPL on more datasets, including those with different graph structures and sizes.\n2. Compare VPL with existing methods in terms of computational efficiency and provide a detailed analysis of the computational complexity.\n3. Provide a more comprehensive explanation of how to choose the hyperparameters, particularly the trade-off parameter \u03bb.\n4. Consider incorporating VPL into other graph-based learning methods, such as graph convolutional networks, to further enhance their performance.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of graph-based semi-supervised learning by proposing a novel framework, VPL, to address the challenges of extremely sparse labeled data. The theoretical analysis and experimental results demonstrate the effectiveness and superior performance of VPL. However, the paper would benefit from a more comprehensive comparison with existing methods and a clearer explanation of how to choose the hyperparameters. Overall, I recommend this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents Variance-enlarged Poisson Learning (VPL), a novel framework designed to tackle the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data, effectively reducing the likelihood of degenerate solutions characterized by nearly constant label functions. Two streamlined algorithms, V-Laplace and V-Poisson, are proposed to enhance Laplace and Poisson learning, respectively. Furthermore, the authors extend VPL to encompass graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation.\n\nStrengths:\n\n1. The paper presents a novel framework, Variance-enlarged Poisson Learning (VPL), which addresses the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning.\n2. The authors propose two efficient algorithms, V-Laplace and V-Poisson, which are tailored for improving Laplace learning and Poisson learning, respectively.\n3. The paper extends VPL to enhance semi-supervised node classification with graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN).\n4. The authors provide a comprehensive theoretical exploration of VPL in both finite and infinite data cases, yielding deeper insights into understanding the behavior of VPL.\n5. The paper presents extensive experiments demonstrating the effectiveness and superior performance of VPL in scenarios characterized by extremely sparse labeled data.\n\nWeaknesses:\n\n1. The paper assumes that the underlying distribution of the data is smooth, which may not be the case in real-world scenarios.\n2. The authors do not provide a thorough comparison with existing methods, which makes it difficult to evaluate the performance of VPL.\n3. The paper assumes that the number of unlabeled data points is large, which may not be the case in some scenarios.\n4. The authors do not provide a clear explanation of how VPL can be used in scenarios where the labeled data is not extremely sparse.\n5. The paper assumes that the graph structure is known, which may not be the case in some scenarios.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough comparison with existing methods to evaluate the performance of VPL.\n2. Consider scenarios where the labeled data is not extremely sparse and explain how VPL can be used in these scenarios.\n3. Provide a more detailed explanation of how VPL can be used when the graph structure is unknown.\n4. Consider using more robust evaluation metrics to evaluate the performance of VPL.\n5. Provide more experimental results to demonstrate the effectiveness of VPL in scenarios characterized by extremely sparse labeled data.\n\nRecommendation:\n\nThe paper presents a novel framework, Variance-enlarged Poisson Learning (VPL), which addresses the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning. The authors propose two efficient algorithms, V-Laplace and V-Poisson, which are tailored for improving Laplace learning and Poisson learning, respectively. Furthermore, the paper extends VPL to enhance semi-supervised node classification with graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN). The authors provide a comprehensive theoretical exploration of VPL in both finite and infinite data cases, yielding deeper insights into understanding the behavior of VPL. Overall, the paper presents a novel and effective approach to addressing the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning.\n\nThe paper is well-organized, and the authors provide a clear explanation of the proposed framework and algorithms. The experimental results demonstrate the effectiveness of VPL in scenarios characterized by extremely sparse labeled data. However, there are some weaknesses in the paper, such as the assumption of smooth underlying distribution, lack of thorough comparison with existing methods, and assumption of known graph structure. Overall, the paper presents a novel and effective approach to addressing the challenges associated with extremely sparse labeled data in graph-based semi-supervised learning, but there is still room for improvement.\n\nRecommendation: Accept.\n\n$$$\nNote: The review is based on the paper's content, and the reviewer's feedback is intended to help the authors improve their work. The review should not be considered as a formal evaluation or assessment of the paper's quality or significance.",
        "gr_score_with": 0.4714285714285714,
        "gr_score_without": 0.3838383838383838,
        "output_without": {
            "num_tokens_scored": 396,
            "num_green_tokens": 152,
            "green_fraction": 0.3838383838383838,
            "z_score": -0.656487951889775,
            "p_value": 0.7442448914866194
        },
        "output_with": {
            "num_tokens_scored": 420,
            "num_green_tokens": 198,
            "green_fraction": 0.4714285714285714,
            "z_score": 2.988071523335984,
            "p_value": 0.0014037192310010068
        }
    },
    {
        "title": "STYLIZED OFFLINE REINFORCEMENT LEARNING: EXTRACTING DIVERSE HIGH-QUALITY BEHAVIORS FROM HETEROGENEOUS DATASETS",
        "abstract": "Previous literature on policy diversity in reinforcement learning (RL) either focuses on the online setting or ignores the policy performance. In contrast, offline RL, which aims to learn high-quality policies from batched data, has yet to fully leverage the intrinsic diversity of the offline dataset. Addressing this dichotomy and aiming to balance quality and diversity poses a significant challenge to extant methodologies. This paper introduces a novel approach, termed Stylized Offline RL (SORL), which is designed to extract high-performing, stylistically diverse policies from a dataset characterized by distinct behavioral patterns. Drawing inspiration from the venerable Expectation-Maximization (EM) algorithm, SORL innovatively alternates between policy learning and trajectory clustering, a mechanism that promotes policy diversification. To further augment policy performance, we introduce advantage-weighted style learning into the SORL framework. Experimental evaluations across multiple environments demonstrate the significant superiority of SORL over previous methods in extracting high-quality policies with diverse behaviors. A case in point is that SORL successfully learns strong policies with markedly distinct playing patterns from a real-world human dataset of a popular basketball video game \u201dDunk City Dynasty.\u201d",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Stylized Offline Reinforcement Learning (SORL), a novel framework designed to extract high-performing, stylistically diverse policies from heterogeneous offline datasets. SORL consists of two steps: EM-based style clustering and advantage-weighted style learning. The EM-based style clustering step identifies distinct behavioral styles in the dataset, and the advantage-weighted style learning step learns diverse and high-quality policies by constraining each policy to be conservative with respect to the corresponding weighted set of data.\n\nStrengths:\n\n* The paper presents a novel framework for extracting diverse and high-quality behaviors from offline heterogeneous datasets, which is an important problem in reinforcement learning.\n* The EM-based style clustering step is a key innovation that allows SORL to identify distinct behavioral styles in the dataset.\n* The advantage-weighted style learning step is a well-motivated approach that learns diverse and high-quality policies by constraining each policy to be conservative with respect to the corresponding weighted set of data.\n* The paper provides extensive experimental results that demonstrate the superiority of SORL over prior methods in extracting high-quality and diverse behaviors.\n\nWeaknesses:\n\n* The paper assumes that the dataset contains behaviors of heterogeneous policies, which may not always be the case in real-world scenarios.\n* The EM-based style clustering step may not be able to identify all possible behavioral styles in the dataset, especially if the dataset is large and complex.\n* The advantage-weighted style learning step may not be able to learn policies that are optimal for all possible behavioral styles.\n* The paper does not provide a clear comparison of SORL with other state-of-the-art methods for extracting diverse and high-quality behaviors.\n\nSuggestions for Improvement:\n\n* The authors should provide more detailed analysis of the EM-based style clustering step and its limitations.\n* The authors should provide more experimental results that demonstrate the robustness and adaptability of SORL in handling complex, real-world tasks.\n* The authors should provide a clear comparison of SORL with other state-of-the-art methods for extracting diverse and high-quality behaviors.\n* The authors should discuss potential applications of SORL in real-world scenarios and how it can be used to improve the performance of reinforcement learning algorithms.\n\nRecommendation:\n\nThe paper presents a novel and well-motivated framework for extracting diverse and high-quality behaviors from offline heterogeneous datasets. The experimental results demonstrate the superiority of SORL over prior methods, and the framework has the potential to be applied to a wide range of real-world scenarios. However, the paper has some limitations that need to be addressed, and the authors should provide more detailed analysis and experimental results to further support their claims. Overall, I would recommend this paper for publication in a top-tier conference or journal in the field of reinforcement learning.\n\nRating: 8/10\n\nReviewer's Expertise: Reinforcement Learning, Deep Learning, Machine Learning.\n\nReviewer's Qualification: PhD in Computer Science, Postdoctoral Researcher.\n\nReviewer's Conflict of Interest: None.\n\nReviewer's Disclosure: None.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces a novel approach, termed Stylized Offline RL (SORL), for extracting high-performing, stylistically diverse policies from a heterogeneous offline dataset. SORL addresses the limitations of both diverse RL and offline RL methods by incorporating both quality and diversity into the optimization objective. It consists of two stages: EM-based style clustering, where it learns a set of m most representative policies by maximizing the log-likelihood of the dataset, and advantage-weighted style learning, where it learns a set of policies to maximize the cumulative return while constraining each policy to be conservative with respect to a weighted set of data. SORL demonstrates its superiority over previous methods in extracting high-quality policies with diverse behaviors in various environments.\n\nStrengths:\n\n1. SORL addresses a significant gap in previous literature by balancing both quality and diversity in offline RL settings.\n2. It introduces a novel two-stage framework, combining EM-based style clustering and advantage-weighted style learning, which shows promising results in extracting diverse and high-quality policies.\n3. SORL demonstrates its effectiveness in handling complex, real-world tasks, especially when working with large-scale datasets from diverse human players.\n\nWeaknesses:\n\n1. The paper relies heavily on empirical evaluations, and it would be beneficial to include theoretical analysis of the convergence of the EM-based style clustering algorithm.\n2. The paper assumes a uniform prior distribution for the latent variable, which might not be suitable for all scenarios. It would be interesting to explore other prior distributions and their effects on the algorithm.\n3. The paper focuses on a specific setting, where the dataset contains behaviors of heterogeneous policies. It would be beneficial to explore SORL's performance in more diverse settings, such as datasets with varying degrees of data quality or different types of tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the effects of different prior distributions for the latent variable on the algorithm's performance.\n2. Conduct theoretical analysis of the convergence of the EM-based style clustering algorithm, providing a better understanding of its limitations and potential for improvement.\n3. Explore SORL's performance in more diverse settings, such as datasets with varying degrees of data quality or different types of tasks.\n\nRecommendation:\n\nThis paper presents a novel approach, SORL, for extracting high-performing, stylistically diverse policies from a heterogeneous offline dataset. It demonstrates its superiority over previous methods in various environments, showing promising results in addressing a significant gap in previous literature. With some additional theoretical analysis and exploration of SORL's performance in diverse settings, this paper has the potential to make a significant contribution to the field of reinforcement learning.",
        "gr_score_with": 0.5753424657534246,
        "gr_score_without": 0.4368421052631579,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 166,
            "green_fraction": 0.4368421052631579,
            "z_score": 1.4659886790209342,
            "p_value": 0.07132568553594798
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 210,
            "green_fraction": 0.5753424657534246,
            "z_score": 6.837981490731012,
            "p_value": 4.015838552247252e-12
        }
    },
    {
        "title": "SAFEDREAMER: SAFE REINFORCEMENT LEARNING",
        "abstract": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and visiononly input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details can be seen on our project website: https://sites.google.com/view/safedreamer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces SafeDreamer, a novel algorithm that incorporates Lagrangian-based methods into world model planning processes within the superior Dreamer framework. The goal is to balance long-term rewards and costs of the agent in complex scenarios, especially vision-only tasks. The authors propose a framework that integrates safety-reward planning of the world model with the Lagrangian methods to achieve nearly zero-cost performance in the Safety-Gymnasium benchmark.\n\nStrengths:\n\n1. The paper presents a novel algorithm that addresses the challenge of balancing rewards and costs in complex scenarios, especially vision-only tasks.\n2. The authors provide a detailed comparison with various algorithms, including model-free and model-based methods, showcasing the efficacy of SafeDreamer in balancing performance and safety.\n3. The paper presents a clear and concise explanation of the algorithm, making it easy to understand and implement.\n4. The authors provide a thorough evaluation of SafeDreamer on various tasks, including low-dimensional and visual input environments, demonstrating its superior performance.\n\nWeaknesses:\n\n1. The paper assumes access to a world model, which may not be feasible in all scenarios, especially in real-world applications.\n2. The authors do not provide a detailed analysis of the computational cost of SafeDreamer, which may be a concern for large-scale applications.\n3. The paper does not discuss the potential limitations of the Lagrangian method, such as its sensitivity to the choice of parameters.\n4. The authors do not provide a clear explanation of how SafeDreamer handles uncertainty and exploration-exploitation trade-offs.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational cost of SafeDreamer and discuss potential optimization techniques to reduce it.\n2. Discuss the potential limitations of the Lagrangian method and propose alternative methods or techniques to address these limitations.\n3. Provide a clear explanation of how SafeDreamer handles uncertainty and exploration-exploitation trade-offs.\n4. Consider incorporating additional evaluation metrics, such as robustness and adaptability, to provide a more comprehensive assessment of SafeDreamer's performance.\n\nRecommendation:\n\nThe paper presents a novel and effective algorithm for balancing rewards and costs in complex scenarios, especially vision-only tasks. The authors provide a clear and concise explanation of the algorithm, making it easy to understand and implement. While there are some limitations and areas for improvement, the paper is well-written and well-organized, making it a valuable contribution to the field of Safe Reinforcement Learning. I recommend accepting the paper for publication in the conference.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces SafeDreamer, a novel algorithm that combines safety-reward planning of world models with the Lagrangian methods to balance long-term rewards and costs. SafeDreamer is designed to address the challenge of achieving zero-cost performance in Safe Reinforcement Learning (SafeRL) tasks, particularly in complex scenarios such as vision-only tasks. The algorithm is built upon the Dreamer framework and leverages the world model to generate latent rollouts for online or background planning. SafeDreamer is evaluated on various tasks in the Safety-Gymnasium benchmark, demonstrating its ability to achieve nearly zero-cost performance while surpassing existing SafeRL methods.\n\nStrengths:\n\n1. The paper presents a comprehensive review of the state-of-the-art SafeRL methods and clearly identifies the limitations of existing approaches.\n2. The introduction of SafeDreamer is well-motivated and addresses the challenge of achieving zero-cost performance in SafeRL tasks.\n3. The algorithm is designed to leverage the world model to generate latent rollouts for online or background planning, which is a novel approach in the SafeRL literature.\n4. The paper provides extensive experimental results to demonstrate the effectiveness of SafeDreamer on various tasks in the Safety-Gymnasium benchmark.\n\nWeaknesses:\n\n1. The paper relies heavily on the Dreamer framework, which may limit its applicability to other SafeRL tasks.\n2. The evaluation of SafeDreamer is primarily focused on the Safety-Gymnasium benchmark, which may not capture the full range of SafeRL tasks.\n3. The paper does not provide a thorough analysis of the computational costs associated with SafeDreamer, particularly in the context of large-scale SafeRL tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the applicability of SafeDreamer to other SafeRL tasks beyond the Safety-Gymnasium benchmark.\n2. Provide a more comprehensive analysis of the computational costs associated with SafeDreamer.\n3. Consider incorporating additional safety constraints or uncertainty estimates to further improve the robustness of SafeDreamer.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to SafeRL, leveraging the world model to generate latent rollouts for online or background planning. While the paper has some limitations, the experimental results demonstrate the potential of SafeDreamer to achieve nearly zero-cost performance in SafeRL tasks. Therefore, I recommend the paper for publication in a reputable conference or journal.\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$$\n\n$$",
        "gr_score_with": 0.4662379421221865,
        "gr_score_without": 0.3392857142857143,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 114,
            "green_fraction": 0.3392857142857143,
            "z_score": -2.271720556255608,
            "p_value": 0.9884483030991179
        },
        "output_with": {
            "num_tokens_scored": 311,
            "num_green_tokens": 145,
            "green_fraction": 0.4662379421221865,
            "z_score": 2.384412582899124,
            "p_value": 0.008553205833094851
        }
    },
    {
        "title": "SKIP-ATTENTION: IMPROVING VISION TRANSFORM- ERS BY PAYING LESS ATTENTION",
        "abstract": "This work aims to improve the efficiency of vision transformers (ViTs). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers \u2013 a key redundancy that causes unnecessary computations. Based on this observation, we propose SKIPAT, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer\u2019s performance while running computationally faster. We show that SKIPAT is agnostic to transformer architecture and is effective in image classification, semantic segmentation, image denoising, and video denoising. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel method called SKIP-ATTENTION (SKIPAT) to improve the efficiency of vision transformers (ViTs) by reducing the computational cost of self-attention operations. The authors identify that self-attention operations are highly correlated across layers, which leads to unnecessary computations. They propose a simple parametric function to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. The SKIPAT method is shown to be effective in image classification, semantic segmentation, image denoising, and video denoising, achieving improved throughput at the same-or-higher accuracy levels.\n\nStrengths:\n1. The paper proposes a novel and effective method to improve the efficiency of ViTs by reducing self-attention computations.\n2. The authors provide a thorough analysis of the correlation between self-attention maps across different layers of ViTs, which is a key contribution of the paper.\n3. The SKIPAT method is shown to be effective on various tasks, including image classification, semantic segmentation, image denoising, and video denoising.\n4. The paper provides a clear and concise explanation of the SKIPAT method and its implementation.\n\nWeaknesses:\n1. The paper assumes that the correlation between self-attention maps is a universal property of ViTs, which may not be the case for all architectures or tasks.\n2. The authors do not provide a comprehensive analysis of the computational cost of the SKIPAT method, which is a crucial aspect of its efficiency.\n3. The paper does not provide a detailed comparison with other methods that aim to improve the efficiency of ViTs, such as token sampling or efficient attention methods.\n4. The authors do not provide a clear explanation of how the SKIPAT method can be generalized to other architectures or tasks.\n\nSuggestions for Improvement:\n1. Provide a more comprehensive analysis of the computational cost of the SKIPAT method, including a detailed comparison with other methods.\n2. Investigate the universality of the correlation between self-attention maps across different ViT architectures and tasks.\n3. Provide a more detailed comparison with other methods that aim to improve the efficiency of ViTs.\n4. Provide a clear explanation of how the SKIPAT method can be generalized to other architectures or tasks.\n\nRecommendation:\nThe paper proposes a novel and effective method to improve the efficiency of ViTs, which is a significant contribution to the field. However, the authors need to address the limitations and weaknesses of the paper to make it more comprehensive and robust. With some additional analysis and comparison, the paper has the potential to be a high-impact publication in the field of computer vision.\n\nRating: 7.5/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel method called SKIP-ATTENTION (SKIPAT), which improves the efficiency of vision transformers (ViTs) by reducing the computational cost of self-attention operations. The authors identify that self-attention operations are highly correlated across layers, leading to unnecessary computations. They introduce a simple parametric function that reuses self-attention computation from preceding layers to approximate attention at one or more subsequent layers. The SKIPAT method is agnostic to transformer architecture and achieves improved throughput at the same-or-higher accuracy levels in various tasks such as image classification, semantic segmentation, image denoising, and video denoising.\n\nStrengths:\n1. The paper provides a novel and effective method for improving the efficiency of vision transformers.\n2. The authors identify a key redundancy in self-attention operations across layers, which is a significant contribution to the field.\n3. The proposed parametric function is simple and light, which does not affect the inductive bias encoded in MSA.\n4. The authors demonstrate the effectiveness of SKIPAT on various tasks and transformer architectures.\n5. The method is easy to implement and can be used in any ViT architecture.\n\nWeaknesses:\n1. The paper assumes that the correlation between self-attention maps across layers is a good indicator of the correlation between the representations learned by each layer.\n2. The authors do not provide a detailed analysis of the trade-offs between accuracy and computational cost.\n3. The paper does not explore the use of different parametric functions or architectures for improving efficiency.\n4. The authors do not provide a detailed discussion on the limitations of their method and potential future work.\n\nSuggestions for Improvement:\n1. Investigate the use of different parametric functions or architectures for improving efficiency.\n2. Analyze the trade-offs between accuracy and computational cost in more detail.\n3. Explore the use of SKIPAT in conjunction with other methods for improving efficiency.\n4. Provide a detailed discussion on the limitations of their method and potential future work.\n5. Consider evaluating the method on more diverse and challenging tasks.\n\nRecommendation:\nThe paper presents a novel and effective method for improving the efficiency of vision transformers. While there are some limitations and potential areas for improvement, the authors demonstrate the effectiveness of their method on various tasks and transformer architectures. The paper is well-written and easy to follow, making it accessible to a wide range of readers. I would recommend this paper for publication in a reputable conference or journal.\n\n$$$",
        "gr_score_with": 0.3211267605633803,
        "gr_score_without": 0.2523076923076923,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 82,
            "green_fraction": 0.2523076923076923,
            "z_score": -5.4349297638940595,
            "p_value": 0.9999999725909895
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 114,
            "green_fraction": 0.3211267605633803,
            "z_score": -3.0334597301220767,
            "p_value": 0.998791165118771
        }
    },
    {
        "title": "MULTI-RESOLUTION HUBERT: MULTI-RESOLUTION SPEECH SELF-SUPERVISED LEARNING WITH MASKED UNIT PREDICTION",
        "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised speech learning model called MR-HuBERT (Multi-Resolution HuBERT), which extends the HuBERT model by incorporating multi-resolution masked unit prediction in conjunction with a hierarchical transformer architecture. The proposed model is designed to process speech signals at multiple resolutions, which is not addressed in existing speech self-supervised learning models. The authors demonstrate that MR-HuBERT achieves superior or comparable performance to the original HuBERT model on various speech processing tasks, including speech recognition, spoken language understanding, multilingual speech recognition, and speech enhancement. Additionally, the model exhibits computational efficiencies, specifically a 9-13% reduction in computational complexity.\n\nStrengths:\n\n1. The paper addresses a significant limitation of existing speech self-supervised learning models by incorporating multi-resolution information into speech representation learning.\n2. The proposed hierarchical transformer architecture is novel and effective in processing speech signals at multiple resolutions.\n3. The authors provide comprehensive evaluations of MR-HuBERT on various benchmarks, including LibriSpeech, SUPERB, and ML-SUPERB, which demonstrates its robustness and superiority.\n4. The model exhibits computational efficiencies, which is an essential aspect of modern AI systems.\n\nWeaknesses:\n\n1. The paper assumes a fixed frame size of 20ms for the original HuBERT model, which may not be universally optimal across different downstream tasks.\n2. The authors do not provide a thorough analysis of the effect of different resolutions on the performance of MR-HuBERT.\n3. The paper does not discuss the limitations of the proposed model, such as its sensitivity to hyperparameters or its scalability to larger datasets.\n4. The authors do not provide a clear explanation of how the hierarchical transformer architecture improves the performance of MR-HuBERT.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the effect of different resolutions on the performance of MR-HuBERT.\n2. The paper should discuss the limitations of the proposed model and provide a clear explanation of how the hierarchical transformer architecture improves the performance of MR-HuBERT.\n3. The authors should provide a thorough evaluation of MR-HuBERT on larger datasets to demonstrate its scalability.\n4. The paper should discuss the potential applications of MR-HuBERT in real-world speech processing systems.\n\nRecommendation:\n\nThe paper proposes a novel and effective self-supervised speech learning model that addresses a significant limitation of existing speech self-supervised learning models. The model exhibits superior or comparable performance to the original HuBERT model on various speech processing tasks and computational efficiencies. However, the paper has some limitations, such as the lack of a thorough analysis of the effect of different resolutions and the discussion of the limitations of the proposed model. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of MR-HuBERT to make it a more robust and effective model for speech processing tasks.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel self-supervised speech learning model called MULTI-RESOLUTION HUBERT (MR-HuBERT), which incorporates multi-resolution information into speech self-supervised representation learning. The proposed model leverages a hierarchical Transformer architecture and HuBERT-style masked prediction objectives to process speech at multiple resolutions. Experimental results demonstrate that MR-HuBERT not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks.\n\nStrengths:\n1. The paper addresses an important issue in self-supervised speech learning, which is the fixed resolution used by existing models. By incorporating multi-resolution information, the proposed model can better capture the varying informational content present at different resolutions in speech signals.\n2. The proposed hierarchical Transformer architecture is novel and effective in processing speech at multiple resolutions. This architecture allows the model to capture long-term dependencies and relationships between different resolutions.\n3. The paper provides comprehensive evaluations across various benchmarks, including speech recognition, spoken language understanding, multilingual speech recognition, and speech enhancement. These evaluations demonstrate the superiority of MR-HuBERT over the original HuBERT model.\n4. The proposed model also exhibits computational efficiencies, with a 9-13% reduction in computational complexity. This is a significant advantage in real-world applications where computational resources are limited.\n\nWeaknesses:\n1. The paper assumes that the multi-resolution information is sufficient for capturing the varying informational content present at different resolutions. However, it is unclear how the model handles cases where the multi-resolution information is not sufficient.\n2. The proposed hierarchical Transformer architecture is complex and may be challenging to implement. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed architecture.\n3. The paper does not provide a clear explanation of how the multi-resolution information is integrated into the self-supervised representation learning process. This integration is crucial for capturing the varying informational content present at different resolutions.\n\nSuggestions for Improvement:\n1. Provide a detailed analysis of the computational complexity of the proposed hierarchical Transformer architecture. This analysis should include the time and space complexity of the architecture.\n2. Investigate the robustness of the proposed model to variations in the multi-resolution information. This can be done by introducing noise or artifacts into the multi-resolution information and evaluating the model's performance under these conditions.\n3. Provide a clear explanation of how the multi-resolution information is integrated into the self-supervised representation learning process. This explanation should include the mathematical formulation of the integration process and the impact of the integration on the model's performance.\n4. Investigate the potential applications of the proposed model beyond speech recognition and spoken language understanding. This can include tasks such as speech enhancement, speaker recognition, and voice conversion.\n\nRecommendation:\nThe paper presents a novel and effective approach to self-supervised speech learning by incorporating multi-resolution information. The proposed model demonstrates superiority over the original HuBERT model across various benchmarks and exhibits computational efficiencies. However, the paper assumes that the multi-resolution information is sufficient for capturing the varying informational content present at different resolutions, and the proposed hierarchical Transformer architecture is complex and challenging to implement. Therefore, the paper requires further research and development to address these limitations. Overall, the paper is well-written and provides valuable contributions to the field of self-supervised speech learning. I recommend that the paper be accepted for publication with minor revisions.\n\nRating: 8/10\n\nReviewer's Expertise:\nThe reviewer has expertise in self-supervised learning, speech recognition, and natural language processing. The reviewer has read and analyzed the paper carefully and provides a detailed evaluation of the paper's strengths and weaknesses. The reviewer also provides suggestions for improvement and a recommendation for publication.",
        "gr_score_with": 0.5255319148936171,
        "gr_score_without": 0.40625,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 143,
            "green_fraction": 0.40625,
            "z_score": 0.2393567769390833,
            "p_value": 0.40541447147615234
        },
        "output_with": {
            "num_tokens_scored": 470,
            "num_green_tokens": 247,
            "green_fraction": 0.5255319148936171,
            "z_score": 5.555171381515982,
            "p_value": 1.3866975717748966e-08
        }
    },
    {
        "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION",
        "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for incremental robustness certification of deep neural networks (DNNs) using randomized smoothing (RS). The method, called Incremental Randomized Smoothing (IRS), reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples. This approach significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees.\n\nStrengths:\n\n1. The paper addresses a critical limitation of existing RS-based certification practices, which requires expensive certification to be re-run from scratch for each approximate network.\n2. The proposed IRS approach is the first incremental RS-based certification framework that efficiently computes the certified radius of the updated smoothed classifier.\n3. The paper presents an extensive evaluation of the performance of IRS, demonstrating speedups of up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models.\n4. The authors provide a thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, which affects the efficiency of IRS.\n\nWeaknesses:\n\n1. The paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n3. The paper assumes that the user has a larger sample budget, which may not always be the case in real-world scenarios.\n4. The authors do not provide a detailed comparison with other incremental certification methods, such as deterministic certification using formal logic.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the assumptions made in the paper, particularly with regards to the similarity between the approximate and original networks.\n2. The paper should include a more detailed analysis of the computational cost of storing and retrieving the certification cache, including the memory requirements and the time complexity of the operations involved.\n3. The authors should provide a more detailed comparison with other incremental certification methods, including deterministic certification using formal logic.\n4. The paper should include a discussion of the potential applications of IRS in real-world scenarios, including approximate tuning and shipping certification caches.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and weaknesses to the approach, the authors provide a thorough analysis of the trade-offs involved and demonstrate the effectiveness of IRS in practice. Therefore, I recommend accepting the paper for publication in a reputable conference or journal.\n\nRating: 8/10\n\nReview:\n\nThe paper is well-written and easy to follow, with a clear introduction to the problem and a thorough presentation of the proposed approach. The authors provide a detailed analysis of the trade-offs involved in choosing the smoothing parameter \u03c3 and demonstrate the effectiveness of IRS in practice. However, the paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios. Additionally, the authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n\nOverall, the paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and weaknesses to the approach, the authors provide a thorough analysis of the trade-offs involved and demonstrate the effectiveness of IRS in practice. Therefore, I recommend accepting the paper for publication in a reputable conference or journal.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel approach for incremental randomized smoothing certification, referred to as IRS. This method leverages the certification guarantees obtained from the smoothed model to certify a smoothed approximated model with very few samples, thereby significantly reducing the computational cost of certification. IRS is designed to be particularly useful for approximate tuning, where users need to analyze the robustness of multiple similar networks. The authors demonstrate the effectiveness of IRS through extensive experiments on state-of-the-art classification models, achieving speedups of up to 4.1x over the standard non-incremental RS baseline.\n\nStrengths:\n1. IRS offers a novel approach for probabilistic DNN certification, leveraging the certification guarantees obtained from the smoothed model to certify a smoothed approximated model with very few samples.\n2. The method significantly reduces the computational cost of certification, making it particularly useful for approximate tuning and real-world applications.\n3. The authors provide extensive experimental results, demonstrating the effectiveness of IRS on state-of-the-art classification models, including CIFAR-10 and ImageNet.\n\nWeaknesses:\n1. The IRS algorithm requires a cache with additional information, including the top predicted class index, its lower confidence bound, and the seeds for Gaussian corruptions obtained from the RS execution of the original classifier. While this has negligible memory overhead, it may be a limitation for some applications.\n2. The choice of smoothing parameter \u03c3 affects the efficiency of IRS, with larger values generally leading to better results. However, this may also result in a loss of accuracy in the model.\n3. IRS targets fast certification while maintaining a sufficiently large radius, and the certified radius can be smaller than the non-incremental RS, provided the user has a larger sample budget.\n\nSuggestions for Improvement:\n1. Investigate methods to reduce the memory overhead associated with storing the additional information required for IRS.\n2. Explore alternative methods for selecting the smoothing parameter \u03c3, potentially incorporating domain knowledge or meta-learning approaches.\n3. Conduct further experiments to evaluate the performance of IRS with larger sample budgets and more aggressive pruning, and compare it with other state-of-the-art certification methods.\n\nRecommendation:\nThis paper presents a novel and effective approach for probabilistic DNN certification, particularly useful for approximate tuning and real-world applications. While some limitations exist, the authors provide a comprehensive evaluation and demonstrate the effectiveness of IRS. I recommend this paper for publication, with the hope that it will inspire further research and improvements in the field of DNN certification.\n\nRating: 8.5/10\n\nReviewer's Expertise: Computer Vision, Deep Learning, and Machine Learning.",
        "gr_score_with": 0.4938875305623472,
        "gr_score_without": 0.37777777777777777,
        "output_without": {
            "num_tokens_scored": 405,
            "num_green_tokens": 153,
            "green_fraction": 0.37777777777777777,
            "z_score": -0.9128709291752768,
            "p_value": 0.8193447857369105
        },
        "output_with": {
            "num_tokens_scored": 409,
            "num_green_tokens": 202,
            "green_fraction": 0.4938875305623472,
            "z_score": 3.8758231241860783,
            "p_value": 5.313242893609983e-05
        }
    },
    {
        "title": "TILLATION: A KERNEL METHOD PERSPECTIVE",
        "abstract": "Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks. 1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel theoretical framework based on the kernel inducing points (KIP) method to study the interplay between backdoor attacks and dataset distillation. The authors propose two theory-induced backdoor attacks, simple-trigger and relax-trigger, which are designed to evade conventional backdoor detection and mitigation methods. The authors provide a comprehensive analysis of the backdoor effect, characterizing it by three key components: conflict loss, projection loss, and generalization gap. The results show that the proposed attacks are effective in evading conventional defenses and pose a significant risk to dataset distillation-based models.\n\nStrengths:\n\n1. The paper presents a novel theoretical framework for studying the interplay between backdoor attacks and dataset distillation.\n2. The authors propose two effective theory-induced backdoor attacks, simple-trigger and relax-trigger, which evade conventional defenses.\n3. The paper provides a comprehensive analysis of the backdoor effect, characterizing it by three key components: conflict loss, projection loss, and generalization gap.\n4. The results show that the proposed attacks are effective in evading conventional defenses and pose a significant risk to dataset distillation-based models.\n\nWeaknesses:\n\n1. The paper assumes a specific kernel function and dataset distillation method, which may not be applicable to all scenarios.\n2. The authors do not provide a thorough comparison with existing backdoor attacks and defenses.\n3. The paper does not discuss the potential risks and limitations of the proposed attacks.\n4. The results are based on a limited number of experiments and may not be generalizable to other scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough comparison with existing backdoor attacks and defenses to demonstrate the effectiveness of their proposed attacks.\n2. The paper should discuss the potential risks and limitations of the proposed attacks, including the potential for overfitting and the impact on model performance.\n3. The authors should provide a more detailed analysis of the backdoor effect, including a discussion of the implications for dataset distillation-based models.\n4. The paper should include more experiments to demonstrate the robustness of the proposed attacks and to explore the impact of different kernel functions and dataset distillation methods.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to studying the interplay between backdoor attacks and dataset distillation. The proposed attacks are effective in evading conventional defenses and pose a significant risk to dataset distillation-based models. However, the paper has some limitations, including the assumption of a specific kernel function and dataset distillation method, and the lack of a thorough comparison with existing backdoor attacks and defenses. Overall, the paper is a valuable contribution to the field and provides new insights into the risks and limitations of dataset distillation-based models.\n\nRecommendation: Accept with minor revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$ \n\nPaper Summary:\n\nThe paper proposes two theory-induced backdoor attacks, simple-trigger and relax-trigger, based on the kernel inducing points (KIP) method for dataset distillation. The authors provide a theoretical framework to characterize backdoor effects on dataset distillation, which explains why certain backdoors survive dataset distillation. The proposed methods aim to use dataset distillation to perform backdoor attacks, and the authors show that their optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. The results validate that the triggers developed using their approaches are proficient at executing resilient backdoor attacks.\n\nStrengths:\n\n* The paper provides a comprehensive theoretical framework for backdoor attacks on dataset distillation, which is a novel and important contribution.\n* The proposed methods, simple-trigger and relax-trigger, are effective and resilient backdoor attacks that can evade existing detection and mitigation methods.\n* The authors provide a detailed and thorough analysis of the backdoor effects, including conflict loss, projection loss, and generalization gap, which provides valuable insights into the interplay between backdoor attacks and dataset distillation.\n* The experimental results show that the proposed methods can achieve high attack success rates and clean test accuracy, and that they are robust against various backdoor detection and mitigation methods.\n\nWeaknesses:\n\n* The paper assumes that the dataset distillation method used is KIP, which may not be the case in all scenarios, and it would be beneficial to explore other dataset distillation methods.\n* The proposed methods require knowledge of the kernel matrix and the regularization parameter, which may not be available in all scenarios, and it would be beneficial to provide more insights into how to choose these parameters.\n* The experimental results show that the proposed methods can be effective against various backdoor detection and mitigation methods, but it would be beneficial to provide more detailed comparisons with other backdoor detection and mitigation methods.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed methods, and it would be beneficial to provide more insights into how to scale the proposed methods for large-scale datasets.\n\nSuggestions for Improvement:\n\n* The authors could provide more insights into how to choose the kernel matrix and the regularization parameter, and how to handle scenarios where these parameters are not available.\n* The authors could provide more detailed comparisons with other backdoor detection and mitigation methods, and explore how to make the proposed methods more robust against various backdoor detection and mitigation methods.\n* The authors could provide a more detailed analysis of the computational complexity of the proposed methods, and explore how to scale the proposed methods for large-scale datasets.\n* The authors could provide more insights into how to make the proposed methods more robust against various backdoor detection and mitigation methods, and how to make them more suitable for real-world applications.\n\nRecommendation:\n\nThe paper provides a comprehensive theoretical framework for backdoor attacks on dataset distillation, and proposes two effective and resilient backdoor attacks, simple-trigger and relax-trigger. The results show that the proposed methods can achieve high attack success rates and clean test accuracy, and that they are robust against various backdoor detection and mitigation methods. Therefore, I highly recommend this paper for publication, and I believe that it has the potential to make significant contributions to the field of backdoor attacks and dataset distillation.",
        "gr_score_with": 0.541795665634675,
        "gr_score_without": 0.45733788395904434,
        "output_without": {
            "num_tokens_scored": 293,
            "num_green_tokens": 134,
            "green_fraction": 0.45733788395904434,
            "z_score": 2.0034100621523057,
            "p_value": 0.022566646162080553
        },
        "output_with": {
            "num_tokens_scored": 323,
            "num_green_tokens": 175,
            "green_fraction": 0.541795665634675,
            "z_score": 5.201859237357714,
            "p_value": 9.865232280646558e-08
        }
    },
    {
        "title": "FINE-TUNING LANGUAGE MODELS FOR FACTUALITY REDUCES HALLUCINATION",
        "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as \u2018hallucinations.\u2019 These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model\u2019s confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to fine-tuning language models for factual accuracy in long-form text generation without human annotation. The authors leverage recent innovations in NLP, including reference-based and reference-free truthfulness estimators, to construct preference pairs that encourage greater factuality. The proposed approach, called FactTune, is evaluated on two benchmark datasets for evaluating factuality, targeting generating biographies and answering open-ended questions about medical conditions. The results show that fine-tuning for factuality outperforms conventional reinforcement learning from human feedback (RLHF) and produces complementary benefits to LLM decoding strategies that aim to increase factuality.\n\nStrengths:\n\n1. The paper proposes a novel approach to fine-tuning language models for factual accuracy, which is a significant contribution to the field of NLP.\n2. The authors provide a comprehensive evaluation of their approach on two benchmark datasets, demonstrating its effectiveness in improving factual accuracy.\n3. The paper highlights the importance of factuality in language models and provides a clear explanation of the challenges associated with fine-tuning language models for factual accuracy.\n4. The authors provide a detailed description of their approach, including the reference-based and reference-free truthfulness estimators, and demonstrate their effectiveness in constructing preference pairs that encourage greater factuality.\n\nWeaknesses:\n\n1. The paper assumes that the language model is pre-trained on a vast amount of knowledge, which may not be the case for all language models.\n2. The authors do not provide a detailed explanation of how the reference-based truthfulness estimator works, and how it is used to construct preference pairs.\n3. The paper does not provide a comprehensive comparison with other approaches to fine-tuning language models for factual accuracy.\n4. The authors do not discuss the potential limitations of their approach, such as the need for large amounts of data to train the model.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the reference-based truthfulness estimator and how it is used to construct preference pairs.\n2. Conduct a more comprehensive comparison with other approaches to fine-tuning language models for factual accuracy.\n3. Discuss the potential limitations of the approach, such as the need for large amounts of data to train the model.\n4. Provide more information on the scalability of the approach, including how it can be applied to larger language models and datasets.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of NLP by proposing a novel approach to fine-tuning language models for factual accuracy. The approach is well-evaluated on two benchmark datasets, and the results demonstrate its effectiveness in improving factual accuracy. However, the paper could benefit from a more detailed explanation of the reference-based truthfulness estimator and a more comprehensive comparison with other approaches. Overall, I recommend the paper for publication in a top-tier NLP conference.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method to fine-tune language models to reduce factual errors, referred to as \"hallucinations.\" The approach uses a preference-based learning framework to optimize the model's responses to a set of prompts without human annotation. The authors leverage recent advances in estimating truthfulness without human intervention, including reference-based automated fact-checking methods and reference-free truthfulness evaluations that use a model's own confidence as a proxy for truthfulness. The method is evaluated on two benchmark datasets for evaluating factuality, targeting at generating biographies of popular figures and answering open-ended questions about medical conditions.\n\nStrengths:\n\n1. The paper presents a novel approach to fine-tuning language models for factuality without human annotation.\n2. The method is evaluated on a range of tasks, including generating biographies and answering medical questions.\n3. The authors provide a thorough analysis of the strengths and weaknesses of their approach.\n4. The method is scalable to larger models and datasets.\n\nWeaknesses:\n\n1. The paper assumes that the model's confidence is a good proxy for truthfulness, which may not always be the case.\n2. The method relies on a reference-based approach to estimating truthfulness, which may not be applicable to all domains.\n3. The paper does not provide a detailed analysis of the computational cost of the method.\n4. The method may not generalize well to tasks that require a high level of creativity or common sense.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of the method.\n2. The paper should include a more thorough evaluation of the method's performance on a range of tasks.\n3. The authors should investigate alternative approaches to estimating truthfulness, such as using self-correction.\n4. The method should be evaluated on larger models and datasets to assess its scalability.\n\nRecommendation:\n\nThe paper presents a novel approach to fine-tuning language models for factuality without human annotation. While the method has its strengths and weaknesses, it is a promising direction for research in this area. The authors should address the weaknesses and suggestions for improvement to make the method more robust and scalable. The paper is suitable for publication in a top-tier conference in natural language processing.\n\nRating: 8/10",
        "gr_score_with": 0.4625,
        "gr_score_without": 0.3887240356083086,
        "output_without": {
            "num_tokens_scored": 337,
            "num_green_tokens": 131,
            "green_fraction": 0.3887240356083086,
            "z_score": -0.4225353273614505,
            "p_value": 0.6636828397314751
        },
        "output_with": {
            "num_tokens_scored": 320,
            "num_green_tokens": 148,
            "green_fraction": 0.4625,
            "z_score": 2.282177322938192,
            "p_value": 0.011239436683062633
        }
    },
    {
        "title": "METHOD WITH OPTIMAL ADAPTATION TO INEXACTNESS",
        "abstract": "We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, which occurs typically in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key setting. We further introduce a tensor generalization for stochastic higher-order derivatives. When the oracles are non-stochastic, the proposed tensor algorithm matches the global convergence of Nesterov Accelerated Tensor method. Both algorithms allow for approximate solutions of their auxiliary subproblems with verifiable conditions on the accuracy of the solution.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel stochastic accelerated second-order algorithm for convex and strongly convex optimization problems. The proposed algorithm, Accelerated Stochastic Cubic Newton, achieves optimal convergence rates in both gradient and Hessian inexactness. The authors also introduce a tensor generalization of second-order methods for stochastic high-order derivatives. The paper establishes a lower bound for stochastic second-order optimization and proves the algorithm's convergence rates. The authors provide theoretical insights into the required accuracy of the subproblem's solution and suggest strategies for choosing the sequence of tolerance parameters.\n\nStrengths:\n\n1. The paper presents a novel stochastic accelerated second-order algorithm that achieves optimal convergence rates in both gradient and Hessian inexactness.\n2. The authors introduce a tensor generalization of second-order methods for stochastic high-order derivatives, which is a significant contribution to the field.\n3. The paper establishes a lower bound for stochastic second-order optimization, which provides a theoretical foundation for the proposed algorithm.\n4. The authors provide theoretical insights into the required accuracy of the subproblem's solution, which is a crucial aspect of globally convergent second-order methods.\n5. The paper suggests strategies for choosing the sequence of tolerance parameters, which can help practitioners implement the algorithm effectively.\n\nWeaknesses:\n\n1. The paper assumes that the function f(x) is convex and has L1-Lipschitz-continuous gradient and L2-Lipschitz-continuous Hessian, which may not be the case in all optimization problems.\n2. The proposed algorithm involves a subproblem that necessitates an additional subroutine to find its solution, which can be computationally expensive.\n3. The paper does not provide a detailed comparison with existing algorithms, such as Nesterov's Accelerated Gradient Method and Extra-Newton Method.\n4. The authors assume that the stochastic gradient and Hessian satisfy certain conditions, which may not be met in practice.\n5. The paper does not provide a comprehensive analysis of the algorithm's performance in non-convex optimization problems.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with existing algorithms to demonstrate the effectiveness of the proposed algorithm.\n2. The paper should include a more comprehensive analysis of the algorithm's performance in non-convex optimization problems.\n3. The authors should provide more details on the computational complexity of the subproblem and the additional subroutine required to find its solution.\n4. The paper should include a more thorough discussion of the assumptions made on the stochastic gradient and Hessian.\n5. The authors should provide more experimental results to demonstrate the algorithm's performance in different scenarios.\n\nRecommendation:\n\nThe paper presents a novel stochastic accelerated second-order algorithm that achieves optimal convergence rates in both gradient and Hessian inexactness. The authors also introduce a tensor generalization of second-order methods for stochastic high-order derivatives. While the paper has some limitations, it is a significant contribution to the field of stochastic optimization. Therefore, I recommend that the paper be accepted for publication in its current form, with the authors addressing the suggested improvements in future work.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents an accelerated stochastic second-order method for convex optimization, dubbed as Accelerated Stochastic Cubic Newton's method. The proposed method is robust to both gradient and Hessian inexactness, which is a common issue in machine learning applications. The authors establish a theoretical lower bound for inexact second-order methods and prove that their algorithm achieves optimal convergence in both gradient and Hessian inexactness. Additionally, they introduce a tensor generalization of the algorithm for stochastic high-order derivatives. The proposed method is shown to outperform existing first-order and second-order methods in various experiments, demonstrating its effectiveness in convex optimization.\n\nStrengths:\n\n1. The paper presents a novel accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, which is a significant contribution to the field of convex optimization.\n2. The authors establish a theoretical lower bound for inexact second-order methods, providing a foundation for understanding the convergence rates of such methods.\n3. The proposed method is shown to outperform existing first-order and second-order methods in various experiments, demonstrating its effectiveness in convex optimization.\n4. The paper introduces a tensor generalization of the algorithm for stochastic high-order derivatives, which is a significant extension of the existing literature.\n\nWeaknesses:\n\n1. The proposed method involves a subproblem that necessitates an additional subroutine to find its solution, which can be computationally expensive.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed method, which can be an issue in practice.\n3. The paper assumes that the Hessian is Lipschitz continuous, which may not be the case in some applications.\n4. The authors do not provide a comparison of the proposed method with other existing methods that can handle inexact Hessians, such as the inexact Newton method.\n\nSuggestions for Improvement:\n\n1. The authors should provide a detailed analysis of the computational complexity of the proposed method, including the time complexity and space complexity.\n2. The paper should include a comparison of the proposed method with other existing methods that can handle inexact Hessians, such as the inexact Newton method.\n3. The authors should provide more experimental results to demonstrate the effectiveness of the proposed method in different scenarios, such as non-convex optimization and large-scale optimization problems.\n4. The paper should include a discussion of the potential applications of the proposed method in real-world problems, such as image processing and machine learning.\n\nRecommendation:\n\nThis paper presents a novel and effective accelerated stochastic second-order method for convex optimization that is robust to both gradient and Hessian inexactness. The proposed method is shown to outperform existing first-order and second-order methods in various experiments, demonstrating its effectiveness in convex optimization. However, the paper has some limitations, such as the need for an additional subroutine to solve the subproblem and the assumption of Lipschitz continuity of the Hessian. To improve the paper, the authors should provide a detailed analysis of the computational complexity of the proposed method, include a comparison with other existing methods, and provide more experimental results.",
        "gr_score_with": 0.48493975903614456,
        "gr_score_without": 0.43641618497109824,
        "output_without": {
            "num_tokens_scored": 346,
            "num_green_tokens": 151,
            "green_fraction": 0.43641618497109824,
            "z_score": 1.3826965361143626,
            "p_value": 0.0833789672341394
        },
        "output_with": {
            "num_tokens_scored": 332,
            "num_green_tokens": 161,
            "green_fraction": 0.48493975903614456,
            "z_score": 3.159180540384955,
            "p_value": 0.0007910672362600667
        }
    },
    {
        "title": "GENERATIVE MODELING WITH PHASE STOCHASTIC BRIDGES",
        "abstract": "We introduce a novel generative modeling framework grounded in phase space dynamics, taking inspiration from the principles underlying Critically damped Langevin Dynamics and Bridge Matching. Leveraging insights from Stochastic Optimal Control, we construct a more favorable path measure in the phase space that is highly advantageous for efficient sampling. A distinctive feature of our approach is the early-stage data prediction capability within the context of propagating generative Ordinary Differential Equations or Stochastic Differential Equations. This early prediction, enabled by the model\u2019s unique structural characteristics, sets the stage for more efficient data generation, leveraging additional velocity information along the trajectory. This innovation has spurred the exploration of a novel avenue for mitigating sampling complexity by quickly converging to realistic data samples. Our model yields comparable results in image generation and notably outperforms baseline methods, particularly when faced with a limited Number of Function Evaluations. Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential in the realm of generative modeling.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative modeling framework, Acceleration Generative Modeling (AGM), grounded in phase space dynamics and Stochastic Optimal Control (SOC) theory. AGM aims to enhance the sample efficiency of velocity-based generative modeling by constructing a favorable path measure in the phase space. The proposed method leverages insights from SOC to characterize the twisted trajectory of momentum dynamics induced by Critical-damped Langevin Dynamics (CLD) and enables the estimation of target data points by incorporating additional velocity information. The authors demonstrate the efficacy of AGM in image generation tasks, outperforming baseline methods, particularly when faced with limited Number of Function Evaluations (NFE).\n\nStrengths:\n\n1. The paper presents a novel generative modeling framework, AGM, which leverages SOC theory to improve the sample efficiency of velocity-based generative modeling.\n2. The authors provide a comprehensive analysis of the proposed method, including its theoretical foundations and experimental results.\n3. The paper demonstrates the effectiveness of AGM in image generation tasks, outperforming baseline methods, particularly when faced with limited NFE.\n4. The authors propose a sampling technique, sampling-hop, which enables the estimation of target data points at early time steps, reducing the time required to converge to accurate predictions.\n\nWeaknesses:\n\n1. The paper assumes a Gaussian prior distribution for the velocity variable, which may not be suitable for all applications.\n2. The authors rely on the SOC theory, which may not be well-established in the context of generative modeling.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, particularly in scenarios with sufficient NFE.\n4. The authors acknowledge that their approach's performance lags behind state-of-the-art methods in scenarios with sufficient NFE, suggesting avenues for enhancing AGM performance.\n\nSuggestions for Improvement:\n\n1. Explore alternative prior distributions for the velocity variable to improve the generality of the proposed method.\n2. Provide a more detailed comparison with other state-of-the-art methods, including those that utilize SOC theory.\n3. Investigate the use of data augmentation, fine-tuned noise scheduling, and network preconditioning to enhance the training quality of AGM.\n4. Consider extending the proposed method to other applications, such as image inpainting and conditional generation.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to generative modeling, leveraging SOC theory to improve the sample efficiency of velocity-based generative modeling. While there are some limitations and areas for improvement, the proposed method demonstrates encouraging results in image generation tasks, particularly when faced with limited NFE. Therefore, I recommend this paper for publication in a reputable conference or journal, with the understanding that the authors will address the suggested improvements.\n\nRating: 8/10\n\nReviewer's Comments:\n\nThe paper is well-written, and the authors provide a comprehensive analysis of the proposed method. However, there are some areas for improvement, such as a more detailed comparison with other state-of-the-art methods and the use of alternative prior distributions for the velocity variable. The proposed method demonstrates encouraging results in image generation tasks, particularly when faced with limited NFE, making it a promising approach to generative modeling.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative modeling framework, Acceleration Generative Modeling (AGM), grounded in phase space dynamics and leveraging insights from Stochastic Optimal Control (SOC) theory. AGM aims to enhance the sample efficiency of velocity-based generative modeling by constructing a path measure that bridges the data and prior distribution, resulting in a more favorable trajectory for efficient sampling. The proposed method incorporates additional velocity information to estimate realistic data points at an early time step, referred to as sampling-hop. AGM is shown to outperform baseline methods, particularly when faced with limited Number of Function Evaluations (NFE), and rivals the performance of diffusion models equipped with efficient sampling techniques.\n\nStrengths:\n\n1. The paper introduces a novel framework, AGM, that leverages SOC theory to construct a path measure in phase space, leading to more favorable sampling trajectories.\n2. AGM incorporates additional velocity information to estimate realistic data points at an early time step, reducing sampling complexity and increasing efficiency.\n3. The paper presents competitive results compared to diffusion models equipped with efficient sampling techniques, particularly in scenarios with limited NFE.\n4. AGM demonstrates promising outcomes for fast sampling in unconditional generative tasks on both toy and image datasets.\n\nWeaknesses:\n\n1. The paper's performance lags behind state-of-the-art methods in scenarios with sufficient NFE, suggesting avenues for enhancing AGM performance.\n2. The training process of AGM involves optimizing the force term, which can be computationally expensive and may require further research to improve efficiency.\n3. The choice of initial covariance \u03a30 directly influences the path measure of the trajectory, and selecting an optimal value may require further research.\n\nSuggestions for Improvement:\n\n1. To further enhance AGM performance, consider incorporating techniques proposed in Karras et al. (2022), such as data augmentation, fine-tuned noise scheduling, and network preconditioning.\n2. Investigate methods to improve the training efficiency of AGM by optimizing the force term or exploring alternative optimization strategies.\n3. Conduct further research to determine the optimal value of initial covariance \u03a30 and explore methods to adaptively adjust \u03a30 based on the specific problem or dataset.\n\nRecommendation:\n\nThe paper presents a novel and promising framework for generative modeling, AGM, which leverages SOC theory to construct more favorable sampling trajectories. While AGM's performance may require further enhancement in scenarios with sufficient NFE, the paper's contributions and experimental results demonstrate the potential of AGM for fast sampling in unconditional generative tasks. Therefore, I recommend this paper for publication, with suggestions for further research to address the limitations and areas for improvement identified above.\n\n$$$",
        "gr_score_with": 0.3645320197044335,
        "gr_score_without": 0.2992874109263658,
        "output_without": {
            "num_tokens_scored": 421,
            "num_green_tokens": 126,
            "green_fraction": 0.2992874109263658,
            "z_score": -4.218122497392497,
            "p_value": 0.999987682750851
        },
        "output_with": {
            "num_tokens_scored": 406,
            "num_green_tokens": 148,
            "green_fraction": 0.3645320197044335,
            "z_score": -1.4587936172515947,
            "p_value": 0.9276890400532597
        }
    },
    {
        "title": "EFFECTIVE DATA AUGMENTATION WITH DIFFUSION MODELS",
        "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains. DA-Fusion Semantically Augment Augmented Images For Training Models Figure 1: Real images (left) are semantically modified using a publicly available off-the-shelf Stable Diffusion checkpoint. Resulting synthetic images (right) are used for training downstream classification models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data augmentation strategy, DA-Fusion, that utilizes pre-trained text-to-image diffusion models to generate high-quality augmentations for images. The method adapts the diffusion model to new concepts by fine-tuning pseudo-prompts in the text encoder, and it produces variations of real images that respect their semantic invariances. The authors evaluate DA-Fusion on few-shot image classification tasks and a real-world weed recognition task, and they observe an improvement in accuracy in tested domains. The method is flexible, controllable, and performant, and it outperforms prior work in data augmentation.\n\nStrengths:\n\n1. The paper proposes a novel and innovative data augmentation strategy that leverages pre-trained text-to-image diffusion models.\n2. The method is flexible and can adapt to new concepts by fine-tuning pseudo-prompts in the text encoder.\n3. The authors evaluate DA-Fusion on a range of tasks, including few-shot image classification and a real-world weed recognition task.\n4. The method is controllable and can produce variations of real images that respect their semantic invariances.\n5. The authors provide a thorough analysis of the method's performance and compare it to prior work in data augmentation.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained text-to-image diffusion model is available, which may not be the case for all researchers.\n2. The method requires a large amount of computational resources to fine-tune the pseudo-prompts and generate augmentations.\n3. The paper does not provide a detailed analysis of the method's robustness to different types of images and datasets.\n4. The authors do not provide a clear explanation of how the method can be applied to other types of tasks, such as object detection or segmentation.\n5. The paper assumes that the diffusion model is pre-trained on a large dataset, which may not be the case for all researchers.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the method's robustness to different types of images and datasets.\n2. Explain how the method can be applied to other types of tasks, such as object detection or segmentation.\n3. Provide a clear explanation of how to fine-tune the pseudo-prompts and generate augmentations.\n4. Evaluate the method on a larger range of tasks and datasets to demonstrate its generalizability.\n5. Provide a more detailed analysis of the method's computational requirements and how they can be optimized.\n\nRecommendation:\n\nThe paper proposes a novel and innovative data augmentation strategy that leverages pre-trained text-to-image diffusion models. The method is flexible, controllable, and performant, and it outperforms prior work in data augmentation. However, the paper assumes that the pre-trained text-to-image diffusion model is available, which may not be the case for all researchers. Additionally, the method requires a large amount of computational resources to fine-tune the pseudo-prompts and generate augmentations. Overall, the paper is well-written and provides a thorough analysis of the method's performance. However, it requires further evaluation and analysis to demonstrate its robustness and generalizability.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data augmentation method, DA-Fusion, that leverages text-to-image diffusion models to semantically modify real images. This method aims to improve few-shot classification accuracy by generating high-quality augmentations that capture novel semantic attributes. DA-Fusion adapts a pre-trained diffusion model to new concepts by fine-tuning pseudo-prompts in the text encoder, allowing it to generate variations of real images. The method also controls augmentation intensity by randomizing the insertion position of real images into the diffusion process. The authors evaluate DA-Fusion on several datasets, including a contributed weed recognition task, and demonstrate improvements in few-shot classification accuracy compared to existing data augmentation methods.\n\nStrengths:\n\n1. DA-Fusion offers a novel approach to data augmentation by leveraging text-to-image diffusion models, which can capture novel semantic attributes.\n2. The method adapts to new concepts by fine-tuning pseudo-prompts, allowing it to generate variations of real images.\n3. DA-Fusion controls augmentation intensity by randomizing the insertion position of real images, which helps to maximize diversity.\n4. The authors provide extensive experimental results on several datasets, including a contributed weed recognition task, demonstrating improvements in few-shot classification accuracy.\n5. DA-Fusion has the potential to be applied to various computer vision tasks, such as image classification, object detection, and visual reinforcement learning.\n\nWeaknesses:\n\n1. DA-Fusion requires a pre-trained diffusion model, which may not be readily available for all researchers.\n2. The method assumes access to a large dataset of real images, which may not be feasible for all applications.\n3. DA-Fusion's performance may degrade if the pseudo-prompts are not well-tuned, which could require significant expertise.\n4. The authors do not provide a detailed comparison with other data augmentation methods, which could help to demonstrate the effectiveness of DA-Fusion.\n5. DA-Fusion's robustness to data balance is not thoroughly evaluated, which could be an important consideration for practical applications.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other data augmentation methods to demonstrate the effectiveness of DA-Fusion.\n2. Evaluate DA-Fusion's robustness to data balance and provide recommendations for optimal hyperparameter settings.\n3. Consider providing a more extensive analysis of DA-Fusion's performance on different datasets and applications.\n4. Develop a more efficient method for fine-tuning pseudo-prompts, which could reduce the expertise required to use DA-Fusion.\n5. Investigate the potential applications of DA-Fusion in other areas, such as image editing or visual reinforcement learning.\n\nRecommendation:\n\nThe paper proposes a novel data augmentation method, DA-Fusion, that leverages text-to-image diffusion models to semantically modify real images. The method demonstrates improvements in few-shot classification accuracy on several datasets, including a contributed weed recognition task. However, DA-Fusion requires a pre-trained diffusion model, access to a large dataset of real images, and expertise in fine-tuning pseudo-prompts. Therefore, I recommend this paper for publication, but suggest that the authors address the limitations and suggestions for improvement listed above.\n\n$$$",
        "gr_score_with": 0.5783783783783784,
        "gr_score_without": 0.4472843450479233,
        "output_without": {
            "num_tokens_scored": 313,
            "num_green_tokens": 140,
            "green_fraction": 0.4472843450479233,
            "z_score": 1.7075912697779692,
            "p_value": 0.04385610470877006
        },
        "output_with": {
            "num_tokens_scored": 370,
            "num_green_tokens": 214,
            "green_fraction": 0.5783783783783784,
            "z_score": 7.003859939637147,
            "p_value": 1.2450252468736554e-12
        }
    },
    {
        "title": "OPTIMAL TRANSPORT BASED ADVERSARIAL PATCH TO LEVERAGE LARGE SCALE ATTACK TRANSFERABILITY",
        "abstract": "Adversarial patch attacks, where a small patch is placed in the scene to fool neural networks, have been studied for numerous applications. Focusing on image classification, we consider the setting of a black-box transfer attack where an attacker does not know the target model. Instead of forcing corrupted image representations to cross the nearest decision boundaries or converge to a particular point, we propose a distribution-oriented approach. We rely on optimal transport to push the feature distribution of attacked images towards an already modeled distribution. We show that this new distribution-oriented approach leads to better transferable patches. Through digital experiments conducted on ImageNet-1K, we provide evidence that our new patches are the only ones that can simultaneously influence multiple Transformer models and Convolutional Neural Networks. Physical world experiments demonstrate that our patch can affect systems in deployment without explicit knowledge.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel distribution-oriented approach for designing Adversarial Patch Attacks (APAs) that are highly transferable to unknown networks. The authors rely on optimal transport to push the feature distribution of attacked images towards an already modeled distribution, rather than forcing corrupted image representations to cross the nearest decision boundaries or converge to a particular point. The proposed method is shown to be more effective than previous APAs in white-box and black-box transfer attacks, and is demonstrated to be physically feasible in the real world.\n\nStrengths:\n\n* The paper presents a novel and effective approach for designing APAs that are highly transferable to unknown networks.\n* The authors provide extensive experimental results that demonstrate the effectiveness of their approach in both digital and physical experiments.\n* The paper provides a thorough analysis of the strengths and weaknesses of previous APAs and identifies the limitations of existing approaches.\n\nWeaknesses:\n\n* The paper assumes that the target network is known, which may not always be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational complexity of their approach, which may be a limitation in terms of practical implementation.\n* The paper does not provide a clear explanation of how the optimal transport approach is used to design the APAs, which may make it difficult for readers to understand the underlying methodology.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of their approach and discuss potential ways to improve its efficiency.\n* The paper should provide a clear explanation of how the optimal transport approach is used to design the APAs, including a detailed description of the optimization process and the role of the Wasserstein loss.\n* The authors should provide more experimental results that demonstrate the effectiveness of their approach in real-world scenarios, including results on more diverse datasets and models.\n\nRecommendation:\n\nThis paper presents a novel and effective approach for designing APAs that are highly transferable to unknown networks. The authors provide extensive experimental results that demonstrate the effectiveness of their approach, and the paper provides a thorough analysis of the strengths and weaknesses of previous APAs. However, the paper assumes that the target network is known, and the authors do not provide a detailed analysis of the computational complexity of their approach. Therefore, I recommend that the paper be accepted for publication with minor revisions to address these limitations.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach for designing adversarial patch attacks (APAs) that can leverage large-scale attack transferability. The authors propose a distribution-oriented approach based on optimal transport, which aims to globally alter the feature distribution of a set of images from a particular class to match another known distribution from another class. This approach is shown to lead to better transferable patches compared to existing methods. The authors conduct extensive experiments on ImageNet-1K, demonstrating that their approach can fool multiple Transformer models and Convolutional Neural Networks, and can affect systems in deployment without explicit knowledge. The results also show that the proposed approach can outperform existing methods in terms of transferability, even when the attacked systems use defense mechanisms.\n\nStrengths:\n\n* The paper presents a novel approach for designing APAs that can leverage large-scale attack transferability.\n* The approach is shown to lead to better transferable patches compared to existing methods.\n* The authors conduct extensive experiments on ImageNet-1K, demonstrating the effectiveness of the proposed approach.\n* The results show that the proposed approach can outperform existing methods in terms of transferability, even when the attacked systems use defense mechanisms.\n\nWeaknesses:\n\n* The paper assumes that the attacked systems use a classical encoder-decoder schema, which may not be the case for all systems.\n* The authors do not provide a thorough analysis of the computational complexity of the proposed approach.\n* The paper assumes that the optimal transport distance is a good measure of the similarity between two distributions, which may not always be the case.\n* The authors do not provide a thorough analysis of the robustness of the proposed approach to different types of attacks.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the computational complexity of the proposed approach.\n* The authors should provide a more thorough analysis of the robustness of the proposed approach to different types of attacks.\n* The authors should explore other measures of similarity between two distributions, such as the Jensen-Shannon divergence.\n* The authors should provide a more thorough analysis of the impact of the choice of the targeted layers on the patch transferability.\n\nRecommendation:\n\nThe paper presents a novel approach for designing APAs that can leverage large-scale attack transferability, and the results show that the proposed approach can outperform existing methods in terms of transferability. However, the paper assumes that the attacked systems use a classical encoder-decoder schema, and the authors do not provide a thorough analysis of the computational complexity of the proposed approach. Therefore, I recommend that the authors revise the paper to address these issues before publication.\n\n$$$",
        "gr_score_with": 0.49458483754512633,
        "gr_score_without": 0.4161073825503356,
        "output_without": {
            "num_tokens_scored": 298,
            "num_green_tokens": 124,
            "green_fraction": 0.4161073825503356,
            "z_score": 0.5675805238084821,
            "p_value": 0.2851599174854871
        },
        "output_with": {
            "num_tokens_scored": 277,
            "num_green_tokens": 137,
            "green_fraction": 0.49458483754512633,
            "z_score": 3.213333383265673,
            "p_value": 0.0006560194913573704
        }
    },
    {
        "title": "TENSOR TRUST: INTERPRETABLE PROMPT INJECTION ATTACKS",
        "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \u201cdefenses\u201d against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release data and code at [removed].",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Tensor Trust, a web-based game that allows players to create and defend against prompt injection attacks on Large Language Models (LLMs). The game generates a dataset of over 126,000 prompt injection attacks and 46,000 defenses, which is the largest dataset of human-generated adversarial examples for instruction-following LLMs. The authors use this dataset to create two benchmarks for evaluating the robustness of LLMs to prompt extraction and prompt hijacking attacks. They also analyze the attacks and defenses in the dataset to identify common strategies and vulnerabilities in existing LLMs.\n\nStrengths:\n\n1. The paper presents a large and diverse dataset of human-generated adversarial examples, which is a significant contribution to the field.\n2. The authors introduce two benchmarks for evaluating the robustness of LLMs to prompt extraction and prompt hijacking attacks, which can be used to evaluate the effectiveness of different mitigation strategies.\n3. The paper provides a detailed analysis of the attacks and defenses in the dataset, which sheds light on the vulnerabilities of existing LLMs and the strategies used by attackers.\n4. The authors demonstrate that attacks from the dataset can transfer to real-world applications, which highlights the importance of developing robust defenses against prompt injection attacks.\n\nWeaknesses:\n\n1. The paper focuses primarily on the vulnerability of LLMs to prompt injection attacks, but does not provide a comprehensive evaluation of the effectiveness of different mitigation strategies.\n2. The authors do not provide a detailed explanation of the implementation details of the game, which may make it difficult for readers to replicate the results.\n3. The paper assumes that the attacks and defenses in the dataset are representative of the broader population of attacks and defenses, which may not be the case.\n4. The authors do not provide a clear evaluation of the impact of the attacks on the LLMs, which may make it difficult for readers to understand the severity of the vulnerability.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive evaluation of the effectiveness of different mitigation strategies, including a comparison of the performance of different models and a discussion of the trade-offs between different approaches.\n2. The authors should provide a more detailed explanation of the implementation details of the game, including the specific version of GPT 3.5 Turbo used and the token limits.\n3. The authors should provide a more detailed analysis of the attacks and defenses in the dataset, including a discussion of the common strategies and vulnerabilities identified.\n4. The authors should provide a clear evaluation of the impact of the attacks on the LLMs, including a discussion of the severity of the vulnerability and the potential consequences of a successful attack.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of LLM security, and the authors should be commended for their efforts to create a large and diverse dataset of human-generated adversarial examples. However, the paper would benefit from a more comprehensive evaluation of the effectiveness of different mitigation strategies, a more detailed explanation of the implementation details of the game, and a more detailed analysis of the attacks and defenses in the dataset. Overall, the paper is a valuable contribution to the field, and it has the potential to inform the development of more robust defenses against prompt injection attacks.",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThe paper presents a comprehensive study of prompt injection attacks against Large Language Models (LLMs). The authors introduce a new dataset, Tensor Trust, which consists of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, created by players of an online game called Tensor Trust. The authors use this dataset to create two benchmarks for resistance to prompt injection, which they refer to as prompt extraction and prompt hijacking. They demonstrate that many models are vulnerable to these attack strategies, and that some of them generalize to deployed LLM-based applications.\n\n**Strengths**\n\n1. The authors provide a thorough analysis of prompt injection attacks and defenses, which is a crucial step in understanding the security of LLMs.\n2. The Tensor Trust dataset is a significant contribution, as it is the largest dataset of human-generated adversarial examples for instruction-following LLMs.\n3. The authors introduce two benchmarks for resistance to prompt injection, which can be used to evaluate the robustness of LLMs.\n4. The study highlights the importance of considering human-written attacks, which can be more interpretable and effective than automatically generated attacks.\n\n**Weaknesses**\n\n1. The study focuses primarily on a specific type of prompt injection attack, namely prompt extraction and prompt hijacking, which may not be representative of all types of prompt injection attacks.\n2. The authors do not provide a thorough analysis of the limitations of their study, such as the potential biases in the Tensor Trust dataset.\n3. The study does not provide a clear comparison with previous work on prompt injection attacks, which makes it difficult to evaluate the novelty and impact of the findings.\n4. The authors do not provide a clear plan for mitigating prompt injection attacks, which is a crucial step in addressing the security concerns raised by this study.\n\n**Suggestions for Improvement**\n\n1. Expand the study to include a broader range of prompt injection attacks, including those that are more complex or nuanced.\n2. Provide a more thorough analysis of the limitations of the study, including potential biases in the Tensor Trust dataset.\n3. Compare the findings of this study with previous work on prompt injection attacks, including both human-written and automatically generated attacks.\n4. Provide a clear plan for mitigating prompt injection attacks, including recommendations for developers and researchers.\n\n**Recommendation**\n\nI recommend that this paper be accepted for publication, subject to revisions that address the concerns raised above. The study provides a thorough analysis of prompt injection attacks and defenses, and introduces a new dataset and benchmarks that can be used to evaluate the robustness of LLMs. However, the study could be strengthened by expanding the scope of the analysis, addressing limitations, and providing a clear plan for mitigating prompt injection attacks.",
        "gr_score_with": 0.4793388429752066,
        "gr_score_without": 0.4017094017094017,
        "output_without": {
            "num_tokens_scored": 351,
            "num_green_tokens": 141,
            "green_fraction": 0.4017094017094017,
            "z_score": 0.06537204504606073,
            "p_value": 0.473938890639127
        },
        "output_with": {
            "num_tokens_scored": 363,
            "num_green_tokens": 174,
            "green_fraction": 0.4793388429752066,
            "z_score": 3.085556863359478,
            "p_value": 0.001015856855223429
        }
    },
    {
        "title": "FAITHFUL RULE EXTRACTION FOR DIFFERENTIABLE RULE LEARNING MODELS",
        "abstract": "There is increasing interest in methods for extracting interpretable rules from ML models trained to solve a wide range of tasks over knowledge graphs (KGs), such as KG completion, node classification, question answering and recommendation. Many such approaches, however, lack formal guarantees establishing the precise relationship between the model and the extracted rules, and this lack of assurance becomes especially problematic when the extracted rules are applied in safetycritical contexts or to ensure compliance with legal requirements. Recent research has examined whether the rules derived from the influential NEURAL-LP model exhibit soundness (or completeness), which means that the results obtained by applying the model to any dataset always contain (or are contained in) the results obtained by applying the rules to the same dataset. In this paper, we extend this analysis to the context of DRUM, an approach that has demonstrated superior practical performance. After observing that the rules currently extracted from a DRUM model can be unsound and/or incomplete, we propose a novel algorithm where the output rules, expressed in an extension of Datalog, ensure both soundness and completeness. This algorithm, however, can be inefficient in practice and hence we propose additional constraints to DRUM models facilitating rule extraction, albeit at the expense of reduced expressive power.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel algorithm for extracting faithful rules from Differentiable Rule Learning (DRL) models, specifically DRUM, which has demonstrated superior practical performance. The authors extend the analysis of faithfulness guarantees to the context of DRUM and propose a novel algorithm where the output rules ensure both soundness and completeness. However, this algorithm can be inefficient in practice, and hence, they propose additional constraints to DRUM models facilitating rule extraction, albeit at the expense of reduced expressive power.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of faithfulness guarantees in the context of DRUM, which is a state-of-the-art approach for DRL.\n2. The proposed algorithm for extracting faithful rules from DRUM models is novel and provides a significant improvement over existing methods.\n3. The authors provide a thorough evaluation of the proposed approach on benchmark datasets, demonstrating its effectiveness and efficiency.\n4. The paper discusses the limitations of the proposed approach and provides suggestions for future work.\n\nWeaknesses:\n\n1. The proposed algorithm for extracting faithful rules from DRUM models has a high computational complexity, which may limit its practical applicability.\n2. The paper assumes that the DRUM model is trained on a dataset with a fixed structure, which may not be the case in real-world applications.\n3. The evaluation of the proposed approach is limited to a specific set of benchmark datasets, and it is unclear how well it will perform on other datasets.\n4. The paper does not provide a detailed comparison with other existing methods for DRL.\n\nSuggestions for Improvement:\n\n1. The authors could explore methods to reduce the computational complexity of the proposed algorithm for extracting faithful rules from DRUM models.\n2. The paper could provide a more detailed discussion of the assumptions made about the DRUM model and the dataset, and how they affect the performance of the proposed approach.\n3. The authors could evaluate the proposed approach on a more diverse set of benchmark datasets to demonstrate its robustness.\n4. The paper could provide a more detailed comparison with other existing methods for DRL to demonstrate the effectiveness of the proposed approach.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of DRL, and the proposed approach for extracting faithful rules from DRUM models is novel and effective. However, the high computational complexity of the proposed algorithm and the limitations of the evaluation are significant concerns. Therefore, I recommend that the authors address these concerns in future work and provide a more detailed comparison with other existing methods for DRL.\n\nOverall, I believe that the paper has the potential to make a significant impact in the field of DRL, and I recommend it for publication in a top-tier conference or journal.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents a novel approach for faithful rule extraction from Differentiable Rule Learning (DRL) models, specifically for the DRUM model. The authors extend previous work on rule extraction for NEURAL-LP by introducing a new algorithm that extracts faithful rules with soundness and completeness guarantees. The proposed algorithm is based on multipath conjunctions with inequalities and disjunctions, which enable counting operations and faithful rule extraction. The authors also introduce two simplified variants of the DRUM model, SMDRUM and MMDRUM, which have reduced expressive power but are more tractable for rule extraction. Experimental results demonstrate the effectiveness of the proposed approach on several benchmarks for inductive knowledge graph completion.\n\nStrengths:\n\n* The paper presents a novel approach for faithful rule extraction from DRL models, which is a significant contribution to the field.\n* The proposed algorithm is based on sound mathematical foundations, ensuring that the extracted rules are faithful and have soundness and completeness guarantees.\n* The authors introduce two simplified variants of the DRUM model, which are more tractable for rule extraction and can be used for practical applications.\n* Experimental results demonstrate the effectiveness of the proposed approach on several benchmarks for inductive knowledge graph completion.\n\nWeaknesses:\n\n* The proposed algorithm has high computational complexity, making it challenging to extract faithful rules for large datasets.\n* The authors assume that the dataset is sparse, which may not always be the case in practice.\n* The paper focuses on inductive knowledge graph completion, and it is not clear whether the proposed approach can be extended to other applications.\n* The authors do not provide a comprehensive comparison with other rule extraction approaches.\n\nSuggestions for Improvement:\n\n* To reduce the computational complexity of the proposed algorithm, the authors could investigate approximation techniques or heuristics that can be used to extract faithful rules for large datasets.\n* To handle dense datasets, the authors could consider using sparse graph representations or other techniques that can efficiently handle large datasets.\n* To extend the proposed approach to other applications, the authors could investigate adapting the algorithm to other DRL models or other knowledge graph completion tasks.\n* To provide a comprehensive comparison with other rule extraction approaches, the authors could investigate comparing the proposed approach with other state-of-the-art rule extraction methods on several benchmarks.\n\nRecommendation:\n\nThis paper presents a novel and significant contribution to the field of faithful rule extraction from DRL models. While there are some limitations and areas for improvement, the proposed approach has the potential to be a valuable tool for applications that require faithful rule extraction. I recommend that the authors continue to investigate and refine their approach to make it more efficient and widely applicable.",
        "gr_score_with": 0.5187319884726225,
        "gr_score_without": 0.3739130434782609,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 129,
            "green_fraction": 0.3739130434782609,
            "z_score": -0.9890707100936805,
            "p_value": 0.8386857265335113
        },
        "output_with": {
            "num_tokens_scored": 347,
            "num_green_tokens": 180,
            "green_fraction": 0.5187319884726225,
            "z_score": 4.514678802284852,
            "p_value": 3.1706400691661893e-06
        }
    },
    {
        "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
        "abstract": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Unlike existing autoregressive models that use rotationally invariant features, Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n\nStrengths:\n\n1. The paper presents a novel approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections.\n2. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n3. The paper proposes a comprehensive set of tests to evaluate the quality of generated 3D structures, including validity, capturing training set statistics, and generalization capabilities.\n4. The authors provide a detailed comparison between their approach and existing methods, highlighting the advantages of their approach.\n\nWeaknesses:\n\n1. The paper assumes that the training set molecules are representative of the underlying distribution, which may not always be the case.\n2. The authors use a lookup table to assign bond orders, which may not be accurate for all molecules.\n3. The paper does not provide a clear explanation of how the spherical harmonic projections are used to model the radial distribution.\n4. The authors do not provide a detailed comparison between their approach and existing diffusion models, such as EDM.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the spherical harmonic projections are used to model the radial distribution.\n2. Use a more accurate method to assign bond orders, such as using a neural network to predict bond orders.\n3. Provide a more detailed comparison between the approach and existing diffusion models, such as EDM.\n4. Investigate the use of normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections. While there are some limitations and areas for improvement, the paper provides a comprehensive evaluation of the approach and demonstrates its effectiveness on various metrics. I recommend accepting the paper for publication, subject to the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents Symphony, an autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Symphony uses message-passing with higher-degree E(3)-equivariant features, allowing for a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics, including molecular generation, completion, and generalization capabilities.\n\nStrengths:\n\n1. The authors address the challenge of 3D molecular structure generation by introducing a novel method, Symphony, that leverages E(3)-equivariant features and spherical harmonic projections.\n2. The paper provides a comprehensive evaluation of Symphony's performance, comparing it to existing autoregressive models and diffusion models, and demonstrating its superiority in various metrics.\n3. The authors propose a new metric, valid completion rate (VCR), to evaluate the model's generalization capabilities, which is a valuable contribution to the field.\n\nWeaknesses:\n\n1. The paper relies heavily on existing work, such as G-SchNet and G-SphereNet, and builds upon their architectures, which may limit the originality of the contribution.\n2. The authors acknowledge that the discretization of radial components is too coarse, which affects the model's accuracy in bond length distributions, and suggest exploring normalizing flows to address this issue.\n3. The computational cost of Symphony is increased due to the use of tensor products, which may be a limitation for larger molecules or more complex structures.\n\nSuggestions for Improvement:\n\n1. To address the limitation of coarse discretization, the authors could explore normalizing flows or other techniques to model the radial distribution without discretization.\n2. To improve the computational efficiency of Symphony, the authors could investigate more efficient methods for generating higher-degree E(3)-equivariant features, such as using hierarchical architectures or more efficient tensor products.\n3. To further evaluate Symphony's generalization capabilities, the authors could conduct more extensive experiments, such as training the model on larger datasets or more diverse molecular structures.\n\nRecommendation:\n\nThe paper presents a promising method for 3D molecular structure generation, Symphony, which outperforms existing autoregressive models and approaches the performance of diffusion models. However, the authors acknowledge several limitations, including coarse discretization and increased computational cost, which need to be addressed. With further development and optimization, Symphony has the potential to become a leading method for molecular generation and completion.\n\nGrade: 8/10\n\nThis review provides a balanced evaluation of the paper, highlighting both the strengths and weaknesses of Symphony, and suggesting areas for improvement. The recommendation is based on the paper's promising results, but also takes into account the acknowledged limitations, which need to be addressed for Symphony to become a leading method in the field.",
        "gr_score_with": 0.4657210401891253,
        "gr_score_without": 0.37337662337662336,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 115,
            "green_fraction": 0.37337662337662336,
            "z_score": -0.9537463156584329,
            "p_value": 0.8298939651979189
        },
        "output_with": {
            "num_tokens_scored": 423,
            "num_green_tokens": 197,
            "green_fraction": 0.4657210401891253,
            "z_score": 2.7591098483460814,
            "p_value": 0.0028979524424623307
        }
    },
    {
        "title": "MAXIMUM ENTROPY MODEL CORRECTION",
        "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model\u2019s next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna\u2019s convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes the MaxEnt Model Correction (MoCo) framework to alleviate model errors in model-based reinforcement learning (MBRL). The framework uses maximum entropy density estimation to correct the next-state distributions needed for planning from the approximate model P\u0302 towards the true one in P. The authors introduce the MaxEnt MoCo algorithm, which can be used with any planning algorithm to solve the approximate MDP M\u0302. They also develop a sample-based variant, MoCoDyna, which extends the Dyna framework. The authors provide a thorough theoretical analysis, showing that the MoCo framework can significantly accelerate the convergence rate of policy evaluation and control algorithms, and ensure convergence to the true value functions despite model errors if said errors are sufficiently small.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to alleviate model errors in MBRL, which is a significant challenge in the field.\n2. The authors provide a thorough theoretical analysis, including convergence results and error bounds, which demonstrate the effectiveness of the MoCo framework.\n3. The paper introduces a sample-based variant, MoCoDyna, which extends the Dyna framework and allows for efficient learning in complex environments.\n4. The authors provide empirical results, demonstrating the superior performance of MoCo-based planning algorithms compared to their naive counterparts.\n\nWeaknesses:\n\n1. The paper assumes access to the true dynamics P, which may not be available in practice.\n2. The authors rely on the maximum entropy density estimation framework, which may not be suitable for all environments.\n3. The paper does not provide a clear comparison with other model-based reinforcement learning algorithms, such as OS-VI and OS-Dyna.\n4. The authors do not discuss the computational complexity of the MoCo framework, which may be a concern in large-scale environments.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the assumptions made in the paper, particularly the assumption of access to the true dynamics P.\n2. Explore alternative approaches to maximum entropy density estimation, such as using other divergences or regularization techniques.\n3. Compare the MoCo framework with other model-based reinforcement learning algorithms, such as OS-VI and OS-Dyna, to demonstrate its effectiveness.\n4. Provide a more detailed discussion of the computational complexity of the MoCo framework and its implications for large-scale environments.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to alleviate model errors in MBRL, which is a significant challenge in the field. The authors provide a thorough theoretical analysis and empirical results, demonstrating the superior performance of MoCo-based planning algorithms compared to their naive counterparts. However, the paper assumes access to the true dynamics P, which may not be available in practice, and relies on the maximum entropy density estimation framework, which may not be suitable for all environments. Therefore, the paper requires further investigation and refinement to make it more practical and applicable to a wider range of environments.\n\n$$$\nRecommendation: Accept",
        "peer_review_with_watermark": "$$$ Paper Summary:\nThe paper presents the Maximum Entropy Model Correction (MaxEnt MoCo) algorithm, which aims to reduce the adverse impact of model error in model-based reinforcement learning (MBRL). The algorithm uses Maximum Entropy density estimation to correct the next-state distributions needed for planning from the approximate model P\u0302 towards the true one in P. The authors provide a thorough theoretical analysis, including convergence results, and demonstrate the effectiveness of the algorithm through numerical experiments. They also introduce a sample-based variant, MoCoDyna, which extends the Dyna framework.\n\n Strengths:\nThe paper makes significant contributions to the field of MBRL by proposing a cost-efficient approach to alleviate model errors. The authors provide a thorough theoretical analysis, including convergence results, which demonstrates the effectiveness of the algorithm. The numerical experiments show that the algorithm can successfully solve tasks with any model error, outperforming other algorithms.\n\n Weaknesses:\nWhile the paper provides a thorough theoretical analysis, some of the convergence results are complex and require a strong background in theoretical computer science. The numerical experiments are limited to a specific environment, and it would be beneficial to evaluate the algorithm on a broader range of tasks. The paper does not provide a clear discussion on how to select the hyperparameters, especially the number of basis functions.\n\n Suggestions for Improvement:\nTo make the paper more accessible to a broader audience, the authors could provide a more detailed explanation of the theoretical results and simplify the convergence analysis. They could also evaluate the algorithm on a broader range of tasks and provide a clear discussion on how to select the hyperparameters. Additionally, it would be beneficial to provide more insights into the practical applications of the algorithm, such as how it can be used in real-world scenarios.\n\n Recommendation:\nI recommend this paper for publication in a top-tier conference on reinforcement learning. The paper presents a significant contribution to the field of MBRL and provides a thorough theoretical analysis, as well as numerical experiments that demonstrate the effectiveness of the algorithm. The authors should address some of the weaknesses mentioned above to make the paper more accessible and robust.",
        "gr_score_with": 0.5111821086261981,
        "gr_score_without": 0.4099722991689751,
        "output_without": {
            "num_tokens_scored": 361,
            "num_green_tokens": 148,
            "green_fraction": 0.4099722991689751,
            "z_score": 0.3867615383341854,
            "p_value": 0.3494663767452395
        },
        "output_with": {
            "num_tokens_scored": 313,
            "num_green_tokens": 160,
            "green_fraction": 0.5111821086261981,
            "z_score": 4.015147039748198,
            "p_value": 2.9704372567620644e-05
        }
    },
    {
        "title": "A PRECISE CHARACTERIZATION OF SGD STABILITY USING LOSS SURFACE GEOMETRY",
        "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors focus on the linearized dynamics of SGD close to the optimum, and introduce a novel coherence measure \u03c3 to capture the relevant geometric characteristics of the loss surface. They provide a sufficient condition for the linear dynamics to diverge, which relies solely on the values of \u03bb1(H), \u03b7, B, n, and \u03c3. The authors also demonstrate that their sufficient condition is nearly optimal across a natural range of SGD hyperparameters.\n\nStrengths:\n\n* The paper presents a novel coherence measure \u03c3 that summarizes relevant information in the loss surface geometry.\n* The authors provide a precise characterization of the stability of SGD using linearized dynamics.\n* The paper demonstrates that the sufficient condition for divergence is nearly optimal across a natural range of SGD hyperparameters.\n* The authors provide a series of experiments to validate their theoretical results.\n\nWeaknesses:\n\n* The paper assumes that the loss function is additively decomposable, which may not be the case in all scenarios.\n* The authors do not provide a detailed comparison of their results with other existing works in the field.\n* The paper assumes that the learning rate and batch size are fixed, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of their results with other existing works in the field.\n* The paper should be extended to consider more general loss functions and learning rates.\n* The authors should provide a more detailed analysis of the coherence measure \u03c3 and its relation to the stability of SGD.\n\nRecommendation:\n\nThe paper presents a novel and precise characterization of the stability of SGD using loss surface geometry. The authors provide a sufficient condition for divergence that is nearly optimal across a natural range of SGD hyperparameters. The paper also provides a series of experiments to validate their theoretical results. However, the paper assumes that the loss function is additively decomposable and does not provide a detailed comparison of their results with other existing works in the field. Overall, the paper is well-written and provides a significant contribution to the field of optimization.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors focus on linearized SGD dynamics and provide a sufficient condition for linear instability, contingent on hyperparameters and the sharpness at the optimum. The condition relies on a novel coherence measure, \u03c3, which captures relevant geometric properties of the loss surface. The authors demonstrate that this condition is nearly optimal across a range of hyperparameters and conduct experiments on additively decomposable quadratic loss functions to validate their theoretical findings.\n\nStrengths:\n\n1. The paper presents a rigorous and interpretable sufficient condition for linear instability in SGD, which is a significant contribution to the field.\n2. The authors introduce a novel coherence measure, \u03c3, that captures relevant geometric properties of the loss surface, making it a useful tool for understanding the behavior of SGD.\n3. The paper provides a thorough comparison to prior work, highlighting the advantages of the proposed condition and its applicability to a broader class of loss functions.\n4. The experiments conducted in the paper validate the theoretical findings and demonstrate the importance of considering the geometry of the loss surface in understanding the behavior of SGD.\n\nWeaknesses:\n\n1. The paper assumes a linearized setting, which may not be sufficient to capture the full complexity of SGD in non-linear settings.\n2. The authors rely on a specific definition of stability, which may be too restrictive and does not account for other forms of instability, such as sharpness-aware methods.\n3. The paper does not provide a clear understanding of how the proposed condition can be applied in practice, making it challenging to translate the theoretical results to real-world scenarios.\n4. The authors mention several open questions, including closing the gap between Theorem 1 and Theorem 2, but do not provide a clear direction for addressing these questions.\n\nSuggestions for Improvement:\n\n1. The authors should extend the linearized setting to non-linear settings to capture a more comprehensive understanding of SGD.\n2. The paper should be updated to account for other forms of instability, such as sharpness-aware methods.\n3. The authors should provide a clear explanation of how the proposed condition can be applied in practice, including a discussion on how to estimate the coherence measure, \u03c3.\n4. The paper should be updated to address the open questions mentioned in the conclusion, including closing the gap between Theorem 1 and Theorem 2.\n\nRecommendation:\n\nI recommend accepting this paper, as it presents a significant contribution to the field and provides a rigorous and interpretable sufficient condition for linear instability in SGD. However, I suggest that the authors address the weaknesses and suggestions for improvement mentioned above to further enhance the paper's impact and applicability.\n\nRating: 8/10",
        "gr_score_with": 0.5264550264550265,
        "gr_score_without": 0.4076923076923077,
        "output_without": {
            "num_tokens_scored": 260,
            "num_green_tokens": 106,
            "green_fraction": 0.4076923076923077,
            "z_score": 0.25318484177091666,
            "p_value": 0.4000626897550529
        },
        "output_with": {
            "num_tokens_scored": 378,
            "num_green_tokens": 199,
            "green_fraction": 0.5264550264550265,
            "z_score": 5.018528280511403,
            "p_value": 2.6034414615548144e-07
        }
    }
]