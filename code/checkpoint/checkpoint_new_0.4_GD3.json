[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "$$$\\boxed{\\text{Paper Summary:}}$\n\nThe paper proposes the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for causal discovery. The CMA is designed to synergize the metadata-based reasoning capabilities of LLMs with the data-driven modelling of DSCMs. The authors evaluate the CMA's performance on synthetic experiments and causal discovery benchmarks, as well as on a real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). The results show that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery.\n\n$\\boxed{\\text{Strengths:}}$\n\n1. The CMA framework is novel and innovative, combining LLMs and DSCMs to tackle the causal discovery problem.\n2. The authors provide a clear and detailed explanation of the CMA's components and how they work together.\n3. The experimental results demonstrate the CMA's effectiveness on synthetic experiments and real-world datasets.\n4. The paper provides a thorough discussion of the limitations of existing causal discovery methods and highlights the potential benefits of the CMA.\n\n$\\boxed{\\text{Weaknesses:}}$\n\n1. The paper assumes that the CMA can handle discrete variables with parents, but does not discuss how to handle this case.\n2. The authors mention that the CMA relies on DSCMs to fit data, which assumes a Markovian DAG. However, they do not discuss how to extend the framework to include chain graph elements or more flexible, non-Markovian causal graphs.\n3. The paper does not provide a comprehensive comparison with other causal discovery methods, and the experimental results are limited to a few benchmarks.\n4. The authors mention that the CMA can propose potentially confounding or modulating variables, but do not discuss how to validate these relationships.\n\n$\\boxed{\\text{Suggestions for Improvement:}}$\n\n1. Develop a more comprehensive comparison with other causal discovery methods to demonstrate the CMA's superiority.\n2. Discuss how to handle discrete variables with parents and provide experimental results to support this.\n3. Extend the framework to include chain graph elements and more flexible, non-Markovian causal graphs.\n4. Provide a more detailed discussion of how to validate the relationships between potentially confounding or modulating variables.\n\n$\\boxed{\\text{Recommendation:}}$\n\nBased on the experimental results and the novelty of the CMA framework, I recommend that the authors revise and resubmit their paper for consideration. The CMA has the potential to make a significant contribution to the field of causal discovery, and with some revisions to address the limitations and weaknesses, it can become a leading method in the field.",
        "peer_review_with_watermark": "$$$\\begin{tabular}{|l|} \\hline \\textbf{Paper Summary:} & The paper introduces the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for causal discovery. The CMA uses LLMs to generate hypotheses, DSCMs to model the data, and a post-processing stage to refine the model. The paper demonstrates the effectiveness of the CMA on synthetic and real-world datasets from various scientific fields. \\\\ \\hline \\textbf{Strengths:} & The paper makes significant contributions to the field of causal discovery, combining LLMs and DSCMs for the first time. The CMA is capable of proposing insightful relationships between variables, both internal and external to the DSCMs. The paper provides a unified framework for causal discovery, which can be applied to multi-modal data. The use of LLMs as priors, critics, and post-processors is an innovative approach. The paper also provides a real-world application of the CMA to the clinical and radiological phenotype of Alzheimer\u2019s Disease. \\\\ \\hline \\textbf{Weaknesses:} & The paper relies on DSCMs, which assume a Markovian DAG. The CMA may struggle to represent chains in a fully automated manner, and human intervention is necessary to allow appropriate Deep Chain Graph modelling. The paper also assumes that the LLMs can effectively reduce the Markov Equivalence Class (MEC) by ruling out implausible graphs, but this is not explicitly demonstrated. The paper could benefit from more explicit evaluation of the CMA's performance on various benchmark datasets. \\\\ \\hline \\textbf{Suggestions for Improvement:} & The paper could benefit from more explicit evaluation of the CMA's performance on various benchmark datasets. The use of DSCMs should be explicitly demonstrated to show that they can handle non-Markovian causal graphs. The paper should also explore the potential of fully automated chain graph modelling. The CMA should be compared to other state-of-the-art causal discovery methods to demonstrate its superiority. The paper could also provide more explicit evaluation of the CMA's performance on real-world datasets, including the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset. \\\\ \\hline \\textbf{Recommendation:} & The paper makes significant contributions to the field of causal discovery, and the CMA is a novel and effective framework for causal discovery. The paper should be considered for publication in a reputable conference or journal. However, the paper could benefit from more explicit evaluation and comparison to other state-of-the-art methods. The paper should also be further developed to address the limitations and assumptions of the CMA. \\\\ \\hline \\end{tabular}$$",
        "gr_score_with": 0.35106382978723405,
        "gr_score_without": 0.3473684210526316,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 132,
            "green_fraction": 0.3473684210526316,
            "z_score": -2.0942695414584773,
            "p_value": 0.9818820119969719
        },
        "output_with": {
            "num_tokens_scored": 376,
            "num_green_tokens": 132,
            "green_fraction": 0.35106382978723405,
            "z_score": -1.9369494184529943,
            "p_value": 0.9736242366424916
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "$$\\begin{aligned}\n$$\\textbf{Paper Summary:}$$\nThe paper proposes a framework for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The proposed framework consists of a set of concept encoders that extract disentangled concept embeddings along various concept axes specified by language. The concept encoders are trained using a combination of reconstruction and anchoring losses to encourage disentanglement and compositionality. The paper presents quantitative and qualitative evaluations of the proposed framework, demonstrating its superiority over prior work in visual concept editing.\n\n$$\\textbf{Strengths:}$$\n1. The paper presents a novel approach to learning language-informed visual concepts from images by distilling from pre-trained vision-language models.\n2. The proposed framework encourages disentanglement and compositionality of concept embeddings, enabling flexible compositions of visual concepts.\n3. The paper presents thorough evaluations of the proposed framework, demonstrating its superiority over prior work in visual concept editing.\n\n$$\\textbf{Weaknesses:}$$\n1. The paper relies heavily on pre-trained vision-language models, which may not capture the nuances of human language and visual perception.\n2. The proposed framework may not generalize well to unseen concepts and images, requiring additional training data or fine-tuning.\n3. The paper does not provide a clear analysis of the effect of hyperparameters on the performance of the proposed framework.\n\n$$\\textbf{Suggestions for Improvement:}$$\n1. The paper should explore the use of additional data sources, such as human-annotated datasets, to improve the generalizability of the proposed framework.\n2. The paper should provide a more detailed analysis of the effect of hyperparameters on the performance of the proposed framework.\n3. The paper should investigate the use of additional techniques, such as adversarial training or regularization, to improve the disentanglement and compositionality of concept embeddings.\n\n$$\\textbf{Recommendation:}$$\nBased on the evaluation of the proposed framework, I recommend its adoption in research and development of vision-language models. However, further exploration of the limitations and potential improvements of the proposed framework is necessary to fully realize its potential.",
        "peer_review_with_watermark": "$$\n\\begin{aligned}\n\\textbf{Paper Summary:} &\\  The paper proposes a framework for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The proposed framework learns disentangled concept embeddings along various concept axes specified by language and enables flexible composition of concepts for image generation. The paper also proposes a method for test-time finetuning of the concept encoders to generalize to unseen concepts.\n\n\\textbf{Strengths:} &\\  The paper makes a significant contribution to the field of visual concept learning by introducing a framework that can learn disentangled concept embeddings from pre-trained vision-language models. The proposed framework is flexible and enables composition of concepts for image generation. The paper also proposes a method for test-time finetuning of the concept encoders, which is a crucial aspect of real-world applications.\n\n\\textbf{Weaknesses:} &\\  The paper has a few limitations. The proposed framework relies on pre-trained vision-language models, which may not be the best choice for all tasks. The paper also assumes that the concept axes are well-defined and specified by language, which may not be the case in all scenarios. The paper also lacks a comprehensive evaluation of the proposed framework, with only a few qualitative results presented.\n\n\\textbf{Suggestions for Improvement:} &\\  To improve the paper, the authors could consider the following: (1) Using a more robust pre-trained vision-language model that can learn more abstract and meaningful concept embeddings. (2) Evaluating the proposed framework on a more comprehensive dataset that captures a wider range of concept axes and visual variations. (3) Providing a more detailed and comprehensive evaluation of the proposed framework, including both qualitative and quantitative results.\n\n\\textbf{Recommendation:} &\\  The paper is well-written and makes a significant contribution to the field of visual concept learning. However, the limitations and weaknesses of the proposed framework need to be addressed to make it more robust and reliable. With some modifications and extensions, the paper has the potential to be a top-notch contribution to the field.\n\n$$\n\\begin{aligned}\n&  The paper is well-written, and the authors have made a significant contribution to the field of visual concept learning. However, there are a few limitations and weaknesses that need to be addressed to make the proposed framework more robust and reliable. With some modifications and extensions, the paper has the potential to be a top-notch contribution to the field.\n\\end{aligned}\n$$",
        "gr_score_with": 0.3223684210526316,
        "gr_score_without": 0.34317343173431736,
        "output_without": {
            "num_tokens_scored": 271,
            "num_green_tokens": 93,
            "green_fraction": 0.34317343173431736,
            "z_score": -1.9095474499772553,
            "p_value": 0.9719042466328024
        },
        "output_with": {
            "num_tokens_scored": 304,
            "num_green_tokens": 98,
            "green_fraction": 0.3223684210526316,
            "z_score": -2.7629281441659983,
            "p_value": 0.9971357311895701
        }
    }
]