[
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential limitations and challenges of applying the concept bottleneck layer to more complex and realistic settings.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the training process for the concept bottleneck layer, including the loss functions and hyperparameters used.\n2. Discuss the potential limitations and challenges of applying the concept bottleneck layer to more complex and realistic settings, such as text or 3D data.\n3. Provide more qualitative and quantitative evaluations of the concept bottleneck layer, including its performance on more challenging tasks and datasets.\n4. Consider incorporating more visualizations and illustrations to help readers understand the concept bottleneck layer and its applications.\n\nRecommendation:\n\nThis paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. While the paper has some limitations, it provides a comprehensive evaluation of CBGMs and demonstrates their effectiveness and robustness. I recommend this paper to readers interested in generative modeling, interpretability, and controllability.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel generative model architecture, called concept bottleneck generative models (CBGMs), which integrates a concept bottleneck (CB) layer to enable interpretability, steerability, and debuggability. The CB layer is a plug-in module that can be applied to various generative models, including generative adversarial networks (GANs), variational autoencoders (VAEs), and diffusion models. The CB layer constrains the model to encode human-understandable concepts, which can be used to steer the model's output and debug the model during and post-training.\n\nStrengths:\n\n1. The CBGM architecture is flexible and can be applied to various generative models, making it a versatile tool for generative modeling.\n2. The CB layer enables interpretability, steerability, and debuggability, which are essential for understanding and controlling the model's behavior.\n3. The paper provides a comprehensive evaluation of the CBGM architecture, including experiments on multiple datasets and generative models.\n4. The authors provide a clear and well-organized presentation of the paper, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the training set is annotated with pre-defined concepts, which may not be feasible in practice, especially for large-scale datasets.\n2. The CB layer requires a large number of concepts to be defined, which can be time-consuming and labor-intensive.\n3. The paper does not provide a detailed analysis of the computational complexity of the CBGM architecture.\n4. The authors do not provide a clear comparison with existing state-of-the-art generative models.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the CBGM architecture and its implications for large-scale datasets.\n2. The paper should include a more thorough evaluation of the CBGM architecture on a wider range of datasets and generative models.\n3. The authors should provide a more detailed comparison with existing state-of-the-art generative models, including a discussion of the strengths and weaknesses of each approach.\n4. The paper should include a more detailed discussion of the potential applications of the CBGM architecture, including its potential uses in real-world settings.\n\nRecommendation:\n\nThis paper presents a novel and innovative generative model architecture that enables interpretability, steerability, and debuggability. The CBGM architecture is flexible and can be applied to various generative models, making it a valuable tool for generative modeling. However, the paper assumes that the training set is annotated with pre-defined concepts, which may not be feasible in practice. The authors should provide a more detailed analysis of the computational complexity of the CBGM architecture and its implications for large-scale datasets. With these improvements, the CBGM architecture has the potential to become a leading approach in generative modeling.\n\nRating: 8/10\n\nReviewer's Expertise: Generative modeling, deep learning, and computer vision.\n\nReviewer's Comments:\n\nThis paper presents a novel and innovative generative model architecture that enables interpretability, steerability, and debuggability. The CBGM architecture is flexible and can be applied to various generative models, making it a valuable tool for generative modeling. However, the paper assumes that the training set is annotated with pre-defined concepts, which may not be feasible in practice. The authors should provide a more detailed analysis of the computational complexity of the CBGM architecture and its implications for large-scale datasets. With these improvements, the CBGM architecture has the potential to become a leading approach in generative modeling.\n\n$$$",
        "gr_score_with": 0.9565217391304348,
        "gr_score_without": 0.9254498714652957,
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 360,
            "green_fraction": 0.9254498714652957,
            "z_score": 1.6733482620544442,
            "p_value": 0.04712937834371358
        },
        "output_with": {
            "num_tokens_scored": 345,
            "num_green_tokens": 330,
            "green_fraction": 0.9565217391304348,
            "z_score": 3.4996269497977806,
            "p_value": 0.0002329548461106293
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called SWAP (Sparse Entropic Wasserstein Regression) for neural network pruning. The method aims to address the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. The authors introduce a formulation of Entropic Wasserstein regression (EWR) for pruning, which capitalizes on the geometric properties of the optimal transport problem. The proposed method, SWAP, is demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost.\n\nStrengths:\n\n1. The paper provides a novel formulation of Entropic Wasserstein regression (EWR) for pruning, which strikes a balance between covariance information preservation and noise reduction.\n2. The authors demonstrate the effectiveness of the proposed method, SWAP, in addressing the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning.\n3. The paper provides extensive experimental results, including comparisons with state-of-the-art (SoTA) methods, which demonstrate the superiority of the proposed method in terms of robustness against noise.\n4. The authors provide a clear and concise explanation of the proposed method, including the mathematical formulation and the algorithmic framework.\n\nWeaknesses:\n\n1. The paper assumes a fixed number of pruning stages T, which may not be optimal in practice. It would be beneficial to investigate the impact of varying T on the performance of the proposed method.\n2. The authors do not provide a comprehensive analysis of the computational complexity of the proposed method, which is an important aspect to consider in practice.\n3. The paper assumes a fixed regularization parameter \u03bb, which may not be optimal in practice. It would be beneficial to investigate the impact of varying \u03bb on the performance of the proposed method.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of varying the number of pruning stages T on the performance of the proposed method.\n2. Provide a comprehensive analysis of the computational complexity of the proposed method.\n3. Investigate the impact of varying the regularization parameter \u03bb on the performance of the proposed method.\n4. Consider incorporating additional techniques, such as early stopping or warm restarts, to further improve the performance of the proposed method.\n\nRecommendation:\n\nThe proposed method, SWAP, is a novel and effective approach to neural network pruning that addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix. The method is demonstrated to offer noise mitigation effects and strikes a balance between covariance information preservation and noise reduction. While there are some weaknesses and suggestions for improvement, the proposed method has the potential to be a valuable tool in the field of neural network pruning. Therefore, I recommend accepting this paper for publication.\n\nRating: 8/10\n\nReviewer's Signature: [Your Name]\n\nDate: [Today's Date]\n\n$$$",
        "gr_score_with": 0.911504424778761,
        "gr_score_without": 0.898876404494382,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 320,
            "green_fraction": 0.898876404494382,
            "z_score": -0.07046939958061729,
            "p_value": 0.5280899722101721
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 309,
            "green_fraction": 0.911504424778761,
            "z_score": 0.70624469965885,
            "p_value": 0.24001798847950157
        }
    },
    {
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines. We release all the above-mentioned resources at https://github.com/RUCAIBox/FIGA.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called FIGA (Fine-grained Quality-Aware Alignment) for aligning language models with human preferences. The approach leverages fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of language models for alignment. The authors curate a refined alignment dataset called SPA (SubPar Alignment) that pairs initial responses with revised responses on queries where a language model performs less well. They also design a new loss function that incorporates fine-grained quality signals to encourage the model to learn good behaviors and penalize bad behaviors. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nStrengths:\n\n1. The paper presents a novel approach for aligning language models with human preferences, which is a critical problem in natural language processing.\n2. The approach leverages fine-grained quality signals, which provides more detailed information about what makes a response good or bad.\n3. The authors curate a refined alignment dataset called SPA, which is a significant contribution to the field.\n4. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nWeaknesses:\n\n1. The paper assumes the availability of a reward model, which may not always be the case in real-world scenarios.\n2. The approach relies on the quality of the revised responses, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the Levenshtein distance metric used to quantify the similarity between initial and revised responses.\n4. The approach may not generalize well to out-of-distribution scenarios, where the reward model may not perform well.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for deriving fine-grained quality signals, such as using other metrics or incorporating additional information.\n2. Evaluate the approach on more diverse and challenging datasets to assess its generalizability.\n3. Provide more detailed analysis of the Levenshtein distance metric and its limitations.\n4. Investigate the use of other reward models or alternative methods for aligning language models with human preferences.\n\nRecommendation:\n\nThe paper presents a novel and promising approach for aligning language models with human preferences. While there are some limitations and areas for improvement, the approach shows superior performance compared to existing alignment methods. I recommend this paper for publication in a top-tier conference, with the suggestion to address the weaknesses and improve the approach as mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper proposes an alignment approach named FIGA, which leverages fine-grained quality signals to improve the alignment of large language models (LLMs) with human preferences. FIGA consists of two main components: a curated alignment dataset called SPA and a new loss function that incorporates fine-grained quality signals. The SPA dataset pairs initial responses with revised responses on queries where the LLM performs less well, and the new loss function assigns different weights to different parts of the responses based on their quality. The authors demonstrate the effectiveness of FIGA by comparing it with several competitive baselines on various tasks, including knowledge utilization, human alignment, and open-ended generation.\n\nStrengths:\n\n* The paper presents a novel approach to aligning LLMs with human preferences by leveraging fine-grained quality signals.\n* The proposed FIGA method has several advantages over traditional SFT and RLHF methods, including ease of implementation, efficient training, and better understanding of correct behaviors for alignment.\n* The authors provide extensive experimental results and comparisons with competitive baselines, demonstrating the effectiveness of FIGA on various tasks.\n* The paper provides detailed implementation details and hyperparameter settings, making it easier for readers to reproduce the results.\n\nWeaknesses:\n\n* The paper assumes the availability of a reward model for assessing the alignment level, which may not be feasible in all scenarios.\n* The construction of the SPA dataset requires careful filtering and revision of initial responses, which may be time-consuming and labor-intensive.\n* The paper does not provide a thorough analysis of the trade-offs between the different components of the FIGA method, such as the importance of fine-grained quality signals and the impact of different hyperparameters.\n* The paper does not discuss potential limitations and challenges of the FIGA method, such as its applicability to diverse domains and tasks.\n\nSuggestions for Improvement:\n\n* Provide more detailed analysis of the trade-offs between the different components of the FIGA method, including the importance of fine-grained quality signals and the impact of different hyperparameters.\n* Discuss potential limitations and challenges of the FIGA method, such as its applicability to diverse domains and tasks.\n* Provide more information on the construction of the SPA dataset, including the filtering and revision process, and the impact of different hyperparameters on the quality of the dataset.\n* Consider evaluating the FIGA method on more diverse tasks and domains to demonstrate its generalizability and robustness.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to aligning LLMs with human preferences by leveraging fine-grained quality signals. The proposed FIGA method has several advantages over traditional SFT and RLHF methods, including ease of implementation, efficient training, and better understanding of correct behaviors for alignment. However, the paper assumes the availability of a reward model for assessing the alignment level, which may not be feasible in all scenarios. The construction of the SPA dataset requires careful filtering and revision of initial responses, which may be time-consuming and labor-intensive. Overall, the paper provides valuable insights and contributions to the field of language model alignment and is recommended for publication in the conference.",
        "gr_score_with": 0.9438502673796791,
        "gr_score_without": 0.9034090909090909,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 318,
            "green_fraction": 0.9034090909090909,
            "z_score": 0.21339280241665434,
            "p_value": 0.41551029703127207
        },
        "output_with": {
            "num_tokens_scored": 374,
            "num_green_tokens": 353,
            "green_fraction": 0.9438502673796791,
            "z_score": 2.826907901475086,
            "p_value": 0.002349992156851359
        }
    },
    {
        "title": "GUAGE MODEL POWERED DIALOGUE AGENTS",
        "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel paradigm for strategizing large language models (LLMs) powered dialogue agents, called Plug-and-Play Dialogue Policy Planner (PPDPP). PPDPP is designed to tackle the challenges of proactive dialogue problems, where the dialogue agent needs to strategically take the initiative to steer the conversation towards an anticipated goal. The proposed framework consists of a tunable language model plug-in that can be fine-tuned with available human-annotated data and reinforced learning from goal-oriented AI feedback. The authors demonstrate the effectiveness of PPDPP on three different proactive dialogue problems, including negotiation, emotional support, and tutoring dialogues.\n\nStrengths:\n1. The paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems.\n2. The framework is designed to be modular and flexible, allowing for easy adaptation to different applications and domains.\n3. The authors provide a thorough evaluation of PPDPP on three different datasets, demonstrating its superiority over existing approaches.\n4. The paper highlights the potential of tunable plug-ins to address specific shortcomings in LLMs and integrate with multiple plug-ins to tackle more complex dialogue challenges.\n\nWeaknesses:\n1. The paper assumes the availability of human-annotated data, which may not be feasible in all scenarios.\n2. The framework relies on the quality of the goal-oriented AI feedback, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the computational resources required for training and deploying PPDPP.\n4. The authors do not discuss the potential limitations and biases of using LLMs in dialogue systems.\n\nSuggestions for Improvement:\n1. Provide more detailed information on the data requirements and preprocessing steps for training PPDPP.\n2. Discuss the potential limitations and biases of using LLMs in dialogue systems and propose potential solutions.\n3. Provide a more comprehensive analysis of the computational resources required for training and deploying PPDPP.\n4. Consider exploring other types of goal-oriented AI feedback, such as multi-modal feedback, to improve the robustness of PPDPP.\n\nRecommendation:\nThe paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems. While there are some limitations and areas for improvement, the framework shows promise for a wide range of applications. I recommend the paper for publication, with the understanding that the authors will address the suggested improvements in future work.\n\nGrade: B+ (87%)\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel paradigm for strategizing large language models (LLMs) powered dialogue agents, called Plug-and-Play Dialogue Policy Planner (PPDPP). PPDPP utilizes a tunable language model plug-in to facilitate supervised fine-tuning and reinforcement learning from goal-oriented AI feedback, enabling the LLM-powered dialogue agent to generalize to different cases and exhibit versatility across diverse applications.\n\nStrengths:\n1. The paper proposes a novel and effective approach to improving the policy planning capability of LLM-powered dialogue agents.\n2. The proposed framework can be generalized to different cases and exhibits versatility across diverse applications.\n3. The experimental results demonstrate the superiority of PPDPP over existing approaches on three different proactive dialogue problems.\n4. The paper provides a comprehensive evaluation of the proposed method, including both automatic and human evaluation.\n\nWeaknesses:\n1. The paper relies heavily on the quality of the pre-trained language model and the human-annotated corpus.\n2. The proposed framework requires a significant amount of computational resources and data to train.\n3. The paper does not provide a clear explanation of how the tunable language model plug-in is learned and updated during the reinforcement learning process.\n4. The paper only evaluates the proposed method on three specific proactive dialogue problems, and it is unclear how well the method would generalize to other domains.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of how the tunable language model plug-in is learned and updated during the reinforcement learning process.\n2. Evaluate the proposed method on a wider range of proactive dialogue problems to demonstrate its generalizability.\n3. Provide a clearer explanation of the role of the pre-trained language model and the human-annotated corpus in the proposed framework.\n4. Consider incorporating additional evaluation metrics, such as turn-level response quality measurements, to provide a more comprehensive evaluation of the proposed method.\n\nRecommendation:\nThe proposed method has the potential to significantly improve the policy planning capability of LLM-powered dialogue agents, making it a valuable contribution to the field of conversational AI research. However, further evaluation and refinement of the method are necessary to demonstrate its robustness and generalizability across different domains and applications.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.9221183800623053,
        "gr_score_without": 0.9052924791086351,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 325,
            "green_fraction": 0.9052924791086351,
            "z_score": 0.33445288670240736,
            "p_value": 0.3690189184315381
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 296,
            "green_fraction": 0.9221183800623053,
            "z_score": 1.3211124662893212,
            "p_value": 0.09323193348841963
        }
    },
    {
        "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
        "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in deep neural networks (DNNs). FOMO alternates between the forgetting phase, where a subset of weights is randomly forgotten through reinitialization, and the relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the efficacy of FOMO on benchmark datasets and adversarial attacks, showing that it alleviates robust overfitting, improves robustness, and provides a better trade-off between standard and robust accuracy.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the problem of robust overfitting in adversarial training and its significance in real-world applications.\n2. The authors propose a novel and innovative solution, FOMO, which is grounded in the concept of active forgetting and relearning, inspired by the human brain's learning mechanisms.\n3. The experimental results demonstrate the effectiveness of FOMO in alleviating robust overfitting, improving robustness, and providing a better trade-off between standard and robust accuracy.\n4. The authors conduct extensive experiments on multiple datasets, architectures, and perturbation types, showcasing the scalability and robustness of FOMO.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase is beneficial for generalization, but it is unclear whether this assumption holds in all cases.\n2. The authors do not provide a thorough analysis of the convergence properties of FOMO, which is crucial for understanding its robustness and generalization capabilities.\n3. The paper relies heavily on empirical results, and more theoretical analysis is needed to understand the underlying mechanisms of FOMO.\n4. The authors do not discuss the computational costs and complexity of implementing FOMO, which may be a limitation in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the forgetting phase and its impact on generalization.\n2. Investigate the convergence properties of FOMO and its robustness to different perturbation types.\n3. Conduct more theoretical analysis to understand the underlying mechanisms of FOMO and its relationship to the effectiveness of adversarial training.\n4. Discuss the computational costs and complexity of implementing FOMO and provide strategies for reducing these costs.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of adversarial training by proposing a novel and innovative solution, FOMO. The experimental results demonstrate the effectiveness of FOMO, and the authors provide a comprehensive overview of the problem and its significance. However, more theoretical analysis and investigation of the forgetting phase are needed to fully understand the underlying mechanisms of FOMO. I recommend this paper for publication in a top-tier conference in the field of machine learning and computer vision.\n\nScore: 8/10\n\nReviewer's Name: [Your Name]\n\nReviewer's Affiliation: [Your Affiliation]\n\nDate: [Current Date]",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in deep neural networks (DNNs). FOMO alternates between the forgetting phase, where a subset of weights is randomly forgotten through reinitialization, and the relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the effectiveness of FOMO in alleviating robust overfitting and improving robust generalization on various benchmark datasets and adversarial attacks. The proposed framework is robust to AutoAttacks and increases generalization in many real-world scenarios.\n\nStrengths:\n\n1. The paper provides a comprehensive review of the existing literature on adversarial training and robust generalization, highlighting the limitations of current methods.\n2. The authors propose a novel approach to addressing robust overfitting, which is a critical issue in adversarial training.\n3. The experimental results demonstrate the effectiveness of FOMO in improving robust generalization and reducing the gap between the best and last robust test accuracy.\n4. The proposed framework is robust to AutoAttacks and natural corruptions, making it a promising solution for real-world applications.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase is a necessary step for achieving robust generalization, but the theoretical justification for this assumption is limited.\n2. The authors do not provide a thorough analysis of the computational cost of the proposed framework, which may be a concern for large-scale applications.\n3. The paper relies heavily on empirical results, and more theoretical analysis is needed to understand the underlying mechanisms of FOMO.\n4. The proposed framework is not compared to other state-of-the-art methods in the field, which makes it difficult to evaluate its overall performance.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough theoretical analysis of the forgetting phase and its relationship to robust generalization.\n2. Investigate the computational cost of the proposed framework and provide a detailed analysis of its scalability.\n3. Compare the proposed framework to other state-of-the-art methods in the field to evaluate its overall performance.\n4. Provide more insights into the role of the stability model in consolidating critical information learned during each relearning phase.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to addressing robust overfitting in adversarial training. While there are some limitations and areas for improvement, the experimental results demonstrate the effectiveness of FOMO in improving robust generalization and reducing the gap between the best and last robust test accuracy. I recommend this paper for publication, but with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9650537634408602,
        "gr_score_without": 0.9384615384615385,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 366,
            "green_fraction": 0.9384615384615385,
            "z_score": 2.532018623437144,
            "p_value": 0.005670398384420182
        },
        "output_with": {
            "num_tokens_scored": 372,
            "num_green_tokens": 359,
            "green_fraction": 0.9650537634408602,
            "z_score": 4.182514369886322,
            "p_value": 1.4415139447230723e-05
        }
    },
    {
        "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
        "abstract": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Unlike existing autoregressive models that use rotationally invariant features, Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n\nStrengths:\n\n1. The paper presents a novel approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections.\n2. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n3. The paper proposes a comprehensive set of tests to evaluate the quality of generated 3D structures, including validity, capturing training set statistics, and generalization capabilities.\n4. The authors provide a detailed comparison between their approach and existing methods, highlighting the advantages of their approach.\n\nWeaknesses:\n\n1. The paper assumes that the training set molecules are representative of the underlying distribution, which may not always be the case.\n2. The authors use a lookup table to assign bond orders, which may not be accurate for all molecules.\n3. The paper does not provide a clear explanation of how the spherical harmonic projections are used to model the radial distribution.\n4. The authors do not provide a detailed comparison between their approach and existing diffusion models, such as EDM.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the spherical harmonic projections are used to model the radial distribution.\n2. Use a more accurate method to assign bond orders, such as using a neural network to predict bond orders.\n3. Provide a more detailed comparison between the approach and existing diffusion models, such as EDM.\n4. Investigate the use of normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections. While there are some limitations and areas for improvement, the paper provides a comprehensive evaluation of the approach and demonstrates its effectiveness on various metrics. I recommend accepting the paper for publication, subject to the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper presents Symphony, a novel E(3)-equivariant autoregressive generative model for 3D molecular geometries. Unlike existing autoregressive models, Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on a variety of metrics, including validity, uniqueness, and bispectrum accuracy.\n\nStrengths:\n\n1. The paper presents a novel approach to molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections, which allows for a more accurate representation of 3D molecular geometries.\n2. The authors demonstrate the effectiveness of Symphony on a variety of metrics, including validity, uniqueness, and bispectrum accuracy, and show that it outperforms existing autoregressive models and approaches the performance of diffusion models.\n3. The paper provides a comprehensive evaluation of Symphony's performance on a range of tasks, including molecule generation, completion, and validation.\n\nWeaknesses:\n\n1. The paper's training procedure is not fully detailed, and the authors do not provide a clear explanation of how they handle the angular frequency bottleneck.\n2. The authors do not provide a thorough comparison of Symphony's performance with other state-of-the-art models, and the evaluation metrics used in the paper are not fully justified.\n3. The paper's conclusion is somewhat premature, as the authors do not provide a clear explanation of the implications of their results or a roadmap for future work.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of their training procedure, including how they handle the angular frequency bottleneck and how they select the hyperparameters for their model.\n2. The authors should provide a more thorough comparison of Symphony's performance with other state-of-the-art models, including diffusion models and other autoregressive models.\n3. The authors should provide a more detailed justification for the evaluation metrics used in the paper and explain how they selected these metrics.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections. However, the paper's training procedure and evaluation metrics require further clarification, and the authors should provide a more thorough comparison with other state-of-the-art models. With these improvements, the paper has the potential to make a significant contribution to the field of molecular generation.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.9461279461279462,
        "gr_score_without": 0.9253246753246753,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 285,
            "green_fraction": 0.9253246753246753,
            "z_score": 1.4816494036728693,
            "p_value": 0.06921680316521923
        },
        "output_with": {
            "num_tokens_scored": 297,
            "num_green_tokens": 281,
            "green_fraction": 0.9461279461279462,
            "z_score": 2.6499911905738576,
            "p_value": 0.004024693484013477
        }
    },
    {
        "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
        "abstract": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel priorinformed state-switching GLMs, called Gaussian HMM-GLM (Gaussian prior) and one-hot HMM-GLM (Gumbel-Softmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data. The code is available at https://github.com/JerrySoybean/onehot-hmmglm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for estimating time-varying functional connectivity in multi-state neural systems, called One-Hot Generalized Linear Model for Switching Brain State Discovery. The model, referred to as One-Hot HMM-GLM (OHG), decomposes the traditional weight matrix in GLMs into a discrete adjacency matrix representing the connection type and a positive-valued strength matrix. The authors argue that the regulated adjacency matrices with their shared prior should inform us about the underlying anatomical connectome, while the less restricted strength matrices can provide flexibility to capture functional variations across multiple brain states. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance in terms of log-likelihood, state accuracy, weight error, adjacency accuracy, and adjacency prior accuracy.\n\nStrengths:\n\n1. The paper proposes a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems.\n2. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance.\n3. The authors provide a clear and concise explanation of the model and its components.\n4. The paper includes a detailed experimental evaluation, including quantitative results and visualizations of the learned models.\n\nWeaknesses:\n\n1. The paper assumes that the underlying anatomical connectome is static, which may not be the case in reality.\n2. The model requires a large number of hyperparameters to be pre-defined, which can be challenging in practice.\n3. The paper does not provide a clear explanation of how the Gumbel-Softmax prior is used to enforce the adjacency matrices to learn shared information.\n4. The experimental evaluation is limited to two real neural datasets, and it would be beneficial to include more datasets to demonstrate the robustness of the model.\n\nSuggestions for Improvement:\n\n1. Consider incorporating a more dynamic model of the underlying anatomical connectome to better capture the complexity of neural systems.\n2. Provide a more detailed explanation of the Gumbel-Softmax prior and its role in enforcing shared information among adjacency matrices.\n3. Include more real neural datasets in the experimental evaluation to demonstrate the robustness of the model.\n4. Consider using more advanced techniques, such as Bayesian inference or deep learning, to improve the accuracy and robustness of the model.\n\nRecommendation:\n\nThe paper presents a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems. While there are some limitations and areas for improvement, the results demonstrate the potential of the model to improve our understanding of neural systems. I recommend this paper for publication in a reputable journal, with the hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel one-hot generalized linear model (OHG) for switching brain state discovery. The authors argue that existing state-switching generalized linear models (GLMs) with hidden Markov models (HMMs) lack biological plausibility, as they do not account for the underlying anatomical connectome. To address this issue, the authors introduce a shared prior for the adjacency matrices of all states, which enforces the adjacency matrices to learn shared information. The proposed model decomposes the traditional weight matrix in GLMs into a discrete adjacency matrix representing the connection type and a positive-valued strength matrix. The authors demonstrate the effectiveness of the proposed model using both simulated and real neural datasets.\n\nStrengths:\n\n1. The paper proposes a novel and biologically plausible model for switching brain state discovery.\n2. The authors provide a comprehensive evaluation of the proposed model using both simulated and real neural datasets.\n3. The paper highlights the importance of accounting for the underlying anatomical connectome in modeling neural interactions.\n4. The proposed model has the potential to improve the understanding of neural circuits and their functional connectivity.\n\nWeaknesses:\n\n1. The paper assumes that the adjacency matrices of all states share a common structure, which may not always be the case in real neural systems.\n2. The proposed model requires a large number of hyperparameters to be tuned, which can be computationally expensive.\n3. The paper does not provide a clear explanation of how the proposed model can be applied to more complex neural systems with multiple brain states.\n4. The paper relies heavily on simulated data to evaluate the proposed model, which may not be representative of real neural systems.\n\nSuggestions for Improvement:\n\n1. The authors should provide more evidence to support the assumption that the adjacency matrices of all states share a common structure.\n2. The paper should provide more details on how to tune the hyperparameters of the proposed model.\n3. The authors should provide more examples of how the proposed model can be applied to more complex neural systems with multiple brain states.\n4. The paper should provide more discussion on the limitations of the proposed model and how it can be improved.\n\nRecommendation:\n\nThe paper proposes a novel and biologically plausible model for switching brain state discovery. However, it has some limitations that need to be addressed. With some modifications and additional evaluations, the proposed model has the potential to improve the understanding of neural circuits and their functional connectivity. I recommend this paper for publication in a peer-reviewed journal after addressing the suggested improvements.\n\n$$$$",
        "gr_score_with": 0.9582089552238806,
        "gr_score_without": 0.9523809523809523,
        "output_without": {
            "num_tokens_scored": 399,
            "num_green_tokens": 380,
            "green_fraction": 0.9523809523809523,
            "z_score": 3.4878549933316196,
            "p_value": 0.0002434560362220796
        },
        "output_with": {
            "num_tokens_scored": 335,
            "num_green_tokens": 321,
            "green_fraction": 0.9582089552238806,
            "z_score": 3.55147041936592,
            "p_value": 0.00019154252551864802
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to overcome the limitations of existing visual RL algorithms in terms of sample efficiency, asymptotic performance, and robustness to random seeds. DrM is based on the observation that visual RL agents often exhibit sustained inactivity during early training, which limits their ability to explore effectively. The authors introduce the concept of dormant ratio, which measures the inactivity of an agent's policy network, and show that it can be used as a standalone indicator of an agent's activity level. DrM uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. The authors demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance compared to state-of-the-art model-free visual RL algorithms.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the limitations of existing visual RL algorithms and identifies a key issue that has not been addressed before, i.e., the inactivity of agents during early training.\n2. The authors introduce a novel concept, dormant ratio, which provides a new perspective on understanding agent behavior in visual RL.\n3. The proposed DrM algorithm is well-motivated and easy to implement, making it a practical solution for real-world applications.\n4. The experimental results demonstrate that DrM achieves state-of-the-art performance in various visual continuous control tasks.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of visual RL and deep learning, which may make it challenging for non-experts to understand the context and significance of the work.\n2. The authors do not provide a detailed analysis of the theoretical foundations of the dormant ratio concept and its connection to agent behavior.\n3. The paper relies heavily on empirical results, which may not be generalizable to other domains or tasks.\n4. The authors do not discuss the potential limitations and challenges of using the dormant ratio as a metric for agent activity.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the theoretical foundations of the dormant ratio concept and its connection to agent behavior.\n2. Conduct a more thorough analysis of the limitations and challenges of using the dormant ratio as a metric for agent activity.\n3. Explore the potential applications of DrM in other domains or tasks, such as discrete action tasks or unsupervised RL.\n4. Provide more detailed information about the hyperparameters and tuning procedures used in the experiments.\n\nRecommendation:\n\nThe paper presents a novel and well-motivated approach to visual RL that addresses a key issue that has not been addressed before. The proposed DrM algorithm is easy to implement and achieves state-of-the-art performance in various visual continuous control tasks. However, the paper relies heavily on empirical results and does not provide a detailed analysis of the theoretical foundations of the dormant ratio concept. Therefore, I recommend that the authors address these limitations and provide a more comprehensive analysis of the work.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "gr_score_with": 0.8864734299516909,
        "gr_score_without": 0.8605898123324397,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 321,
            "green_fraction": 0.8605898123324397,
            "z_score": -2.5368878865461637,
            "p_value": 0.9944078617290022
        },
        "output_with": {
            "num_tokens_scored": 414,
            "num_green_tokens": 367,
            "green_fraction": 0.8864734299516909,
            "z_score": -0.9171923780938213,
            "p_value": 0.8204790799518318
        }
    },
    {
        "title": "ING VIA TASK-DRIVEN FEATURE SELECTION",
        "abstract": "This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a userspecified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most informative channel-subset whilst simultaneously training a network to execute the task given the subset. Experiments demonstrate the potential of TADRED in diverse imaging applications: several clinicallyrelevant tasks in magnetic resonance imaging; and remote sensing and physiological applications of hyperspectral imaging. Results show substantial improvement over classical experimental design, two recent application-specific methods within the new paradigm, and state-of-the-art approaches in supervised feature selection. We anticipate further applications of our approach. Code is available: Code Link.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to experimental design in imaging applications, specifically for multi-channel imaging. The proposed method, TADRED, is a task-driven experimental design paradigm that simultaneously optimizes the design and trains a machine-learning model to execute a user-specified image-analysis task. TADRED uses a dual-network architecture, consisting of a scoring network and a task network, to identify the most informative channel-subset and train a high-performing network for the task. The paper demonstrates the effectiveness of TADRED in various imaging applications, including clinically relevant tasks in magnetic resonance imaging and remote sensing and physiological applications of hyperspectral imaging.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to experimental design in imaging applications, which is a significant contribution to the field.\n2. The proposed method, TADRED, is well-motivated and has a clear and logical structure.\n3. The paper provides a thorough evaluation of TADRED using various benchmark datasets and tasks, demonstrating its effectiveness in different imaging applications.\n4. The authors provide a comprehensive analysis of the strengths and limitations of TADRED, as well as comparisons with other state-of-the-art methods.\n\nWeaknesses:\n\n1. The paper assumes a priori knowledge of the task, which may not always be the case in real-world applications.\n2. The proposed method requires a large amount of training data, which may not be feasible in all scenarios.\n3. The paper does not provide a detailed explanation of the hyperparameter tuning process, which is crucial for the success of the method.\n4. The comparison with other methods is limited to a few benchmark datasets, and it would be beneficial to evaluate TADRED on a broader range of datasets and tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the possibility of learning the task from the data, rather than relying on a priori knowledge.\n2. Explore alternative methods for hyperparameter tuning, such as Bayesian optimization or evolutionary algorithms.\n3. Provide a more detailed explanation of the hyperparameter tuning process and its impact on the performance of TADRED.\n4. Evaluate TADRED on a broader range of datasets and tasks to demonstrate its robustness and generalizability.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to experimental design in imaging applications, which has the potential to significantly impact the field. However, there are some limitations and areas for improvement that need to be addressed. With further refinement and evaluation, TADRED has the potential to become a widely used and effective method for experimental design in imaging applications.\n\n$$$\nNote: The review is based on the provided paper and does not include any external knowledge or information. The review is intended to provide a critical evaluation of the paper, highlighting its strengths and weaknesses, and suggesting areas for improvement.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to experimental design in imaging applications, specifically for multi-channel imaging. The proposed method, TADRED, is a task-driven experimental design paradigm that simultaneously optimizes the design and trains a machine learning model to execute a user-specified image-analysis task. TADRED uses a dual-network architecture, consisting of a scoring network and a task network, to identify the most informative channel-subset and train a high-performing network for the task. The paper demonstrates the effectiveness of TADRED in various imaging applications, including clinically-relevant MRI and remote sensing and physiological applications of hyperspectral imaging.\n\nStrengths:\n\n* The paper presents a novel approach to experimental design in imaging applications, which is a significant contribution to the field.\n* The proposed method, TADRED, is demonstrated to be effective in various imaging applications, including clinically-relevant MRI and remote sensing and physiological applications of hyperspectral imaging.\n* The paper provides a comprehensive review of related work in experimental design and supervised feature selection.\n* The paper presents a detailed description of the TADRED algorithm, including its architecture, training procedure, and optimization.\n\nWeaknesses:\n\n* The paper assumes that the task network is a deep neural network, which may not be the case in all imaging applications.\n* The paper does not provide a detailed analysis of the computational cost of TADRED compared to other methods.\n* The paper assumes that the scoring network is a separate network from the task network, which may not be necessary in all cases.\n* The paper does not provide a detailed evaluation of the robustness of TADRED to different types of noise and artifacts.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of TADRED compared to other methods.\n* Evaluate the robustness of TADRED to different types of noise and artifacts.\n* Consider using a more general architecture for the task network, such as a graph neural network or a recurrent neural network.\n* Provide a more detailed evaluation of the performance of TADRED on different types of imaging data, such as 3D medical images or 2D natural images.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to experimental design in imaging applications. However, there are some limitations and areas for improvement. With further development and evaluation, TADRED has the potential to become a widely-used method in the field of imaging and machine learning.\n\n$$$",
        "gr_score_with": 0.9545454545454546,
        "gr_score_without": 0.9388753056234719,
        "output_without": {
            "num_tokens_scored": 409,
            "num_green_tokens": 384,
            "green_fraction": 0.9388753056234719,
            "z_score": 2.6208552509421104,
            "p_value": 0.004385474690541176
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 294,
            "green_fraction": 0.9545454545454546,
            "z_score": 3.1910343567544763,
            "p_value": 0.0007088221064441898
        }
    },
    {
        "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
        "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which admits separate control over the camera and the content of the scene. The model is trained using a synthetic dataset, DySO, and real-world videos, and it learns to disentangle camera pose and scene dynamics from monocular real-world videos. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\nStrengths:\n\n1. The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task.\n2. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n3. The paper presents a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n4. The authors propose a novel synthetic dataset, DySO, which is used for co-training DyST.\n\nWeaknesses:\n\n1. The paper assumes that the scene dynamics are known, which may not be the case in real-world scenarios.\n2. The authors use a simple set of camera and object motions in the synthetic dataset, which may not be sufficient to capture the complexity of real-world scenes.\n3. The paper does not provide a detailed analysis of the model's performance on more complex types of videos, such as videos with several independent moving objects or changing lighting conditions.\n4. The authors use a L2 loss for training the model, which may not be suitable for all types of scenes.\n\nSuggestions for Improvement:\n\n1. The authors could explore more complex types of videos and analyze the model's performance on these scenarios.\n2. The paper could benefit from a more detailed analysis of the model's performance on videos with changing lighting conditions.\n3. The authors could explore the use of more advanced loss functions, such as a adversarial loss, to improve the model's performance.\n4. The paper could benefit from a more detailed analysis of the model's learned latent representations and their relationship to the scene dynamics and camera pose.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes. While there are some limitations to the paper, it is a significant contribution to the field and has the potential to be improved upon. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which can admit separate control over the camera and the content of the scene. The model is trained on a synthetic dataset, DySO, and a real-world video dataset, SSv2, through a co-training scheme. The authors demonstrate that DyST can learn to disentangle camera pose and scene dynamics, and show promising view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\nStrengths:\n\n* The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which can admit separate control over the camera and the content of the scene.\n* The authors demonstrate that DyST can learn to disentangle camera pose and scene dynamics, which is a challenging task.\n* The paper provides a thorough analysis of the model's performance on both synthetic and real-world datasets.\n* The authors provide a detailed description of the model's architecture and training procedure.\n\nWeaknesses:\n\n* The paper assumes that the camera pose and scene dynamics are separable, which may not always be the case in real-world scenarios.\n* The authors do not provide a clear evaluation of the model's performance on more complex types of videos, such as videos with several independent moving objects or changing lighting conditions.\n* The paper does not provide a clear comparison with other state-of-the-art models for generative modeling of dynamic 3D visual scenes.\n* The authors do not provide a clear discussion of the limitations of the model and potential future work.\n\nSuggestions for Improvement:\n\n* Provide a more detailed evaluation of the model's performance on more complex types of videos.\n* Provide a clear comparison with other state-of-the-art models for generative modeling of dynamic 3D visual scenes.\n* Discuss the limitations of the model and potential future work.\n* Provide a more detailed analysis of the model's performance on videos with changing lighting conditions.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which can admit separate control over the camera and the content of the scene. The authors demonstrate that DyST can learn to disentangle camera pose and scene dynamics, and show promising view synthesis and scene control capabilities on real-world videos of dynamic scenes. While the paper has some limitations, it provides a thorough analysis of the model's performance and provides a clear description of the model's architecture and training procedure. Therefore, I recommend accepting the paper for publication in a top-tier conference.\n\n$$$",
        "gr_score_with": 0.9486166007905138,
        "gr_score_without": 0.9172413793103448,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 266,
            "green_fraction": 0.9172413793103448,
            "z_score": 0.9788638331807119,
            "p_value": 0.16382363219324025
        },
        "output_with": {
            "num_tokens_scored": 253,
            "num_green_tokens": 240,
            "green_fraction": 0.9486166007905138,
            "z_score": 2.5777775410902324,
            "p_value": 0.004971899571143526
        }
    },
    {
        "title": "LEARNING MULTI-FACETED PROTOTYPICAL USER INTERESTS",
        "abstract": "We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the granularity of item space to that of user interests, which we postulate is not ideal as user interests would likely map to a small subset of item space. In addition, the compositionality of user interests has received inadequate attention, preventing the modeling of interactions between explanatory factors driving a user\u2019s decision. To resolve this, we propose to align user interests with multi-faceted item characteristics. First, we involve prototype-based representation learning to discover item characteristics along multiple facets. Second, we compose user interests from uncovered item characteristics via binding mechanism, separating the granularity of user preferences from that of item space. Third, we design a dedicated bi-directional binding block, aiding the derivation of compositional user interests. On real-world datasets, the experimental results demonstrate the strong performance of our proposed method compared to a series of baselines.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to address the shortcomings of existing VAE-based disentangled recommendation models, including inadequate item space discovering, same level of granularity between user interests and item space assumption, and improper user interest complexity handling. FACETVAE is characterized by three main innovations: 1) disentangling item space under a multi-faceted manner, 2) binding compositional user interests from low-level ones discovered from item space, and 3) effectively binding user interests via a bi-directional binding block. The method is evaluated on three real-world datasets, and the results show that FACETVAE achieves significantly higher accuracy than existing baselines.\n\nStrengths:\n\n* The paper provides a clear and concise introduction to the problem of learning multi-faceted prototypical user interests.\n* The proposed method, FACETVAE, is well-motivated and addresses the shortcomings of existing VAE-based disentangled recommendation models.\n* The experimental results demonstrate the effectiveness of FACETVAE in achieving higher accuracy than existing baselines.\n* The paper provides a comprehensive analysis of the results, including the efficiency of FACETVAE, multi-faceted item space disentangling, and the interpretability of user's interests.\n\nWeaknesses:\n\n* The paper assumes a uniform distribution of facets underlying item space, which may not be realistic in all cases.\n* The method requires tuning several hyperparameters, including the number of facets and the number of clusters per facet, which may be challenging in practice.\n* The paper does not provide a clear explanation of how the binding block works and how it affects the recommendation accuracy.\n* The results are not always consistent across different datasets and metrics, which may indicate that the method is sensitive to the specific characteristics of the data.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how the binding block works and how it affects the recommendation accuracy.\n* The authors should investigate alternative methods for tuning the hyperparameters, such as using a Bayesian optimization or a gradient-based optimization.\n* The authors should provide more results on the interpretability of user's interests, including a qualitative analysis of the interests produced by FACETVAE.\n* The authors should investigate the robustness of FACETVAE to noisy and missing data, which is a common issue in recommendation systems.\n\nRecommendation:\n\nThe paper proposes a novel and effective method for learning multi-faceted prototypical user interests. While there are some limitations and areas for improvement, the method shows promising results and has the potential to be a useful tool for recommendation systems. Therefore, I recommend accepting the paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to address the shortcomings of existing VAE-based disentangled recommendation models by disentangling item space under a multi-faceted manner, binding compositional user interests from low-level ones discovered from item space, and effectively binding user interests via a bi-directional binding block. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines on three real-world datasets.\n\nStrengths:\n\n1. The paper proposes a novel method that addresses the shortcomings of existing VAE-based disentangled recommendation models.\n2. The method disentangles item space under a multi-faceted manner, which is a novel and interesting approach.\n3. The bi-directional binding block is a clever mechanism that effectively binds user interests from low-level ones.\n4. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines.\n5. The paper provides a thorough analysis of the results and discusses the implications of the findings.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known, which may not be the case in real-world scenarios.\n2. The method requires a large number of hyperparameters to be tuned, which can be time-consuming and may require expert knowledge.\n3. The paper does not provide a clear explanation of how the bi-directional binding block works and how it is related to the low-level user interests.\n4. The experimental results are based on three datasets, and it would be beneficial to see the results on more datasets to ensure the generalizability of the method.\n5. The paper does not provide a clear explanation of how the FACETVAE method can be used in real-world scenarios and how it can be integrated with existing recommendation systems.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how the bi-directional binding block works and how it is related to the low-level user interests.\n2. Develop a method to automatically determine the number of facets and the number of clusters per facet in a data-driven manner.\n3. Provide more experimental results on more datasets to ensure the generalizability of the method.\n4. Provide a clear explanation of how the FACETVAE method can be used in real-world scenarios and how it can be integrated with existing recommendation systems.\n5. Discuss the limitations of the method and how they can be addressed in future work.\n\nRecommendation:\n\nThe paper proposes a novel method that addresses the shortcomings of existing VAE-based disentangled recommendation models. The method disentangles item space under a multi-faceted manner and effectively binds user interests via a bi-directional binding block. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines. However, the paper assumes that the number of facets and the number of clusters per facet are known, which may not be the case in real-world scenarios. Therefore, I recommend that the authors develop a method to automatically determine the number of facets and the number of clusters per facet in a data-driven manner. Additionally, providing more experimental results on more datasets and discussing the limitations of the method would strengthen the paper.\n\n$$$",
        "gr_score_with": 0.9283489096573209,
        "gr_score_without": 0.8897435897435897,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 347,
            "green_fraction": 0.8897435897435897,
            "z_score": -0.6749449205533474,
            "p_value": 0.750144620270218
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 298,
            "green_fraction": 0.9283489096573209,
            "z_score": 1.6932043568414685,
            "p_value": 0.04520828779311806
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of its effectiveness. However, the paper assumes that the input images are already preprocessed, and does not provide a detailed analysis of the computational cost of the Win-Win approach. To improve the paper, the authors should provide a more detailed analysis of the computational cost, and provide a more thorough evaluation of the Win-Win approach on other tasks and datasets. With these improvements, the paper would be a significant contribution to the field of computer vision.\n\nOverall, I would recommend this paper for publication in a top-tier conference, with the suggested improvements.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach is effective when using relative positional embedding such as rotary embeddings.\n* The authors provide a thorough analysis of the strengths and weaknesses of the Win-Win approach.\n\nWeaknesses:\n\n* The paper assumes that the relative positional embedding such as rotary embeddings is used, which may not be the case for all vision transformer models.\n* The paper does not provide a thorough analysis of the trade-off between training speed and accuracy.\n* The paper assumes that the number of windows N is fixed, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the trade-off between training speed and accuracy.\n* The authors should investigate the use of different types of positional embeddings, such as absolute positional embeddings.\n* The authors should provide more details on how the number of windows N is chosen, and how it affects the accuracy of the model.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough analysis of the strengths and weaknesses of the Win-Win approach. However, the paper assumes that the relative positional embedding such as rotary embeddings is used, and does not provide a thorough analysis of the trade-off between training speed and accuracy. Therefore, I recommend that the authors provide a more thorough analysis of the trade-off between training speed and accuracy, and investigate the use of different types of positional embeddings. With these improvements, the paper would be more suitable for publication in a top-tier conference.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.9390681003584229,
        "gr_score_without": 0.9124293785310734,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 323,
            "green_fraction": 0.9124293785310734,
            "z_score": 0.779709328052847,
            "p_value": 0.21778099349902197
        },
        "output_with": {
            "num_tokens_scored": 279,
            "num_green_tokens": 262,
            "green_fraction": 0.9390681003584229,
            "z_score": 2.175363261590621,
            "p_value": 0.014801452651641869
        }
    },
    {
        "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
        "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL. FedCOG consists of two key components: complementary data generation and knowledge-distillation-based model training. The complementary data generation module generates data from the shared global model to complement the original dataset, reducing the heterogeneity level. The knowledge-distillation-based model training module distills knowledge from the global model to the local model based on the generated data, mitigating the effects of heterogeneity. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy. The authors conduct extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n1. The paper addresses a critical issue in FL, data heterogeneity, from a novel perspective of modifying local dataset.\n2. FedCOG is a well-designed algorithm that consists of two key components, complementary data generation and knowledge-distillation-based model training, which are both orthogonal to most existing FL methods.\n3. The authors provide extensive experimental results on classical and real-world FL datasets, demonstrating the effectiveness of FedCOG.\n4. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy.\n\nWeaknesses:\n\n1. The paper does not provide a thorough analysis of the theoretical guarantees of FedCOG.\n2. The authors do not provide a detailed comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The paper assumes a centralized setting, where the global model is shared among all clients, which may not be realistic in many FL scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the theoretical guarantees of FedCOG, including convergence rates and optimality guarantees.\n2. The authors should conduct a more thorough comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The authors should consider a decentralized setting, where the global model is not shared among all clients, and evaluate the performance of FedCOG in such a setting.\n\nRecommendation:\n\nThe paper is well-written, and the proposed algorithm, FedCOG, is innovative and effective. However, the paper lacks a thorough theoretical analysis and a detailed comparison with other state-of-the-art methods. With some improvements, the paper can be considered for publication in a top-tier conference.",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL from the perspective of modifying local dataset. FedCOG consists of two key components: complementary data generation to reduce heterogeneity level and knowledge-distillation-based model training to mitigate the effects of heterogeneity. The algorithm generates data to complement the original dataset for each client, which is achieved by inversely optimizing inputs given the global and local model. The generated dataset is then utilized for the process of local model training, with the optimization objective defined as a balance between task-driven loss and knowledge distillation loss. The authors demonstrate the effectiveness of FedCOG through extensive experiments on classical and real-world FL datasets, showing that it consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n*   The paper proposes a novel approach to tackle data heterogeneity in FL from the perspective of modifying local dataset.\n*   The algorithm is plug-and-play in most existing FL methods and is compatible with standard FL protocol such as Secure Aggregation.\n*   The authors demonstrate the effectiveness of FedCOG through extensive experiments on classical and real-world FL datasets.\n*   The paper provides a comprehensive analysis of the algorithm, including its advantages and limitations.\n\nWeaknesses:\n\n*   The paper assumes a fixed number of clients and rounds, which may not be realistic in many scenarios.\n*   The algorithm may not be suitable for very large-scale FL systems, as the computation overhead may be too high.\n*   The paper does not provide a detailed analysis of the computational complexity of the algorithm.\n*   The authors do not provide a comparison with other state-of-the-art methods in terms of computational efficiency.\n\nSuggestions for Improvement:\n\n*   The authors should provide a more detailed analysis of the computational complexity of the algorithm and compare it with other state-of-the-art methods.\n*   The paper should assume a more realistic number of clients and rounds, and provide an analysis of the algorithm's performance under these conditions.\n*   The authors should provide a comparison with other state-of-the-art methods in terms of computational efficiency.\n*   The paper should provide a more detailed analysis of the algorithm's advantages and limitations, and discuss potential applications and future work.\n\nRecommendation:\n\nThe paper proposes a novel approach to tackle data heterogeneity in FL from the perspective of modifying local dataset, and demonstrates its effectiveness through extensive experiments on classical and real-world FL datasets. However, the paper has some limitations, such as assuming a fixed number of clients and rounds, and not providing a detailed analysis of the computational complexity of the algorithm. Therefore, I recommend the paper for publication in a conference with a high acceptance rate, but suggest that the authors address these limitations in their final version.\n\n$$",
        "gr_score_with": 0.9230769230769231,
        "gr_score_without": 0.8615384615384616,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 280,
            "green_fraction": 0.8615384615384616,
            "z_score": -2.3110313700609444,
            "p_value": 0.9895844388899724
        },
        "output_with": {
            "num_tokens_scored": 351,
            "num_green_tokens": 324,
            "green_fraction": 0.9230769230769231,
            "z_score": 1.4413281726264027,
            "p_value": 0.0747459958912584
        }
    },
    {
        "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
        "abstract": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework, GIM, for learning generalizable image matching models using self-training on diverse internet videos. GIM first trains a model on standard domain-specific datasets and then uses the trained model to generate dense labels on nearby frames of downloaded videos. The final model is trained on propagated data with strong augmentations. The authors also introduce a new zero-shot evaluation benchmark, ZEB, consisting of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks.\n\nStrengths:\n1. The paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos.\n2. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments.\n3. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks.\n4. The authors provide a detailed analysis of the GIM framework and its components, including the self-training process, label propagation, and strong data augmentation.\n\nWeaknesses:\n1. The paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios.\n2. The authors do not provide a thorough analysis of the limitations and potential failures of the GIM framework.\n3. The paper does not discuss the potential issues with data quality and consistency across different internet videos.\n4. The authors do not provide a clear comparison of the computational efficiency of GIM with other state-of-the-art image matching methods.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the limitations and potential failures of the GIM framework.\n2. Discuss the potential issues with data quality and consistency across different internet videos.\n3. Compare the computational efficiency of GIM with other state-of-the-art image matching methods.\n4. Provide more information on the hyperparameters and training settings used for the GIM framework.\n\nRecommendation:\nThe paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks. However, the paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios. Therefore, I recommend that the authors provide a more detailed analysis of the limitations and potential failures of the GIM framework, discuss the potential issues with data quality and consistency across different internet videos, and compare the computational efficiency of GIM with other state-of-the-art image matching methods.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes GIM, a self-training framework that learns generalizable image matching from internet videos. GIM first trains an image matching architecture on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. The labels are filtered by robust fitting and enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. The authors also propose ZEB, the first zero-shot evaluation benchmark for image matching, which consists of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks.\n\nStrengths:\n1. The paper addresses a significant problem in image matching, which is the limited scalability of existing data construction pipelines and the need for domain-specific training.\n2. The proposed GIM framework is novel and efficient, using internet videos as a data source, which is abundant and diverse.\n3. The authors provide a thorough evaluation of the framework using the proposed ZEB benchmark, which is a significant contribution to the field.\n4. The paper demonstrates the effectiveness of GIM for various downstream tasks, such as visual localization and 3D reconstruction.\n\nWeaknesses:\n1. The paper relies heavily on the availability of internet videos, which may not always be accessible or diverse enough.\n2. The authors do not provide a thorough analysis of the computational cost of the framework, which may be a limitation for large-scale applications.\n3. The paper assumes that the internet videos are labeled, which may not always be the case.\n4. The authors do not provide a clear explanation of how the ZEB benchmark is constructed and evaluated.\n\nSuggestions for Improvement:\n1. The authors should provide more details on how the ZEB benchmark is constructed and evaluated, including the process of sampling image pairs and computing the AUC of the relative pose error.\n2. The paper should provide a more thorough analysis of the computational cost of the framework, including the time and resources required for training and testing.\n3. The authors should investigate the use of other data sources, such as RGBD scans or 3D reconstruction data, to complement the internet videos.\n4. The paper should provide more details on how the GIM framework can be applied to real-world applications, including the potential challenges and limitations.\n\nRecommendation:\nThe paper is well-written and provides a significant contribution to the field of image matching. However, the authors should address the weaknesses and suggestions for improvement to make the paper more comprehensive and impactful. I would recommend this paper for publication in a top-tier conference, but with the condition that the authors address the suggested improvements.\n\nRating: 7.5/10\n$$$",
        "gr_score_with": 0.9292682926829269,
        "gr_score_without": 0.889261744966443,
        "output_without": {
            "num_tokens_scored": 298,
            "num_green_tokens": 265,
            "green_fraction": 0.889261744966443,
            "z_score": -0.6177153866656149,
            "p_value": 0.7316185162379348
        },
        "output_with": {
            "num_tokens_scored": 410,
            "num_green_tokens": 381,
            "green_fraction": 0.9292682926829269,
            "z_score": 1.9756423093120623,
            "p_value": 0.02409765157934451
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method employs a single pair of frozen random matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings, making it suitable for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n\nStrengths:\n\n1. VeRA significantly reduces the number of trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable results to LoRA on various benchmarks, including GLUE, E2E, and image classification tasks.\n3. VeRA is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The paper provides a thorough analysis of the method's components, including the impact of different initializations and the magnitude of adaptation.\n\nWeaknesses:\n\n1. The paper assumes that the frozen random matrices can be regenerated from a random number generator (RNG) seed, which may not be feasible in all scenarios.\n2. The method's performance may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques.\n3. The paper focuses on language and vision models with Transformer architecture, and its applicability to other architectures and domains remains an area for future research.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of regenerating frozen random matrices from an RNG seed in various scenarios.\n2. Explore additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques, to improve the method's performance.\n3. Conduct experiments on other architectures and domains to evaluate VeRA's applicability and performance.\n\nRecommendation:\n\nVeRA is a promising parameter-efficient finetuning method that achieves comparable results to LoRA while reducing the number of trainable parameters. The method's memory savings make it suitable for scenarios that require frequent swapping of numerous finetuned models. However, further research is needed to explore the method's limitations and potential areas for improvement. I recommend accepting this paper for publication in a reputable conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method while maintaining comparable results. The method employs a single pair of low-rank matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings and is particularly well-suited for scenarios that require frequent swapping of numerous finetuned models.\n\nStrengths:\n\n1. VeRA achieves a significant reduction in trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method is shown to be effective on various downstream tasks, including language understanding and image classification.\n3. VeRA is particularly well-suited for scenarios that require frequent swapping of numerous finetuned models.\n4. The paper provides a comprehensive comparison with other parameter-efficient adaptation methods.\n\nWeaknesses:\n\n1. The paper assumes that the random frozen matrices can be regenerated from a random number generator (RNG) seed, which may not always be the case.\n2. The method relies on the choice of the initial values for the scaling vectors, which may impact performance.\n3. The paper does not provide a thorough analysis of the trade-offs between trainable parameters and accuracy.\n4. The ablation study is limited to a few experiments and does not provide a comprehensive understanding of the method's components.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough analysis of the trade-offs between trainable parameters and accuracy.\n2. Conduct a more comprehensive ablation study to understand the impact of different components of the method.\n3. Investigate the impact of different initialization schemes for the scaling vectors.\n4. Provide more details on the implementation of VeRA and how it can be integrated into existing frameworks.\n\nRecommendation:\n\nThis paper presents a novel and effective parameter-efficient finetuning method for large language models. While there are some limitations to the paper, the method shows promising results and has the potential to be widely adopted. I recommend this paper for publication in a reputable conference or journal.\n\nOverall, the paper is well-written and easy to follow. The authors provide a clear and concise overview of the method and its components. The experiments are thorough and provide a good comparison with other parameter-efficient adaptation methods. However, there are some areas for improvement, and I would like to see more comprehensive analysis and ablation studies in future work.\n\nScore: 8/10\n\nRecommendation: Accept",
        "gr_score_with": 0.936,
        "gr_score_without": 0.8977272727272727,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 316,
            "green_fraction": 0.8977272727272727,
            "z_score": -0.1419367996132358,
            "p_value": 0.556435036661808
        },
        "output_with": {
            "num_tokens_scored": 375,
            "num_green_tokens": 351,
            "green_fraction": 0.936,
            "z_score": 2.3239591105410846,
            "p_value": 0.010063842829889898
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors examine two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis, and (2) bounds that may also depend on the learning algorithm. They prove mathematically that no generalization bound of the first type can be uniformly tight in the overparameterized setting, and establish a trade-off between learning performance and estimability for the second type of bounds.\n\nStrengths:\n\n* The paper provides a rigorous and formal analysis of the notion of uniformly tight generalization bounds.\n* The authors introduce a framework for understanding the limitations of generalization bounds in the overparameterized setting.\n* The paper presents several theorems and lemmas that provide a deeper understanding of the relationships between learning performance, estimability, and generalization bounds.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and theoretical computer science.\n* The authors do not provide a comprehensive review of existing generalization bounds and their limitations, which may make it difficult for readers to understand the significance of their results.\n* The paper's conclusions and implications may be subject to interpretation, and the authors acknowledge that different scholarly positions exist on this matter.\n\nSuggestions for Improvement:\n\n* The authors could provide a more accessible introduction to the mathematical framework and notation used in the paper.\n* The paper could benefit from a more comprehensive review of existing generalization bounds and their limitations, which would provide context for the authors' results.\n* The authors could provide more concrete examples and illustrations to demonstrate the implications of their results and make the paper more engaging for readers.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of machine learning and theoretical computer science by shedding light on the limitations of generalization bounds in the overparameterized setting. The authors' formal analysis and rigorous proofs provide a solid foundation for understanding the relationships between learning performance, estimability, and generalization bounds. However, the paper's assumptions and conclusions may be subject to interpretation, and the authors could benefit from providing a more comprehensive review of existing generalization bounds and their limitations. Overall, the paper is well-written and well-structured, and it is a valuable contribution to the field.\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research:\n\n* Investigate the implications of the authors' results for existing generalization bounds and their limitations.\n* Explore the relationships between learning performance, estimability, and generalization bounds in more detail.\n* Develop new generalization bounds that take into account the limitations and trade-offs identified by the authors.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper studies the concept of generalization bounds in the context of neural networks, specifically in the overparameterized setting. The authors investigate whether it is possible to obtain generalization bounds that are uniformly tight, meaning that they are tight for all learning algorithms and population distributions. The paper provides two main results: (1) a negative result showing that algorithm-independent generalization bounds cannot be uniformly tight in the overparameterized setting, and (2) a trade-off between learnability and estimability, which implies that algorithm-dependent generalization bounds are also limited.\n\nStrengths:\n\n* The paper provides a comprehensive and rigorous treatment of the concept of generalization bounds in the overparameterized setting.\n* The authors present two main results, which are well-motivated and have important implications for the field of machine learning.\n* The paper includes a thorough discussion of related work and provides a clear overview of the current state of the art.\n* The authors provide a concrete example of a hypothesis class and a set of algorithms to illustrate their results.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and statistics.\n* The authors do not provide a clear explanation of why the overparameterized setting is relevant to neural networks, which may make the paper difficult to understand for readers without a strong background in neural networks.\n* The paper does not discuss the implications of their results for the design of neural networks, which may limit the paper's impact.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the overparameterized setting and its relevance to neural networks.\n* Consider adding more concrete examples or illustrations to help readers understand the results.\n* Discuss the implications of the results for the design of neural networks, such as the choice of architecture or hyperparameters.\n* Consider providing a more detailed discussion of the limitations of the paper, such as the assumptions made about the hypothesis class and the algorithms.\n\nRecommendation:\n\nThe paper is well-written and provides a comprehensive treatment of the concept of generalization bounds in the overparameterized setting. The authors present two main results, which are well-motivated and have important implications for the field of machine learning. However, the paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and statistics. Therefore, I recommend this paper for readers with a strong background in machine learning and statistics, but with the caveat that it may be challenging for readers without a strong background in the field.\n\nRecommendation: Accept\n\nRating: 8/10\n\nReviewer: [Your Name]\n\nDate: [Today's Date]\n\n$$$",
        "gr_score_with": 0.9317507418397626,
        "gr_score_without": 0.914364640883978,
        "output_without": {
            "num_tokens_scored": 362,
            "num_green_tokens": 331,
            "green_fraction": 0.914364640883978,
            "z_score": 0.9112049389111346,
            "p_value": 0.181093701116102
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 314,
            "green_fraction": 0.9317507418397626,
            "z_score": 1.9430510450725738,
            "p_value": 0.026004995562295772
        }
    },
    {
        "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
        "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that enables large language model (LLM)-based embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three essential functions: multimodal perception, foundational knowledge base, and skill prediction and planning. The authors construct an extensive dataset of 850K open-world instruction pairs to train Steve-Eye and develop three open-world evaluation benchmarks to validate the model's capability to strategically act and plan.\n\nStrengths:\n\n1. The paper presents a novel approach to multimodal interaction in open-world environments, addressing the limitations of text-only agents.\n2. The proposed model, Steve-Eye, demonstrates improved performance in multimodal perception, foundational knowledge question answering, and skill prediction and planning.\n3. The authors provide a comprehensive evaluation of the model's performance on three open-world benchmarks, including environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of pre-trained LLMs and visual encoders, which may limit the model's ability to generalize to new tasks and environments.\n2. The dataset collection process is labor-intensive and requires significant human annotation effort, which may not be scalable for larger datasets.\n3. The paper does not provide a thorough analysis of the model's limitations and failure cases, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more robust and generalizable pre-trained models that can handle a wide range of tasks and environments.\n2. Explore more efficient and scalable methods for dataset collection, such as active learning or transfer learning.\n3. Conduct a more comprehensive analysis of the model's limitations and failure cases to better understand its robustness and reliability.\n\nRecommendation:\n\nThis paper presents a novel and promising approach to multimodal interaction in open-world environments. While there are some limitations and areas for improvement, the proposed model demonstrates improved performance on three open-world benchmarks. I recommend this paper for publication in a reputable conference, with the understanding that the authors will address the limitations and suggestions for improvement outlined above.\n\nRating: 7.5/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that enables large language model (LLM)-based embodied agents to engage with the open world via visual-text interfaces. Steve-Eye is equipped with three indispensable functions: (1) multimodal perception; (2) foundational knowledge base; and (3) skill prediction and planning. The authors develop a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs and adopt a two-stage strategy to train the model. Experiments on three open-world benchmarks demonstrate that Steve-Eye outperforms LLM-based agents in open-world setups.\n\nStrengths:\n\n1. The paper proposes a novel approach to enable LLM-based agents to engage with the open world via visual-text interfaces, which is a significant contribution to the field of embodied AI.\n2. The authors develop a semi-automatic strategy to collect a large-scale dataset, which is essential for training a multimodal model.\n3. The paper presents a comprehensive evaluation of the proposed model on three open-world benchmarks, demonstrating its advantages over a wide range of perspectives.\n\nWeaknesses:\n\n1. The paper assumes that the visual encoder is pre-trained, which may not be the case in practice. The authors should provide more details on how to pre-train the visual encoder.\n2. The paper does not provide a thorough comparison with other state-of-the-art multimodal models, which makes it difficult to evaluate the novelty and effectiveness of the proposed approach.\n3. The paper mentions that the model still falls short in fine-grained reasoning tasks, which requires further investigation and improvement.\n\nSuggestions for Improvement:\n\n1. Provide more details on how to pre-train the visual encoder and its impact on the performance of the proposed model.\n2. Conduct a more thorough comparison with other state-of-the-art multimodal models to evaluate the novelty and effectiveness of the proposed approach.\n3. Investigate and improve the fine-grained reasoning capabilities of the proposed model.\n\nRecommendation:\n\nThe paper presents a novel approach to enable LLM-based agents to engage with the open world via visual-text interfaces. While there are some limitations and areas for improvement, the paper demonstrates the potential of the proposed approach and provides a comprehensive evaluation on three open-world benchmarks. With further investigation and improvement, the proposed model has the potential to become a state-of-the-art multimodal model for embodied AI applications.\n\n$$$",
        "gr_score_with": 0.95625,
        "gr_score_without": 0.933933933933934,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 311,
            "green_fraction": 0.933933933933934,
            "z_score": 2.0642817122619426,
            "p_value": 0.019495506958654468
        },
        "output_with": {
            "num_tokens_scored": 320,
            "num_green_tokens": 306,
            "green_fraction": 0.95625,
            "z_score": 3.35424144011788,
            "p_value": 0.00039791444925883567
        }
    },
    {
        "title": "PERCEPTUAL GROUP TOKENIZER: BUILDING PERCEPTION WITH ITERATIVE GROUPING",
        "abstract": "Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping (Palmer, 2002; Wagemans et al., 2012; Herzog, 2018). Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-training, and interpretability. Specifically, Perceptual Group Tokenizer achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear probe evaluation, establishing a new milestone for this paradigm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT), which entirely relies on perceptual grouping principles to extract visual features and perform self-supervised representation learning. The model consists of a series of grouping operations that iteratively hypothesize the context for pixels or superpixels to refine feature representations. The authors show that PGT achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles.\n2. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks.\n3. The authors provide a thorough analysis of the model's properties, including adaptive computation and high model interpretability.\n4. The paper provides a clear and concise explanation of the model's architecture and training procedure.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision.\n2. The model's performance on other tasks, such as object detection and segmentation, is not evaluated.\n3. The paper does not provide a thorough comparison with other state-of-the-art models, such as ViT and ResNet.\n4. The model's computational cost is relatively high due to the iterative grouping processes.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision.\n2. Evaluate the model's performance on other tasks, such as object detection and segmentation.\n3. Compare the model's performance with other state-of-the-art models, such as ViT and ResNet.\n4. Explore ways to reduce the model's computational cost, such as using more efficient grouping operations.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability. However, the paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision, and the model's performance on other tasks is not evaluated. Therefore, I recommend that the authors provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision, evaluate the model's performance on other tasks, and compare the model's performance with other state-of-the-art models.\n\nScore: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT) that relies entirely on perceptual grouping principles for self-supervised representation learning. PGT achieves competitive performance on the ImageNet-1K benchmark and has desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper presents a novel approach to visual recognition that is based on perceptual grouping principles, which is a promising direction for future research.\n2. The proposed model, PGT, shows strong performance on the ImageNet-1K benchmark with linear probe evaluation, which is a challenging task.\n3. The paper highlights the desirable properties of PGT, including adaptive computation and high model interpretability, which make it a valuable contribution to the field.\n4. The authors provide a thorough analysis of the design space of PGT and explore various ablation studies, which provides valuable insights into the model's behavior.\n\nWeaknesses:\n1. The paper assumes that the input image is divided into small patches, which may not be the most efficient way to represent images, especially for large images.\n2. The iterative grouping process in PGT can be computationally expensive, which may limit its practical applications.\n3. The paper does not provide a thorough comparison with other state-of-the-art models, which makes it difficult to evaluate the performance of PGT in a broader context.\n4. The paper assumes that the grouping operation can automatically learn and emerge both pairwise and higher-order information exchange, which is a strong assumption that may not always hold.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough comparison with other state-of-the-art models, including those that use self-attention mechanisms.\n2. The paper should provide a more detailed analysis of the computational cost of the iterative grouping process and explore ways to reduce it.\n3. The authors should provide more insights into the limitations of the proposed model and how it can be improved.\n4. The paper should provide more experimental results on downstream tasks, such as object detection and segmentation, to evaluate the generalizability of the proposed model.\n\nRecommendation:\nThe paper presents a novel approach to visual recognition that is based on perceptual grouping principles, which is a promising direction for future research. The proposed model, PGT, shows strong performance on the ImageNet-1K benchmark and has desirable properties such as adaptive computation and high model interpretability. However, the paper assumes that the input image is divided into small patches, which may not be the most efficient way to represent images, and the iterative grouping process can be computationally expensive. Therefore, I recommend that the authors provide a more thorough comparison with other state-of-the-art models and explore ways to reduce the computational cost of the iterative grouping process.\n\nOverall, I believe that the paper presents a valuable contribution to the field of visual recognition and provides a promising direction for future research.",
        "gr_score_with": 0.9292035398230089,
        "gr_score_without": 0.9071428571428571,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 254,
            "green_fraction": 0.9071428571428571,
            "z_score": 0.39857796800869966,
            "p_value": 0.3451020991036301
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 315,
            "green_fraction": 0.9292035398230089,
            "z_score": 1.7924805365131784,
            "p_value": 0.03652801210510132
        }
    },
    {
        "title": "FOSI: Hybrid First and Second Order Optimization",
        "abstract": "Popular machine learning approaches forgo second-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any base first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We formally analyze FOSI\u2019s convergence and the conditions under which it improves a base optimizer. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods such as Heavy-Ball and Adam, and outperforms second-order methods (K-FAC and L-BFGS).",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that combines a first-order base optimizer with Newton's method to improve the convergence of any base first-order optimizer. FOSI iteratively splits the optimization problem into two quadratic problems on orthogonal subspaces, uses Newton's method to minimize one, and the base optimizer to minimize the other. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems. The paper formally analyzes FOSI's convergence and the conditions under which it improves a base optimizer. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nStrengths:\n\n1. FOSI is a hybrid optimization algorithm that combines the benefits of first-order and second-order methods, making it a robust and efficient optimization technique.\n2. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems.\n3. The paper provides a thorough analysis of FOSI's convergence and the conditions under which it improves a base optimizer.\n4. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nWeaknesses:\n\n1. The algorithm requires the number of Lanczos iterations (m) to be set, which can be a hyperparameter that needs to be tuned.\n2. The paper assumes that the Hessian is symmetric and positive definite, which may not be the case in all optimization problems.\n3. The algorithm requires the use of a base first-order optimizer, which may not be the best choice for all optimization problems.\n4. The paper does not provide a thorough analysis of the computational complexity of FOSI.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically tuning the number of Lanczos iterations (m) and other hyperparameters of FOSI.\n2. Consider relaxing the assumption that the Hessian is symmetric and positive definite to make FOSI more applicable to a wider range of optimization problems.\n3. Investigate the use of other base first-order optimizers, such as stochastic gradient descent, to see if they can be improved by FOSI.\n4. Provide a thorough analysis of the computational complexity of FOSI and compare it to other optimization algorithms.\n\nRecommendation:\n\nFOSI is a promising optimization algorithm that combines the benefits of first-order and second-order methods. While there are some limitations and areas for improvement, the algorithm shows great potential for improving the convergence rate and optimization time of first-order methods and outperforming second-order methods on various deep neural network training tasks. Therefore, I recommend accepting this paper for publication in a top-tier conference or journal.\n\nRating: 8/10\n\nReviewer's Confidence: 9/10\n\nReviewer's Expertise: Optimization algorithms, machine learning, deep learning.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that improves the performance of any base first-order optimizer by incorporating second-order information. FOSI iteratively splits the optimization problem into pairs of quadratic problems on orthogonal subspaces and uses a second-order method to minimize one and the base optimizer to minimize the other. The paper provides a detailed description of the FOSI algorithm, a thorough spectral analysis of its preconditioner, and convergence guarantees in the stochastic setting. The empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods such as Heavy-Ball and Adam, and outperforms second-order methods (K-FAC and L-BFGS).\n\nStrengths:\n\n1. FOSI is a hybrid optimization algorithm that improves the performance of any base first-order optimizer by incorporating second-order information.\n2. The paper provides a detailed description of the FOSI algorithm and a thorough spectral analysis of its preconditioner.\n3. The empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods.\n4. The paper provides convergence guarantees in the stochastic setting, which is a significant contribution.\n\nWeaknesses:\n\n1. The paper assumes that the Hessian of the objective function is available, which may not be the case in practice.\n2. The paper does not provide a detailed analysis of the computational complexity of FOSI.\n3. The paper assumes that the ESE procedure provides accurate estimates of the extreme eigenvalues and eigenvectors of the Hessian, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a detailed analysis of the computational complexity of FOSI.\n2. Investigate the robustness of FOSI to inaccurate estimates of the extreme eigenvalues and eigenvectors of the Hessian.\n3. Provide more experimental results to demonstrate the effectiveness of FOSI on a wider range of optimization problems.\n4. Investigate the use of FOSI in conjunction with other optimization algorithms to further improve its performance.\n\nRecommendation:\n\nThe paper proposes a novel and effective optimization algorithm that improves the performance of first-order methods by incorporating second-order information. The empirical evaluation demonstrates the effectiveness of FOSI, and the convergence guarantees in the stochastic setting are a significant contribution. However, the paper assumes that the Hessian of the objective function is available, and the computational complexity of FOSI is not analyzed. Therefore, I recommend that the authors provide a detailed analysis of the computational complexity of FOSI and investigate the robustness of FOSI to inaccurate estimates of the extreme eigenvalues and eigenvectors of the Hessian.\n\nOverall, the paper is well-written, and the results are impressive. However, to make the paper more comprehensive, the authors should address the limitations and suggestions for improvement mentioned above.\n\n$$$\nNote: The reviewer has carefully read the paper and provided a detailed review. However, the review is not perfect, and the reviewer may have missed some important details. The author should carefully read the review and address any concerns or suggestions for improvement.",
        "gr_score_with": 0.9532163742690059,
        "gr_score_without": 0.9260273972602739,
        "output_without": {
            "num_tokens_scored": 365,
            "num_green_tokens": 338,
            "green_fraction": 0.9260273972602739,
            "z_score": 1.6576847239185242,
            "p_value": 0.04869055937555084
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 326,
            "green_fraction": 0.9532163742690059,
            "z_score": 3.280624407952807,
            "p_value": 0.0005178878691633204
        }
    },
    {
        "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n* The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a promising direction for future research.\n* The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings.\n* The paper provides a comprehensive evaluation of GR-1, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nWeaknesses:\n\n* The paper relies heavily on pre-training on a large-scale video dataset, which may not be feasible for all applications or domains.\n* The authors do not provide a thorough analysis of the trade-offs between pre-training and finetuning, which may be important for practical applications.\n* The paper assumes access to a large-scale video dataset, which may not be available for all researchers or applications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the trade-offs between pre-training and finetuning, including the impact on performance, computational resources, and data requirements.\n* Explore alternative approaches to pre-training, such as using smaller datasets or different types of data, to make the method more widely applicable.\n* Provide more information about the scalability of GR-1, including the computational resources required for training and inference.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to visual robot manipulation, and the authors demonstrate its effectiveness on challenging benchmarks. However, the paper relies heavily on pre-training on a large-scale video dataset, and the trade-offs between pre-training and finetuning are not thoroughly analyzed. To make the method more widely applicable and scalable, the authors should explore alternative approaches to pre-training and provide a more detailed analysis of the trade-offs between pre-training and finetuning. With these improvements, the paper has the potential to make a significant impact in the field of visual robot manipulation.\n\nRating: 7/10\n\nRecommendation for Publication: Accept with revisions.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n* The paper presents a novel approach to visual robot manipulation that leverages large-scale video generative pre-training.\n* The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot.\n* The paper provides extensive experiments and quantitative results to support the claims.\n* The authors discuss the potential of GR-1 in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nWeaknesses:\n\n* The paper assumes access to a large-scale video dataset, which may not be available in all scenarios.\n* The authors do not provide a detailed analysis of the video prediction performance of GR-1.\n* The paper does not discuss the potential limitations of GR-1, such as its sensitivity to noise or its performance in complex scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the video prediction performance of GR-1.\n* The paper should discuss the potential limitations of GR-1 and provide suggestions for future work to address these limitations.\n* The authors should provide more information about the Ego4D dataset and the CALVIN benchmark, including the size of the dataset and the complexity of the tasks.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to visual robot manipulation that leverages large-scale video generative pre-training. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, and provide extensive experiments and quantitative results to support the claims. However, the paper assumes access to a large-scale video dataset, and the authors do not provide a detailed analysis of the video prediction performance of GR-1. Therefore, the paper is suitable for publication in a top-tier conference, but the authors should address the limitations mentioned above to make the paper more comprehensive.\n\nRating: 8.5/10\n\nRecommendation for publication: Yes\n\n$$$",
        "gr_score_with": 0.9455128205128205,
        "gr_score_without": 0.9262536873156342,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 314,
            "green_fraction": 0.9262536873156342,
            "z_score": 1.6114412303707903,
            "p_value": 0.05354179341554227
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 295,
            "green_fraction": 0.9455128205128205,
            "z_score": 2.67986879619925,
            "p_value": 0.0036825511027531012
        }
    },
    {
        "title": "DOUBLY ROBUST INSTANCE-REWEIGHTED ADVERSARIAL TRAINING",
        "abstract": "Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which combines distributionally robust optimization (DRO) and bilevel optimization to determine instance weights for adversarial training in a theoretically grounded way. The framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples. The authors also develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem.\n\nStrengths:\n\n* The paper provides a theoretically grounded approach to determine instance weights for adversarial training, which is a significant improvement over existing heuristics-based methods.\n* The proposed framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples.\n* The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem.\n* The paper provides experimental results on standard benchmarks, demonstrating the effectiveness of the proposed framework.\n\nWeaknesses:\n\n* The paper assumes that the attack loss function \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice.\n* The authors use a log-barrier penalty method to penalize the linear constraint into the attack objective, which may not be effective in all cases.\n* The paper does not provide a comprehensive comparison with other existing methods, such as robust overfitting mitigation methods.\n* The experimental results are limited to standard benchmarks and do not provide a thorough analysis of the robustness of the proposed framework against different types of attacks.\n\nSuggestions for Improvement:\n\n* Provide a more comprehensive comparison with other existing methods, including robust overfitting mitigation methods.\n* Analyze the robustness of the proposed framework against different types of attacks, such as white-box and black-box attacks.\n* Provide more experimental results on other benchmarks and datasets to demonstrate the effectiveness of the proposed framework.\n* Consider using more robust optimization techniques, such as robust gradient descent, to improve the stability of the optimization process.\n\nRecommendation:\n\nThe paper provides a theoretically grounded approach to determine instance weights for adversarial training, which is a significant improvement over existing heuristics-based methods. However, the paper assumes that the attack loss function \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice. Additionally, the authors use a log-barrier penalty method to penalize the linear constraint into the attack objective, which may not be effective in all cases. Therefore, I recommend that the authors provide a more comprehensive comparison with other existing methods, analyze the robustness of the proposed framework against different types of attacks, and provide more experimental results on other benchmarks and datasets to demonstrate the effectiveness of the proposed framework.\n\nOverall, the paper provides a promising approach to adversarial training, and with further improvements, it has the potential to become a leading method in the field.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which combines distributionally robust optimization (DRO) and bilevel optimization to determine instance weights for adversarial training in a theoretically grounded way. The framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples. The authors also develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values. The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be of great interest to the optimization community.\n\nStrengths:\n\n1. The paper proposes a novel and theoretically grounded approach to instance reweighted adversarial training, which addresses the non-uniform issues of traditional adversarial training.\n2. The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values, which provides a theoretical guarantee for the proposed framework.\n3. The paper provides extensive experimental evaluations on standard benchmarks, which demonstrate the effectiveness of the proposed framework.\n4. The authors provide a comprehensive review of related work and provide a clear explanation of the proposed framework and its benefits.\n\nWeaknesses:\n\n1. The paper assumes that the adversarial loss function is twice differentiable, which may not be the case in practice.\n2. The authors assume that the inner level problem has a unique minimizer, which may not be the case in practice.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed algorithm.\n4. The authors do not provide a comparison with other instance reweighted adversarial training methods that do not use DRO.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the assumptions made in the paper, and provide a more robust analysis of the proposed framework.\n2. The paper should provide a more detailed comparison with other instance reweighted adversarial training methods that do not use DRO.\n3. The authors should provide a more detailed analysis of the computational complexity of the proposed algorithm, and provide a more efficient implementation.\n4. The paper should provide a more detailed analysis of the experimental evaluations, and provide more insights into the benefits of the proposed framework.\n\nRecommendation:\n\nThe paper proposes a novel and theoretically grounded approach to instance reweighted adversarial training, which addresses the non-uniform issues of traditional adversarial training. The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values, which provides a theoretical guarantee for the proposed framework. The paper provides extensive experimental evaluations on standard benchmarks, which demonstrate the effectiveness of the proposed framework. However, the paper assumes that the adversarial loss function is twice differentiable, which may not be the case in practice. The authors should provide a more detailed analysis of the assumptions made in the paper, and provide a more robust analysis of the proposed framework. Overall, the paper is well-written and provides a comprehensive review of related work. I recommend the paper for publication in a top-tier conference.\n\n$$$",
        "gr_score_with": 0.9652777777777778,
        "gr_score_without": 0.9278996865203761,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 296,
            "green_fraction": 0.9278996865203761,
            "z_score": 1.6611770998290096,
            "p_value": 0.04833894169778824
        },
        "output_with": {
            "num_tokens_scored": 288,
            "num_green_tokens": 278,
            "green_fraction": 0.9652777777777778,
            "z_score": 3.6927939841658337,
            "p_value": 0.00011090185113074415
        }
    },
    {
        "title": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS",
        "abstract": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models\u2019 (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to automatically generate rich simulation environments and expert demonstrations for robotic policy training. The framework operates in two distinct modes: goal-directed generation, where a target task is given to the LLM, and exploratory generation, where the LLM bootstraps from previous tasks to propose novel tasks. The authors demonstrate that LLMs can generate high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies. They evaluate several LLMs, including finetuned GPTs and Code Llama, on code generation for robotic simulation tasks and show that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities by 25%.\n\nStrengths:\n\n* The paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can augment diverse simulation tasks for robotic policy training.\n* The authors demonstrate the effectiveness of their framework in generating high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies.\n* The paper provides a thorough evaluation of several LLMs on code generation for robotic simulation tasks and shows that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities.\n\nWeaknesses:\n\n* The paper relies heavily on a single LLM model, GPT4, and does not explore the robustness of their framework to different LLM models.\n* The evaluation of the framework is limited to a specific task domain (table-top pick-and-place) and does not consider more complex robotic tasks.\n* The paper does not provide a thorough analysis of the limitations of their framework, such as the potential for hallucinations and lack of grounding in physical and geometric details.\n\nSuggestions for Improvement:\n\n* To improve the robustness of the framework, the authors should explore the use of multiple LLM models and evaluate their framework on different task domains.\n* The authors should provide a more thorough analysis of the limitations of their framework, including the potential for hallucinations and lack of grounding in physical and geometric details.\n* The paper should include a more detailed discussion of the potential applications of their framework, such as generating dexterous and complex robotic tasks.\n\nRecommendation:\n\nThe paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can enhance task-level generalization of robotic policies. While the paper has some limitations, it provides a thorough evaluation of their framework and demonstrates its effectiveness in generating high-quality, achievable, and diverse tasks. Therefore, I recommend this paper for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to automatically generate rich simulation environments and expert demonstrations for robotic policy training. The framework operates in two distinct modes: goal-directed generation, where a target task is given to the LLM, and exploratory generation, where the LLM bootstraps from previous tasks to propose novel tasks. The authors demonstrate that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. They also show that pretraining on GPT4-generated simulation tasks exhibits stronger transfer to unseen long-horizon tasks in the real world and outperforms baselines by 25%.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to generating simulation tasks using LLMs, which has the potential to revolutionize robotic policy training.\n2. The authors provide a thorough evaluation of their framework, demonstrating its effectiveness in generating high-quality tasks and improving policy generalization.\n3. The paper highlights the potential of LLMs in scaling simulation tasks and incorporating human feedback, making the task generation process more efficient and scalable.\n\nWeaknesses:\n1. The paper relies heavily on the capabilities of LLMs, which may not be universally applicable to all robotic tasks and environments.\n2. The evaluation metrics used in the paper may not capture all aspects of task generation, such as physical and geometric details, which could lead to hallucinations and basic syntax errors in the generated code.\n3. The paper only explores table-top pick-and-place task generation, and generating dexterous and complex robotic tasks could be more challenging.\n\nSuggestions for Improvement:\n1. Investigate the limitations of LLMs in generating simulation tasks and explore ways to improve their capabilities, such as incorporating domain knowledge and physical constraints.\n2. Develop more comprehensive evaluation metrics that capture the nuances of task generation, including physical and geometric details.\n3. Expand the scope of the paper to include more complex robotic tasks, such as dexterous manipulation and assembly.\n\nRecommendation:\nThe paper presents a novel and innovative approach to generating simulation tasks using LLMs, which has the potential to revolutionize robotic policy training. However, the authors should address the limitations and weaknesses of their framework to make it more robust and applicable to a wider range of robotic tasks and environments. With further development and refinement, GenSim has the potential to become a powerful tool for robotic policy training and simulation.",
        "gr_score_with": 0.9264305177111717,
        "gr_score_without": 0.8918128654970761,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 305,
            "green_fraction": 0.8918128654970761,
            "z_score": -0.5044896195705005,
            "p_value": 0.6930413225634213
        },
        "output_with": {
            "num_tokens_scored": 367,
            "num_green_tokens": 340,
            "green_fraction": 0.9264305177111717,
            "z_score": 1.6879620209194042,
            "p_value": 0.04570925983746274
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. The authors also demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n\nStrengths:\n\n1. The paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n2. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n3. The authors demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n4. The paper provides a comprehensive analysis of the framework's performance and limitations, including the use of pseudo-labels and the Pathway module.\n\nWeaknesses:\n\n1. The paper relies heavily on pseudo-labels, which may not be accurate and can lead to performance loss on certain datasets.\n2. The Pathway module has limitations, including not handling multi-path scenarios, which may lead to suboptimal performance in certain cases.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, which makes it difficult to evaluate the framework's performance in a broader context.\n4. The paper assumes that the input utterance is well-formed and does not handle cases where the utterance is ambiguous or contains negations.\n\nSuggestions for Improvement:\n\n1. The authors should investigate alternative methods for collecting pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. The Pathway module should be modified to handle multi-path scenarios, such as using a graph-based approach to reason about different possibilities.\n3. The paper should provide a more thorough comparison with other state-of-the-art methods, including a detailed analysis of the framework's performance and limitations.\n4. The paper should address the assumption that the input utterance is well-formed and provide a more robust solution for handling ambiguous or negated utterances.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field. However, the paper relies heavily on pseudo-labels, which may not be accurate, and the Pathway module has limitations. To improve the framework, the authors should investigate alternative methods for collecting pseudo-labels and modify the Pathway module to handle multi-path scenarios. Additionally, the paper should provide a more thorough comparison with other state-of-the-art methods and address the assumption that the input utterance is well-formed. Overall, the paper is well-written and provides a comprehensive analysis of the framework's performance and limitations.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently used to localize the final target object. The approach enhances interpretability and improves overall performance and data efficiency. The authors demonstrate the effectiveness of their framework through extensive experiments on three 3D visual grounding benchmarks, achieving state-of-the-art results without requiring any additional manual annotations.\n\nStrengths:\n\n*   The proposed framework, CoT3DRef, is novel and interpretable, providing a clear and logical sequence of steps for 3D visual grounding.\n*   The framework demonstrates excellent performance on three challenging benchmarks, Nr3D, Sr3D, and ScanRefer, achieving state-of-the-art results without requiring any additional manual annotations.\n*   The authors provide a comprehensive analysis of the framework's performance, including ablation studies and comparisons with state-of-the-art methods.\n*   The framework's data efficiency is demonstrated through experiments on limited data, showcasing its potential for real-world applications.\n\nWeaknesses:\n\n*   The accuracy of the pseudo-labels plays a vital role in the overall performance, and a significant accuracy gap is measured between automatically collected pseudo-labels and ground-truth data, contributing to the performance loss observed on the Nr3D dataset.\n*   The pseudo module limitations hinder achieving more gain on Nr3D, and the anchor localization block in the pseudo module is tailored on ScanNet dataset and will need adaptations to be usable on other 3D scene datasets.\n*   The Pathway module limitations do not handle the multi-path scenario, where multiple paths are valid, and a possible solution to handle this limitation implicitly through building a graph that reasons the different possibilities.\n\nSuggestions for Improvement:\n\n*   Enhance the pseudo module performance to reduce the accuracy gap between automatically collected pseudo-labels and ground-truth data.\n*   Adapt the anchor localization block in the pseudo module to be usable on other 3D scene datasets.\n*   Develop a more robust Pathway module that can handle the multi-path scenario and provide a clear and logical sequence of steps for 3D visual grounding.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, demonstrating excellent performance on three challenging benchmarks. While there are some limitations, the framework's data efficiency and potential for real-world applications make it a valuable contribution to the field. I recommend accepting the paper for publication, with the suggestion for improvement to enhance the pseudo module performance and adapt the anchor localization block to other 3D scene datasets.\n\n$$",
        "gr_score_with": 0.9267605633802817,
        "gr_score_without": 0.8967391304347826,
        "output_without": {
            "num_tokens_scored": 368,
            "num_green_tokens": 330,
            "green_fraction": 0.8967391304347826,
            "z_score": -0.20831209918109106,
            "p_value": 0.5825073574574943
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 329,
            "green_fraction": 0.9267605633802817,
            "z_score": 1.6808647341732337,
            "p_value": 0.04639459564708004
        }
    },
    {
        "title": "REWARD DESIGN FOR JUSTIFIABLE SEQUENTIAL DECISION-MAKING",
        "abstract": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to designing rewards for reinforcement learning (RL) agents that can justify their decisions using supporting evidence. The authors introduce a debate-based reward model, where two argumentative agents take turns providing supporting evidence for two competing decisions. A proxy of a human judge evaluates which decision is better justified based on the proposed evidence. The authors demonstrate the effectiveness of their approach in learning policies for treating septic patients, showing that the debate-based feedback yields policies highly favored by the judge while hardly sacrificing any performance.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence.\n2. The authors provide a thorough and detailed explanation of their approach, including the formal setup, learning framework, and experiments.\n3. The paper presents a comprehensive evaluation of the effectiveness of the debate-based reward model, including quantitative and qualitative results.\n4. The authors provide a clear and concise description of the experimental setup, including the patient cohort, environment, and evaluation metrics.\n\nWeaknesses:\n\n1. The paper assumes a well-defined argument space, which may not be the case in all domains. The authors acknowledge this limitation and suggest potential alternative approaches.\n2. The paper relies on a proxy of a human judge, which may not accurately capture human preferences and biases.\n3. The authors do not provide a thorough discussion of the potential biases and limitations of the debate-based reward model.\n4. The paper assumes a fixed number of turns in the debate game, which may not be realistic in all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed discussion of the potential biases and limitations of the debate-based reward model, including the impact of human biases and preferences.\n2. The paper should include a more thorough evaluation of the effectiveness of the debate-based reward model in different domains and scenarios.\n3. The authors should provide a more detailed explanation of the learning framework and the role of the proxy judge in the debate-based reward model.\n4. The paper should include a more comprehensive discussion of the potential applications and limitations of the debate-based reward model.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence. While there are some limitations and weaknesses to the approach, the paper provides a thorough and detailed evaluation of the effectiveness of the debate-based reward model. I recommend acceptance of the paper for publication, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to designing rewards for reinforcement learning (RL) agents that can justify their decisions using evidence. The authors introduce a debate-based reward model, where two argumentative agents engage in a zero-sum debate game to quantify the justifiability of a decision. The outcome of the debate game is used to train a justifiable policy that can provide evidence to justify its decisions. The authors demonstrate the effectiveness of their approach on a real-world problem of treating septic patients, where they show that the justifiable policy trained using the debate-based reward model yields policies that are highly favored by a human judge when compared to the policy obtained solely from the environment rewards.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to designing rewards for RL agents that can justify their decisions using evidence.\n2. The debate-based reward model is a well-designed and effective way to quantify the justifiability of a decision.\n3. The paper provides a comprehensive evaluation of the approach on a real-world problem of treating septic patients, including both quantitative and qualitative evaluation.\n4. The authors provide a thorough analysis of the performance and justifiability of the trained policies, including the effect of the debate coefficient \u03bb on the trade-off between performance and justifiability.\n\nWeaknesses:\n\n1. The paper assumes that the human judge's preferences can be elicited and used to evaluate the justifiability of decisions, which may not always be the case in real-world scenarios.\n2. The debate-based reward model may not be scalable to larger state spaces, as the number of possible evidence sets grows exponentially with the size of the state space.\n3. The paper does not provide a thorough analysis of the robustness of the approach to different types of biases and adversarial attacks.\n4. The paper assumes that the argumentative agents can provide evidence that is resilient to refutations, which may not always be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the robustness of the approach to different types of biases and adversarial attacks.\n2. The paper should provide a more detailed discussion on how to elicit and use human preferences in real-world scenarios.\n3. The authors should provide a more thorough evaluation of the approach on larger state spaces and more complex decision-making problems.\n4. The paper should provide a more detailed analysis of the trade-off between performance and justifiability in different scenarios.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for RL agents that can justify their decisions using evidence. While there are some limitations to the approach, the paper provides a thorough evaluation and analysis of the approach on a real-world problem of treating septic patients. The approach has the potential to be applied to a wide range of decision-making problems, including healthcare, finance, and education. Therefore, I recommend accepting the paper for publication.\n\n$$$\nNote: The review is based on the provided paper and does not include any external knowledge or information. The review is written in a standard conference review format and includes a summary, strengths, weaknesses, suggestions for improvement, and a recommendation.",
        "gr_score_with": 0.9384615384615385,
        "gr_score_without": 0.8794788273615635,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 270,
            "green_fraction": 0.8794788273615635,
            "z_score": -1.198334545045815,
            "p_value": 0.8846065984123186
        },
        "output_with": {
            "num_tokens_scored": 390,
            "num_green_tokens": 366,
            "green_fraction": 0.9384615384615385,
            "z_score": 2.532018623437144,
            "p_value": 0.005670398384420182
        }
    },
    {
        "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
        "abstract": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach named TGeo-KD for learning sample-wise knowledge fusion ratios during knowledge distillation (KD). The authors argue that existing methods for determining the knowledge fusion ratio often rely on heuristic rules or constant values, which may not capture the nuanced dynamics of the knowledge transfer process. To address this limitation, the authors introduce a novel adaptive method that learns a sample-wise knowledge fusion ratio by exploiting both the correctness of the teacher and student, as well as the discrepancy between their predictions. The proposed method, TGeo-KD, leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations.\n\nStrengths:\n\n1. The paper presents a well-structured and clear introduction to the problem of knowledge distillation and the limitations of existing methods.\n2. The proposed approach, TGeo-KD, is innovative and well-motivated, leveraging the trilateral geometry among the signals from the student, teacher, and ground truth.\n3. The authors provide extensive experiments across diverse domains, including image classification, attack detection, and click-through rate prediction, demonstrating the consistent superiority of TGeo-KD over other re-weighting methods.\n4. The paper provides a clear and concise explanation of the proposed method, including the bilevel optimization problem and the intra- and inter-sample geometric relations.\n\nWeaknesses:\n\n1. The paper assumes a relatively simple neural network architecture for learning the knowledge fusion ratio, which may not be optimal for all scenarios.\n2. The authors do not provide a thorough analysis of the computational complexity of the proposed method, which may be a concern for large-scale applications.\n3. The paper focuses primarily on image classification tasks and does not provide a comprehensive evaluation of TGeo-KD on other tasks, such as object detection or segmentation.\n4. The authors do not provide a clear explanation of how the proposed method can be extended to handle multiple teachers or students.\n\nSuggestions for Improvement:\n\n1. The authors could provide a more detailed analysis of the computational complexity of the proposed method and discuss potential optimizations.\n2. The paper could benefit from a more comprehensive evaluation of TGeo-KD on a wider range of tasks and datasets.\n3. The authors could provide a clear explanation of how the proposed method can be extended to handle multiple teachers or students.\n4. The paper could benefit from a more detailed discussion of the limitations of the proposed method and potential areas for future research.\n\nRecommendation:\n\nI highly recommend this paper for publication in a top-tier conference or journal in the field of machine learning. The proposed approach, TGeo-KD, is innovative and well-motivated, and the authors provide extensive experiments demonstrating its consistent superiority over other re-weighting methods. However, the authors should address the limitations and suggestions for improvement mentioned above to further strengthen the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method named TGeo-KD for learning sample-wise knowledge fusion ratios during knowledge distillation (KD). The method leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations. The TGeo-KD method is designed to adaptively learn the knowledge fusion ratio based on the trilateral geometry within the (S, T, G) triplet. The method is evaluated on three different tasks: image classification on CIFAR-100 and ImageNet, attack detection on HIL, and click-through rate prediction on Criteo. The results show that TGeo-KD consistently outperforms other re-weighting methods.\n\nStrengths:\n\n* The paper proposes a novel method for learning sample-wise knowledge fusion ratios during KD.\n* The method leverages the trilateral geometry among the signals from the student, teacher, and ground truth.\n* The method is designed to adaptively learn the knowledge fusion ratio based on the trilateral geometry within the (S, T, G) triplet.\n* The method is evaluated on three different tasks: image classification on CIFAR-100 and ImageNet, attack detection on HIL, and click-through rate prediction on Criteo.\n* The results show that TGeo-KD consistently outperforms other re-weighting methods.\n\nWeaknesses:\n\n* The paper does not provide a clear explanation of the motivation behind the proposed method.\n* The paper does not provide a detailed analysis of the trilateral geometry within the (S, T, G) triplet.\n* The paper does not provide a comparison of the proposed method with other KD methods that do not use trilateral geometry.\n* The paper does not provide a discussion on the limitations of the proposed method.\n* The paper does not provide a clear explanation of the experimental settings and the choice of hyperparameters.\n\nSuggestions for Improvement:\n\n* Provide a clear explanation of the motivation behind the proposed method.\n* Provide a detailed analysis of the trilateral geometry within the (S, T, G) triplet.\n* Compare the proposed method with other KD methods that do not use trilateral geometry.\n* Discuss the limitations of the proposed method.\n* Provide a clear explanation of the experimental settings and the choice of hyperparameters.\n\nRecommendation:\n\nThe paper proposes a novel method for learning sample-wise knowledge fusion ratios during KD. The method leverages the trilateral geometry among the signals from the student, teacher, and ground truth. The results show that TGeo-KD consistently outperforms other re-weighting methods. However, the paper has several weaknesses that need to be addressed. With some improvements, the paper has the potential to make a significant contribution to the field of KD.",
        "gr_score_with": 0.9110169491525424,
        "gr_score_without": 0.8770053475935828,
        "output_without": {
            "num_tokens_scored": 374,
            "num_green_tokens": 328,
            "green_fraction": 0.8770053475935828,
            "z_score": -1.4820964546364568,
            "p_value": 0.9308426839886741
        },
        "output_with": {
            "num_tokens_scored": 236,
            "num_green_tokens": 215,
            "green_fraction": 0.9110169491525424,
            "z_score": 0.5643038302197159,
            "p_value": 0.28627368758976707
        }
    },
    {
        "title": "TAPMO: SHAPE-AWARE MOTION GENERATION OF SKELETON-FREE CHARACTERS",
        "abstract": "Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of meshspecific motions for various characters. Specifically, TapMo comprises two main components Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character\u2019s shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of nonhuman meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters. The project page: https://semanticdh.github.io/TapMo.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. The pipeline consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning to train TapMo with limited ground-truth data. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. The paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters.\n2. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective.\n3. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n4. The authors provide a clear and concise explanation of the proposed method and its components.\n\nWeaknesses:\n\n1. The paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n2. The paper does not provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. The authors should provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. The paper assumes that the text description is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n\nSuggestions for Improvement:\n\n1. Provide more information on how to handle the case where the input mesh is not provided.\n2. Provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. Provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. Provide more information on how to handle the case where the text description is not provided.\n5. Consider providing more information on the robustness of the proposed method to different types of meshes and text descriptions.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods. However, the paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue. Overall, the paper is well-written and provides a clear and concise explanation of the proposed method and its components. I recommend this paper for publication in the conference.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. TapMo consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. To train TapMo with limited ground-truth data, the authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning. The authors conduct extensive experiments to validate the effectiveness of TapMo and show that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. TapMo is a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters, which has the potential to democratize animation creation.\n2. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is essential for training TapMo with limited ground-truth data.\n3. The Shape-aware Motion Diffusion module is designed to generate text-guided motions considering the specific deformation properties of the mesh, which ensures coherent and plausible character animation without introducing mesh distortion.\n4. The authors conduct extensive experiments to validate the effectiveness of TapMo and show that it achieves state-of-the-art performance compared to baseline methods.\n\nWeaknesses:\n\n1. The paper assumes that the input mesh is already pre-processed and cleaned, which may not be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of TapMo, which is essential for its practical deployment.\n3. The paper focuses primarily on the qualitative results of TapMo, and the quantitative results could be more comprehensive.\n4. The authors do not provide a clear explanation of how the Mesh Handle Predictor is able to predict the skinning weights and cluster mesh vertices into adaptive handles.\n\nSuggestions for Improvement:\n\n1. Provide a detailed analysis of the computational cost of TapMo and its practical deployment.\n2. Conduct more comprehensive experiments to validate the effectiveness of TapMo, including quantitative results.\n3. Provide a clear explanation of how the Mesh Handle Predictor is able to predict the skinning weights and cluster mesh vertices into adaptive handles.\n4. Consider adding more diverse and challenging scenarios to the experiments to further evaluate the robustness and generalizability of TapMo.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters, which has the potential to democratize animation creation. However, there are some weaknesses and areas for improvement that need to be addressed. With further refinement and improvement, TapMo has the potential to become a state-of-the-art solution for text-driven animation of skeleton-free 3D characters.\n\n$$$",
        "gr_score_with": 0.956140350877193,
        "gr_score_without": 0.9470198675496688,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 286,
            "green_fraction": 0.9470198675496688,
            "z_score": 2.7238701703045543,
            "p_value": 0.003226093239619197
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 327,
            "green_fraction": 0.956140350877193,
            "z_score": 3.4608679330729646,
            "p_value": 0.0002692184445651806
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. The framework consists of four core modules: Environment, Reasoning, Reflection, and Memory. The Reasoning module utilizes the experiences derived from the Memory module and the common-sense knowledge of the LLM to perform decision-making for the current traffic scenario. The Reflection module assesses past decision sequences, categorizing them as either safe or unsafe, and updates the Memory module with refined decisions. The authors conduct extensive experiments to demonstrate the effectiveness of DiLu in accumulating experience, generalization ability, and transformation ability.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to autonomous driving by leveraging LLMs and knowledge-driven capabilities.\n2. The framework is well-structured, and the authors provide a clear explanation of the Reasoning and Reflection modules.\n3. The experiments are thorough, and the authors provide a comprehensive comparison with the state-of-the-art RL-based method.\n4. The paper highlights the importance of generalization and transformation ability in autonomous driving systems.\n\nWeaknesses:\n\n1. The paper lacks a clear explanation of how the LLM is trained and fine-tuned for the autonomous driving task.\n2. The authors do not provide a detailed analysis of the decision-making latency and hallucinations generated by LLMs.\n3. The paper assumes that the LLM has access to a vast amount of common-sense knowledge, which may not be feasible in real-world scenarios.\n4. The authors do not provide a clear comparison with other knowledge-driven approaches in autonomous driving.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the LLM training and fine-tuning process.\n2. Conduct a more thorough analysis of the decision-making latency and hallucinations generated by LLMs.\n3. Consider incorporating other knowledge-driven approaches in autonomous driving for a more comprehensive comparison.\n4. Provide a clear explanation of how the framework can be scaled up for real-world applications.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to autonomous driving, and the authors provide a clear explanation of the framework. However, the paper lacks a clear explanation of the LLM training and fine-tuning process, and the authors do not provide a detailed analysis of the decision-making latency and hallucinations generated by LLMs. With some modifications and improvements, the paper can be a strong contribution to the field of autonomous driving.\n\nRating: 7/10\n\nRecommendation for Publication: Accept after revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) with emergent abilities to facilitate reasoning and reflection processes. The framework consists of four core modules: Environment, Reasoning, Reflection, and Memory. The Reasoning module utilizes the experiences derived from the Memory module and the common-sense knowledge of the LLM to perform decision-making for the current traffic scenario. The Reflection module continuously learns from past driving experiences and accumulates valuable experiences to enrich the Memory module.\n\nStrengths:\n\n1. The paper presents a novel knowledge-driven approach to autonomous driving, which is a significant departure from traditional data-driven methods.\n2. The DiLu framework is well-structured, and the authors provide a clear explanation of each module's functionality.\n3. The experimental results demonstrate the effectiveness of DiLu in accumulating experience and exhibiting strong generalization ability compared to the SOTA RL-based method.\n4. The paper provides a comprehensive literature review of related works, including recent advancements in LLMs and their applications in autonomous driving.\n\nWeaknesses:\n\n1. The paper relies heavily on the performance of LLMs, which may not be robust in all scenarios, especially when faced with novel or ambiguous situations.\n2. The authors do not provide a thorough evaluation of the framework's performance in real-world scenarios, which is essential for its deployment in practical autonomous driving systems.\n3. The decision-making latency of DiLu is relatively high, which may not be suitable for real-time autonomous driving applications.\n4. The paper does not provide a clear explanation of how the framework handles edge cases, such as sudden changes in the environment or unexpected events.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive evaluation of DiLu's performance in real-world scenarios, including its robustness in handling novel or ambiguous situations.\n2. The framework should be modified to reduce the decision-making latency, which is essential for real-time autonomous driving applications.\n3. The authors should provide a more detailed explanation of how the framework handles edge cases, such as sudden changes in the environment or unexpected events.\n4. The paper should include more experiments on the generalization ability of DiLu in different environments, including its performance in scenarios with varying traffic density and lane configurations.\n\nRecommendation:\n\nThe paper presents a novel knowledge-driven approach to autonomous driving, which has the potential to overcome the limitations of traditional data-driven methods. The DiLu framework is well-structured, and the authors provide a clear explanation of each module's functionality. However, the paper relies heavily on the performance of LLMs, which may not be robust in all scenarios. Therefore, I recommend that the authors provide a more comprehensive evaluation of DiLu's performance in real-world scenarios and modify the framework to reduce the decision-making latency.",
        "gr_score_with": 0.9018567639257294,
        "gr_score_without": 0.886039886039886,
        "output_without": {
            "num_tokens_scored": 351,
            "num_green_tokens": 311,
            "green_fraction": 0.886039886039886,
            "z_score": -0.8716019894395989,
            "p_value": 0.8082872275014561
        },
        "output_with": {
            "num_tokens_scored": 377,
            "num_green_tokens": 340,
            "green_fraction": 0.9018567639257294,
            "z_score": 0.12037296349967683,
            "p_value": 0.45209385382657574
        }
    },
    {
        "title": "RTFS-NET: RECURRENT TIME-FREQUENCY MOD-",
        "abstract": "Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method called RTFS-Net, which operates in the time-frequency domain. The proposed method utilizes a multi-layered recurrent neural network (RNN) to model and capture the time and frequency dimensions of the audio independently. Additionally, it introduces a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. The experimental evaluations conducted on three widely used datasets demonstrate the superiority of RTFS-Net over state-of-the-art (SOTA) methods, achieving better separation quality and reducing computational complexity by 83% and parameter count by 90%.\n\nStrengths:\n\n1. The proposed method, RTFS-Net, is the first time-frequency domain AVSS method to outperform all contemporary T-domain counterparts.\n2. The method achieves remarkable performance improvements while maintaining a significantly reduced computational complexity and parameter count.\n3. The use of a multi-layered RNN to model and capture the time and frequency dimensions of the audio independently is a novel and effective approach.\n4. The attention-based fusion technique and the new mask separation approach are unique and efficient ways to integrate audio and visual information.\n5. The experimental evaluations conducted on multiple datasets demonstrate the superiority of RTFS-Net over SOTA methods.\n\nWeaknesses:\n\n1. The paper assumes that the audio and video inputs are aligned in time, which may not always be the case in real-world scenarios.\n2. The method relies on a pre-trained video encoder, which may not be available for all applications.\n3. The experimental evaluations were conducted on a limited number of datasets, and it would be beneficial to evaluate the method on a larger variety of datasets.\n4. The paper does not provide a detailed analysis of the computational complexity of the method, which may be a concern for large-scale applications.\n5. The code for RTFS-Net is not provided, which makes it difficult for readers to reproduce the results.\n\nSuggestions for Improvement:\n\n1. Investigate methods to align the audio and video inputs in time, even if they are not perfectly aligned.\n2. Provide a more detailed analysis of the computational complexity of the method and explore ways to reduce it further.\n3. Evaluate the method on a larger variety of datasets to demonstrate its robustness and generalizability.\n4. Provide a more detailed explanation of the attention-based fusion technique and the new mask separation approach.\n5. Provide a more detailed comparison with SOTA methods and highlight the advantages of RTFS-Net over these methods.\n\nRecommendation:\n\nThe paper presents a novel and effective AVSS method that outperforms SOTA methods. However, it has some limitations that need to be addressed. With some improvements, RTFS-Net has the potential to become a leading method in the field of AVSS. I recommend this paper for publication in a reputable conference, and I hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method, Recurrent Time-Frequency Separation Network (RTFS-Net), which operates in the time-frequency domain. RTFS-Net employs a multi-layered RNN to model and capture the time and frequency dimensions of the audio independently. The method also introduces a unique attention-based fusion technique for the efficient integration of audio and visual information and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. The authors conducted comprehensive experimental evaluations on three widely used datasets and demonstrated the superiority of RTFS-Net over state-of-the-art (SOTA) methods in terms of separation quality and computational complexity.\n\nStrengths:\n\n1. The paper presents a novel and efficient approach to AVSS that operates in the time-frequency domain, which is a significant departure from existing methods that operate in the time domain.\n2. The use of a multi-layered RNN to model and capture the time and frequency dimensions of the audio independently is a unique and innovative approach that has not been explored in previous works.\n3. The attention-based fusion technique and the new mask separation approach are also innovative and efficient ways to integrate audio and visual information and to separate the target speaker's voice, respectively.\n4. The authors provide a comprehensive evaluation of RTFS-Net on three widely used datasets and demonstrate its superiority over SOTA methods in terms of separation quality and computational complexity.\n\nWeaknesses:\n\n1. The paper assumes that the audio and video inputs are preprocessed and that the audio signal is represented in the time-frequency domain, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity and memory requirements of RTFS-Net, which may be a limitation for large-scale applications.\n3. The paper relies heavily on the use of pre-trained video and audio encoders, which may not be available for all scenarios or may require additional training data.\n4. The authors do not provide a detailed comparison with other time-frequency domain AVSS methods, which may be a limitation for evaluating the novelty and impact of RTFS-Net.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity and memory requirements of RTFS-Net and discuss its scalability for large-scale applications.\n2. Explore the use of other pre-trained models or training methods for the video and audio encoders to improve the performance and robustness of RTFS-Net.\n3. Compare RTFS-Net with other time-frequency domain AVSS methods and discuss its novelty and impact in the context of existing works.\n4. Provide a more detailed evaluation of the robustness and generalizability of RTFS-Net to different scenarios, such as varying audio and video quality, speaker diversity, and environmental conditions.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach to AVSS that operates in the time-frequency domain. While there are some limitations and areas for improvement, the paper provides a comprehensive evaluation of RTFS-Net and demonstrates its superiority over SOTA methods in terms of separation quality and computational complexity. Therefore, I recommend accepting this paper for publication in its current form, with the understanding that the authors will address the suggested areas for improvement in future work.\n\n$$$",
        "gr_score_with": 0.8791469194312796,
        "gr_score_without": 0.8604118993135011,
        "output_without": {
            "num_tokens_scored": 437,
            "num_green_tokens": 376,
            "green_fraction": 0.8604118993135011,
            "z_score": -2.7583152149009753,
            "p_value": 0.9970949928498039
        },
        "output_with": {
            "num_tokens_scored": 422,
            "num_green_tokens": 371,
            "green_fraction": 0.8791469194312796,
            "z_score": -1.4276909770169846,
            "p_value": 0.9233095877989509
        }
    },
    {
        "title": "SOHES: SELF-SUPERVISED OPEN-WORLD HIERARCHICAL ENTITY SEGMENTATION",
        "abstract": "Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that sidesteps the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pretrained self-supervised representation, we produce abundant high-quality pseudolabels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach, called Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), for open-world entity segmentation. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the method generates initial pseudo-labels by clustering visual features from a pre-trained self-supervised representation. In the self-instruction phase, a segmentation model is trained on the initial pseudo-labels to refine the segmentation. Finally, in the self-correction phase, a teacher-student mutual-learning framework is employed to further improve the segmentation model. SOHES achieves state-of-the-art performance on various datasets, including MS-COCO, LVIS, ADE20K, EntitySeg, and SA-1B, and significantly closes the gap between self-supervised and supervised methods.\n\nStrengths:\n\n1. SOHES is a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision.\n2. The method is self-supervised, which means that it can learn from unlabeled data, making it more efficient and cost-effective than traditional supervised learning approaches.\n3. SOHES can learn hierarchical structures of visual entities, which is a unique feature of this approach.\n4. The method achieves state-of-the-art performance on various datasets, including those with complex scenes and diverse classes.\n5. The paper provides a thorough evaluation of the method, including ablation studies and comparisons with state-of-the-art methods.\n\nWeaknesses:\n\n1. The method requires a large amount of unlabeled data, which may not be readily available in all scenarios.\n2. The self-exploration phase can be computationally expensive, which may limit the scalability of the method.\n3. The teacher-student mutual-learning framework used in the self-correction phase may not be suitable for all types of data and may require additional hyperparameter tuning.\n4. The paper does not provide a detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n5. The method may not be suitable for real-time applications due to its computational complexity.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more efficient clustering algorithms in the self-exploration phase to reduce computational complexity.\n2. Explore the use of different teacher-student mutual-learning frameworks to improve the performance of the method.\n3. Provide a more detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n4. Investigate the use of transfer learning to adapt the method to new domains and scenarios.\n5. Provide a more detailed evaluation of the method's performance on real-world datasets and scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision. The method is self-supervised, which makes it more efficient and cost-effective than traditional supervised learning approaches. However, the method requires a large amount of unlabeled data and may not be suitable for real-time applications due to its computational complexity. Overall, the paper is well-written and provides a thorough evaluation of the method, but it requires additional work to address the weaknesses and limitations of the approach.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach to open-world entity segmentation, called Self-Supervised Open-World Hierarchical Entity Segmentation (SOHES). SOHES consists of three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the model generates initial pseudo-labels by clustering visual features based on similarity and locality. In the self-instruction phase, the model learns from the initial pseudo-labels and refines the segmentation. In the self-correction phase, the model further refines the segmentation by mutual-learning with a teacher-student framework. The proposed approach achieves state-of-the-art performance in self-supervised open-world segmentation and closes the gap between self-supervised and supervised methods.\n\nStrengths:\n\n1. The paper proposes a novel self-supervised approach to open-world entity segmentation, which is a challenging task.\n2. The approach consists of three phases: self-exploration, self-instruction, and self-correction, which are well-defined and easy to understand.\n3. The paper provides a thorough analysis of the proposed approach and compares it with state-of-the-art methods.\n4. The approach achieves state-of-the-art performance in self-supervised open-world segmentation and closes the gap between self-supervised and supervised methods.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained self-supervised representation is available, which may not be the case in practice.\n2. The approach requires a large amount of computational resources and time to train the model.\n3. The paper does not provide a thorough analysis of the robustness of the proposed approach to noisy data or adversarial attacks.\n4. The paper assumes that the pseudo-labels generated in the self-exploration phase are accurate, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n1. Investigate the use of other pre-trained self-supervised representations and compare their performance with the proposed approach.\n2. Develop a more efficient algorithm for generating pseudo-labels in the self-exploration phase.\n3. Analyze the robustness of the proposed approach to noisy data or adversarial attacks.\n4. Investigate the use of other teacher-student mutual-learning frameworks to improve the performance of the proposed approach.\n\nRecommendation:\n\nThe paper proposes a novel self-supervised approach to open-world entity segmentation, which achieves state-of-the-art performance and closes the gap between self-supervised and supervised methods. The approach is well-defined and easy to understand, and the paper provides a thorough analysis of the proposed approach. However, the paper assumes that the pre-trained self-supervised representation is available and requires a large amount of computational resources and time to train the model. Therefore, the paper is suitable for researchers and practitioners who are interested in self-supervised open-world entity segmentation and have access to large computational resources.",
        "gr_score_with": 0.9075907590759076,
        "gr_score_without": 0.8615023474178404,
        "output_without": {
            "num_tokens_scored": 426,
            "num_green_tokens": 367,
            "green_fraction": 0.8615023474178404,
            "z_score": -2.648357376977115,
            "p_value": 0.9959558014907994
        },
        "output_with": {
            "num_tokens_scored": 303,
            "num_green_tokens": 275,
            "green_fraction": 0.9075907590759076,
            "z_score": 0.4406133445993162,
            "p_value": 0.329746470168531
        }
    },
    {
        "title": "PRE-TRAINING WITH SYNTHETIC DATA HELPS OFFLINE REINFORCEMENT LEARNING",
        "abstract": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline deep reinforcement learning (DRL) models using synthetic data, rather than language data. The authors show that pre-training Decision Transformer (DT) with synthetic data generated from a one-step Markov Chain can match or even surpass the performance of pre-training with a large language corpus. They also demonstrate that pre-training Conservative Q-Learning (CQL) with synthetic Markov Decision Process (MDP) data can provide a significant performance boost. The results suggest that synthetic data can be a surprisingly effective approach to improve offline DRL performance for both transformer-based and Q-learning-based algorithms.\n\nStrengths:\n\n* The paper provides a clear and concise overview of the related work and the contributions of the paper.\n* The authors conduct extensive ablations to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* The results are presented in a clear and organized manner, with tables and figures that facilitate easy comparison of the different methods.\n* The authors provide theoretical insights into why IID data can still achieve a good performance, which adds depth to the understanding of the results.\n\nWeaknesses:\n\n* The paper assumes that the reader is familiar with the basics of DRL and reinforcement learning, which may make it difficult for non-experts to understand the paper.\n* The authors do not provide a clear explanation of why pre-training with synthetic data is effective, which is a limitation of the paper.\n* The results are based on a limited set of experiments, which may not be representative of all possible scenarios.\n* The paper does not provide a detailed comparison with other pre-training methods, such as pre-training with generic image data or using offline DRL data itself to learn representations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of why pre-training with synthetic data is effective, including a discussion of the potential benefits of using synthetic data for pre-training.\n* Conduct more extensive experiments to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* Provide a more detailed comparison with other pre-training methods, including a discussion of the advantages and disadvantages of each method.\n* Consider including more visualizations to help illustrate the results and facilitate easy comparison of the different methods.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of offline DRL by demonstrating the effectiveness of pre-training with synthetic data. The results are clear and well-presented, and the authors provide theoretical insights into why IID data can still achieve a good performance. However, the paper assumes a high level of prior knowledge and does not provide a clear explanation of why pre-training with synthetic data is effective. Overall, I would recommend this paper for publication, but with the suggestion that the authors provide more explanation and comparison with other pre-training methods.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline Deep Reinforcement Learning (DRL) models using synthetic data, specifically focusing on Decision Transformer (DT) and Conservative Q-Learning (CQL). The authors demonstrate that pre-training with synthetic data generated from a one-step Markov Chain or Independent and Identically Distributed (IID) states can significantly improve the performance of DT and CQL, often outperforming pre-training with a large language corpus or no pre-training at all. The results show that the performance gains obtained by pre-training with synthetic data are robust across different settings of the synthetic data, including the degree of past dependence, state-space size, and the degree of randomness in the transitions.\n\nStrengths:\n\n* The paper provides a comprehensive study on the effectiveness of pre-training offline DRL models using synthetic data, which is a novel and timely contribution to the field.\n* The authors demonstrate the robustness of synthetic pre-training across different settings of the synthetic data, which is an important finding that challenges the previous view that language pre-training can provide unique benefits for DRL.\n* The paper provides a clear and concise explanation of the experimental setup and the results, making it easy to follow and understand.\n\nWeaknesses:\n\n* The paper assumes that the synthetic data is generated from a one-step Markov Chain or IID states, which may not be the case in real-world scenarios where the data may have more complex dependencies and correlations.\n* The authors do not provide a detailed analysis of the limitations and potential biases of the synthetic data generation scheme used in the paper.\n* The paper does not explore the potential applications of synthetic pre-training in other domains, such as online DRL or other types of machine learning tasks.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the limitations and potential biases of the synthetic data generation scheme used in the paper, including a discussion of the assumptions made and the potential impact on the results.\n* The paper should explore the potential applications of synthetic pre-training in other domains, such as online DRL or other types of machine learning tasks, to provide a more comprehensive understanding of its benefits and limitations.\n* The authors should provide more details on the experimental setup, including the hardware and software used, to make the results more reproducible and reliable.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of offline DRL and synthetic pre-training, and the results are impressive and robust. However, the paper would benefit from a more detailed analysis of the limitations and potential biases of the synthetic data generation scheme used, and a discussion of the potential applications of synthetic pre-training in other domains. With these improvements, the paper would be an even stronger contribution to the field.\n\nRating: 8/10\n\nRecommendation for publication: Yes, but with revisions to address the suggestions for improvement.\n\nReviewer's expertise: Research Scientist in Deep Learning and Reinforcement Learning.\n\nReviewer's conflict of interest: None.\n\nReviewer's affiliation: [Affiliation]\n\nReviewer's contact information: [Contact information]",
        "gr_score_with": 0.9507389162561576,
        "gr_score_without": 0.9023746701846965,
        "output_without": {
            "num_tokens_scored": 379,
            "num_green_tokens": 342,
            "green_fraction": 0.9023746701846965,
            "z_score": 0.1542998964133283,
            "p_value": 0.4386866394943147
        },
        "output_with": {
            "num_tokens_scored": 406,
            "num_green_tokens": 386,
            "green_fraction": 0.9507389162561576,
            "z_score": 3.408031678925688,
            "p_value": 0.0003271664888342428
        }
    },
    {
        "title": "CROSSQ: BATCH NORMALIZATION",
        "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ\u2019s contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces CrossQ, a lightweight algorithm for continuous control tasks that achieves superior performance by removing much of the algorithmic design complexity that was added over the years. CrossQ uses Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low update-to-data (UTD) ratio of 1. The authors present three design choices that constitute CrossQ: removing target networks, using Batch Normalization, and wider critic networks. They provide empirical investigations and hypotheses for CrossQ's success and show that CrossQ matches or surpasses the state-of-the-art algorithms in sample efficiency while being up to 4\u00d7 faster in terms of wallclock time.\n\nStrengths:\n\n* The paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks.\n* The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* The paper includes a comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* The authors provide a clear and concise description of the algorithm and its components.\n\nWeaknesses:\n\n* The paper assumes a good understanding of deep reinforcement learning and its components, which may make it challenging for readers without a background in the field.\n* The authors do not provide a detailed theoretical analysis of the algorithm's performance, which may limit its applicability to other domains.\n* The paper focuses on continuous control tasks, and it is unclear whether CrossQ can be extended to other domains, such as discrete control tasks or multi-agent systems.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* Include a more comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* Provide a detailed theoretical analysis of the algorithm's performance, including a discussion of its convergence properties and stability.\n* Consider extending CrossQ to other domains, such as discrete control tasks or multi-agent systems.\n\nRecommendation:\n\nThe paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks. The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance. While the paper has some limitations, it is a valuable contribution to the field of deep reinforcement learning. I recommend accepting this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces CrossQ, a lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. The authors claim that CrossQ is computationally more efficient than REDQ and DroQ, the current state-of-the-art methods, and is easy to implement, requiring just a few lines of code on top of SAC.\n\nStrengths:\n\n1. The paper presents a new algorithm, CrossQ, which matches or surpasses the current state-of-the-art methods in terms of sample efficiency while being computationally more efficient.\n2. The authors provide empirical evidence for CrossQ's performance, including experiments on a wide range of continuous-control MuJoCo environments.\n3. The paper provides a clear and concise explanation of the design choices behind CrossQ and their contributions to the algorithm's performance.\n4. The authors conduct thorough ablation studies to isolate the effects of each design choice and provide insights into the importance of each component.\n\nWeaknesses:\n\n1. The paper relies heavily on empirical results, and it would be beneficial to have more theoretical analysis or insights into the design choices behind CrossQ.\n2. The authors claim that CrossQ is computationally more efficient than REDQ and DroQ, but the paper does not provide a detailed analysis of the computational costs.\n3. The paper does not provide a clear explanation of why the combination of removing target networks and adding BatchNorm is what makes CrossQ so efficient.\n4. The authors mention that the relationship between the Q estimation bias and the learning performance of CrossQ is complex, but they do not provide a thorough analysis or insights into this relationship.\n\nSuggestions for Improvement:\n\n1. Provide more theoretical analysis or insights into the design choices behind CrossQ, including the use of Batch Normalization and the removal of target networks.\n2. Include a detailed analysis of the computational costs of CrossQ and compare it to REDQ and DroQ.\n3. Provide a clearer explanation of why the combination of removing target networks and adding BatchNorm is what makes CrossQ so efficient.\n4. Conduct a more thorough analysis of the relationship between the Q estimation bias and the learning performance of CrossQ.\n\nRecommendation:\n\nThis paper presents a new algorithm, CrossQ, which has the potential to be a significant improvement over current state-of-the-art methods in terms of sample efficiency and computational efficiency. However, the paper relies heavily on empirical results, and it would be beneficial to have more theoretical analysis or insights into the design choices behind CrossQ. I recommend accepting this paper for publication with the suggestion to include more theoretical analysis or insights into the design choices behind CrossQ.\n\n$$$",
        "gr_score_with": 0.877742946708464,
        "gr_score_without": 0.8412698412698413,
        "output_without": {
            "num_tokens_scored": 315,
            "num_green_tokens": 265,
            "green_fraction": 0.8412698412698413,
            "z_score": -3.4742903815448205,
            "p_value": 0.999743896802574
        },
        "output_with": {
            "num_tokens_scored": 319,
            "num_green_tokens": 280,
            "green_fraction": 0.877742946708464,
            "z_score": -1.3248749022196118,
            "p_value": 0.9073936768066949
        }
    },
    {
        "title": "Never Train from Scratch: FAIR COMPARISON OF LONG- SEQUENCE MODELS REQUIRES DATA-DRIVEN PRIORS",
        "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to evaluate the performance of long-range sequence models by incorporating self-supervised pretraining on the task data itself. The authors argue that training models from scratch using random initialization can lead to gross overestimation of the differences between architectures, and that pretraining with standard denoising objectives using only the downstream task data can lead to dramatic gains across multiple architectures. The paper demonstrates the effectiveness of self-supervised pretraining on the Long Range Arena (LRA) benchmark, a standard benchmark for long sequence modeling, and shows that pretraining leads to large empirical gains over previously-reported performances on LRA across a range of architectures. The authors also analyze the utility of structured parameterizations for State Space Models (SSMs) and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nStrengths:\n\n* The paper presents a novel and efficient approach to evaluate the performance of long-range sequence models.\n* The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark.\n* The paper highlights the importance of accounting for the pretraining stage while designing and evaluating novel architectures.\n* The authors provide a thorough analysis of the utility of structured parameterizations for SSMs and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nWeaknesses:\n\n* The paper assumes that the downstream task data is available, which may not be the case in all scenarios.\n* The authors do not provide a detailed analysis of the computational overhead of self-supervised pretraining.\n* The paper does not provide a thorough comparison with other pretraining methods, such as supervised pretraining or transfer learning.\n* The authors do not provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational overhead of self-supervised pretraining and discuss ways to reduce it.\n* Compare self-supervised pretraining with other pretraining methods, such as supervised pretraining or transfer learning, to demonstrate its effectiveness.\n* Provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n* Discuss the potential limitations of self-supervised pretraining and how to address them.\n\nRecommendation:\n\nThis paper presents a novel and efficient approach to evaluate the performance of long-range sequence models. The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark. While there are some weaknesses and limitations to the paper, it makes a significant contribution to the field and has the potential to impact the design and evaluation of novel architectures. I recommend this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the underestimation of the capabilities of various architectures on long-range benchmarks due to inadequate training and evaluation practices. The authors propose a simple and efficient solution, self-supervised pretraining on the task data itself (SPT), which leads to dramatic gains in performance for multiple architectures across data scales. The paper demonstrates the effectiveness of SPT for both Transformers and State Space Models (SSMs), and shows that manual-designed biases become increasingly redundant with pretraining. The authors also analyze the utility of previously-proposed structured parameterizations for SSMs and find that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the underestimation of model capabilities on long-range benchmarks.\n2. The authors propose a simple and efficient solution, SPT, which leads to dramatic gains in performance for multiple architectures.\n3. The paper demonstrates the effectiveness of SPT for both Transformers and SSMs.\n4. The authors provide a comprehensive array of experiments, including experiments on additional datasets and tasks.\n\nWeaknesses:\n\n1. The paper relies heavily on the assumption that SPT is a reliable method for evaluating model capabilities.\n2. The authors do not provide a clear explanation of why SPT is effective, and the paper could benefit from a more detailed analysis of the underlying mechanisms.\n3. The paper focuses primarily on the evaluation of model capabilities on long-range benchmarks, and does not explore other aspects of model evaluation.\n4. The authors do not provide a clear roadmap for how SPT could be integrated into existing model development workflows.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the underlying mechanisms of SPT and why it is effective.\n2. Explore other aspects of model evaluation, such as model interpretability and fairness.\n3. Provide a clear roadmap for how SPT could be integrated into existing model development workflows.\n4. Consider conducting experiments on other types of models and tasks to further validate the effectiveness of SPT.\n\nRecommendation:\n\nI recommend accepting this paper for publication, pending revisions to address the weaknesses and suggestions for improvement outlined above. The paper provides a valuable contribution to the field by highlighting the underestimation of model capabilities on long-range benchmarks and proposing a simple and efficient solution. However, the paper could benefit from further refinement and exploration of the underlying mechanisms of SPT.",
        "gr_score_with": 0.9375,
        "gr_score_without": 0.8944281524926686,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 305,
            "green_fraction": 0.8944281524926686,
            "z_score": -0.34277250535208437,
            "p_value": 0.6341151941092547
        },
        "output_with": {
            "num_tokens_scored": 320,
            "num_green_tokens": 300,
            "green_fraction": 0.9375,
            "z_score": 2.236222948240698,
            "p_value": 0.01266858536372437
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework combines the strengths of LLMs, which can perform high-level abstract reasoning, and automated reasoners, which can perform precise low-level reasoning. The authors present a formal calculus, LEMUR, that operates over a configuration, which is either a distinguished symbol or a tuple of a program, an assumption, and a list of properties. The calculus performs oracle calls to LLMs to propose new properties and revise them. The authors also present a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n\nStrengths:\n\n1. The paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification.\n2. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners.\n3. The paper presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n4. The authors demonstrate the practicality of the framework by implementing it as a fully automated framework and evaluating its efficiency on standard benchmark sets.\n5. The paper provides a detailed analysis of the limitations of the framework and points to future research directions to extend the framework.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs can generate correct and relevant properties, which may not always be the case.\n2. The paper does not provide a detailed analysis of the limitations of the automated reasoners and how they can be improved.\n3. The paper does not provide a detailed comparison with existing approaches to automated program verification.\n4. The paper assumes that the program verification tasks can be decomposed into smaller, more manageable sub-tasks, which may not always be the case.\n5. The paper does not provide a detailed analysis of the scalability of the framework to large programs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the limitations of the LLMs and how they can be improved.\n2. The authors should provide a detailed comparison with existing approaches to automated program verification.\n3. The authors should provide a detailed analysis of the scalability of the framework to large programs.\n4. The authors should provide a more detailed analysis of the limitations of the automated reasoners and how they can be improved.\n5. The authors should provide a more detailed evaluation of the framework on a larger set of benchmarks.\n\nRecommendation:\n\nThe paper is well-written and presents a novel and innovative framework for automated program verification. The authors provide a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners. The paper also presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure. The authors demonstrate the practicality of the framework by implementing it as a fully automated framework and evaluating its efficiency on standard benchmark sets. However, the paper assumes that the LLMs can generate correct and relevant properties, which may not always be the case. Therefore, the paper requires further improvement to address these limitations.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework, LEMUR, is a proof system that combines the power of LLMs to perform high-level abstract reasoning with the precision of automated reasoners to perform low-level reasoning. The authors provide a formal calculus for LEMUR, which is sound and terminating. They also present a practical algorithmic instantiation of the calculus, which they demonstrate on several benchmark sets.\n\nStrengths:\n\n* The paper provides a formal calculus for LEMUR, which is sound and terminating.\n* The authors demonstrate the practical efficiency of LEMUR on several benchmark sets, including Code2Inv and SV-COMP benchmarks.\n* The paper highlights the complementary strengths of LLMs and automated reasoners, which can be combined to achieve more efficient and effective automated program verification.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used in LEMUR are capable of generating correct and useful properties, which may not always be the case.\n* The paper does not provide a comprehensive evaluation of the performance of LEMUR on a wide range of benchmark sets.\n* The paper assumes that the automated reasoners used in LEMUR are sound and complete, which may not always be the case.\n\nSuggestions for Improvement:\n\n* The authors could provide more detailed explanations of the LLMs used in LEMUR and how they are trained to generate correct and useful properties.\n* The authors could provide a more comprehensive evaluation of the performance of LEMUR on a wide range of benchmark sets, including both synthetic and real-world programs.\n* The authors could provide more information on how to ensure the soundness and completeness of the automated reasoners used in LEMUR.\n\nRecommendation:\n\nThis paper provides a significant contribution to the field of automated program verification by proposing a novel framework that integrates LLMs with automated reasoners. The framework, LEMUR, is sound and terminating, and the authors demonstrate its practical efficiency on several benchmark sets. While there are some weaknesses and limitations to the paper, the authors provide a clear and concise presentation of the framework and its evaluation. Therefore, I highly recommend this paper for publication in a top-tier conference on automated program verification.\n\nRating: 8/10",
        "gr_score_with": 0.911864406779661,
        "gr_score_without": 0.9078947368421053,
        "output_without": {
            "num_tokens_scored": 304,
            "num_green_tokens": 276,
            "green_fraction": 0.9078947368421053,
            "z_score": 0.45900636371913206,
            "p_value": 0.3231147981156139
        },
        "output_with": {
            "num_tokens_scored": 295,
            "num_green_tokens": 269,
            "green_fraction": 0.911864406779661,
            "z_score": 0.6794287321386968,
            "p_value": 0.24843312493007763
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method combines the Eulerian and Lagrangian views to model scene motion, enabling the discovery of rigid parts. The authors propose a hybrid representation of feature volume and neural network, which allows for fast and high-quality reconstruction. The method is evaluated on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks, demonstrating its ability to achieve high-quality rendering and part discovery.\n\nStrengths:\n\n* The paper presents a novel approach to dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n* The method combines the Eulerian and Lagrangian views to model scene motion, providing a more comprehensive understanding of the scene.\n* The authors propose a hybrid representation of feature volume and neural network, which allows for fast and high-quality reconstruction.\n* The method is evaluated on multiple datasets, demonstrating its ability to achieve high-quality rendering and part discovery.\n\nWeaknesses:\n\n* The paper assumes a piece-wise rigid scene, which may not be applicable to all scenes.\n* The method requires a large number of parameters, which may lead to overfitting.\n* The paper does not provide a clear explanation of how the Lagrangian view is used to discover parts.\n* The method is limited to monocular input, which may not be applicable to all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of how the Lagrangian view is used to discover parts.\n* The method could be improved by incorporating more prior knowledge about the scene, such as object shapes and motion patterns.\n* The authors should provide more quantitative results to demonstrate the effectiveness of the method.\n* The method could be evaluated on more challenging datasets, such as scenes with complex geometry and topology.\n\nRecommendation:\n\nThe paper presents a novel approach to dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method combines the Eulerian and Lagrangian views to model scene motion, providing a more comprehensive understanding of the scene. The authors propose a hybrid representation of feature volume and neural network, which allows for fast and high-quality reconstruction. However, the paper has some limitations, such as assuming a piece-wise rigid scene and requiring a large number of parameters. Overall, the paper is well-written and provides a clear explanation of the method. With some improvements, the method has the potential to be a powerful tool for dynamic scene reconstruction and part discovery.\n\nRating: 7.5/10\n\nRecommendation: Accept",
        "gr_score_with": 0.9249146757679181,
        "gr_score_without": 0.8689655172413793,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 252,
            "green_fraction": 0.8689655172413793,
            "z_score": -1.761459207558479,
            "p_value": 0.9609196442602054
        },
        "output_with": {
            "num_tokens_scored": 293,
            "num_green_tokens": 271,
            "green_fraction": 0.9249146757679181,
            "z_score": 1.421726754206811,
            "p_value": 0.07755279508383521
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks with small initialization. The authors consider a training dataset with well-separated input vectors and provide a careful analysis of the neurons' directional dynamics. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nStrengths:\n\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5, which is a significant improvement over previous work.\n3. The paper provides a quantitative bound on the time it takes for good alignment to be achieved, which is O(logn\u221a\u00b5).\n4. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nWeaknesses:\n\n1. The paper assumes that the training dataset has well-separated input vectors, which may not be a realistic assumption in practice.\n2. The authors use a balanced initialization scheme, which may not be a common initialization scheme in practice.\n3. The paper does not provide a detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n2. The paper should be extended to consider more realistic initialization schemes and training datasets.\n3. The authors should provide more numerical experiments to validate their theoretical findings.\n\nRecommendation:\n\nThis paper provides a significant contribution to the understanding of the dynamics of gradient flow for training two-layer ReLU networks with small initialization. The authors' analysis is thorough and provides a quantitative bound on the time it takes for good alignment to be achieved. However, the paper assumes a balanced initialization scheme and well-separated input vectors, which may not be realistic in practice. Therefore, I recommend that the authors extend their analysis to more realistic initialization schemes and training datasets.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. The authors consider a training dataset with well-separated input vectors and provide a complete analysis of the dynamics of gradient flow. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on their corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using numerical experiments on the MNIST dataset.\n\nStrengths:\nThe paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network with small initialization. The authors provide a clear and concise presentation of the problem, the analysis, and the numerical experiments. The paper is well-organized, and the authors provide a clear explanation of the main contributions and the implications of their results. The authors also provide a thorough comparison with prior work and highlight the differences between their analysis and previous studies.\n\nWeaknesses:\nThe paper assumes a well-separated input dataset, which might not be the case in many real-world applications. The authors also assume that the data points are not orthogonal to each other, which might not be the case in some datasets. The paper does not provide a clear explanation of how the analysis can be extended to more general cases. The authors also do not provide a thorough comparison with other optimization algorithms, such as stochastic gradient descent or Adam.\n\nSuggestions for Improvement:\nThe authors should provide a more detailed explanation of how the analysis can be extended to more general cases, such as non-well-separated datasets or orthogonal data points. The authors should also provide a thorough comparison with other optimization algorithms to demonstrate the effectiveness of gradient flow in this setting. The authors should also provide more numerical experiments to validate their theoretical findings, such as experiments with different initialization schemes or different datasets.\n\nRecommendation:\nThis paper provides a complete and thorough analysis of the dynamics of gradient flow for training a two-layer ReLU network with small initialization. The authors provide a clear and concise presentation of the problem, the analysis, and the numerical experiments. The paper is well-organized, and the authors provide a clear explanation of the main contributions and the implications of their results. I recommend this paper for publication in a top-tier machine learning conference, with the suggestions for improvement noted above.",
        "gr_score_with": 0.9505813953488372,
        "gr_score_without": 0.9223880597014925,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 309,
            "green_fraction": 0.9223880597014925,
            "z_score": 1.3660672524425643,
            "p_value": 0.0859589278434365
        },
        "output_with": {
            "num_tokens_scored": 344,
            "num_green_tokens": 327,
            "green_fraction": 0.9505813953488372,
            "z_score": 3.127299890100284,
            "p_value": 0.0008820993625979148
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper addresses a significant challenge in smart city applications, namely data scarcity in many cities and regions.\n2. The proposed framework is novel and effective, leveraging pre-training paradigm to achieve effective fine-grained spatio-temporal knowledge transfer across different cities.\n3. The framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The authors provide a comprehensive experimental evaluation, demonstrating the effectiveness of their framework on multiple real-world datasets.\n\nWeaknesses:\n\n1. The paper assumes access to a collection of optimized neural network parameters from source cities, which may not be feasible in practice.\n2. The framework relies on the quality of the prompts, which can be challenging to design and optimize.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed framework.\n4. The authors do not discuss potential limitations and challenges of scaling the framework to larger cities or more complex spatio-temporal prediction tasks.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically generating high-quality prompts, such as leveraging large language models or transfer learning techniques.\n2. Explore ways to reduce the computational complexity of the framework, such as using more efficient diffusion models or approximations.\n3. Conduct a more detailed analysis of the framework's scalability and limitations, including its performance on larger cities or more complex spatio-temporal prediction tasks.\n4. Provide more insight into the interpretability of the framework's generated neural networks and the transferable knowledge they represent.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for spatio-temporal few-shot learning with urban knowledge transfer. While there are some limitations and challenges to be addressed, the framework has the potential to revolutionize smart city applications in data-scarce environments. I recommend accepting the paper for publication in its current form, with the suggestion that the authors address the limitations and challenges mentioned above in future work.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper presents a novel approach to address the challenges of spatio-temporal few-shot learning, which is a pressing problem in smart city applications.\n2. The authors provide a comprehensive overview of the existing methods and highlight the limitations of the current approaches.\n3. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The experimental results demonstrate the effectiveness of the proposed framework in various scenarios, including short-term and long-term predictions.\n5. The authors provide a thorough analysis of the influence of different source cities and conditioning strategies on the model's performance.\n\nWeaknesses:\n\n1. The paper assumes that the source cities have rich data, which might not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of the proposed framework.\n3. The paper relies heavily on the Transformer-based diffusion model, which might not be suitable for all types of spatio-temporal prediction tasks.\n4. The authors do not provide a clear explanation of how the region prompts are designed and how they are used in the framework.\n5. The paper assumes that the target city has a similar structure to the source cities, which might not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational cost of the proposed framework and how it can be optimized for real-world applications.\n2. Investigate the use of other types of models, such as graph neural networks, to improve the adaptability of the framework.\n3. Provide a more thorough explanation of how the region prompts are designed and how they are used in the framework.\n4. Investigate the use of transfer learning techniques to improve the performance of the framework in scenarios where the source cities have limited data.\n5. Provide a more detailed analysis of the influence of different hyperparameters on the model's performance.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to address the challenges of spatio-temporal few-shot learning. The proposed framework is model-agnostic and demonstrates excellent performance in various scenarios. However, there are some limitations and areas for improvement that need to be addressed. Overall, this paper is a valuable contribution to the field of urban computing and has the potential to revolutionize smart city applications in data-scarce environments.",
        "gr_score_with": 0.9139784946236559,
        "gr_score_without": 0.9005376344086021,
        "output_without": {
            "num_tokens_scored": 372,
            "num_green_tokens": 335,
            "green_fraction": 0.9005376344086021,
            "z_score": 0.03476508298573597,
            "p_value": 0.4861335317659007
        },
        "output_with": {
            "num_tokens_scored": 372,
            "num_green_tokens": 340,
            "green_fraction": 0.9139784946236559,
            "z_score": 0.8988795177566913,
            "p_value": 0.18435841946176373
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of widely used, open-source language models (LLMs) to prompt formatting choices, finding that performance spreads across tasks, models, and number of few-shot examples. They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats, allowing for the efficient exploration of the space. The authors demonstrate that FORMATSPREAD can be used to estimate the performance spread across prompt formatting choices, and that this spread is large regardless of model choice, even when increasing model size, number of few-shots, or using instruction tuning.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of using these models in real-world applications.\n2. The authors propose a novel grammar that defines the space of plausible prompt formats and semantic equivalence between formats, which allows for the efficient exploration of the space.\n3. The paper introduces FORMATSPREAD, an algorithm that can be used to estimate the performance spread across prompt formatting choices, which is a valuable tool for researchers and practitioners.\n4. The authors provide a thorough evaluation of the algorithm on a large set of tasks and models, demonstrating its effectiveness in estimating performance spread.\n\nWeaknesses:\n\n1. The paper assumes that the grammar is manually constructed, which may not be practical for large-scale applications.\n2. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation for large-scale applications.\n3. The paper focuses on classification tasks and does not explore other types of tasks, such as open-ended text generation.\n4. The authors do not provide a detailed analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nSuggestions for Improvement:\n\n1. Develop a more automated method for constructing the grammar, which could be based on machine learning algorithms or other techniques.\n2. Provide a more detailed analysis of the computational cost of FORMATSPREAD and explore ways to reduce it.\n3. Explore the use of FORMATSPREAD on other types of tasks, such as open-ended text generation.\n4. Conduct a more thorough analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of natural language processing and language modeling, and the authors should be commended for their thorough analysis of the sensitivity of LLMs to prompt formatting choices. However, to make the paper more impactful, the authors should address the limitations mentioned above and provide a more detailed analysis of the algorithm's computational cost and robustness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of large language models (LLMs) to prompt formatting choices, finding significant performance spread across tasks, even with increased model size, number of few-shot examples, or instruction tuning. The authors propose a grammar to define the space of plausible prompt formats and semantic equivalence between formats. They use FORMATSPREAD to evaluate the spread of several widely-used open-source LLMs for classification tasks in few-shot learning settings. The results show that spread is large regardless of model choice, and the authors recommend that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible formats.\n\nStrengths:\n\n* The paper introduces a novel algorithm, FORMATSPREAD, to efficiently search the space of plausible prompt formats.\n* The authors provide a thorough analysis of the sensitivity of LLMs to prompt formatting choices.\n* The results show significant performance spread across tasks, even with increased model size, number of few-shot examples, or instruction tuning.\n* The authors propose a grammar to define the space of plausible prompt formats and semantic equivalence between formats.\n\nWeaknesses:\n\n* The paper assumes a fixed set of plausible prompt formats, which may not capture all possible formats used by humans.\n* The authors focus on classification tasks and few-shot learning settings, which may not be representative of all use cases.\n* The results may be sensitive to the choice of evaluation metrics and the specific LLMs used.\n* The paper does not provide a clear explanation of how to apply FORMATSPREAD to more complex tasks or tasks with longer input instructions.\n\nSuggestions for Improvement:\n\n* Consider extending the grammar to capture more complex prompt formats and semantic equivalence relationships.\n* Evaluate FORMATSPREAD on a broader range of tasks and use cases, including more complex tasks and tasks with longer input instructions.\n* Provide more detailed analysis of the results, including the impact of different evaluation metrics and LLMs on the performance spread.\n* Consider incorporating more domain-specific knowledge into the grammar and evaluation process to improve the relevance of the results.\n\nRecommendation:\n\nThe paper presents an important contribution to the field of LLMs and prompt engineering. The introduction of FORMATSPREAD and the analysis of prompt formatting sensitivity are valuable insights for the research community. However, the results may be limited by the assumptions made in the paper, and further work is needed to extend and generalize the findings. I recommend this paper for publication in a top-tier conference or journal, with the understanding that the authors will address the limitations and suggestions for improvement outlined above.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9296875,
        "gr_score_without": 0.9265536723163842,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 328,
            "green_fraction": 0.9265536723163842,
            "z_score": 1.6655203888990466,
            "p_value": 0.04790448967333774
        },
        "output_with": {
            "num_tokens_scored": 384,
            "num_green_tokens": 357,
            "green_fraction": 0.9296875,
            "z_score": 1.9393562149667491,
            "p_value": 0.02622898908337817
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem induced by a soft DSF. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nStrengths:\n\n* The paper proposes a novel sorting network that learns to sort high-dimensional inputs.\n* The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n* The authors provide a detailed analysis of the error accumulation problem and propose an error-free DSF to resolve it.\n* The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nWeaknesses:\n\n* The paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications.\n* The proposed method may not be effective for sorting low-dimensional inputs or inputs with clear ordering information.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n* The paper assumes that the supervision is provided in the form of permutation matrices, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The paper should discuss the limitations of the proposed method and provide a more detailed analysis of the cases where it may not be effective.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method on diverse sorting benchmarks.\n* The paper should discuss the potential negative societal impact of the proposed method and provide a more detailed analysis of the cases where it may be considered as an unethical use of artificial intelligence.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, the paper requires further research to address the limitations and potential negative societal impact of the proposed method.\n\nOverall, the paper is well-written and provides a novel contribution to the field of sorting networks. However, it requires further research to address the limitations and potential negative societal impact of the proposed method.\n\nRecommendation: Accept\n\n$$$\nNote: The paper requires further research to address the limitations and potential negative societal impact of the proposed method. Therefore, the recommendation is to accept the paper with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem of conventional DSFs and is still differentiable. The paper presents a robust Transformer-based network that satisfies the permutation-equivariant property, which is essential for sorting tasks. The authors demonstrate the effectiveness of their methods on diverse sorting benchmarks, including sorting multi-digit images and image fragments.\n\nStrengths:\n\n1. The paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF.\n2. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n3. The authors propose an error-free DSF that resolves the error accumulation problem of conventional DSFs and is still differentiable.\n4. The paper presents a robust Transformer-based network that satisfies the permutation-equivariant property, which is essential for sorting tasks.\n5. The authors demonstrate the effectiveness of their methods on diverse sorting benchmarks, including sorting multi-digit images and image fragments.\n\nWeaknesses:\n\n1. The paper assumes that the inputs are high-dimensional vectors, which may not be the case for all sorting tasks.\n2. The proposed method may not be effective for sorting tasks that require a large number of comparisons.\n3. The authors do not provide a thorough analysis of the computational complexity of their method.\n4. The paper does not provide a clear explanation of how the proposed method can be applied to real-world sorting tasks.\n5. The authors do not provide a thorough discussion of the potential negative societal impacts of their method.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of their method.\n2. The paper should include a thorough discussion of the potential negative societal impacts of the proposed method.\n3. The authors should provide a clear explanation of how the proposed method can be applied to real-world sorting tasks.\n4. The paper should include more experimental results to demonstrate the effectiveness of the proposed method on a wider range of sorting tasks.\n5. The authors should provide a more detailed analysis of the properties of the proposed DSF, including its non-decreasing condition and differentiability.\n\nRecommendation:\n\nThe paper presents a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem of conventional DSFs and is still differentiable. The paper presents a robust Transformer-based network that satisfies the permutation-equivariant property, which is essential for sorting tasks. However, the paper assumes that the inputs are high-dimensional vectors, which may not be the case for all sorting tasks. The authors should provide a more detailed analysis of the computational complexity of their method and a clear explanation of how the proposed method can be applied to real-world sorting tasks. With these improvements, the paper has the potential to be a significant contribution to the field of sorting algorithms.\n\nRating: 7.5/10\n\n$$$",
        "gr_score_with": 0.9571428571428572,
        "gr_score_without": 0.9111111111111111,
        "output_without": {
            "num_tokens_scored": 270,
            "num_green_tokens": 246,
            "green_fraction": 0.9111111111111111,
            "z_score": 0.608743003309306,
            "p_value": 0.2713473990874823
        },
        "output_with": {
            "num_tokens_scored": 280,
            "num_green_tokens": 268,
            "green_fraction": 0.9571428571428572,
            "z_score": 3.1874060671301137,
            "p_value": 0.0007177753901147316
        }
    },
    {
        "title": "WEAKER MVI CONDITION: EXTRAGRADIENT METH-",
        "abstract": "This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems. One of these follows the weak Minty variational inequality (weak MVI), which is motivated by relaxing Minty variational inequality (MVI) without compromising convergence guarantee of extragradient algorithm. Existing extragradient-type algorithms involve one exploration step and one update step per iteration. We analyze the algorithms with multiple exploration steps and show that current assumption can be further relaxed when more exploration is introduced. Furthermore, we design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors focus on the weak Minty variational inequality (weak MVI) problem, which is a relaxation of the Minty variational inequality without compromising convergence guarantees. They analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. Furthermore, they design an adaptive algorithm that explores until the optimal improvement is achieved. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases.\n\nStrengths:\n\n1. The paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm.\n2. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced.\n3. The paper designs an adaptive algorithm that explores until the optimal improvement is achieved.\n4. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms.\n5. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which is a significant improvement.\n\nWeaknesses:\n\n1. The paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, which may not be the case in practice.\n2. The authors use a tolerance \u03b52 to prevent the algorithm from incorrect convergence, which may not be necessary in some cases.\n3. The paper does not provide a detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n4. The authors do not provide a theoretical analysis of the convergence rate of the adaptive algorithm.\n5. The paper assumes that the weak MVI condition holds, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n2. Theoretically analyze the convergence rate of the adaptive algorithm.\n3. Relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave.\n4. Provide a more detailed explanation of the tolerance \u03b52 and its role in preventing incorrect convergence.\n5. Provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nRecommendation:\n\nThis paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases. However, the paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, and uses a tolerance \u03b52 to prevent the algorithm from incorrect convergence. Therefore, I recommend that the authors provide a more detailed comparison with other algorithms, theoretically analyze the convergence rate of the adaptive algorithm, relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave, provide a more detailed explanation of the tolerance \u03b52, and provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nOverall, this paper provides a significant contribution to the field of optimization and machine learning, and I recommend that it be published in a reputable journal.\n\nRating: 8/10\n\nRecommendation: Publish",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework for solving weak Minty variational inequality (MVI) problems by generalizing the extragradient algorithm to multi-step cases. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. They also design an adaptive algorithm that explores until the optimal improvement is achieved. The proposed framework provides a convergence guarantee for problems where \u03c1 \u2208 (\u2212(1\u22121/e)/L,\u221e), where e is Euler\u2019s number.\n\nStrengths:\n\n1. The authors provide a comprehensive analysis of the extragradient algorithm with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced.\n2. The proposed framework provides a convergence guarantee for problems where \u03c1 \u2208 (\u2212(1\u22121/e)/L,\u221e), which is a significant improvement over the existing result of \u03c1 > \u22121/2L.\n3. The authors design an adaptive algorithm that explores until the optimal improvement is achieved, which is a novel approach to addressing weak MVI problems.\n\nWeaknesses:\n\n1. The proposed framework requires knowledge of the problem parameter \u03c1, which may not be available in practice.\n2. The authors do not provide a clear explanation of how to choose the parameters \u03c3k and \u03bbk in the adaptive algorithm.\n3. The paper focuses on weak MVI problems and does not provide a general framework for solving nonconvex-nonconcave problems.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to choose the parameters \u03c3k and \u03bbk in the adaptive algorithm.\n2. Develop a framework for solving nonconvex-nonconcave problems that does not require knowledge of the problem parameter \u03c1.\n3. Provide more experimental results to demonstrate the effectiveness of the proposed framework.\n\nRecommendation:\n\nThis paper provides a novel framework for solving weak MVI problems by generalizing the extragradient algorithm to multi-step cases. The proposed framework provides a convergence guarantee for problems where \u03c1 \u2208 (\u2212(1\u22121/e)/L,\u221e), which is a significant improvement over the existing result of \u03c1 > \u22121/2L. However, the framework requires knowledge of the problem parameter \u03c1, which may not be available in practice. Therefore, I recommend that the authors provide a clear explanation of how to choose the parameters \u03c3k and \u03bbk in the adaptive algorithm and develop a framework for solving nonconvex-nonconcave problems that does not require knowledge of the problem parameter \u03c1.\n\n$$$\nNote: The above review is based on the provided text and does not include any external knowledge or references.",
        "gr_score_with": 0.8676470588235294,
        "gr_score_without": 0.8303571428571429,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 279,
            "green_fraction": 0.8303571428571429,
            "z_score": -4.254999321451358,
            "p_value": 0.9999895475175401
        },
        "output_with": {
            "num_tokens_scored": 272,
            "num_green_tokens": 236,
            "green_fraction": 0.8676470588235294,
            "z_score": -1.7783984800277717,
            "p_value": 0.9623307839216716
        }
    },
    {
        "title": "TAIL: TASK-SPECIFIC ADAPTERS FOR IMITATION LEARNING WITH LARGE PRETRAINED MODELS",
        "abstract": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly due to data scarcity and computational challenges associated with training or fine-tuning large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to a stream of new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques\u2014e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)\u2014in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained models to new control tasks. The authors explore parameter-efficient fine-tuning (PEFT) techniques, specifically Low-Rank Adaptation (LoRA), and demonstrate their potential in enhancing adaptation efficiency, mitigating catastrophic forgetting, and ensuring robust performance across diverse tasks. The proposed framework is evaluated on the LIBERO robotic manipulation continual learning benchmark, showcasing its effectiveness in adapting to new tasks while preserving performance on previous tasks.\n\nStrengths:\n\n1. The paper proposes a novel framework for efficient adaptation of large pretrained models, addressing the challenges of catastrophic forgetting and overfitting in continual learning scenarios.\n2. The authors provide a comprehensive exploration of PEFT techniques, including LoRA, Bottleneck Adapter, and Prefix Tuning, and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting.\n3. The paper presents extensive experimental results on the LIBERO benchmark, showcasing the effectiveness of TAIL in adapting to new tasks while preserving performance on previous tasks.\n4. The authors provide a thorough analysis of the strengths and limitations of each adaptation strategy, providing valuable insights for future research.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the pretrained model, which may not be optimal for all applications.\n2. The authors do not provide a detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. The paper assumes a fixed dataset for pretraining, which may not be representative of all real-world scenarios.\n4. The authors do not provide a detailed analysis of the robustness of TAIL to noisy or incomplete data.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of varying the architecture of the pretrained model on the performance of TAIL.\n2. Conduct a more detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. Evaluate the robustness of TAIL to noisy or incomplete data.\n4. Explore the application of TAIL to other domains, such as reinforcement learning.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for efficient adaptation of large pretrained models in continual learning scenarios. The authors provide a comprehensive exploration of PEFT techniques and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting. While there are some limitations to the paper, the proposed framework has the potential to significantly impact the field of artificial intelligence and robotics. Therefore, I recommend acceptance of the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained decision-making models to a stream of new control tasks. TAIL incorporates lightweight adapter modules into pretrained decision-making models and comprehensively compares efficient adaptation techniques implemented in TAIL in a continual imitation learning setting. The authors explore parameter-efficient adaptation techniques such as Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) to adapt large pretrained models for new tasks with limited demonstration data. The experiments demonstrate that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity.\n\nStrengths:\n\n1. The paper proposes a novel framework for efficient adaptation of large pretrained decision-making models to a stream of new control tasks.\n2. The authors comprehensively compare efficient adaptation techniques implemented in TAIL in a continual imitation learning setting.\n3. The experiments demonstrate that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning.\n4. The paper highlights the importance of preserving adaptation plasticity and avoiding catastrophic forgetting in continual learning scenarios.\n\nWeaknesses:\n\n1. The paper relies heavily on the performance of LoRA, which may not be the most effective adaptation technique for all tasks.\n2. The experiments are conducted on a specific benchmark (LIBERO) and may not generalize to other tasks or domains.\n3. The paper does not provide a thorough analysis of the computational costs of TAIL compared to other adaptation methods.\n\nSuggestions for Improvement:\n\n1. Conduct experiments on other benchmarks and tasks to evaluate the generalizability of TAIL.\n2. Investigate other adaptation techniques and their performance on TAIL.\n3. Provide a more detailed analysis of the computational costs of TAIL compared to other adaptation methods.\n\nRecommendation:\n\nThe paper proposes a novel and effective framework for efficient adaptation of large pretrained decision-making models. The experiments demonstrate the potential of TAIL with LoRA to achieve state-of-the-art performance in continual learning scenarios. However, further investigation is needed to evaluate the generalizability of TAIL and to explore other adaptation techniques. Therefore, I recommend acceptance of the paper with minor revisions.\n\n$$$",
        "gr_score_with": 0.8957654723127035,
        "gr_score_without": 0.8654434250764526,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 283,
            "green_fraction": 0.8654434250764526,
            "z_score": -2.082754567091218,
            "p_value": 0.9813631996112063
        },
        "output_with": {
            "num_tokens_scored": 307,
            "num_green_tokens": 275,
            "green_fraction": 0.8957654723127035,
            "z_score": -0.24713082047071036,
            "p_value": 0.5975965089746569
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel regularization technique, zero-mean regularization, for spectral contrastive learning (SpeCL). The authors argue that SpeCL has two limitations: (i) it requires the orthogonality of representations between negative pairs, and (ii) it does not account for wrong connections in the positive-pair graph. To address these limitations, they introduce an additive factor, \u03c4, into the term of SpeCL involving negative pairs. This modification is equivalent to regularizing the mean of representations to be zero, referred to as zero-mean regularization. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem and the proposed solution.\n2. The authors provide a thorough theoretical analysis of the benefits of zero-mean regularization in both unsupervised and supervised scenarios.\n3. The paper presents extensive experimental results to verify the effectiveness of zero-mean regularization on various benchmark datasets.\n4. The authors provide a clear explanation of the implications of zero-mean regularization on the structure of the learned representations.\n\nWeaknesses:\n\n1. The paper assumes a specific form of the positive-pair graph, which may not be realistic in all scenarios.\n2. The authors do not provide a clear explanation of how to choose the value of \u03c4 in practice.\n3. The paper does not discuss the computational cost of incorporating zero-mean regularization into the SpeCL algorithm.\n4. The authors do not provide a comparison with other regularization techniques, such as dropout or weight decay.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the assumptions made about the positive-pair graph and how they affect the results.\n2. Discuss the practical implications of choosing the value of \u03c4 and provide guidance on how to select it.\n3. Investigate the computational cost of incorporating zero-mean regularization into the SpeCL algorithm and discuss potential ways to reduce it.\n4. Compare the performance of zero-mean regularization with other regularization techniques, such as dropout or weight decay, to provide a more comprehensive evaluation of its benefits.\n\nRecommendation:\n\nThe paper presents a novel and effective regularization technique for spectral contrastive learning. The theoretical analysis and experimental results demonstrate the benefits of zero-mean regularization in mitigating adverse effects of wrong connections. However, the paper could be improved by providing more detailed explanations of the assumptions made about the positive-pair graph and the practical implications of choosing the value of \u03c4. Additionally, a comparison with other regularization techniques would provide a more comprehensive evaluation of the benefits of zero-mean regularization. Overall, the paper is well-written and presents a significant contribution to the field of contrastive learning.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes zero-mean regularization to enhance spectral contrastive learning (SpeCL), which is a self-supervised learning paradigm. The authors introduce an additive factor into the loss component involving negative pairs, equivalent to regularizing the mean of representations to be zero. The proposed modification, referred to as zero-mean regularization, is shown to implicitly alleviate the adverse effects of wrong connections in the positive-pair graph, leading to better performance and robustness. The paper investigates the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios, with theoretical analysis and quantitative evaluation. The results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.\n\nStrengths:\n\n1. The paper proposes a novel modification to spectral contrastive learning, which is shown to be effective in alleviating the adverse effects of wrong connections in the positive-pair graph.\n2. The authors provide thorough theoretical analysis and quantitative evaluation of the proposed modification in both unsupervised and supervised scenarios.\n3. The paper highlights the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.\n4. The results demonstrate that the proposed modification can improve the discriminativeness of representations, especially in the supervised scenario.\n\nWeaknesses:\n\n1. The paper assumes that the underlying structure of the positive-pair graph is unknown, which may limit the applicability of the proposed modification.\n2. The authors do not provide a detailed comparison with other state-of-the-art self-supervised learning paradigms.\n3. The paper assumes that the noise in the positive-pair graph is independent, which may not be the case in practice.\n4. The results are limited to the specific datasets and models used in the experiments.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other state-of-the-art self-supervised learning paradigms.\n2. Investigate the robustness of the proposed modification to different types of noise in the positive-pair graph.\n3. Provide more experimental results to demonstrate the effectiveness of the proposed modification in various tasks.\n4. Investigate the computational cost of the proposed modification and provide guidance on how to implement it efficiently.\n\nRecommendation:\n\nThe paper proposes a novel modification to spectral contrastive learning, which is shown to be effective in alleviating the adverse effects of wrong connections in the positive-pair graph. The results demonstrate the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks. However, the paper assumes that the underlying structure of the positive-pair graph is unknown, which may limit the applicability of the proposed modification. Therefore, I recommend that the authors provide more experimental results to demonstrate the effectiveness of the proposed modification in various tasks and investigate the robustness of the proposed modification to different types of noise in the positive-pair graph.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9016393442622951,
        "gr_score_without": 0.8689458689458689,
        "output_without": {
            "num_tokens_scored": 351,
            "num_green_tokens": 305,
            "green_fraction": 0.8689458689458689,
            "z_score": -1.9391082180854458,
            "p_value": 0.9737559188857186
        },
        "output_with": {
            "num_tokens_scored": 305,
            "num_green_tokens": 275,
            "green_fraction": 0.9016393442622951,
            "z_score": 0.09561328657180058,
            "p_value": 0.46191385619555936
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline, RDesign, which aims to systematically address the challenge of designing RNA sequences that fold into a specified tertiary structure. The authors develop a comprehensive structural modeling approach to represent the complex RNA tertiary structure and design a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. They also incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method is evaluated on standard RNA design benchmarks and generalization ability assessments, demonstrating its efficacy and providing a reliable pipeline for future research in this field.\n\nStrengths:\n\n1. The authors provide a thorough understanding of the challenges associated with RNA tertiary structure modeling and design.\n2. The proposed RDesign pipeline is comprehensive, incorporating both tertiary structure modeling and secondary structure imposing strategies.\n3. The hierarchical data-efficient representation learning framework is an innovative approach to capturing the intrinsic relationships within the data.\n4. The experimental results demonstrate the effectiveness of the proposed method, outperforming existing baseline models on both recovery and Macro-F1 metrics.\n5. The authors provide a clear and concise explanation of the methodology, making the paper accessible to a broad audience.\n\nWeaknesses:\n\n1. The paper assumes a large dataset of RNA tertiary structures, which may not be available or easily accessible.\n2. The authors do not provide a detailed explanation of the training process, making it difficult to replicate the results.\n3. The ablation study is limited, and further experiments are needed to fully understand the impact of each component.\n4. The paper does not discuss the limitations of the proposed method, such as its applicability to different types of RNA structures.\n5. The conclusion could be more comprehensive, summarizing the main contributions and highlighting the potential applications of the proposed method.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the training process, including the hyperparameter tuning and data preprocessing steps.\n2. Conduct additional experiments to further validate the proposed method, such as testing its performance on different types of RNA structures or evaluating its robustness to noise and outliers.\n3. Discuss the limitations of the proposed method and potential avenues for future research.\n4. Provide a more comprehensive conclusion, summarizing the main contributions and highlighting the potential applications of the proposed method.\n5. Consider including more visualization examples to illustrate the effectiveness of the proposed method.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to RNA design, leveraging both tertiary structure modeling and secondary structure imposing strategies. The experimental results demonstrate the effectiveness of the proposed method, and the hierarchical data-efficient representation learning framework is an innovative approach to capturing the intrinsic relationships within the data. However, the paper could benefit from a more detailed explanation of the training process and additional experiments to further validate the proposed method. Overall, the paper is well-written and provides a valuable contribution to the field of RNA design.",
        "gr_score_with": 0.9205955334987593,
        "gr_score_without": 0.9161290322580645,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 284,
            "green_fraction": 0.9161290322580645,
            "z_score": 0.9467729721084766,
            "p_value": 0.171877238534891
        },
        "output_with": {
            "num_tokens_scored": 403,
            "num_green_tokens": 371,
            "green_fraction": 0.9205955334987593,
            "z_score": 1.3783644220371378,
            "p_value": 0.08404540110402003
        }
    },
    {
        "title": "ONE-TO-MANY POLICY TRANSFER",
        "abstract": "We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named Meta-Evolve that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2\u00d7 and one-to-six transfer of agile locomotion policy by 2.4\u00d7 in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers. Supplementary videos available at the project website: https://sites.google.com/view/meta-evolve.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for transferring an expert policy from a source robot to multiple target robots. The method uses continuous robot evolution to efficiently transfer the policy through a tree-structured evolutionary robot sequence. The paper presents a heuristic approach to determine an optimized robot evolution tree and shows that the method can significantly outperform one-to-one policy transfer baselines in terms of simulation cost and training iterations.\n\nStrengths:\n\n* The paper introduces a new research problem of transferring an expert policy from a source robot to multiple target robots, which is an important problem in robotics.\n* The method proposed, Meta-Evolve, is a novel and efficient approach to solve this problem.\n* The paper presents a heuristic approach to determine the robot evolution tree, which is a challenging problem.\n* The experiments conducted on Hand Manipulation Suite tasks and an agile locomotion task show that the method can significantly outperform one-to-one policy transfer baselines.\n\nWeaknesses:\n\n* The paper assumes that the source and target robots can be matched in their kinematic tree topology, which may not always be the case.\n* The method proposed relies on the assumption that the target robots are mutually similar in morphology and kinematics, which may not always be true.\n* The paper does not provide a formal proof of the convergence of the method, which is an important aspect of the method's theoretical validity.\n* The experiments conducted are limited to two tasks and it is unclear whether the method can generalize to other tasks.\n\nSuggestions for Improvement:\n\n* Provide a formal proof of the convergence of the method to establish its theoretical validity.\n* Investigate the assumption that the source and target robots can be matched in their kinematic tree topology and propose a method to relax this assumption.\n* Explore the possibility of using other machine learning techniques, such as meta-learning, to improve the performance of the method.\n* Conduct more extensive experiments on a variety of tasks to demonstrate the method's generalizability.\n\nRecommendation:\n\nThe paper proposes a novel and efficient method for transferring an expert policy from a source robot to multiple target robots. While the method has some limitations, it shows promising results in the experiments conducted. With further development and refinement, the method has the potential to make significant contributions to the field of robotics. Therefore, I recommend the paper for publication in a reputable conference.\n\nRating: 7/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for efficiently transferring a well-trained policy from one source robot to multiple target robots. The method utilizes continuous robot evolution to find a set of tree-structured evolutionary robot sequences that connect the source robot to each target robot. This allows for the sharing of evolution paths among target robots, reducing the cost of exploration and training during policy transfer. The authors present a heuristic approach to determine the structure of the evolution tree and conduct experiments on manipulation tasks and agile locomotion tasks to demonstrate the effectiveness of their method.\n\nStrengths:\n\n* The paper introduces a new research problem of one-to-many policy transfer and provides a novel solution using continuous robot evolution.\n* The method allows for the sharing of evolution paths among target robots, reducing the cost of exploration and training during policy transfer.\n* The authors provide a heuristic approach to determine the structure of the evolution tree, which is efficient and effective.\n* The experiments demonstrate the effectiveness of the method on manipulation tasks and agile locomotion tasks.\n\nWeaknesses:\n\n* The paper assumes that the state and action spaces of the source and target robots are shared, which may not always be the case in real-world scenarios.\n* The method requires a well-trained expert policy on the source robot, which may not always be available.\n* The heuristic approach to determine the structure of the evolution tree may not always yield the optimal solution.\n* The experiments are conducted on a limited number of tasks and robot morphologies, and it is unclear how the method will perform on more diverse scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how to handle cases where the state and action spaces of the source and target robots are not shared.\n* The authors should provide more information on how to obtain a well-trained expert policy on the source robot.\n* The authors should investigate more advanced algorithms for determining the structure of the evolution tree, such as genetic algorithms or reinforcement learning.\n* The authors should conduct experiments on a larger number of tasks and robot morphologies to demonstrate the robustness and generalizability of the method.\n\nRecommendation:\n\nThe paper provides a novel and effective solution to the problem of one-to-many policy transfer using continuous robot evolution. The method has the potential to significantly reduce the cost of exploration and training during policy transfer and can be applied to a wide range of tasks and robot morphologies. However, the authors should address the weaknesses and suggestions for improvement mentioned above to further improve the method and its robustness.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9197530864197531,
        "gr_score_without": 0.8918918918918919,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 297,
            "green_fraction": 0.8918918918918919,
            "z_score": -0.4930004215138797,
            "p_value": 0.6889938552466321
        },
        "output_with": {
            "num_tokens_scored": 324,
            "num_green_tokens": 298,
            "green_fraction": 0.9197530864197531,
            "z_score": 1.1853558806963236,
            "p_value": 0.1179383839448232
        }
    },
    {
        "title": "WHAT\u2019S IN MY BIG DATA?",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. The authors apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama, and uncover several surprising and previously undocumented findings about these corpora.\n\nStrengths:\n\n* The paper proposes a novel platform and set of analyses for understanding the contents of large text corpora.\n* The authors provide a comprehensive analysis of ten different corpora used to train popular language models.\n* The paper highlights the importance of understanding the contents of large text corpora and provides a framework for doing so.\n* The authors provide a clear and concise description of the WIMBD platform and its capabilities.\n* The paper includes a thorough analysis of the results, including both quantitative and qualitative findings.\n\nWeaknesses:\n\n* The paper assumes that the reader has a strong background in natural language processing and machine learning.\n* The authors do not provide a detailed description of the Elasticsearch search engine used in WIMBD.\n* The paper does not provide a clear explanation of the hash function used in the counting and searching algorithms.\n* The authors do not provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* The paper does not provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n\nSuggestions for Improvement:\n\n* Provide a more detailed description of the Elasticsearch search engine used in WIMBD.\n* Provide a clear explanation of the hash function used in the counting and searching algorithms.\n* Provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* Provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n* Consider including a more detailed analysis of the results, including a discussion of the implications of the findings.\n\nRecommendation:\n\nThis paper provides a novel and comprehensive framework for understanding the contents of large text corpora. The authors provide a clear and concise description of the WIMBD platform and its capabilities, and the paper includes a thorough analysis of the results. However, the paper assumes a strong background in natural language processing and machine learning, and some of the technical details are not clearly explained. Overall, this paper is a valuable contribution to the field of natural language processing and machine learning, and it has the potential to be a highly influential paper in the field.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. The authors apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama, and uncover several surprising and previously undocumented findings about these corpora.\n\nStrengths:\n\n* The paper proposes a novel platform and set of analyses for understanding the contents of large text corpora.\n* The authors provide a comprehensive analysis of ten different corpora used to train popular language models.\n* The paper highlights the importance of understanding the contents of large text corpora and provides a set of tools for doing so.\n* The authors provide a detailed description of the WIMBD platform and its capabilities.\n* The paper includes a thorough analysis of the results, including both quantitative and qualitative findings.\n\nWeaknesses:\n\n* The paper assumes that the reader has a strong background in natural language processing and machine learning.\n* The authors do not provide a detailed description of the experimental setup and the hardware used to run the analyses.\n* The paper does not provide a clear explanation of the methodology used to collect and preprocess the corpora.\n* The authors do not provide a clear explanation of the results in terms of their practical implications for the development of language models.\n* The paper does not provide a clear comparison with existing methods for analyzing the contents of large text corpora.\n\nSuggestions for Improvement:\n\n* Provide a more detailed description of the experimental setup and the hardware used to run the analyses.\n* Provide a clear explanation of the methodology used to collect and preprocess the corpora.\n* Provide a clear explanation of the results in terms of their practical implications for the development of language models.\n* Compare the results with existing methods for analyzing the contents of large text corpora.\n* Provide a more detailed analysis of the results, including both quantitative and qualitative findings.\n\nRecommendation:\n\nThe paper proposes a novel platform and set of analyses for understanding the contents of large text corpora, which is a timely and important topic in the field of natural language processing. The authors provide a comprehensive analysis of ten different corpora used to train popular language models and highlight the importance of understanding the contents of large text corpora. However, the paper assumes a strong background in natural language processing and machine learning, and does not provide a clear explanation of the results in terms of their practical implications for the development of language models. Therefore, I recommend that the authors revise the paper to address these weaknesses and provide a more detailed analysis of the results.",
        "gr_score_with": 0.9370629370629371,
        "gr_score_without": 0.9035369774919614,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 281,
            "green_fraction": 0.9035369774919614,
            "z_score": 0.20809804041276342,
            "p_value": 0.4175762085965142
        },
        "output_with": {
            "num_tokens_scored": 286,
            "num_green_tokens": 268,
            "green_fraction": 0.9370629370629371,
            "z_score": 2.089450647449199,
            "p_value": 0.018333588065301144
        }
    },
    {
        "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
        "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The model, called the recurrent circuit model, uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with a modulated global signal to sample from the prior at a rate prescribed by the period of the global oscillator. The model can flexibly use sensory and contextual sources of information to encode task-specific posteriors. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n\nStrengths:\n\n1. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions.\n2. The model is flexible and can be used to encode task-specific posteriors using sensory and contextual sources of information.\n3. The authors demonstrate that the model can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n4. The paper provides a thorough analysis of the model's performance and compares it to traditional diffusion models.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the dendritic tree, which may not be biologically realistic.\n2. The model's performance is evaluated on a limited set of tasks and distributions, and it is unclear how well it generalizes to other scenarios.\n3. The paper does not provide a clear explanation of how the model's parameters are learned, which is a critical component of any machine learning model.\n4. The paper's conclusion is somewhat overstated, as the model's performance is not significantly better than traditional diffusion models in all cases.\n\nSuggestions for Improvement:\n\n1. Consider exploring more biologically realistic architectures for the dendritic tree, such as those with variable branching factors or depths.\n2. Evaluate the model's performance on a wider range of tasks and distributions to assess its generalizability.\n3. Provide a more detailed explanation of how the model's parameters are learned, including any optimization algorithms or techniques used.\n4. Be more cautious in the paper's conclusion, as the model's performance is not uniformly better than traditional diffusion models.\n\nRecommendation:\n\nI recommend accepting this paper for publication, subject to the above suggestions for improvement. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions, and the authors demonstrate its flexibility and performance on a range of tasks. However, the paper's assumptions and limitations should be carefully considered and addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The model, called the recurrent circuit model, uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with a modulated global signal to sample from the prior at a rate prescribed by the period of the global oscillator. The model can flexibly use sensory and contextual sources of information to encode task-specific posteriors. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n\nStrengths:\n\n1. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions.\n2. The model is flexible and can be used to sample from a variety of distributions, including low-dimensional nonlinear manifolds and multimodal posteriors.\n3. The authors provide a clear and well-structured explanation of the model and its components.\n4. The model is supported by simulations and numerical experiments, which demonstrate its ability to sample from complex distributions.\n\nWeaknesses:\n\n1. The paper assumes a specific neural architecture for the model, which may not be the only possible implementation.\n2. The model's ability to generalize to new tasks and distributions is not fully explored.\n3. The authors do not provide a detailed analysis of the computational complexity of the model.\n4. The model's neural signatures, which are proposed as a way to experimentally validate the model, may be difficult to measure in practice.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the model and its limitations.\n2. The model's ability to generalize to new tasks and distributions should be further explored.\n3. The authors should provide more experimental evidence to support the neural signatures proposed as a way to experimentally validate the model.\n4. The model's architecture should be more flexible and modular, to allow for easier extension to new tasks and distributions.\n\nRecommendation:\n\nThe paper is well-written and provides a novel and biologically-inspired model for representing complex high-dimensional distributions. However, the model's limitations and potential applications should be further explored. The authors should address the weaknesses mentioned above and provide more experimental evidence to support the model's claims. With further refinement and experimentation, the model has the potential to make significant contributions to the field of neural computation and probabilistic inference.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Further Research: Yes",
        "gr_score_with": 0.973293768545994,
        "gr_score_without": 0.9404761904761905,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 316,
            "green_fraction": 0.9404761904761905,
            "z_score": 2.4732923665299413,
            "p_value": 0.0066937285814301015
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 328,
            "green_fraction": 0.973293768545994,
            "z_score": 4.48511112614923,
            "p_value": 3.6437971734055166e-06
        }
    },
    {
        "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
        "abstract": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. The authors leverage a body of work in information theory called partial information decomposition (PID) to identify three sources of unfairness in FL: Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples and provide a framework for quantifying accuracy-fairness trade-offs in FL.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective.\n2. The authors leverage PID to identify three sources of unfairness in FL, which is a novel contribution to the field.\n3. The paper provides a framework for quantifying accuracy-fairness trade-offs in FL, which is a significant contribution to the field.\n4. The authors provide experimental results on synthetic and Adult datasets to support their theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes a binary classification problem, which may not be applicable to all real-world scenarios.\n2. The authors do not provide a clear explanation of how to implement the PID decomposition in practice.\n3. The paper does not provide a comprehensive discussion of the limitations of the PID decomposition.\n4. The authors do not provide a clear explanation of how to choose the parameters of the PID decomposition.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive explanation of how to implement the PID decomposition in practice.\n2. The paper should provide a more detailed discussion of the limitations of the PID decomposition.\n3. The authors should provide a clear explanation of how to choose the parameters of the PID decomposition.\n4. The paper should provide more experimental results on real-world datasets to support the theoretical findings.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of fairness in FL and provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective. However, the paper assumes a binary classification problem and does not provide a clear explanation of how to implement the PID decomposition in practice. Therefore, I recommend that the authors provide a more comprehensive explanation of how to implement the PID decomposition in practice and provide more experimental results on real-world datasets to support the theoretical findings.\n\nRating: 8/10\n\nReviewer's Expertise: I have expertise in machine learning, fairness, and information theory.\n\nReviewer's Conflict of Interest: None.\n\nReviewer's Disclosure: None.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" proposes an information-theoretic approach to understand the interplay between local and global fairness in Federated Learning (FL). The authors leverage partial information decomposition (PID) to identify three sources of unfairness: Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these disparities contribute to global and local fairness using canonical examples. The paper introduces the Accuracy-Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization framework that defines the theoretical limits of accuracy and fairness trade-offs in FL. The authors provide experimental results on synthetic datasets and the Adult dataset to support their theoretical findings.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach.\n2. The authors introduce the PID framework, which is a novel and effective way to decompose global and local disparity into three sources of unfairness.\n3. The paper provides a rigorous mathematical framework for understanding the trade-offs between accuracy and fairness in FL.\n4. The authors provide experimental results that support their theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes that the sensitive attribute is binary, which may not be the case in real-world scenarios.\n2. The authors assume that the data is distributed uniformly across clients, which may not always be the case.\n3. The paper does not provide a comprehensive evaluation of the AGLFOP framework, which is a critical component of the paper.\n4. The authors do not discuss the computational complexity of the AGLFOP framework, which is an important consideration in practice.\n\nSuggestions for Improvement:\n\n1. The authors should extend the PID framework to handle non-binary sensitive attributes.\n2. The paper should provide a more comprehensive evaluation of the AGLFOP framework, including its computational complexity.\n3. The authors should discuss the implications of their findings for real-world FL applications.\n4. The paper should provide more detailed experimental results, including the sensitivity of the results to different hyperparameters.\n\nRecommendation:\n\nThis paper makes significant contributions to the field of FL, providing a comprehensive understanding of the interplay between local and global fairness. The authors introduce a novel and effective framework for decomposing global and local disparity, which is a critical component of the paper. While there are some limitations to the paper, the authors provide a rigorous mathematical framework for understanding the trade-offs between accuracy and fairness in FL. I recommend this paper for publication in a top-tier conference or journal in the field of machine learning.\n\nRating: 8/10\n\nRecommendation: Accept\n\n$$$",
        "gr_score_with": 0.9057591623036649,
        "gr_score_without": 0.8774193548387097,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 272,
            "green_fraction": 0.8774193548387097,
            "z_score": -1.3250428752029413,
            "p_value": 0.9074215344282184
        },
        "output_with": {
            "num_tokens_scored": 382,
            "num_green_tokens": 346,
            "green_fraction": 0.9057591623036649,
            "z_score": 0.3754039560974213,
            "p_value": 0.35368003157566896
        }
    },
    {
        "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION",
        "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for incremental robustness certification of deep neural networks (DNNs) using randomized smoothing (RS). The method, called Incremental Randomized Smoothing (IRS), reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples. This approach significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees.\n\nStrengths:\n\n1. The paper addresses a critical limitation of existing RS-based certification practices, which requires expensive certification to be re-run from scratch for each approximate network.\n2. The proposed IRS approach is the first incremental RS-based certification framework that efficiently computes the certified radius of the updated smoothed classifier.\n3. The paper presents an extensive evaluation of the performance of IRS, demonstrating speedups of up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models.\n4. The authors provide a thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, which affects the efficiency of IRS.\n\nWeaknesses:\n\n1. The paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n3. The paper assumes that the user has a larger sample budget, which may not always be the case in real-world scenarios.\n4. The authors do not provide a detailed comparison with other incremental certification methods, such as deterministic certification using formal logic.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of storing and retrieving the certification cache, including a discussion of potential optimization techniques.\n2. The paper should include a more detailed comparison with other incremental certification methods, including deterministic certification using formal logic.\n3. The authors should provide a more thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, including a discussion of potential optimization techniques.\n4. The paper should include a discussion of potential applications of IRS in real-world scenarios, including approximate tuning and shipping certification caches.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and areas for improvement, the proposed IRS approach has the potential to significantly reduce the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. Therefore, I recommend accepting this paper for publication in the conference proceedings.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for incremental robustness certification of deep neural networks (DNNs) using randomized smoothing (RS). The method, called Incremental Randomized Smoothing (IRS), reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples. This approach significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees.\n\nStrengths:\n\n1. The paper addresses a critical limitation of existing RS-based certification practices, which requires expensive certification to be re-run from scratch for each approximated network.\n2. The proposed IRS approach is the first incremental RS-based certification framework, which reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples.\n3. The paper provides a thorough analysis of the benefits of IRS, including reduced computational cost and improved robustness guarantees.\n4. The experimental results demonstrate the effectiveness of IRS, showing significant speedups over the standard non-incremental RS baseline on state-of-the-art classification models.\n\nWeaknesses:\n\n1. The paper assumes that the approximated model is similar to the original model, which may not always be the case in real-world scenarios.\n2. The proposed IRS approach requires a cache with additional information, which may have negligible memory overhead but still requires additional storage.\n3. The choice of hyperparameters, such as the smoothing parameter \u03c3, can significantly impact the performance of IRS.\n4. The paper does not provide a comprehensive comparison with existing incremental certification methods.\n\nSuggestions for Improvement:\n\n1. Investigate the performance of IRS in scenarios where the approximated model is significantly different from the original model.\n2. Explore alternative approaches to storing the additional information required by IRS, such as using more efficient data structures or compression techniques.\n3. Provide a more detailed analysis of the impact of hyperparameters on the performance of IRS.\n4. Compare the performance of IRS with existing incremental certification methods to demonstrate its advantages.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach for incremental robustness certification of DNNs using RS. While there are some limitations and areas for improvement, the paper makes a significant contribution to the field of DNN certification. I recommend accepting the paper for publication, subject to the suggested improvements.\n\nRating: 8/10",
        "gr_score_with": 0.9544159544159544,
        "gr_score_without": 0.9300291545189504,
        "output_without": {
            "num_tokens_scored": 343,
            "num_green_tokens": 319,
            "green_fraction": 0.9300291545189504,
            "z_score": 1.8539925846451775,
            "p_value": 0.031870110443007776
        },
        "output_with": {
            "num_tokens_scored": 351,
            "num_green_tokens": 335,
            "green_fraction": 0.9544159544159544,
            "z_score": 3.3984229251437883,
            "p_value": 0.00033887778610597536
        }
    },
    {
        "title": "SELF-CONSUMING GENERATIVE MODELS GO MAD",
        "abstract": "Seismic advances in generative AI algorithms for imagery, text, and other data types have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates an autophagous (\u201cself-consuming\u201d) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), by analogy to mad cow disease, and show that appreciable MADness arises in just a few generations. Generation t = 1 t = 3 t = 5 t = 7 t = 9 Figure 1: Training generative artificial intelligence (AI) models on synthetic data progressively amplifies artifacts. As AI-synthesized data proliferates in standard datasets and the Internet, future AI models will train on both real and synthetic data, forming autophagous (\u201cself-consuming\u201d) loops. Here we highlight a potential unintended consequence of autophagous training. We trained a sequence of StyleGAN2 (Karras et al., 2019a) models wherein the model at generation t \u2265 2 trains only on data synthesized by the model at generation t\u2212 1. This setup is a fully synthetic loop (Figure 3) without sampling bias (\u03bb = 1). Note how the cross-hatched artifacts (possibly an architectural fingerprint (Karras et al., 2021)) are progressively amplified at each generation. Appendix D has more samples.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of autophagous (self-consuming) loops in generative AI models, where AI-synthesized data is used to train future models, creating a loop of synthetic data being used to train subsequent models. The authors propose three families of autophagous loops: fully synthetic, synthetic augmentation, and fresh data loops. They conduct a thorough analytical and empirical analysis using state-of-the-art generative image models and demonstrate that without enough fresh real data in each generation, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease, a condition they term Model Autophagy Disorder (MAD).\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of autophagous loops, proposing three realistic models that mimic real-world generative model training scenarios.\n2. The authors demonstrate the negative impact of autophagous loops on the quality and diversity of synthetic data, providing empirical evidence of Model Autophagy Disorder (MAD).\n3. The paper highlights the importance of sampling biases in autophagous loops, showing that biased sampling can boost synthetic data quality but at the expense of diversity.\n4. The authors provide a detailed analysis of the fully synthetic loop, showing that unbiased sampling degrades synthetic data quality and diversity, while biased sampling can increase quality but decrease diversity.\n\nWeaknesses:\n\n1. The paper assumes that the generative models used are perfect, which may not be the case in real-world scenarios.\n2. The authors do not provide a comprehensive analysis of the potential benefits of autophagous loops, such as the ability to generate high-quality synthetic data.\n3. The paper focuses primarily on image generation models and does not explore the implications of autophagous loops on other data types, such as text or audio.\n4. The authors do not provide a clear solution to mitigate the negative effects of autophagous loops, such as developing autophagy-aware watermarking techniques.\n\nSuggestions for Improvement:\n\n1. The authors should consider exploring the potential benefits of autophagous loops and provide a more comprehensive analysis of their implications.\n2. The paper should be extended to include a more detailed analysis of the potential effects of autophagous loops on other data types, such as text or audio.\n3. The authors should provide a clear solution to mitigate the negative effects of autophagous loops, such as developing autophagy-aware watermarking techniques.\n4. The paper should be expanded to include a more detailed analysis of the limitations of the proposed models and the potential for future research in this area.\n\nRecommendation:\n\nThe paper provides a comprehensive analysis of autophagous loops and their negative impact on the quality and diversity of synthetic data. While there are some limitations to the paper, the authors' findings are significant and provide a valuable contribution to the field of generative AI. Therefore, I recommend that the paper be accepted for publication in its current form, with the suggested improvements for future research.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper explores the concept of autophagous generative models, where a sequence of generative models train on synthetic data generated by previous models, forming a self-consuming loop. The authors analyze three families of autophagous loops: the fully synthetic loop, the synthetic augmentation loop, and the fresh data loop. They demonstrate that without enough fresh real data in each generation, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease, a condition they term Model Autophagy Disorder (MAD). The authors also investigate the impact of sampling biases on the autophagous loops and find that biased sampling can boost synthetic data quality but at the expense of diversity.\n\nStrengths:\n1. The paper provides a thorough analysis of autophagous generative models and their potential consequences.\n2. The authors propose a novel framework for understanding the behavior of autophagous loops and identify key factors that affect their performance.\n3. The experiments conducted in the paper are comprehensive and well-designed, providing valuable insights into the dynamics of autophagous loops.\n\nWeaknesses:\n1. The paper assumes a specific generative model architecture and dataset, which may limit its generalizability to other models and datasets.\n2. The analysis is focused primarily on the fully synthetic loop, and the results may not be directly applicable to other autophagous loop variants.\n3. The paper does not provide a clear explanation of how to control or mitigate the effects of Model Autophagy Disorder (MAD) in practice.\n\nSuggestions for Improvement:\n1. The authors could explore the applicability of their framework to other generative model architectures and datasets.\n2. A more detailed analysis of the synthetic augmentation loop and the fresh data loop would be beneficial to understand the behavior of these autophagous loop variants.\n3. The paper could provide more concrete recommendations for practitioners to control or mitigate the effects of Model Autophagy Disorder (MAD) in practice.\n\nRecommendation:\nThe paper provides a valuable contribution to the field of generative models and highlights the potential risks associated with autophagous loops. However, the limitations mentioned above need to be addressed to make the framework more generalizable and applicable to real-world scenarios. With additional research and experimentation, the findings presented in this paper have the potential to significantly impact the development and deployment of generative models in various applications.\n\n$$$\nI hope this review meets your expectations.",
        "gr_score_with": 0.9189944134078212,
        "gr_score_without": 0.8615384615384616,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 336,
            "green_fraction": 0.8615384615384616,
            "z_score": -2.5316080249688953,
            "p_value": 0.9943229581222798
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 329,
            "green_fraction": 0.9189944134078212,
            "z_score": 1.1981506494501766,
            "p_value": 0.11542918688029119
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method constructs a directed graph to represent unlabeled data and uses a diffusion process to quantify the influence of each candidate subset. A greedy algorithm is introduced to select the subset with maximum influence, which approximates the vast unlabeled data. The paper provides theoretical analysis and empirical evaluations on various benchmarks, demonstrating the superiority of IDEAL over previous state-of-the-art selective annotation baselines.\n\nStrengths:\n\n1. The paper presents a novel approach to selective annotations, addressing the challenges of balancing diversity and representativeness.\n2. The method is end-to-end, unsupervised, and efficient, reducing unnecessary costs at inference time.\n3. The paper provides theoretical analysis, establishing an upper limit for the global optimal solution and demonstrating the reliability of the greedy search algorithm.\n4. Empirical evaluations demonstrate the superiority of IDEAL across various benchmarks, including classification, multi-choice, dialogue, and generation tasks.\n\nWeaknesses:\n\n1. The paper assumes access to a large-scale unlabeled data pool, which may not be feasible in practice.\n2. The method relies on a similarity-based prompt retrieval method, which may not be optimal in all cases.\n3. The paper does not provide a comprehensive comparison with other graph-based methods, such as K-Means and MFL.\n4. The case study on automatic annotation is limited to a small set of datasets and may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for constructing the directed graph, such as using different similarity metrics or graph neural networks.\n2. Evaluate the performance of IDEAL on other datasets and tasks, including those with varying levels of complexity and domain adaptation.\n3. Compare IDEAL with other graph-based methods, such as K-Means and MFL, to demonstrate its superiority.\n4. Provide a more detailed analysis of the time complexity and scalability of IDEAL, especially for large datasets.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach to selective annotations, addressing the challenges of balancing diversity and representativeness. The theoretical analysis and empirical evaluations demonstrate the superiority of IDEAL over previous state-of-the-art selective annotation baselines. While there are some limitations and areas for improvement, the paper provides a valuable contribution to the field of in-context learning and large language models. I recommend acceptance of the paper, with the suggested improvements and additional evaluations to further strengthen the results.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method constructs a directed graph to represent unlabeled data, quantifies the influence of candidate subsets using a diffusion process, and selects a subset of data to annotate based on the influence metric. Theoretical analysis provides a lower bound for the subset influence selected by the method, demonstrating that it is at least as large as a certain proportion of the optimal solution. Empirical evaluations on various benchmarks show that IDEAL achieves better performance than previous state-of-the-art selective annotation baselines, Vote-k, in most cases, while reducing time consumption during subset selection.\n\nStrengths:\n1. The paper proposes a novel influence-driven selective annotation method, IDEAL, which addresses the challenge of high annotation costs in in-context learning.\n2. The method provides a theoretical guarantee, demonstrating that the selected subset has influence at least as substantial as a specific proportion of the optimal solution's influence.\n3. Empirical evaluations show that IDEAL achieves better performance than previous state-of-the-art selective annotation baselines, Vote-k, in most cases.\n4. The method reduces time consumption during subset selection, making it more efficient than previous baselines.\n\nWeaknesses:\n1. The paper assumes that the influence function satisfies the submodular condition, which may not hold in all cases.\n2. The method requires constructing a directed graph to represent unlabeled data, which may be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the trade-off between diversity and representativeness in the selective annotation process.\n4. The method relies on the quality of the prompt retrieval technology used, which may not always produce high-quality prompts.\n\nSuggestions for Improvement:\n1. Investigate alternative influence functions that may not satisfy the submodular condition, but still provide good performance.\n2. Develop more efficient algorithms for constructing the directed graph to represent unlabeled data.\n3. Analyze the trade-off between diversity and representativeness in the selective annotation process and provide guidance on how to balance them.\n4. Investigate the impact of different prompt retrieval technologies on the performance of IDEAL.\n\nRecommendation:\nThe paper proposes a novel influence-driven selective annotation method, IDEAL, which addresses the challenge of high annotation costs in in-context learning. The method provides a theoretical guarantee and empirical evaluations show that it achieves better performance than previous state-of-the-art selective annotation baselines, Vote-k, in most cases. However, there are some limitations and areas for improvement, such as the assumption of the submodular condition and the reliance on the quality of the prompt retrieval technology. Overall, the paper makes a significant contribution to the field of in-context learning and selective annotations, and it has the potential to inspire future research in this area.",
        "gr_score_with": 0.9056603773584906,
        "gr_score_without": 0.835509138381201,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 320,
            "green_fraction": 0.835509138381201,
            "z_score": -4.206775044738334,
            "p_value": 0.9999870479617498
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 336,
            "green_fraction": 0.9056603773584906,
            "z_score": 0.36361709074557697,
            "p_value": 0.3580719813161001
        }
    },
    {
        "title": "TOWARDS TRANSPARENT TIME SERIES FORECASTING",
        "abstract": "Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in highstakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. In particular, currently used bottom-up approaches that focus on the values of the time series at specific time points (usually regularly spaced) do not provide a holistic understanding of the entire time series. This limits the applicability of ML in many critical areas. To open up these domains for ML, we propose a top-down framework of bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. Applying this framework, we develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. Through a series of experiments, we demonstrate the efficacy and interpretability of our approach, paving the way for more transparent and reliable applications of ML in various domains.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel top-down approach to transparent time series forecasting, which is essential for decision-making systems in high-stakes domains. The authors introduce bi-level transparency, a framework that involves understanding the higher-level trends and the lower-level properties of the predicted time series. They develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the challenges of transparent time series forecasting and the limitations of current bottom-up approaches.\n2. The authors introduce a novel top-down approach to trajectory comprehension, which is more consistent with the natural way people understand trajectories.\n3. The bi-level transparency framework is well-defined and provides a clear understanding of the impact of the input on the trends and properties of the predicted trajectory.\n4. The authors develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, which is accompanied by an interactive visualization tool.\n5. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on real-world and synthetic datasets.\n\nWeaknesses:\n\n1. The paper assumes that the input space is static features, which may not be the case in many real-world applications where the input may contain the previous part of the trajectory or other exogenous time series.\n2. The authors do not provide a thorough comparison of their approach with existing methods for time series forecasting, which may make it difficult to evaluate the novelty and impact of their contribution.\n3. The paper does not provide a detailed analysis of the computational complexity of the bi-level transparency framework and the TIMEVIEW model.\n4. The authors do not discuss the potential limitations and risks of their approach, such as the potential for overfitting or the difficulty of interpreting complex models.\n\nSuggestions for Improvement:\n\n1. The authors should extend their approach to handle dynamic inputs, such as previous parts of the trajectory or other exogenous time series.\n2. The paper should provide a more thorough comparison of their approach with existing methods for time series forecasting, including both transparent and black-box models.\n3. The authors should provide a detailed analysis of the computational complexity of the bi-level transparency framework and the TIMEVIEW model.\n4. The paper should discuss the potential limitations and risks of their approach, such as the potential for overfitting or the difficulty of interpreting complex models.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of transparent time series forecasting by introducing a novel top-down approach and developing a transparent ML model for time series forecasting based on static features. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on real-world and synthetic datasets. However, the paper has some limitations, such as assuming static inputs and not providing a thorough comparison with existing methods. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of their approach to make it more impactful and widely applicable.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel framework for transparent time series forecasting, called TIMEVIEW, which enables the interpretation of the predicted trajectory at two levels: trends and properties. The authors propose a top-down approach to trajectory comprehension, which is more consistent with the natural way people understand trajectories, and provides a holistic understanding of the entire trajectory. They formalize the notion of a trend by defining the composition of a trajectory, which is a sequence of motifs that describe the current shape of the trajectory at a specific interval. The authors demonstrate the efficacy and interpretability of their approach through a series of experiments on four real-world datasets and three synthetic ones.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to transparent time series forecasting, which addresses the limitations of current bottom-up approaches.\n2. The authors provide a clear and concise formalism of the composition of a trajectory, which enables the interpretation of the predicted trajectory at two levels: trends and properties.\n3. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on four real-world datasets and three synthetic ones.\n4. The authors provide a comprehensive discussion of the related work and highlight the differences between their approach and existing methods.\n\nWeaknesses:\n\n1. The paper assumes that the input data is static, which may not be the case in many real-world scenarios.\n2. The authors do not provide a detailed comparison with existing methods, especially those that are specifically designed for time series forecasting.\n3. The paper does not provide a clear explanation of how the composition map is calculated, which is a critical component of the TIMEVIEW framework.\n4. The authors do not provide a discussion of the potential limitations and challenges of their approach, especially in scenarios where the input data is complex or high-dimensional.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with existing methods, especially those that are specifically designed for time series forecasting.\n2. The paper should provide a clear explanation of how the composition map is calculated, which is a critical component of the TIMEVIEW framework.\n3. The authors should discuss the potential limitations and challenges of their approach, especially in scenarios where the input data is complex or high-dimensional.\n4. The paper should provide a more detailed discussion of the implications of their approach for practical applications, especially in high-stakes domains such as medicine and finance.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to transparent time series forecasting, which addresses the limitations of current bottom-up approaches. While there are some weaknesses and areas for improvement, the paper demonstrates the efficacy and interpretability of their approach through a series of experiments on four real-world datasets and three synthetic ones. Therefore, I recommend that the paper be published in a reputable conference or journal.\n\n$$$\nNote: The above review is based on the provided paper and may not reflect the actual opinions or views of the reviewer.",
        "gr_score_with": 0.9480122324159022,
        "gr_score_without": 0.9166666666666666,
        "output_without": {
            "num_tokens_scored": 348,
            "num_green_tokens": 319,
            "green_fraction": 0.9166666666666666,
            "z_score": 1.0365550151689586,
            "p_value": 0.14997164369389976
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 310,
            "green_fraction": 0.9480122324159022,
            "z_score": 2.894187819912441,
            "p_value": 0.0019007035120270962
        }
    },
    {
        "title": "DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional finetuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel decoding strategy, Decoding by Contrasting Layers (DoLa), to reduce hallucinations in large language models (LLMs). The approach exploits the hierarchical encoding of factual knowledge within transformer LLMs by contrasting the logits of higher and lower layers to improve factuality in the decoding process. The method dynamically selects the premature layer based on a distributional distance measure, and the contrasted output is used as the next-word prediction. The authors conduct experiments on multiple tasks, including multiple-choice, open-ended generation, and chain-of-thought reasoning, and demonstrate that DoLa significantly improves truthfulness without external information retrieval or model fine-tuning.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of hallucinations in LLMs and the need for a novel decoding strategy to address this issue.\n2. The proposed method, DoLa, is well-motivated and exploits the hierarchical encoding of factual knowledge within transformer LLMs.\n3. The authors provide a thorough experimental evaluation of DoLa on multiple tasks, including multiple-choice, open-ended generation, and chain-of-thought reasoning.\n4. The results demonstrate that DoLa significantly improves truthfulness without external information retrieval or model fine-tuning.\n\nWeaknesses:\n\n1. The paper assumes that the hierarchical encoding of factual knowledge within transformer LLMs is a reliable and accurate representation of the model's knowledge, which may not always be the case.\n2. The dynamic layer selection strategy used in DoLa may not be suitable for all tasks or datasets, and may require additional hyperparameter tuning.\n3. The paper does not provide a detailed analysis of the computational cost of DoLa, which may be a concern for large-scale applications.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of DoLa and explore ways to optimize it for large-scale applications.\n2. The paper should provide a more thorough evaluation of the robustness of DoLa to different tasks, datasets, and hyperparameter settings.\n3. The authors should explore ways to combine DoLa with other methods, such as reinforcement learning from human feedback or external knowledge retrieval, to address the limitations of the current approach.\n\nRecommendation:\n\nThe paper provides a novel and well-motivated approach to reducing hallucinations in LLMs, and the experimental results demonstrate its effectiveness on multiple tasks. However, the paper assumes a reliable and accurate representation of the model's knowledge, and the dynamic layer selection strategy may require additional hyperparameter tuning. Therefore, I recommend that the authors address these limitations and provide a more detailed analysis of the computational cost of DoLa to make it a more robust and scalable approach.\n\nOverall, the paper provides a significant contribution to the field of NLP and demonstrates the potential of DoLa as a novel decoding strategy for reducing hallucinations in LLMs.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel decoding strategy, Decoding by Contrasting Layers (DoLa), to reduce hallucinations in large language models (LLMs). The approach exploits the hierarchical encoding of factual knowledge within transformer LLMs by contrasting the logits of higher and lower layers to improve factuality in the decoding process. The method dynamically selects suitable early layers based on token complexity, avoiding the need for training and using smaller LMs. Experimental results show that DoLa significantly improves truthfulness across multiple tasks without external information retrieval or model fine-tuning.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to reducing hallucinations in LLMs, which is a significant problem in natural language processing.\n2. The method exploits the hierarchical encoding of factual knowledge within transformer LLMs, which is a well-established phenomenon in NLP research.\n3. The paper provides a comprehensive evaluation of DoLa on multiple tasks and datasets, including multiple-choice and open-ended generation tasks.\n4. The results show that DoLa significantly improves truthfulness across multiple tasks without external information retrieval or model fine-tuning.\n\nWeaknesses:\n\n1. The paper relies on the assumption that factual knowledge is localized to particular transformer layers, which may not always be the case.\n2. The method requires a significant amount of computational resources to dynamically select suitable early layers based on token complexity.\n3. The paper does not explore other dimensions of improving LLMs, such as reinforcement learning from human feedback or grounding on external knowledge.\n4. The limitations of DoLa, such as focusing on factuality and inference only, are not fully addressed.\n\nSuggestions for Improvement:\n\n1. Investigate other dimensions of improving LLMs, such as reinforcement learning from human feedback or grounding on external knowledge.\n2. Explore other methods for selecting suitable early layers, such as using pre-trained models or external knowledge bases.\n3. Provide a more detailed analysis of the computational resources required for DoLa and how they can be optimized.\n4. Evaluate DoLa on a wider range of tasks and datasets to demonstrate its generalizability.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to reducing hallucinations in LLMs, which has significant potential for improving the reliability and trustworthiness of these models. While there are some limitations and weaknesses to the method, the results demonstrate its effectiveness and potential for wide applicability. Therefore, I recommend accepting the paper for publication, with the suggestions for improvement provided above.\n\n$$$",
        "gr_score_with": 0.8820224719101124,
        "gr_score_without": 0.8632707774798928,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 322,
            "green_fraction": 0.8632707774798928,
            "z_score": -2.3642968211409285,
            "p_value": 0.9909678334984978
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 314,
            "green_fraction": 0.8820224719101124,
            "z_score": -1.1304525871425728,
            "p_value": 0.8708572169496984
        }
    },
    {
        "title": "DOUBLY ROBUST PROXIMAL CAUSAL LEARNING FOR CONTINUOUS TREATMENTS",
        "abstract": "Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatments can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments for proximal causal learning. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications1.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a kernel-based doubly robust estimator for continuous treatments within the proximal causal framework. The authors address the challenge of estimating causal effects with continuous treatments by integrating a kernel function into the proximal inverse probability weighted estimator. They derive the kernel-based doubly robust estimator and show that it can approximate the influence function of the causal effect. The authors also propose a new approach to efficiently solve the nuisance functions, which is achieved by estimating the policy function and incorporating it into a min-max optimization problem. The convergence analysis of the proposed estimator is provided, and the authors demonstrate the utility of their method using synthetic data and the legalized abortion and crime dataset.\n\nStrengths:\n\n1. The paper addresses a significant challenge in causal inference, which is the estimation of causal effects with continuous treatments.\n2. The proposed kernel-based doubly robust estimator is a significant contribution to the field of causal inference, and it can handle continuous treatments.\n3. The authors provide a comprehensive convergence analysis of the proposed estimator, which is essential for understanding its performance.\n4. The paper demonstrates the utility of the proposed method using synthetic data and real-world data, which provides evidence of its effectiveness.\n\nWeaknesses:\n\n1. The paper assumes that the kernel function is bounded and differentiable, which may not be true for all kernel functions.\n2. The authors do not provide a detailed comparison of their method with existing methods, which makes it difficult to evaluate its performance.\n3. The paper assumes that the policy function can be estimated accurately, which may not be the case in practice.\n4. The authors do not provide a detailed discussion of the limitations of their method and potential future directions.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison of the proposed method with existing methods, including a discussion of their strengths and weaknesses.\n2. Investigate the effect of different kernel functions on the performance of the proposed estimator.\n3. Provide a more detailed discussion of the limitations of the proposed method and potential future directions.\n4. Consider incorporating additional regularization techniques to improve the stability of the proposed estimator.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of causal inference. However, it would benefit from a more detailed comparison with existing methods and a more detailed discussion of the limitations and potential future directions. With these improvements, the paper would be more comprehensive and provide a clearer understanding of the proposed method's performance.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nNote: The paper has the potential to be published in a top-tier journal in the field of causal inference, but it requires further improvements to make it more comprehensive and convincing.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a kernel-based doubly robust estimator for continuous treatments within the proximal causal framework. The authors address the challenge of estimating causal effects with continuous treatments by integrating a kernel function into the proximal inverse probability weighted estimator. They derive the kernel-based doubly robust estimator and show that it can approximate the influence function of the causal effect. The authors also propose a new approach to estimating the nuisance function q0 by estimating the policy function and incorporating it into a min-max optimization. They provide a comprehensive convergence analysis of the estimator and demonstrate its utility in synthetic and real-world datasets.\n\nStrengths:\n\n* The paper addresses an important challenge in causal inference, i.e., estimating causal effects with continuous treatments.\n* The authors propose a novel kernel-based doubly robust estimator that can approximate the influence function of the causal effect.\n* The paper provides a comprehensive convergence analysis of the estimator and demonstrates its utility in synthetic and real-world datasets.\n* The authors propose a new approach to estimating the nuisance function q0 by estimating the policy function and incorporating it into a min-max optimization.\n\nWeaknesses:\n\n* The paper assumes that the kernel function is bounded differentiable, which may not hold in all cases.\n* The authors do not provide a detailed comparison of their method with existing methods for estimating causal effects with continuous treatments.\n* The paper assumes that the nuisance function q0 is smooth, which may not hold in all cases.\n\nSuggestions for Improvement:\n\n* Provide a more detailed comparison of the proposed method with existing methods for estimating causal effects with continuous treatments.\n* Investigate the robustness of the proposed method to misspecification of the kernel function and the nuisance function q0.\n* Provide more experimental results to demonstrate the utility of the proposed method in various scenarios.\n\nRecommendation:\n\nThe paper proposes a novel kernel-based doubly robust estimator for continuous treatments within the proximal causal framework. The authors provide a comprehensive convergence analysis of the estimator and demonstrate its utility in synthetic and real-world datasets. The paper is well-written and the authors provide a clear and concise explanation of their method. However, the paper assumes that the kernel function is bounded differentiable and the nuisance function q0 is smooth, which may not hold in all cases. Therefore, I recommend that the authors provide a more detailed comparison of their method with existing methods and investigate the robustness of the proposed method to misspecification of the kernel function and the nuisance function q0.\n\nOverall, the paper is a significant contribution to the field of causal inference and provides a novel approach to estimating causal effects with continuous treatments. However, the authors should address the limitations of their method and provide more experimental results to demonstrate its utility.\n\nRating: 8/10\n\nReviewer's Signature: [Your Name]\n\nDate: [Today's Date]\n\n$$$",
        "gr_score_with": 0.9436619718309859,
        "gr_score_without": 0.9129213483146067,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 325,
            "green_fraction": 0.9129213483146067,
            "z_score": 0.8128499233876789,
            "p_value": 0.20815205419241894
        },
        "output_with": {
            "num_tokens_scored": 284,
            "num_green_tokens": 268,
            "green_fraction": 0.9436619718309859,
            "z_score": 2.4528232893747988,
            "p_value": 0.007086997472020422
        }
    },
    {
        "title": "GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS",
        "abstract": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson\u2019s r) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces GeoLLM, a novel method that extracts geospatial knowledge from large language models (LLMs) using auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that their method can effectively unlock this knowledge across various models and tasks. They fine-tune LLMs using prompts constructed with map data and evaluate their performance on a range of geospatial prediction tasks. The results show that GeoLLM outperforms traditional baselines, including k-NN and XGBoost, and even exceeds the performance of satellite-based methods. The authors also observe that the performance of their method scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper presents a novel method for extracting geospatial knowledge from LLMs, which is a significant contribution to the field.\n2. The authors provide a comprehensive evaluation of their method on a range of geospatial prediction tasks, demonstrating its effectiveness and scalability.\n3. The paper highlights the potential of LLMs for geospatial tasks, which is an exciting area of research with many potential applications.\n4. The authors provide a clear and concise explanation of their method and its evaluation, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with LLMs and their capabilities, which may not be the case for all readers.\n2. The evaluation of the method is limited to a few geospatial prediction tasks, and it would be beneficial to see a more comprehensive evaluation on a wider range of tasks.\n3. The paper does not provide a detailed explanation of the map data used in the prompts, which is a crucial component of the method.\n4. The authors do not discuss the potential biases of LLMs in geospatial tasks, which is an important consideration for applications in real-world settings.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the map data used in the prompts and its role in the method.\n2. Evaluate the method on a wider range of geospatial prediction tasks to demonstrate its scalability and effectiveness.\n3. Discuss the potential biases of LLMs in geospatial tasks and how they can be mitigated.\n4. Provide more information on the computational cost of the method and how it can be optimized for real-world applications.\n\nRecommendation:\n\nThe paper presents a novel and effective method for extracting geospatial knowledge from LLMs, which has significant potential for applications in geospatial tasks. While there are some limitations and areas for improvement, the paper provides a clear and concise explanation of the method and its evaluation, making it an excellent contribution to the field. I recommend the paper for publication in a top-tier conference or journal in the field of geospatial AI.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces GeoLLM, a novel method that extracts geospatial knowledge from large language models (LLMs) using auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that GeoLLM can efficiently unlock this knowledge across various tasks and models. The method outperforms traditional baselines, including k-NN and XGBoost, and exceeds the performance of satellite-based methods. The authors also show that the performance of GeoLLM scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper introduces a novel method for extracting geospatial knowledge from LLMs, which has the potential to revolutionize the field of geospatial analysis.\n2. The authors provide a comprehensive evaluation of GeoLLM on various tasks and datasets, demonstrating its effectiveness and robustness.\n3. The paper highlights the sample efficiency and geospatial information richness of LLMs, which is a significant contribution to the field.\n4. The authors provide a clear and concise explanation of the method and its implementation, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper relies heavily on the performance of GPT-3.5, which may not be widely available or accessible to all researchers.\n2. The authors do not provide a detailed comparison of GeoLLM with other state-of-the-art methods for geospatial analysis.\n3. The paper does not discuss the potential limitations and biases of using LLMs for geospatial analysis.\n4. The authors do not provide a clear explanation of how GeoLLM can be applied to real-world scenarios and how it can be integrated with existing geospatial analysis tools.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison of GeoLLM with other state-of-the-art methods for geospatial analysis.\n2. Discuss the potential limitations and biases of using LLMs for geospatial analysis and provide a clear explanation of how to mitigate these issues.\n3. Provide a clear explanation of how GeoLLM can be applied to real-world scenarios and how it can be integrated with existing geospatial analysis tools.\n4. Consider providing more information on the pretraining dataset and the training process of the LLMs used in the paper.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of geospatial analysis. However, it would benefit from a more detailed comparison with other state-of-the-art methods and a clear explanation of the limitations and biases of using LLMs for geospatial analysis. With these improvements, the paper would be even more compelling and would have a greater impact on the field.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9528023598820059,
        "gr_score_without": 0.9289473684210526,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 353,
            "green_fraction": 0.9289473684210526,
            "z_score": 1.8811368905964194,
            "p_value": 0.029976649931722606
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 323,
            "green_fraction": 0.9528023598820059,
            "z_score": 3.240794985652283,
            "p_value": 0.0005959843756494507
        }
    },
    {
        "title": "PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS",
        "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs) that combines two novel components: Relative Importance and Activations (RIA) and Channel Permutation. RIA is a new pruning metric that jointly considers the weight and activations of LLMs, while Channel Permutation is a new approach to maximize the preservation of important weights under N:M sparsity. The authors demonstrate that RIA alone can outperform existing post-training pruning methods on prevalent LLMs, and that Channel Permutation can even outperform the original LLaMA2 70B on zero-shot tasks.\n\nStrengths:\n\n1. The paper proposes two novel and innovative components, RIA and Channel Permutation, which can be combined to further enhance the N:M structured pruned LLMs.\n2. The authors demonstrate the effectiveness of RIA and Channel Permutation through extensive experiments on prominent LLMs like LLaMA, LLaMA2, and OPT across varying model sizes.\n3. The paper provides a thorough analysis of the performance of RIA and Channel Permutation under different sparsity levels, calibration data, and zero-shot performance.\n4. The authors demonstrate the hardware-friendliness of their method, ensuring enhanced inference acceleration.\n\nWeaknesses:\n\n1. The paper assumes that the input and output channels of weight have unique information, which may not always be the case in LLMs.\n2. The authors do not provide a thorough analysis of the computational complexity of their method, which may be a concern for large-scale LLMs.\n3. The paper assumes that the Hungarian algorithm can efficiently solve the linear sum assignment problem, which may not always be the case in practice.\n4. The authors do not provide a clear comparison with other pruning methods that do not require calibration data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational complexity of their method and its implications for large-scale LLMs.\n2. The paper should include a more detailed comparison with other pruning methods that do not require calibration data.\n3. The authors should provide more insights into the assumption that the input and output channels of weight have unique information in LLMs.\n4. The paper should include more experiments to evaluate the robustness of RIA and Channel Permutation under different scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective plug-and-play post-training pruning method for LLMs that combines RIA and Channel Permutation. The authors demonstrate the effectiveness of their method through extensive experiments and provide a thorough analysis of its performance under different sparsity levels, calibration data, and zero-shot performance. However, the paper assumes that the input and output channels of weight have unique information, and does not provide a thorough analysis of the computational complexity of their method. Therefore, the paper is recommended for publication after addressing these concerns.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs) called PLUG-AND-PLAY. The method comprises two key components: Relative Importance and Activations (RIA) and Channel Permutation. RIA is a new pruning metric that jointly considers the weight and activations efficiently on LLMs, while Channel Permutation is a new approach to maximally preserve important weights under N:M sparsity. The authors demonstrate the effectiveness of RIA and Channel Permutation through extensive experiments on popular LLMs, showing that RIA consistently outperforms existing SOTA one-shot pruning techniques SparseGPT and Wanda. Additionally, Channel Permutation reduces the performance drop when adapting the model to N:M constraint.\n\nStrengths:\n\n* The paper proposes a novel and effective post-training pruning method for LLMs.\n* The authors provide extensive experimental results on popular LLMs, demonstrating the effectiveness of RIA and Channel Permutation.\n* The paper highlights the importance of considering both weight and activations in pruning metrics.\n* The authors provide a comprehensive analysis of the performance drop when adapting the model to N:M constraint.\n\nWeaknesses:\n\n* The paper assumes a specific hardware environment (NVIDIA Ampere GPU architecture) and may not be applicable to other hardware environments.\n* The authors do not provide a thorough analysis of the computational complexity of the proposed method.\n* The paper does not provide a detailed comparison with other post-training pruning methods.\n* The authors assume a fixed value of the power factor a in the RIA formula and do not provide a thorough analysis of its impact on the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational complexity of the proposed method and its implications on large-scale LLMs.\n* Compare the proposed method with other post-training pruning methods and provide a more comprehensive analysis of their strengths and weaknesses.\n* Investigate the impact of the power factor a in the RIA formula on the results and provide a more thorough analysis of its effects.\n* Consider extending the proposed method to other types of neural networks and provide a more comprehensive analysis of its applicability.\n\nRecommendation:\n\nThe paper proposes a novel and effective post-training pruning method for LLMs. While it has some limitations, the authors provide extensive experimental results and a comprehensive analysis of the performance drop when adapting the model to N:M constraint. Therefore, I recommend accepting the paper with minor revisions to address the suggestions mentioned above.\n\nRating: 8/10\n\n$$$\nNote: The rating is based on the overall quality and impact of the paper, as well as the clarity and thoroughness of the presentation. The paper has some limitations and areas for improvement, but it makes a significant contribution to the field of post-training pruning for LLMs.",
        "gr_score_with": 0.8777173913043478,
        "gr_score_without": 0.8165680473372781,
        "output_without": {
            "num_tokens_scored": 338,
            "num_green_tokens": 276,
            "green_fraction": 0.8165680473372781,
            "z_score": -5.112663963717727,
            "p_value": 0.9999998411764675
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 323,
            "green_fraction": 0.8777173913043478,
            "z_score": -1.424629321787657,
            "p_value": 0.9228678102563647
        }
    },
    {
        "title": "TEST: TEXT PROTOTYPE ALIGNED EMBEDDING TO ACTIVATE LLM\u2019S ABILITY FOR TIME SERIES",
        "abstract": "This work summarizes two ways to accomplish Time-Series (TS) tasks in today\u2019s Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TSfor-LLM, where we aim to activate LLM\u2019s ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM\u2019s embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today\u2019s SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM\u2019s ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called TEST (Text-Prototype-Aligned Embedding to Activate LLM's Ability for Time Series) to enable Large Language Models (LLMs) to handle Time-Series (TS) data. The method focuses on the TS-for-LLM paradigm, where TS is converted into a model-friendly representation to enable LLMs to understand TS data. The proposed TEST method tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, and creates soft prompts to make LLM more open to the embeddings. The authors demonstrate the feasibility of TS-for-LLM through theory and experiments on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes.\n\nStrengths:\n\n1. The paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field.\n2. The authors provide a thorough analysis of the TS-for-LLM paradigm and its potential methods, which is a valuable addition to the existing literature.\n3. The experiments demonstrate the effectiveness of TEST on various TS tasks, including classification, forecasting, and representation tasks.\n4. The authors provide a detailed comparison with existing methods, including LLM-QA methods, SFT LLM-for-TS methods, classical TS models, and SOTA TS models.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs used are pre-trained and frozen, which may not be the case in real-world applications where LLMs are fine-tuned or trained from scratch.\n2. The authors do not provide a thorough analysis of the impact of the LLM size and type on the results, which is an important aspect to consider in real-world applications.\n3. The paper assumes that the TS data is available in a format that can be easily tokenized and embedded, which may not be the case in real-world applications where TS data may be noisy or missing.\n4. The authors do not provide a detailed comparison with other TS-for-LLM methods, which would be a valuable addition to the existing literature.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the impact of the LLM size and type on the results, including the effect of different architectures and training procedures.\n2. The authors should provide a more thorough comparison with other TS-for-LLM methods, including a discussion of the strengths and weaknesses of each method.\n3. The authors should provide a more detailed analysis of the TS data used in the experiments, including a discussion of the noise and missing values present in the data.\n4. The authors should provide a more detailed discussion of the potential applications of TEST in real-world scenarios, including a discussion of the potential benefits and limitations of the method.\n\nRecommendation:\n\nThe paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field. The experiments demonstrate the effectiveness of TEST on various TS tasks, including classification, forecasting, and representation tasks. However, the paper assumes that the LLMs used are pre-trained and frozen, which may not be the case in real-world applications. Therefore, the authors should provide a more detailed analysis of the impact of the LLM size and type on the results, including the effect of different architectures and training procedures. Additionally, the authors should provide a more thorough comparison with other TS-for-LLM methods, including a discussion of the strengths and weaknesses of each method.\n\nIn conclusion, the paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field. However, the paper has some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called TEST (Text Embedding Space Transformation) to enable Large Language Models (LLMs) to perform Time-Series (TS) tasks without compromising their language ability. TEST is a data-centric approach that converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. The proposed method first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to the LLM's embedding layer space, and finally creates soft prompts to make the LLM more open to that embedding.\n\nStrengths:\n\n1. The paper proposes a novel approach to enable LLMs to perform TS tasks, which is a significant contribution to the field of TS+LLM.\n2. The authors provide a comprehensive summary of the existing work in the field and clearly explain the motivation for their approach.\n3. The proposed method, TEST, is well-defined and easy to understand, with clear explanations of the key components and their functions.\n4. The experiments demonstrate the effectiveness of TEST in achieving comparable performance to SOTA methods on various TS tasks, including classification, forecasting, and representation.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained LLM has sufficient language ability, which may not be the case in all scenarios.\n2. The authors do not provide a thorough comparison with other TS-for-LLM approaches, such as SFT, which may be more effective in certain situations.\n3. The paper relies heavily on the choice of text prototypes and prompts, which may not be universally applicable and require further investigation.\n4. The authors do not provide a clear explanation of how the TS embedding space is aligned to the LLM's embedding layer space, which is a critical component of the proposed method.\n\nSuggestions for Improvement:\n\n1. Provide a more thorough comparison with other TS-for-LLM approaches, such as SFT, to demonstrate the effectiveness of TEST in various scenarios.\n2. Investigate the impact of different text prototypes and prompts on the performance of TEST and provide guidance on how to choose them optimally.\n3. Provide a more detailed explanation of how the TS embedding space is aligned to the LLM's embedding layer space, including the specific techniques used and their theoretical foundations.\n4. Conduct more experiments to investigate the correlation between corpora and TS data, as suggested by the authors, to provide a more comprehensive understanding of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to enable LLMs to perform TS tasks, which has significant potential for real-world applications. However, further investigation is required to address the limitations and weaknesses mentioned above. With some revisions to address these concerns, the paper has the potential to make a significant contribution to the field of TS+LLM.\n\n$$$",
        "gr_score_with": 0.8905472636815921,
        "gr_score_without": 0.8634020618556701,
        "output_without": {
            "num_tokens_scored": 388,
            "num_green_tokens": 335,
            "green_fraction": 0.8634020618556701,
            "z_score": -2.4027478446345945,
            "p_value": 0.9918637982216666
        },
        "output_with": {
            "num_tokens_scored": 402,
            "num_green_tokens": 358,
            "green_fraction": 0.8905472636815921,
            "z_score": -0.6315387224555874,
            "p_value": 0.7361558306416556
        }
    },
    {
        "title": "SHARING RATIO DECOMPOSITION",
        "abstract": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep learning models. SRD analyzes the model using Pointwise Feature Vectors (PFVs) and decomposes relevance with sharing ratios, accounting for nonlinear interactions between filters. The method is based on the observation of Activation-Pattern-Only Prediction (APOP), which highlights the importance of inactive neurons in model behavior. SRD is shown to outperform other saliency map methods across various metrics, including effectiveness, sophistication, and resilience, particularly in terms of robustness against random noise perturbation and targeted adversarial attacks.\n\nStrengths:\n\n* The paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods.\n* The method is based on a thorough reflection of the model's behavior, which is reflected in its robustness against random noise perturbation and targeted adversarial attacks.\n* The paper provides a clear and well-structured explanation of the method, including its mathematical formulation and experimental results.\n* The authors have conducted a comprehensive analysis, including qualitative and quantitative results, which demonstrate the superiority of SRD over other saliency map methods.\n\nWeaknesses:\n\n* The paper assumes that the sharing ratio can be derived at any point, even during inference, which may not be feasible in practice.\n* The method requires prior knowledge of the sharing ratio, which may not be available in all cases.\n* The paper does not provide a clear explanation of how the sharing ratio is calculated, which may be a limitation of the method.\n* The experimental results are limited to a specific dataset and model architecture, which may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the sharing ratio is calculated and how it is used in the method.\n* Investigate the feasibility of deriving the sharing ratio at any point during inference and provide a clear explanation of the challenges and limitations.\n* Conduct experiments on a wider range of datasets and model architectures to demonstrate the robustness and generalizability of SRD.\n* Provide a more detailed analysis of the APOP phenomenon and its implications for model behavior.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the method and demonstrates its superiority over other saliency map methods. Therefore, I recommend this paper for publication in a reputable conference or journal.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD) to address the limitations of existing explanation methods. SRD adopts a vector perspective to consider the intricate nonlinear interactions between filters and introduces an interesting observation termed Activation-PatternOnly Prediction (APOP), which highlights the importance of inactive neurons. The method decomposes a Pointwise Feature Vector (PFV) into shares of PFVs in its receptive field and provides a high-resolution Effective Receptive Field (ERF) at any layer. SRD demonstrates superior performance across various metrics, including effectiveness, sophistication, and resilience, and showcases notable proficiency in robustness, withstanding both random noise perturbation and targeted adversarial attacks.\n\nStrengths:\n\n1. The paper proposes a novel method that addresses the limitations of existing explanation methods.\n2. SRD adopts a vector perspective, which is a departure from conventional neuron-level analyses.\n3. The method introduces an interesting observation termed Activation-PatternOnly Prediction (APOP), which highlights the importance of inactive neurons.\n4. SRD demonstrates superior performance across various metrics, including effectiveness, sophistication, and resilience.\n5. The method showcases notable proficiency in robustness, withstanding both random noise perturbation and targeted adversarial attacks.\n\nWeaknesses:\n\n1. The paper assumes that the sharing ratio can be derived at any point, even during inference, which might not be feasible in practice.\n2. The method relies on the availability of model-generated information, which might not be available in some scenarios.\n3. The paper does not provide a thorough analysis of the computational complexity of the method.\n4. The method assumes that the PFV can be decomposed into shares of PFVs in its receptive field, which might not be possible in all cases.\n5. The paper does not provide a clear explanation of how the method can be extended to other types of neural networks.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of the method.\n2. Investigate the feasibility of deriving the sharing ratio at any point, even during inference.\n3. Explore alternative methods for deriving the sharing ratio, such as using gradient-based methods.\n4. Provide a more thorough analysis of the limitations of the method and how they can be addressed.\n5. Investigate the extension of the method to other types of neural networks.\n\nRecommendation:\n\nThe paper proposes a novel method that addresses the limitations of existing explanation methods. However, it requires further investigation and refinement to address the limitations and challenges mentioned above. With some modifications and improvements, SRD has the potential to become a powerful tool for generating robust and trustworthy explanations in AI models.\n\nOverall, the paper is well-written and easy to follow. The authors provide a clear explanation of the method and its benefits. However, the paper could benefit from a more detailed analysis of the computational complexity and limitations of the method.",
        "gr_score_with": 0.9375,
        "gr_score_without": 0.9072164948453608,
        "output_without": {
            "num_tokens_scored": 388,
            "num_green_tokens": 352,
            "green_fraction": 0.9072164948453608,
            "z_score": 0.4740264148953115,
            "p_value": 0.31774053540046465
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 345,
            "green_fraction": 0.9375,
            "z_score": 2.3980819492615506,
            "p_value": 0.008240588760600309
        }
    },
    {
        "title": "CROSSLOCO: HUMAN MOTION DRIVEN CONTROL OF LEGGED ROBOTS VIA GUIDED UNSUPERVISED REIN-",
        "abstract": "Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents CrossLoco, a guided unsupervised reinforcement learning framework that enables simultaneous learning of human-robot motion correspondence and robot motion control. The framework introduces a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and synthesized robot movements. The authors demonstrate that CrossLoco can translate a large set of human motions for robots, including walking, running, and dancing, and achieve better quantitative results in terms of accuracy, diversity, and user preference compared to baseline methods.\n\nStrengths:\n\n* The paper presents a novel framework for learning human-robot motion correspondence and robot motion control in an unsupervised manner.\n* The cycle-consistency-based correspondence reward term is a key innovation that enables the learning of complex robot skills and human-robot motion correspondence.\n* The authors provide a thorough evaluation of their method, including quantitative and qualitative results, as well as a comparison with baseline methods.\n* The paper explores the potential applications of CrossLoco, including language2text motion synthesis and interactive robot control.\n\nWeaknesses:\n\n* The paper assumes that the human motion dataset is available, which may not be the case in many real-world scenarios.\n* The authors do not provide a clear explanation of how the cycle-consistency-based correspondence reward term is computed, which may make it difficult for readers to reproduce the results.\n* The paper does not discuss the scalability of CrossLoco to more complex scenarios, such as long-horizon human demonstrations or tool manipulation on a legged-manipulation robot.\n* The authors do not provide a clear comparison with other unsupervised reinforcement learning methods that do not use cycle-consistency.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the cycle-consistency-based correspondence reward term is computed, including the specific mathematical formulation and the parameters used.\n* Discuss the scalability of CrossLoco to more complex scenarios, such as long-horizon human demonstrations or tool manipulation on a legged-manipulation robot.\n* Compare CrossLoco with other unsupervised reinforcement learning methods that do not use cycle-consistency to provide a more comprehensive evaluation of the method.\n* Provide more details on the implementation of the framework, including the specific neural network architectures used and the hyperparameters employed.\n\nRecommendation:\n\nThe paper presents a novel and innovative framework for learning human-robot motion correspondence and robot motion control in an unsupervised manner. While there are some limitations and areas for improvement, the paper provides a thorough evaluation of the method and explores its potential applications. I recommend this paper for publication in a top-tier conference in the field of robotics and machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents CrossLoco, a guided unsupervised reinforcement learning framework that enables simultaneous learning of human-robot motion correspondence and robot motion control. The framework introduces a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and synthesized robot movements. The authors demonstrate that CrossLoco can translate a large set of human motions for robots, including walking, running, and dancing, and achieve better quantitative results in terms of accuracy, diversity, and user preference compared to baseline methods.\n\nStrengths:\n1. The paper presents a novel approach to human motion driven control of legged robots, which is a challenging problem due to the mismatches in kinematics and dynamics properties between humans and robots.\n2. The authors introduce a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and synthesized robot movements, which is a key innovation of the paper.\n3. The paper presents a thorough experimental evaluation of CrossLoco, including quantitative comparisons with baseline methods and user studies, which provides strong evidence for the effectiveness of the approach.\n4. The authors provide a clear and concise explanation of the framework, including the problem formulation, method overview, and experimental setup, which makes it easy to follow and understand.\n\nWeaknesses:\n1. The paper assumes that the human motion dataset is available, which may not be the case in many real-world scenarios.\n2. The authors do not provide a detailed analysis of the limitations of the approach, such as the potential for overfitting or the need for large amounts of training data.\n3. The paper does not provide a clear explanation of how the cycle-consistency-based correspondence reward term is used in practice, which may make it difficult for readers to replicate the results.\n4. The authors do not provide a detailed discussion of the potential applications of CrossLoco, such as in robotics, computer graphics, or virtual reality.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of the approach and discuss potential solutions to address these limitations.\n2. The paper should provide a clear explanation of how the cycle-consistency-based correspondence reward term is used in practice, including any modifications or extensions that may be necessary.\n3. The authors should provide a more detailed discussion of the potential applications of CrossLoco and how it can be used in real-world scenarios.\n4. The paper should include more experimental results, such as comparisons with other state-of-the-art methods, to provide a more comprehensive evaluation of CrossLoco.\n\nRecommendation:\nThe paper presents a novel and effective approach to human motion driven control of legged robots, and the experimental results provide strong evidence for its effectiveness. However, the paper could benefit from a more detailed analysis of the limitations of the approach and a clearer explanation of how the cycle-consistency-based correspondence reward term is used in practice. With these improvements, the paper would be a strong contribution to the field of robotics and computer graphics.\n\nRating: 8/10",
        "gr_score_with": 0.9321148825065274,
        "gr_score_without": 0.8757062146892656,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 310,
            "green_fraction": 0.8757062146892656,
            "z_score": -1.5233994301472722,
            "p_value": 0.9361705977455217
        },
        "output_with": {
            "num_tokens_scored": 383,
            "num_green_tokens": 357,
            "green_fraction": 0.9321148825065274,
            "z_score": 2.095176544267345,
            "p_value": 0.01807765077492688
        }
    },
    {
        "title": "A PRECISE CHARACTERIZATION OF SGD STABILITY USING LOSS SURFACE GEOMETRY",
        "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors focus on the linearized dynamics of SGD close to the optimum, and introduce a novel coherence measure \u03c3 to capture the relevant geometric characteristics of the loss surface. They provide a sufficient condition for the linear dynamics to diverge, which relies solely on the values of \u03bb1(H), \u03b7, B, n, and \u03c3. The authors also demonstrate that their sufficient condition is nearly optimal across a natural range of SGD hyperparameters.\n\nStrengths:\n\n* The paper presents a novel coherence measure \u03c3 that summarizes relevant information in the loss surface geometry.\n* The authors provide a precise characterization of the stability of SGD using linearized dynamics.\n* The paper demonstrates that the sufficient condition for divergence is nearly optimal across a natural range of SGD hyperparameters.\n* The authors provide a series of experiments to validate their theoretical results.\n\nWeaknesses:\n\n* The paper assumes that the loss function is additively decomposable, which may not be the case in all scenarios.\n* The authors do not provide a detailed comparison of their results with other existing works in the field.\n* The paper assumes that the learning rate and batch size are fixed, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of their results with other existing works in the field.\n* The paper should be extended to consider more general loss functions and learning rates.\n* The authors should provide a more detailed analysis of the coherence measure \u03c3 and its relation to the stability of SGD.\n\nRecommendation:\n\nThe paper presents a novel and precise characterization of the stability of SGD using loss surface geometry. The authors provide a sufficient condition for divergence that is nearly optimal across a natural range of SGD hyperparameters. The paper also provides a series of experiments to validate their theoretical results. However, the paper assumes that the loss function is additively decomposable and does not provide a detailed comparison of their results with other existing works in the field. Overall, the paper is well-written and provides a significant contribution to the field of optimization.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of stochastic gradient descent (SGD) stability using loss surface geometry. The authors focus on the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks. They introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function relevant to the linear stability of SGD. The authors provide a simplified sufficient condition for identifying linear instability at an optimum and demonstrate that their condition is nearly optimal across a natural range of SGD hyperparameters.\n\nStrengths:\n\n* The paper provides a novel and interpretable sufficient condition for identifying linear instability at an optimum using a coherence measure of the loss Hessian.\n* The authors demonstrate that their condition is nearly optimal across a natural range of SGD hyperparameters, which implies that their analysis is precise and closely aligned with the behavior of SGD.\n* The paper presents a series of experiments conducted on additively decomposable quadratic loss functions, which corroborate the validity of their theoretical findings.\n\nWeaknesses:\n\n* The paper relies on several assumptions, such as additively decomposable loss functions and PSD matrices, which may not be realistic in all cases.\n* The authors do not provide a detailed comparison of their results with previous work in the area of linear stability analysis of SGD.\n* The paper does not provide a clear explanation of how the coherence measure \u03c3 can be computed in practice, especially for large neural networks.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of their results with previous work in the area of linear stability analysis of SGD.\n* The authors should provide a clear explanation of how the coherence measure \u03c3 can be computed in practice, especially for large neural networks.\n* The authors should investigate whether their results can be extended to more general classes of loss functions and neural networks.\n\nRecommendation:\n\nThe paper provides a novel and interpretable sufficient condition for identifying linear instability at an optimum using a coherence measure of the loss Hessian. The authors demonstrate that their condition is nearly optimal across a natural range of SGD hyperparameters, which implies that their analysis is precise and closely aligned with the behavior of SGD. However, the paper relies on several assumptions and does not provide a clear explanation of how the coherence measure \u03c3 can be computed in practice. Overall, the paper is a solid contribution to the area of linear stability analysis of SGD, but it requires further work to make it more practical and applicable to real-world scenarios.\n\nRating: 7/10\n\nRecommendation for publication: Yes, but with revisions to address the weaknesses mentioned above.\n\n$$$",
        "gr_score_with": 0.8896103896103896,
        "gr_score_without": 0.8653846153846154,
        "output_without": {
            "num_tokens_scored": 260,
            "num_green_tokens": 225,
            "green_fraction": 0.8653846153846154,
            "z_score": -1.8603276043603976,
            "p_value": 0.968580404820887
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 274,
            "green_fraction": 0.8896103896103896,
            "z_score": -0.6075988723157615,
            "p_value": 0.7282732255346435
        }
    },
    {
        "title": "DON\u2019T TRUST: VERIFY \u2013 GROUNDING LLM QUANTI-",
        "abstract": "Large language models (LLM), such as Google\u2019s Minerva and OpenAI\u2019s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code \u2014 which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting \u2014 the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among multiple informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. DTV leverages the autoformalization capability of LLMs through few-shot prompting to translate informal statements into formal statements, and then uses a formal theorem proving environment to verify the correctness of the formal statements. The authors demonstrate the effectiveness of DTV by evaluating it on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. The results show that DTV consistently outperforms vanilla majority voting, the best previous approach, and leads to improvement across different model sizes from 8B, 62B, and 540B.\n\nStrengths:\n\n* The paper proposes a novel approach to identify the correct answer among multiple informal solution samples generated by LLMs.\n* The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith.\n* The paper provides a clear and concise description of the DTV approach and its implementation details.\n* The authors discuss the limitations of the current approach and provide suggestions for future work.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used for autoformalization have sufficient training data and capabilities to translate informal statements into formal statements accurately.\n* The authors do not provide a thorough analysis of the errors made by the LLMs in translating informal statements into formal statements.\n* The paper relies on the consistency of the formal environment and its automated theorem prover to verify the correctness of the formal statements, which may not always be the case.\n* The authors do not provide a detailed discussion of the computational cost and time complexity of the DTV approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the errors made by the LLMs in translating informal statements into formal statements and discuss ways to mitigate these errors.\n* The paper should include a more detailed discussion of the computational cost and time complexity of the DTV approach and provide suggestions for improving its efficiency.\n* The authors should provide more case studies and examples to demonstrate the effectiveness of DTV on a wider range of quantitative reasoning problems.\n* The paper should include a more detailed discussion of the limitations of the current approach and provide suggestions for future work to overcome these limitations.\n\nRecommendation:\n\nThe paper is well-written and provides a clear and concise description of the DTV approach and its implementation details. The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. However, the paper assumes that the LLMs used for autoformalization have sufficient training data and capabilities to translate informal statements into formal statements accurately, which may not always be the case. Therefore, I recommend that the authors provide a more thorough analysis of the errors made by the LLMs in translating informal statements into formal statements and discuss ways to mitigate these errors. I also recommend that the authors provide more case studies and examples to demonstrate the effectiveness of DTV on a wider range of quantitative reasoning problems.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among multiple informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. The method leverages the autoformalization capability of LLMs through few-shot prompting to translate informal statements into formal Isabelle code, which can be verified automatically for internal consistency. The approach is lightweight and does not require training or finetuning. The authors evaluate DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith, and demonstrate that it consistently outperforms vanilla majority voting, the best previous approach.\n\nStrengths:\n\n* The paper presents a novel approach to addressing the issue of incorrect answers generated by LLMs for quantitative reasoning problems.\n* The method is lightweight and does not require training or finetuning, making it a practical solution for real-world applications.\n* The authors provide a thorough evaluation of DTV on three quantitative reasoning datasets and demonstrate its effectiveness in improving the accuracy of LLM-generated answers.\n\nWeaknesses:\n\n* The approach relies on the autoformalization capability of LLMs, which may not be perfect and can lead to errors in translating informal statements into formal Isabelle code.\n* The method may not be effective for problems that require advanced mathematical concepts or reasoning that is beyond the scope of current formal theorem proving environments.\n* The paper does not provide a detailed analysis of the limitations of the approach and potential areas for future improvement.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the limitations of the approach and potential areas for future improvement.\n* They should investigate the use of more effective filters and reinforcement learning from human feedback to further finetune language models on aligned mathematical formalizations directly.\n* The approach could be extended to handle problems that require advanced mathematical concepts or reasoning that is beyond the scope of current formal theorem proving environments.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to addressing the issue of incorrect answers generated by LLMs for quantitative reasoning problems. The method is lightweight and does not require training or finetuning, making it a practical solution for real-world applications. However, the approach relies on the autoformalization capability of LLMs, which may not be perfect and can lead to errors in translating informal statements into formal Isabelle code. Therefore, the approach should be further developed and evaluated to address these limitations. Overall, the paper is well-written and provides a thorough evaluation of the approach. It is a significant contribution to the field of natural language processing and has the potential to impact real-world applications.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9190751445086706,
        "gr_score_without": 0.8847352024922118,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 284,
            "green_fraction": 0.8847352024922118,
            "z_score": -0.9114388770235636,
            "p_value": 0.8189679113523514
        },
        "output_with": {
            "num_tokens_scored": 346,
            "num_green_tokens": 318,
            "green_fraction": 0.9190751445086706,
            "z_score": 1.1829043054504813,
            "p_value": 0.11842353756492519
        }
    }
]