[
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential limitations and challenges of applying the concept bottleneck layer to more complex and realistic settings.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the training process for the concept bottleneck layer, including the loss functions and hyperparameters used.\n2. Discuss the potential limitations and challenges of applying the concept bottleneck layer to more complex and realistic settings, such as text or 3D data.\n3. Provide more qualitative and quantitative evaluations of the concept bottleneck layer, including its performance on more challenging tasks and datasets.\n4. Consider incorporating more visualizations and illustrations to help readers understand the concept bottleneck layer and its applications.\n\nRecommendation:\n\nThis paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. While the paper has some limitations, it provides a comprehensive evaluation of CBGMs and demonstrates their effectiveness and robustness. I recommend this paper to readers interested in generative modeling, interpretability, and controllability.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThis paper proposes concept bottleneck generative models (CBGMs) to address the challenges of interpreting and steering generative models. The authors introduce a concept bottleneck layer that constrains the model to encode human-understandable concepts. They complement the traditional task-based loss function with a concept loss and an orthogonality loss to train the CBGMs. The CB layer is model-agnostic and can be applied to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. The authors demonstrate the effectiveness of the CBGMs in steering generative models and interpretability.\n\nStrengths:\n\n* The concept bottleneck layer is a novel and effective approach to address the challenges of interpreting and steering generative models.\n* The authors provide a comprehensive analysis of the concept bottleneck layer and its application to different generative models.\n* The experiments demonstrate the effectiveness of the CBGMs in steering generative models and interpretability.\n* The authors provide a detailed comparison with existing conditional generation approaches.\n\nWeaknesses:\n\n* The requirement for pre-defined human-understandable features can be laborious and might not be feasible in all cases.\n* The CB layer can only be applied to image tasks and not to text tasks.\n* The orthogonality loss can be sensitive to hyperparameters and might not be effective in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how to handle the requirement for pre-defined human-understandable features.\n* The authors should explore the possibility of extending the CB layer to text tasks.\n* The authors should provide more insights on how to choose the hyperparameters for the orthogonality loss.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of generative models by proposing a novel and effective approach to address the challenges of interpreting and steering generative models. The experiments demonstrate the effectiveness of the CBGMs, and the authors provide a comprehensive analysis of the concept bottleneck layer and its application to different generative models. However, there are some limitations and weaknesses that need to be addressed. Overall, this paper is well-written and well-structured, and it has the potential to make a significant impact in the field.\n\nRating: 8/10\n\nRecommendation: Accept\n\nReason: The paper makes a significant contribution to the field of generative models and provides a comprehensive analysis of the concept bottleneck layer and its application to different generative models. However, there are some limitations and weaknesses that need to be addressed.\n\n$$$",
        "gr_score_with": 0.6182432432432432,
        "gr_score_without": 0.5038560411311054,
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 196,
            "green_fraction": 0.5038560411311054,
            "z_score": -3.8705189837256593,
            "p_value": 0.9999456980505043
        },
        "output_with": {
            "num_tokens_scored": 296,
            "num_green_tokens": 183,
            "green_fraction": 0.6182432432432432,
            "z_score": 0.6408446890197651,
            "p_value": 0.2608117974050842
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning, called Sparse Entropic Wasserstein Regression (SWAP), which utilizes Entropic Wasserstein regression (EWR) to prune neural networks. The method aims to mitigate the problem of inaccurate gradients in computing the empirical Fisher Information Matrix, which is a critical challenge in network pruning. SWAP strikes a balance between noise reduction and covariance information preservation, leveraging the geometric properties of the optimal transport problem. The proposed method is compared with several existing state-of-the-art (SoTA) methods, including Magnitude Pruning (MP), WoodFisher (WF), Combinatorial Brain Surgeon (CBS), and Sparse Linear Regression (LR), on various networks and datasets. The results show that SWAP outperforms or matches the SoTA methods, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper proposes a novel method, SWAP, which addresses the challenge of inaccurate gradients in network pruning.\n2. The method utilizes Entropic Wasserstein regression (EWR) to prune neural networks, which strikes a balance between noise reduction and covariance information preservation.\n3. The paper provides a comprehensive analysis of the proposed method, including its theoretical aspects, algorithm design, and numerical results.\n4. The results show that SWAP outperforms or matches the SoTA methods, especially in the presence of noisy gradients.\n5. The paper provides a well-organized and readable presentation, with clear explanations of the methods and results.\n\nWeaknesses:\n\n1. The paper assumes that the true gradients are not available, which may not be the case in practice.\n2. The method requires careful tuning of hyperparameters, such as the regularization parameter \u03bb and the entropic regularization term \u03b5.\n3. The paper does not provide a comprehensive comparison with other pruning methods, such as Lottery Ticket Hypothesis (LTH) or Pruning with Reweighting (PRW).\n4. The results are based on a limited set of experiments, and more extensive experiments are needed to confirm the effectiveness of the proposed method.\n5. The paper does not provide a clear explanation of how to choose the optimal value of \u03b5, which is crucial for the performance of the proposed method.\n\nSuggestions for Improvement:\n\n1. Provide a more comprehensive comparison with other pruning methods, including LTH and PRW.\n2. Conduct more extensive experiments to confirm the effectiveness of the proposed method.\n3. Provide a clear explanation of how to choose the optimal value of \u03b5.\n4. Investigate the robustness of the proposed method to different types of noise, such as label noise or data corruption.\n5. Provide more details on the implementation of the proposed method, including the code and the computational resources required.\n\nRecommendation:\n\nThe paper proposes a novel method, SWAP, which addresses the challenge of inaccurate gradients in network pruning. The method utilizes Entropic Wasserstein regression (EWR) to prune neural networks, which strikes a balance between noise reduction and covariance information preservation. While the results show that SWAP outperforms or matches the SoTA methods, especially in the presence of noisy gradients, more extensive experiments are needed to confirm the effectiveness of the proposed method. I recommend acceptance with minor revisions, including providing a more comprehensive comparison with other pruning methods, conducting more extensive experiments, and providing a clear explanation of how to choose the optimal value of \u03b5.\n\n$$",
        "gr_score_with": 0.65,
        "gr_score_without": 0.5786516853932584,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 206,
            "green_fraction": 0.5786516853932584,
            "z_score": -0.8220286359550896,
            "p_value": 0.7944697003483685
        },
        "output_with": {
            "num_tokens_scored": 400,
            "num_green_tokens": 260,
            "green_fraction": 0.65,
            "z_score": 2.041428457627046,
            "p_value": 0.020604129123423975
        }
    },
    {
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines. We release all the above-mentioned resources at https://github.com/RUCAIBox/FIGA.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called FIGA (Fine-grained Quality-Aware Alignment) for aligning language models with human preferences. The approach leverages fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of language models for alignment. The authors curate a refined alignment dataset called SPA (SubPar Alignment) that pairs initial responses with revised responses on queries where a language model performs less well. They also design a new loss function that incorporates fine-grained quality signals to encourage the model to learn good behaviors and penalize bad behaviors. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nStrengths:\n\n1. The paper presents a novel approach for aligning language models with human preferences, which is a critical problem in natural language processing.\n2. The approach leverages fine-grained quality signals, which provides more detailed information about what makes a response good or bad.\n3. The authors curate a refined alignment dataset called SPA, which is a significant contribution to the field.\n4. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nWeaknesses:\n\n1. The paper assumes the availability of a reward model, which may not always be the case in real-world scenarios.\n2. The approach relies on the quality of the revised responses, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the Levenshtein distance metric used to quantify the similarity between initial and revised responses.\n4. The approach may not generalize well to out-of-distribution scenarios, where the reward model may not perform well.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for deriving fine-grained quality signals, such as using other metrics or incorporating additional information.\n2. Evaluate the approach on more diverse and challenging datasets to assess its generalizability.\n3. Provide more detailed analysis of the Levenshtein distance metric and its limitations.\n4. Investigate the use of other reward models or alternative methods for aligning language models with human preferences.\n\nRecommendation:\n\nThe paper presents a novel and promising approach for aligning language models with human preferences. While there are some limitations and areas for improvement, the approach shows superior performance compared to existing alignment methods. I recommend this paper for publication in a top-tier conference, with the suggestion to address the weaknesses and improve the approach as mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper proposes an alignment approach for large language models (LLMs) called FIGA, which stands for \"Fine-grained Quality-aware Alignment\". The main idea behind FIGA is to leverage fine-grained quality signals to enhance the alignment quality during fine-tuning. The authors curate a high-quality alignment dataset that pairs initial responses with revised responses on queries that a LLM cannot perform well. They also design a new learning objective that can leverage the fine-grained quality signals by contrasting initial with revised responses.\n\nStrengths:\n\n* The paper proposes an innovative approach to alignment that leverages fine-grained quality signals.\n* The authors provide a clear and well-structured explanation of the FIGA approach.\n* The paper presents extensive experimental results that demonstrate the effectiveness of FIGA.\n* The authors provide a comprehensive review of the related work and discuss the limitations and potential applications of FIGA.\n\nWeaknesses:\n\n* The paper assumes that the reward model used to evaluate the quality of responses is accurate and reliable. However, the authors do not provide any information about the specific reward model used or how it was trained.\n* The paper does not provide any information about the scalability and computational efficiency of the FIGA approach.\n* The authors do not discuss the potential limitations and challenges of using fine-grained quality signals for alignment.\n\nSuggestions for Improvement:\n\n* Provide more information about the reward model used to evaluate the quality of responses and how it was trained.\n* Discuss the scalability and computational efficiency of the FIGA approach and provide more information about the computational resources required to train and deploy the model.\n* Discuss the potential limitations and challenges of using fine-grained quality signals for alignment and provide more information about the potential applications and future work.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of alignment for large language models and provides an innovative approach to alignment that leverages fine-grained quality signals. The experimental results demonstrate the effectiveness of the FIGA approach, and the authors provide a comprehensive review of the related work. However, the paper assumes that the reward model used to evaluate the quality of responses is accurate and reliable, and does not provide information about the scalability and computational efficiency of the FIGA approach. Therefore, I recommend that the authors address these limitations and provide more information about the reward model and the computational resources required to train and deploy the model.\n\nRating: 8/10",
        "gr_score_with": 0.721830985915493,
        "gr_score_without": 0.6193181818181818,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 218,
            "green_fraction": 0.6193181818181818,
            "z_score": 0.7400077546682751,
            "p_value": 0.22964764448828728
        },
        "output_with": {
            "num_tokens_scored": 284,
            "num_green_tokens": 205,
            "green_fraction": 0.721830985915493,
            "z_score": 4.191091451057354,
            "p_value": 1.3880786243260323e-05
        }
    },
    {
        "title": "GUAGE MODEL POWERED DIALOGUE AGENTS",
        "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel paradigm for strategizing large language models (LLMs) powered dialogue agents, called Plug-and-Play Dialogue Policy Planner (PPDPP). PPDPP is designed to tackle the challenges of proactive dialogue problems, where the dialogue agent needs to strategically take the initiative to steer the conversation towards an anticipated goal. The proposed framework consists of a tunable language model plug-in that can be fine-tuned with available human-annotated data and reinforced learning from goal-oriented AI feedback. The authors demonstrate the effectiveness of PPDPP on three different proactive dialogue problems, including negotiation, emotional support, and tutoring dialogues.\n\nStrengths:\n1. The paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems.\n2. The framework is designed to be modular and flexible, allowing for easy adaptation to different applications and domains.\n3. The authors provide a thorough evaluation of PPDPP on three different datasets, demonstrating its superiority over existing approaches.\n4. The paper highlights the potential of tunable plug-ins to address specific shortcomings in LLMs and integrate with multiple plug-ins to tackle more complex dialogue challenges.\n\nWeaknesses:\n1. The paper assumes the availability of human-annotated data, which may not be feasible in all scenarios.\n2. The framework relies on the quality of the goal-oriented AI feedback, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the computational resources required for training and deploying PPDPP.\n4. The authors do not discuss the potential limitations and biases of using LLMs in dialogue systems.\n\nSuggestions for Improvement:\n1. Provide more detailed information on the data requirements and preprocessing steps for training PPDPP.\n2. Discuss the potential limitations and biases of using LLMs in dialogue systems and propose potential solutions.\n3. Provide a more comprehensive analysis of the computational resources required for training and deploying PPDPP.\n4. Consider exploring other types of goal-oriented AI feedback, such as multi-modal feedback, to improve the robustness of PPDPP.\n\nRecommendation:\nThe paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems. While there are some limitations and areas for improvement, the framework shows promise for a wide range of applications. I recommend the paper for publication, with the understanding that the authors will address the suggested improvements in future work.\n\nGrade: B+ (87%)\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel dialogue policy planning paradigm, Plug-and-Play Dialogue Policy Planner (PPDPP), to enhance the proactivity of large language models (LLMs) in dialogue tasks. PPDPP utilizes a tunable language model plug-in to predict the dialogue strategy, which can be supervisedly fine-tuned with human-annotated corpora and further refined through reinforcement learning from goal-oriented AI feedback. The framework consists of three components: (1) self-play interaction, where two LLMs engage in conversations to simulate dynamic user-assistant interactions; (2) LLM-based reward model, which provides goal-oriented verbal feedback to evaluate the policy outcome; and (3) reinforcement learning, which optimizes the policy agent to maximize the expected cumulative rewards. The authors conduct extensive experiments on three different proactive dialogue problems, including negotiation, emotional support, and tutoring dialogues, and demonstrate the superiority of PPDPP over existing approaches.\n\nStrengths:\n1. The paper proposes a novel and effective dialogue policy planning paradigm, PPDPP, which addresses the challenges of existing approaches, such as limited policy planning capability and lack of transferability.\n2. The framework consists of three components, which provide a clear and structured approach to dialogue policy planning.\n3. The authors conduct extensive experiments on three different dialogue tasks, which demonstrate the effectiveness and efficiency of PPDPP.\n4. The paper highlights the potential of tunable plug-ins to address specific shortcomings in LLMs, which can be extended to various applications and integrated with multiple plug-ins to tackle more complex dialogue challenges.\n\nWeaknesses:\n1. The paper assumes the availability of human-annotated corpora for supervised fine-tuning, which may not be feasible in all cases.\n2. The framework relies on the quality of the LLM-based reward model, which may be subject to biases and variability.\n3. The paper does not provide a clear explanation of how the LLM-based reward model is trained and evaluated.\n4. The authors do not provide a detailed comparison with existing dialogue policy planning methods, which makes it difficult to evaluate the novelty and effectiveness of PPDPP.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the LLM-based reward model, including its training and evaluation procedures.\n2. Conduct a more thorough comparison with existing dialogue policy planning methods to evaluate the novelty and effectiveness of PPDPP.\n3. Investigate the impact of different LLM architectures and training procedures on the performance of PPDPP.\n4. Explore the application of PPDPP to more complex dialogue tasks and domains, such as multi-turn conversations and dialogue management.\n\nRecommendation:\nThe paper proposes a novel and effective dialogue policy planning paradigm, PPDPP, which addresses the challenges of existing approaches. While the paper has some limitations, the authors provide extensive experimental results and highlight the potential of tunable plug-ins to address specific shortcomings in LLMs. I recommend this paper for publication, subject to the authors addressing the suggestions for improvement and providing a more detailed explanation of the LLM-based reward model.\n$$",
        "gr_score_with": 0.6761904761904762,
        "gr_score_without": 0.5766016713091922,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 207,
            "green_fraction": 0.5766016713091922,
            "z_score": -0.9047711730671455,
            "p_value": 0.8172066867921248
        },
        "output_with": {
            "num_tokens_scored": 420,
            "num_green_tokens": 284,
            "green_fraction": 0.6761904761904762,
            "z_score": 3.187465779408985,
            "p_value": 0.0007176272022922288
        }
    },
    {
        "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
        "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in deep neural networks (DNNs). FOMO alternates between the forgetting phase, where a subset of weights is randomly forgotten through reinitialization, and the relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the efficacy of FOMO on benchmark datasets and adversarial attacks, showing that it alleviates robust overfitting, improves robustness, and provides a better trade-off between standard and robust accuracy.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the problem of robust overfitting in adversarial training and its significance in real-world applications.\n2. The authors propose a novel and innovative solution, FOMO, which is grounded in the concept of active forgetting and relearning, inspired by the human brain's learning mechanisms.\n3. The experimental results demonstrate the effectiveness of FOMO in alleviating robust overfitting, improving robustness, and providing a better trade-off between standard and robust accuracy.\n4. The authors conduct extensive experiments on multiple datasets, architectures, and perturbation types, showcasing the scalability and robustness of FOMO.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase is beneficial for generalization, but it is unclear whether this assumption holds in all cases.\n2. The authors do not provide a thorough analysis of the convergence properties of FOMO, which is crucial for understanding its robustness and generalization capabilities.\n3. The paper relies heavily on empirical results, and more theoretical analysis is needed to understand the underlying mechanisms of FOMO.\n4. The authors do not discuss the computational costs and complexity of implementing FOMO, which may be a limitation in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the forgetting phase and its impact on generalization.\n2. Investigate the convergence properties of FOMO and its robustness to different perturbation types.\n3. Conduct more theoretical analysis to understand the underlying mechanisms of FOMO and its relationship to the effectiveness of adversarial training.\n4. Discuss the computational costs and complexity of implementing FOMO and provide strategies for reducing these costs.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of adversarial training by proposing a novel and innovative solution, FOMO. The experimental results demonstrate the effectiveness of FOMO, and the authors provide a comprehensive overview of the problem and its significance. However, more theoretical analysis and investigation of the forgetting phase are needed to fully understand the underlying mechanisms of FOMO. I recommend this paper for publication in a top-tier conference in the field of machine learning and computer vision.\n\nScore: 8/10\n\nReviewer's Name: [Your Name]\n\nReviewer's Affiliation: [Your Affiliation]\n\nDate: [Current Date]",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to improve the performance and generalization of Deep Neural Networks (DNNs) in adversarial training. FOMO alternates between a forgetting phase, where a random subset of weights is reinitialized, and a relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the effectiveness of FOMO in alleviating robust overfitting and achieving significant results across multiple architectures and datasets.\n\nStrengths:\n\n1. The paper presents a novel approach to addressing robust overfitting in adversarial training, which is a critical problem in the field of adversarial robustness.\n2. The authors provide a comprehensive analysis of the problem and its implications, as well as a thorough review of the related work.\n3. The proposed FOMO method is well-structured and easy to follow, with clear explanations of the forgetting and relearning phases.\n4. The authors provide extensive experimental results on multiple datasets and architectures, demonstrating the effectiveness of FOMO in improving robust performance and generalization.\n\nWeaknesses:\n\n1. The paper relies heavily on empirical results, and the theoretical understanding of the robust overfitting phenomenon is not fully explored.\n2. The authors do not provide a detailed analysis of the forgetting phase, including the selection of the subset of weights to be reinitialized and the impact of the forgetting rate on the performance of FOMO.\n3. The paper focuses primarily on the adversarial training paradigm and does not explore other aspects of robustness, such as the impact of data augmentation and regularization techniques.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the forgetting phase, including the selection of the subset of weights to be reinitialized and the impact of the forgetting rate on the performance of FOMO.\n2. Explore the impact of FOMO on other aspects of robustness, such as data augmentation and regularization techniques.\n3. Develop a more comprehensive understanding of the robust overfitting phenomenon and its implications for adversarial training.\n4. Investigate the scalability of FOMO to larger and more complex datasets, as well as its application to other tasks and domains.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to addressing robust overfitting in adversarial training, and the authors provide a comprehensive analysis of the problem and its implications. However, the paper relies heavily on empirical results, and the theoretical understanding of the robust overfitting phenomenon is not fully explored. To improve the paper, the authors should provide a more detailed analysis of the forgetting phase, explore the impact of FOMO on other aspects of robustness, and develop a more comprehensive understanding of the robust overfitting phenomenon. Overall, the paper presents a promising solution for achieving better robust generalization in adversarial training, and its results are likely to be of interest to researchers and practitioners in the field of adversarial robustness.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.7478510028653295,
        "gr_score_without": 0.6871794871794872,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 268,
            "green_fraction": 0.6871794871794872,
            "z_score": 3.5144992116412967,
            "p_value": 0.00022029198978675102
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 261,
            "green_fraction": 0.7478510028653295,
            "z_score": 5.638249136151536,
            "p_value": 8.589392535977496e-09
        }
    },
    {
        "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
        "abstract": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Unlike existing autoregressive models that use rotationally invariant features, Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n\nStrengths:\n\n1. The paper presents a novel approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections.\n2. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n3. The paper proposes a comprehensive set of tests to evaluate the quality of generated 3D structures, including validity, capturing training set statistics, and generalization capabilities.\n4. The authors provide a detailed comparison between their approach and existing methods, highlighting the advantages of their approach.\n\nWeaknesses:\n\n1. The paper assumes that the training set molecules are representative of the underlying distribution, which may not always be the case.\n2. The authors use a lookup table to assign bond orders, which may not be accurate for all molecules.\n3. The paper does not provide a clear explanation of how the spherical harmonic projections are used to model the radial distribution.\n4. The authors do not provide a detailed comparison between their approach and existing diffusion models, such as EDM.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the spherical harmonic projections are used to model the radial distribution.\n2. Use a more accurate method to assign bond orders, such as using a neural network to predict bond orders.\n3. Provide a more detailed comparison between the approach and existing diffusion models, such as EDM.\n4. Investigate the use of normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections. While there are some limitations and areas for improvement, the paper provides a comprehensive evaluation of the approach and demonstrates its effectiveness on various metrics. I recommend accepting the paper for publication, subject to the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that builds molecules from molecular fragments. Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to model the 3D geometry of molecules, allowing for a novel representation of probability distributions.\n\nStrengths:\n1. Symphony's use of higher-degree E(3)-equivariant features and spherical harmonic projections enables a more accurate representation of 3D molecular geometries, outperforming existing autoregressive models and approaching the performance of diffusion models.\n2. The paper introduces a new metric for evaluating the generalization capabilities of molecular generation models, which measures the valid completion rate of fragments of unseen molecules from the test set.\n3. Symphony's inference speed is approximately 3\u00d7 faster than diffusion models, making it a more efficient option for molecular generation.\n\nWeaknesses:\n1. The discretization of radial components in Symphony's model affects its accuracy in bond length distributions, particularly when using lookup tables to assign bond orders.\n2. The increased computational cost of Symphony due to the use of tensor products to create higher-degree E(3)-equivariant features may be a limitation for larger molecules.\n3. The paper mentions that Symphony's performance on the valid completion rate task decreases as training progresses, which needs further investigation.\n\nSuggestions for Improvement:\n1. Explore normalizing flows to smoothly model the radial distribution without discretization, which may improve Symphony's accuracy in bond length distributions.\n2. Investigate methods to reduce the computational cost of Symphony, such as using more efficient tensor products or parallelizing the computation of higher-degree E(3)-equivariant features.\n3. Further analysis of Symphony's performance on the valid completion rate task is needed to understand the relationship between training progression and performance.\n\nRecommendation:\nThe paper presents a promising approach to autoregressively generating 3D molecular geometries using spherical harmonic projections and higher-degree E(3)-equivariant features. While there are some limitations and areas for improvement, Symphony's performance and efficiency make it a valuable contribution to the field of molecular generation.\n\n$$$",
        "gr_score_with": 0.6677524429967426,
        "gr_score_without": 0.5941558441558441,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 183,
            "green_fraction": 0.5941558441558441,
            "z_score": -0.20919095144298339,
            "p_value": 0.5828504120665974
        },
        "output_with": {
            "num_tokens_scored": 307,
            "num_green_tokens": 205,
            "green_fraction": 0.6677524429967426,
            "z_score": 2.4233584971089464,
            "p_value": 0.007688871430007269
        }
    },
    {
        "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
        "abstract": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel priorinformed state-switching GLMs, called Gaussian HMM-GLM (Gaussian prior) and one-hot HMM-GLM (Gumbel-Softmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data. The code is available at https://github.com/JerrySoybean/onehot-hmmglm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for estimating time-varying functional connectivity in multi-state neural systems, called One-Hot Generalized Linear Model for Switching Brain State Discovery. The model, referred to as One-Hot HMM-GLM (OHG), decomposes the traditional weight matrix in GLMs into a discrete adjacency matrix representing the connection type and a positive-valued strength matrix. The authors argue that the regulated adjacency matrices with their shared prior should inform us about the underlying anatomical connectome, while the less restricted strength matrices can provide flexibility to capture functional variations across multiple brain states. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance in terms of log-likelihood, state accuracy, weight error, adjacency accuracy, and adjacency prior accuracy.\n\nStrengths:\n\n1. The paper proposes a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems.\n2. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance.\n3. The authors provide a clear and concise explanation of the model and its components.\n4. The paper includes a detailed experimental evaluation, including quantitative results and visualizations of the learned models.\n\nWeaknesses:\n\n1. The paper assumes that the underlying anatomical connectome is static, which may not be the case in reality.\n2. The model requires a large number of hyperparameters to be pre-defined, which can be challenging in practice.\n3. The paper does not provide a clear explanation of how the Gumbel-Softmax prior is used to enforce the adjacency matrices to learn shared information.\n4. The experimental evaluation is limited to two real neural datasets, and it would be beneficial to include more datasets to demonstrate the robustness of the model.\n\nSuggestions for Improvement:\n\n1. Consider incorporating a more dynamic model of the underlying anatomical connectome to better capture the complexity of neural systems.\n2. Provide a more detailed explanation of the Gumbel-Softmax prior and its role in enforcing shared information among adjacency matrices.\n3. Include more real neural datasets in the experimental evaluation to demonstrate the robustness of the model.\n4. Consider using more advanced techniques, such as Bayesian inference or deep learning, to improve the accuracy and robustness of the model.\n\nRecommendation:\n\nThe paper presents a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems. While there are some limitations and areas for improvement, the results demonstrate the potential of the model to improve our understanding of neural systems. I recommend this paper for publication in a reputable journal, with the hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel one-hot generalized linear model (OHG) for switching brain state discovery. The authors aim to improve upon the existing state-switching generalized linear models (GLMs) with hidden Markov models (HMMs) by introducing a biologically plausible constraint on the weight matrices. The OHG decomposes the traditional weight matrix into a discrete adjacency matrix representing connection type and a positive-valued strength matrix. A Gumbel-Softmax prior is placed on the adjacency matrix, enforcing it to learn shared information across states. The authors argue that the regulated adjacency matrices should inform us about the underlying anatomical connectome, while the less restricted strength matrices capture functional variations across multiple brain states.\n\nStrengths:\n\n* The proposed OHG model is biologically plausible, as it takes into account the anatomical connectome and its influence on functional connectivity.\n* The use of a Gumbel-Softmax prior on the adjacency matrix allows for a more interpretable and sparse representation of the weight matrix.\n* The experiments demonstrate the effectiveness of OHG in recovering true connectivities for simulated data and achieving the best predictive likelihood on test spike trains for real neural datasets.\n* The uncovered connectivities and hidden state sequence from OHG are more interpretable for the real neural datasets.\n\nWeaknesses:\n\n* The paper assumes a known number of hidden states, which may not be the case in real-world scenarios.\n* The choice of hyperparameters, such as the temperature \u03c4, may not be robust and may require further investigation.\n* The paper does not provide a comprehensive comparison with other state-of-the-art methods, and the evaluation metrics may not be sufficient to fully assess the performance of OHG.\n* The experiments are limited to two real neural datasets, and it is unclear whether OHG will perform well on other datasets with different characteristics.\n\nSuggestions for Improvement:\n\n* Investigate the robustness of the OHG model to the choice of hyperparameters, particularly the temperature \u03c4.\n* Compare OHG with other state-of-the-art methods, including those that do not assume a known number of hidden states.\n* Evaluate the performance of OHG on a more comprehensive set of datasets, including those with varying characteristics.\n* Provide a more detailed analysis of the anatomical connectome and its influence on the inferred weight matrices.\n\nRecommendation:\n\nThis paper presents a novel and biologically plausible model for switching brain state discovery. While there are some limitations and areas for improvement, the proposed OHG model shows promising results in recovering true connectivities and achieving the best predictive likelihood on test spike trains. I recommend acceptance, with the understanding that the authors will address the limitations and suggestions for improvement.\n\n$$$",
        "gr_score_with": 0.7665847665847666,
        "gr_score_without": 0.6616541353383458,
        "output_without": {
            "num_tokens_scored": 399,
            "num_green_tokens": 264,
            "green_fraction": 0.6616541353383458,
            "z_score": 2.5140571370782534,
            "p_value": 0.005967554789697645
        },
        "output_with": {
            "num_tokens_scored": 407,
            "num_green_tokens": 312,
            "green_fraction": 0.7665847665847666,
            "z_score": 6.86022275740672,
            "p_value": 3.437662666000413e-12
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper presents DrM, a novel approach to visual reinforcement learning that addresses the issue of agents becoming motorically inactive during the early stages of training. DrM introduces three simple yet effective mechanisms to balance exploration and exploitation: (1) periodic neural network weight perturbation, (2) dormant-ratio-based exploration scheduler, and (3) dormant-ratio-guided exploitation. Empirical studies on three visual continuous control benchmarks (DeepMind Control Suite, MetaWorld, and Adroit) demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance compared to existing model-free visual RL algorithms. DrM is the first documented model-free algorithm that reliably solves complex tasks such as Dog and Manipulator in the DeepMind Control Suite and demonstration-free Adroit dexterous hand manipulation tasks from pixels.\n\nStrengths:\n\n1. DrM addresses a crucial issue in visual RL: agents becoming motorically inactive during early training, which limits their ability to explore effectively.\n2. The three mechanisms of DrM are simple yet effective and easy to implement.\n3. Empirical studies demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance compared to existing model-free visual RL algorithms.\n4. DrM is the first documented model-free algorithm that reliably solves complex tasks such as Dog and Manipulator in the DeepMind Control Suite and demonstration-free Adroit dexterous hand manipulation tasks from pixels.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with visual RL and the challenges associated with training agents from raw visual input.\n2. The empirical studies are primarily focused on three visual continuous control benchmarks, and it would be beneficial to explore the performance of DrM on other tasks and environments.\n3. While the three mechanisms of DrM are simple, they may require fine-tuning of hyperparameters, which can be time-consuming and challenging.\n4. The paper does not provide a detailed analysis of the dormant ratio and how it affects the agent's behavior and performance.\n\nSuggestions for Improvement:\n\n1. Provide more detailed analysis of the dormant ratio and how it affects the agent's behavior and performance.\n2. Explore the performance of DrM on other tasks and environments beyond the three visual continuous control benchmarks.\n3. Investigate the effect of fine-tuning hyperparameters on the performance of DrM.\n4. Consider incorporating other exploration strategies, such as uncertainty-oriented or intrinsic motivation-oriented exploration, to further enhance the performance of DrM.\n\nRecommendation:\n\nThe paper presents a novel approach to visual reinforcement learning that addresses a crucial issue in the field. The empirical studies demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance compared to existing model-free visual RL algorithms. While there are some limitations and areas for further research, the paper provides a valuable contribution to the field of visual RL. Therefore, I recommend accepting the paper for publication in the conference proceedings.",
        "gr_score_with": 0.6830601092896175,
        "gr_score_without": 0.5522788203753352,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 206,
            "green_fraction": 0.5522788203753352,
            "z_score": -1.881120181362766,
            "p_value": 0.9700222138554369
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 250,
            "green_fraction": 0.6830601092896175,
            "z_score": 3.243777322949292,
            "p_value": 0.0005897796088279961
        }
    },
    {
        "title": "ING VIA TASK-DRIVEN FEATURE SELECTION",
        "abstract": "This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a userspecified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most informative channel-subset whilst simultaneously training a network to execute the task given the subset. Experiments demonstrate the potential of TADRED in diverse imaging applications: several clinicallyrelevant tasks in magnetic resonance imaging; and remote sensing and physiological applications of hyperspectral imaging. Results show substantial improvement over classical experimental design, two recent application-specific methods within the new paradigm, and state-of-the-art approaches in supervised feature selection. We anticipate further applications of our approach. Code is available: Code Link.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to experimental design in imaging applications, specifically for multi-channel imaging. The proposed method, TADRED, is a task-driven experimental design paradigm that simultaneously optimizes the design and trains a machine-learning model to execute a user-specified image-analysis task. TADRED uses a dual-network architecture, consisting of a scoring network and a task network, to identify the most informative channel-subset and train a high-performing network for the task. The paper demonstrates the effectiveness of TADRED in various imaging applications, including clinically relevant tasks in magnetic resonance imaging and remote sensing and physiological applications of hyperspectral imaging.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to experimental design in imaging applications, which is a significant contribution to the field.\n2. The proposed method, TADRED, is well-motivated and has a clear and logical structure.\n3. The paper provides a thorough evaluation of TADRED using various benchmark datasets and tasks, demonstrating its effectiveness in different imaging applications.\n4. The authors provide a comprehensive analysis of the strengths and limitations of TADRED, as well as comparisons with other state-of-the-art methods.\n\nWeaknesses:\n\n1. The paper assumes a priori knowledge of the task, which may not always be the case in real-world applications.\n2. The proposed method requires a large amount of training data, which may not be feasible in all scenarios.\n3. The paper does not provide a detailed explanation of the hyperparameter tuning process, which is crucial for the success of the method.\n4. The comparison with other methods is limited to a few benchmark datasets, and it would be beneficial to evaluate TADRED on a broader range of datasets and tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the possibility of learning the task from the data, rather than relying on a priori knowledge.\n2. Explore alternative methods for hyperparameter tuning, such as Bayesian optimization or evolutionary algorithms.\n3. Provide a more detailed explanation of the hyperparameter tuning process and its impact on the performance of TADRED.\n4. Evaluate TADRED on a broader range of datasets and tasks to demonstrate its robustness and generalizability.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to experimental design in imaging applications, which has the potential to significantly impact the field. However, there are some limitations and areas for improvement that need to be addressed. With further refinement and evaluation, TADRED has the potential to become a widely used and effective method for experimental design in imaging applications.\n\n$$$\nNote: The review is based on the provided paper and does not include any external knowledge or information. The review is intended to provide a critical evaluation of the paper, highlighting its strengths and weaknesses, and suggesting areas for improvement.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents TADRED, a novel method for task-driven experimental design in imaging applications. TADRED is a feature selection algorithm that enables a new subsampling paradigm for experimental design, particularly in multi-channel imaging applications. The approach couples feature scoring and task execution in consecutive networks, allowing for efficient identification of subsets of complementarily informative channels while training a high-performing network for the task.\n\nStrengths:\n\n* TADRED outperforms classical experimental design and recent supervised feature selection approaches in several imaging applications, including model parameter estimation,'model-free' experimental design, and remote sensing.\n* The approach is demonstrated on six datasets/tasks in both clinically-relevant MRI and remote sensing and physiological applications in hyperspectral imaging.\n* TADRED is robust to randomness in deep learning training and performs favorably compared to alternative approaches.\n\nWeaknesses:\n\n* The approach requires extensive hyperparameter search for each approach, including TADRED, for different experimental settings and subsample values C.\n* The computational cost of TADRED is similar to the two state-of-the-art supervised feature selection baselines, SSEFS and FIRDL, although the random supervised feature selection baseline is more computationally economical.\n\nSuggestions for Improvement:\n\n* Further research is needed to explore alternative strategies for TADRED's iterative subsampling procedure, as it decreases the upper bound on performance.\n* The approach could be extended to other imaging problems, such as autofocus for specialized equipment and potentially beyond imaging to studies of cell populations.\n\nRecommendation:\n\nTADRED is a novel and effective method for task-driven experimental design in imaging applications, with significant potential for improving the accuracy and efficiency of imaging experiments. However, further research is needed to improve the approach and extend its applications.\n\n$$$",
        "gr_score_with": 0.726027397260274,
        "gr_score_without": 0.6528117359413202,
        "output_without": {
            "num_tokens_scored": 409,
            "num_green_tokens": 267,
            "green_fraction": 0.6528117359413202,
            "z_score": 2.180339378523189,
            "p_value": 0.014616157072356247
        },
        "output_with": {
            "num_tokens_scored": 292,
            "num_green_tokens": 212,
            "green_fraction": 0.726027397260274,
            "z_score": 4.396084648022542,
            "p_value": 5.511048269510976e-06
        }
    },
    {
        "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
        "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which admits separate control over the camera and the content of the scene. The model is trained using a synthetic dataset, DySO, and real-world videos, and it learns to disentangle camera pose and scene dynamics from monocular real-world videos. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\nStrengths:\n\n1. The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task.\n2. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n3. The paper presents a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n4. The authors propose a novel synthetic dataset, DySO, which is used for co-training DyST.\n\nWeaknesses:\n\n1. The paper assumes that the scene dynamics are known, which may not be the case in real-world scenarios.\n2. The authors use a simple set of camera and object motions in the synthetic dataset, which may not be sufficient to capture the complexity of real-world scenes.\n3. The paper does not provide a detailed analysis of the model's performance on more complex types of videos, such as videos with several independent moving objects or changing lighting conditions.\n4. The authors use a L2 loss for training the model, which may not be suitable for all types of scenes.\n\nSuggestions for Improvement:\n\n1. The authors could explore more complex types of videos and analyze the model's performance on these scenarios.\n2. The paper could benefit from a more detailed analysis of the model's performance on videos with changing lighting conditions.\n3. The authors could explore the use of more advanced loss functions, such as a adversarial loss, to improve the model's performance.\n4. The paper could benefit from a more detailed analysis of the model's learned latent representations and their relationship to the scene dynamics and camera pose.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes. While there are some limitations to the paper, it is a significant contribution to the field and has the potential to be improved upon. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces DyST, a novel approach to generative modeling of dynamic 3D visual scenes that allows for separate control over the camera and the content of the scene. The model learns a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose, and achieves this through a novel co-training scheme on monocular videos and the synthetic dataset DySO. The paper demonstrates the capabilities of DyST on real-world videos of dynamic scenes, including view synthesis and scene control.\n\nStrengths:\n\n* The paper addresses an important and challenging problem in computer vision, namely the representation of dynamic 3D visual scenes.\n* The proposed approach is innovative and well-motivated, and the use of a synthetic dataset for training and evaluation is a good approach.\n* The paper provides a clear and concise description of the model and its training procedure.\n* The experimental results are thorough and demonstrate the capabilities of the model.\n\nWeaknesses:\n\n* The paper assumes that the camera pose is not known, which may not be the case in real-world applications.\n* The model is trained on a synthetic dataset, which may not be representative of real-world data.\n* The paper does not provide a detailed comparison with other state-of-the-art methods.\n* The model is not evaluated on more complex types of videos, such as those with multiple moving objects or changing lighting conditions.\n\nSuggestions for Improvement:\n\n* The paper could benefit from a more detailed comparison with other state-of-the-art methods.\n* The model could be evaluated on more complex types of videos, such as those with multiple moving objects or changing lighting conditions.\n* The paper could provide more information on the synthetic dataset used for training and evaluation.\n* The model could be extended to handle more challenging cases, such as those with partial occlusions or complex camera motions.\n\nRecommendation:\n\nOverall, the paper provides an innovative and well-motivated approach to generative modeling of dynamic 3D visual scenes, and the experimental results are thorough and demonstrate the capabilities of the model. However, the paper could benefit from a more detailed comparison with other state-of-the-art methods and evaluation on more complex types of videos.",
        "gr_score_with": 0.7340425531914894,
        "gr_score_without": 0.6517241379310345,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 189,
            "green_fraction": 0.6517241379310345,
            "z_score": 1.7981464930323297,
            "p_value": 0.036076898078233034
        },
        "output_with": {
            "num_tokens_scored": 282,
            "num_green_tokens": 207,
            "green_fraction": 0.7340425531914894,
            "z_score": 4.59489863459083,
            "p_value": 2.1647984230057788e-06
        }
    },
    {
        "title": "LEARNING MULTI-FACETED PROTOTYPICAL USER INTERESTS",
        "abstract": "We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the granularity of item space to that of user interests, which we postulate is not ideal as user interests would likely map to a small subset of item space. In addition, the compositionality of user interests has received inadequate attention, preventing the modeling of interactions between explanatory factors driving a user\u2019s decision. To resolve this, we propose to align user interests with multi-faceted item characteristics. First, we involve prototype-based representation learning to discover item characteristics along multiple facets. Second, we compose user interests from uncovered item characteristics via binding mechanism, separating the granularity of user preferences from that of item space. Third, we design a dedicated bi-directional binding block, aiding the derivation of compositional user interests. On real-world datasets, the experimental results demonstrate the strong performance of our proposed method compared to a series of baselines.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to address the shortcomings of existing VAE-based disentangled recommendation models, including inadequate item space discovering, same level of granularity between user interests and item space assumption, and improper user interest complexity handling. FACETVAE is characterized by three main innovations: 1) disentangling item space under a multi-faceted manner, 2) binding compositional user interests from low-level ones discovered from item space, and 3) effectively binding user interests via a bi-directional binding block. The method is evaluated on three real-world datasets, and the results show that FACETVAE achieves significantly higher accuracy than existing baselines.\n\nStrengths:\n\n* The paper provides a clear and concise introduction to the problem of learning multi-faceted prototypical user interests.\n* The proposed method, FACETVAE, is well-motivated and addresses the shortcomings of existing VAE-based disentangled recommendation models.\n* The experimental results demonstrate the effectiveness of FACETVAE in achieving higher accuracy than existing baselines.\n* The paper provides a comprehensive analysis of the results, including the efficiency of FACETVAE, multi-faceted item space disentangling, and the interpretability of user's interests.\n\nWeaknesses:\n\n* The paper assumes a uniform distribution of facets underlying item space, which may not be realistic in all cases.\n* The method requires tuning several hyperparameters, including the number of facets and the number of clusters per facet, which may be challenging in practice.\n* The paper does not provide a clear explanation of how the binding block works and how it affects the recommendation accuracy.\n* The results are not always consistent across different datasets and metrics, which may indicate that the method is sensitive to the specific characteristics of the data.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how the binding block works and how it affects the recommendation accuracy.\n* The authors should investigate alternative methods for tuning the hyperparameters, such as using a Bayesian optimization or a gradient-based optimization.\n* The authors should provide more results on the interpretability of user's interests, including a qualitative analysis of the interests produced by FACETVAE.\n* The authors should investigate the robustness of FACETVAE to noisy and missing data, which is a common issue in recommendation systems.\n\nRecommendation:\n\nThe paper proposes a novel and effective method for learning multi-faceted prototypical user interests. While there are some limitations and areas for improvement, the method shows promising results and has the potential to be a useful tool for recommendation systems. Therefore, I recommend accepting the paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel recommendation model, FACETVAE, which addresses the limitations of existing Variational AutoEncoder (VAE)-based disentangled recommendation models. FACETVAE disentangles item space into multiple facets and composes user interests from these facets using a bi-directional binding block. The model learns to represent user interests as a set of compositional factors, which can capture the diversity and complexity of user preferences. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines on three real-world datasets.\n\nStrengths:\n\n1. FACETVAE addresses the limitations of existing VAE-based disentangled recommendation models by disentangling item space into multiple facets and composing user interests from these facets.\n2. The model learns to represent user interests as a set of compositional factors, which can capture the diversity and complexity of user preferences.\n3. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines on three real-world datasets.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known in advance, which may not be the case in real-world scenarios.\n2. The model requires a large number of hyperparameters to be tuned, which can be time-consuming and challenging.\n3. The paper does not provide a clear explanation of how the bi-directional binding block works and how it contributes to the overall performance of the model.\n\nSuggestions for Improvement:\n\n1. Investigate methods to automatically determine the number of facets and the number of clusters per facet in a data-driven manner.\n2. Provide a more detailed explanation of the bi-directional binding block and its contribution to the overall performance of the model.\n3. Evaluate the performance of FACETVAE on more diverse and challenging datasets to demonstrate its robustness and generalizability.\n\nRecommendation:\n\nThe paper proposes a novel and promising approach to recommendation systems by disentangling item space into multiple facets and composing user interests from these facets. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines. However, there are some limitations and challenges that need to be addressed in future work. Overall, the paper is well-written and easy to follow, and it makes a significant contribution to the field of recommendation systems.\n\n$$$",
        "gr_score_with": 0.6769759450171822,
        "gr_score_without": 0.6076923076923076,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 237,
            "green_fraction": 0.6076923076923076,
            "z_score": 0.3102748136938235,
            "p_value": 0.3781759912210601
        },
        "output_with": {
            "num_tokens_scored": 291,
            "num_green_tokens": 197,
            "green_fraction": 0.6769759450171822,
            "z_score": 2.6805347981690524,
            "p_value": 0.003675231080467435
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of its effectiveness. However, the paper assumes that the input images are already preprocessed, and does not provide a detailed analysis of the computational cost of the Win-Win approach. To improve the paper, the authors should provide a more detailed analysis of the computational cost, and provide a more thorough evaluation of the Win-Win approach on other tasks and datasets. With these improvements, the paper would be a significant contribution to the field of computer vision.\n\nOverall, I would recommend this paper for publication in a top-tier conference, with the suggested improvements.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel strategy, called Win-Win, for efficient training and inference of high-resolution vision transformers. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors demonstrate the effectiveness of Win-Win on two dense prediction tasks: semantic segmentation and optical flow estimation. They show that Win-Win achieves state-of-the-art performance on the Spring benchmark and outperforms other approaches in terms of inference time.\n\nStrengths:\n1. The paper presents a novel and efficient approach for training high-resolution vision transformers.\n2. The authors demonstrate the effectiveness of Win-Win on two challenging tasks: semantic segmentation and optical flow estimation.\n3. The paper provides a clear and concise description of the Win-Win strategy and its implementation.\n4. The authors compare Win-Win with other state-of-the-art approaches and demonstrate its superiority in terms of inference time.\n5. The paper includes extensive experimental results and visualizations to support the claims made in the paper.\n\nWeaknesses:\n1. The paper assumes that the input image is divisible into rectangular windows, which may not be the case in all scenarios.\n2. The authors do not provide a detailed analysis of the trade-off between the number of windows and the training time.\n3. The paper assumes that the attention mechanism is the main bottleneck in high-resolution vision transformers, which may not be the case in all scenarios.\n4. The authors do not provide a detailed comparison with other approaches that use hierarchical architectures or attention sparsification.\n5. The paper assumes that the Win-Win strategy is applicable to all vision transformers, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the trade-off between the number of windows and the training time.\n2. Investigate the applicability of the Win-Win strategy to other types of vision transformers, such as hierarchical architectures or attention sparsification.\n3. Provide a more detailed comparison with other approaches that use hierarchical architectures or attention sparsification.\n4. Investigate the robustness of the Win-Win strategy to different types of input images, such as images with irregular shapes or images with variable resolutions.\n5. Provide more visualizations and experimental results to support the claims made in the paper.\n\nRecommendation:\nThe paper presents a novel and efficient approach for training high-resolution vision transformers. The authors demonstrate the effectiveness of Win-Win on two challenging tasks: semantic segmentation and optical flow estimation. While there are some limitations and areas for improvement, the paper is well-written and well-structured, and the results are convincing. Therefore, I recommend accepting the paper for publication.\n$$$",
        "gr_score_with": 0.7385057471264368,
        "gr_score_without": 0.6412429378531074,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 227,
            "green_fraction": 0.6412429378531074,
            "z_score": 1.5841416633427037,
            "p_value": 0.05658074254762852
        },
        "output_with": {
            "num_tokens_scored": 348,
            "num_green_tokens": 257,
            "green_fraction": 0.7385057471264368,
            "z_score": 5.27430957769713,
            "p_value": 6.662839994666293e-08
        }
    },
    {
        "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
        "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL. FedCOG consists of two key components: complementary data generation and knowledge-distillation-based model training. The complementary data generation module generates data from the shared global model to complement the original dataset, reducing the heterogeneity level. The knowledge-distillation-based model training module distills knowledge from the global model to the local model based on the generated data, mitigating the effects of heterogeneity. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy. The authors conduct extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n1. The paper addresses a critical issue in FL, data heterogeneity, from a novel perspective of modifying local dataset.\n2. FedCOG is a well-designed algorithm that consists of two key components, complementary data generation and knowledge-distillation-based model training, which are both orthogonal to most existing FL methods.\n3. The authors provide extensive experimental results on classical and real-world FL datasets, demonstrating the effectiveness of FedCOG.\n4. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy.\n\nWeaknesses:\n\n1. The paper does not provide a thorough analysis of the theoretical guarantees of FedCOG.\n2. The authors do not provide a detailed comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The paper assumes a centralized setting, where the global model is shared among all clients, which may not be realistic in many FL scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the theoretical guarantees of FedCOG, including convergence rates and optimality guarantees.\n2. The authors should conduct a more thorough comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The authors should consider a decentralized setting, where the global model is not shared among all clients, and evaluate the performance of FedCOG in such a setting.\n\nRecommendation:\n\nThe paper is well-written, and the proposed algorithm, FedCOG, is innovative and effective. However, the paper lacks a thorough theoretical analysis and a detailed comparison with other state-of-the-art methods. With some improvements, the paper can be considered for publication in a top-tier conference.",
        "peer_review_with_watermark": "$$\n\\textbf{Paper Summary:}\nThe paper proposes a novel approach to tackle the issue of data heterogeneity in Federated Learning (FL) by modifying the local dataset through data generation and knowledge distillation. The proposed method, FedCOG, generates data to complement the original dataset and distills knowledge from the global model to the local model through the generated data. FedCOG is shown to consistently outperform state-of-the-art methods on classical and real-world FL datasets.\n\n\\textbf{Strengths:}\n1. The paper proposes a novel approach to tackle data heterogeneity in FL, which is a significant problem in the field.\n2. The proposed method, FedCOG, is shown to be effective in reducing the heterogeneity level and improving the performance of FL.\n3. FedCOG is plug-and-play, meaning it can be combined with existing FL methods to further enhance their performance.\n4. The paper provides extensive experiments on classical and real-world FL datasets, which demonstrate the effectiveness of FedCOG.\n\n\\textbf{Weaknesses:}\n1. The paper assumes that the global model is available, which may not be the case in some FL scenarios.\n2. The proposed method requires additional computational resources to generate data and train the local models.\n3. The paper does not provide a detailed analysis of the impact of the hyperparameters on the performance of FedCOG.\n4. The paper assumes that the data distribution is Dirichlet, which may not be the case in all FL scenarios.\n\n\\textbf{Suggestions for Improvement:}\n1. The authors should provide a more detailed analysis of the impact of the hyperparameters on the performance of FedCOG.\n2. The authors should provide more information about the computational resources required to implement FedCOG.\n3. The authors should provide more details about the scenarios where the global model is not available.\n4. The authors should provide more information about the data distribution and how FedCOG handles different data distributions.\n\n\\textbf{Recommendation:}\nThis paper proposes a novel approach to tackle data heterogeneity in FL, which is a significant problem in the field. The proposed method, FedCOG, is shown to be effective in reducing the heterogeneity level and improving the performance of FL. However, there are some limitations and areas for improvement. With some modifications and additional analysis, this paper can be considered for publication in a top-tier conference.\n\n$$",
        "gr_score_with": 0.7406143344709898,
        "gr_score_without": 0.6276923076923077,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 204,
            "green_fraction": 0.6276923076923077,
            "z_score": 1.0192194951162106,
            "p_value": 0.15404938669367424
        },
        "output_with": {
            "num_tokens_scored": 293,
            "num_green_tokens": 217,
            "green_fraction": 0.7406143344709898,
            "z_score": 4.913278555702823,
            "p_value": 4.4782941544583055e-07
        }
    },
    {
        "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
        "abstract": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework, GIM, for learning generalizable image matching models using self-training on diverse internet videos. GIM first trains a model on standard domain-specific datasets and then uses the trained model to generate dense labels on nearby frames of downloaded videos. The final model is trained on propagated data with strong augmentations. The authors also introduce a new zero-shot evaluation benchmark, ZEB, consisting of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks.\n\nStrengths:\n1. The paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos.\n2. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments.\n3. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks.\n4. The authors provide a detailed analysis of the GIM framework and its components, including the self-training process, label propagation, and strong data augmentation.\n\nWeaknesses:\n1. The paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios.\n2. The authors do not provide a thorough analysis of the limitations and potential failures of the GIM framework.\n3. The paper does not discuss the potential issues with data quality and consistency across different internet videos.\n4. The authors do not provide a clear comparison of the computational efficiency of GIM with other state-of-the-art image matching methods.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the limitations and potential failures of the GIM framework.\n2. Discuss the potential issues with data quality and consistency across different internet videos.\n3. Compare the computational efficiency of GIM with other state-of-the-art image matching methods.\n4. Provide more information on the hyperparameters and training settings used for the GIM framework.\n\nRecommendation:\nThe paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks. However, the paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios. Therefore, I recommend that the authors provide a more detailed analysis of the limitations and potential failures of the GIM framework, discuss the potential issues with data quality and consistency across different internet videos, and compare the computational efficiency of GIM with other state-of-the-art image matching methods.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper presents GIM, a self-training framework for learning a single generalizable image matcher from any image matching architecture using internet videos as a diverse and large-scale data source. GIM first trains a model on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of downloaded videos. These labels are filtered by robust fitting and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. The paper also introduces ZEB, the first zero-shot evaluation benchmark for image matching, which consists of data from 8 real-world and 4 simulated domains.\n\nStrengths:\n1. GIM presents a novel approach to learning generalizable image matching models from internet videos, addressing the issue of limited scalability of existing data construction pipelines.\n2. The paper proposes ZEB, a novel zero-shot evaluation benchmark for image matching, which thoroughly assesses the cross-domain generalization performance of different methods.\n3. Experiments demonstrate the effectiveness and generality of GIM, consistently improving the zero-shot performance of 3 state-of-the-art image matching architectures as the number of downloaded videos increases.\n4. GIM enables generalization to extreme cross-domain data, such as Bird Eye View (BEV) images of projected 3D point clouds.\n\nWeaknesses:\n1. The paper assumes that internet videos are readily available and accessible, which may not be the case in all scenarios.\n2. The performance improvement of GIM may saturate with increasing video data size, as shown in Fig. 1.\n3. The paper does not provide a thorough analysis of the computational cost and time required for generating video training data.\n\nSuggestions for Improvement:\n1. Investigate the use of other video sources, such as surveillance footage or social media platforms, to expand the diversity of video data.\n2. Analyze the performance of GIM on datasets with varying levels of image quality, such as those with high noise or blur.\n3. Provide more details on the computational cost and time required for generating video training data, as well as the scalability of GIM to larger video datasets.\n\nRecommendation:\nThis paper presents a novel approach to learning generalizable image matching models from internet videos, addressing a significant issue in the field of computer vision. The introduction of ZEB, a novel zero-shot evaluation benchmark, further contributes to the advancement of image matching research. I recommend this paper for publication in a top-tier conference in computer vision.\n\nFinal rating: 8.5/10\n$$$",
        "gr_score_with": 0.6134663341645885,
        "gr_score_without": 0.5268456375838926,
        "output_without": {
            "num_tokens_scored": 298,
            "num_green_tokens": 157,
            "green_fraction": 0.5268456375838926,
            "z_score": -2.577591676331837,
            "p_value": 0.9950254256898504
        },
        "output_with": {
            "num_tokens_scored": 401,
            "num_green_tokens": 246,
            "green_fraction": 0.6134663341645885,
            "z_score": 0.5506377118812746,
            "p_value": 0.2909410257812026
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method employs a single pair of frozen random matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings, making it suitable for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n\nStrengths:\n\n1. VeRA significantly reduces the number of trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable results to LoRA on various benchmarks, including GLUE, E2E, and image classification tasks.\n3. VeRA is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The paper provides a thorough analysis of the method's components, including the impact of different initializations and the magnitude of adaptation.\n\nWeaknesses:\n\n1. The paper assumes that the frozen random matrices can be regenerated from a random number generator (RNG) seed, which may not be feasible in all scenarios.\n2. The method's performance may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques.\n3. The paper focuses on language and vision models with Transformer architecture, and its applicability to other architectures and domains remains an area for future research.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of regenerating frozen random matrices from an RNG seed in various scenarios.\n2. Explore additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques, to improve the method's performance.\n3. Conduct experiments on other architectures and domains to evaluate VeRA's applicability and performance.\n\nRecommendation:\n\nVeRA is a promising parameter-efficient finetuning method that achieves comparable results to LoRA while reducing the number of trainable parameters. The method's memory savings make it suitable for scenarios that require frequent swapping of numerous finetuned models. However, further research is needed to explore the method's limitations and potential areas for improvement. I recommend accepting this paper for publication in a reputable conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel parameter-efficient finetuning method called Vector-based Random Matrix Adaptation (VeRA) for large language models. VeRA builds upon the low-rank adaptation (LoRA) method by introducing a reparameterization of the low-rank matrices using a single pair of frozen random matrices shared across all adapted layers and trainable scaling vectors. This approach significantly reduces the number of trainable parameters while maintaining comparable results to LoRA. The authors demonstrate the effectiveness of VeRA on the GLUE and E2E benchmarks, image classification tasks, and instruction-tuning of large language models.\n\nStrengths:\n\n1. VeRA offers a significant reduction in trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable or better results on various downstream tasks, including GLUE, E2E, and image classification.\n3. VeRA is particularly well-suited for scenarios requiring frequent swapping of numerous finetuned models, such as cloud-based AI services.\n4. The use of frozen random matrices and trainable scaling vectors provides a flexible and efficient way to adapt large language models.\n\nWeaknesses:\n\n1. The paper assumes a fixed rank for the frozen random matrices, which may limit the flexibility of the method.\n2. The choice of initialization for the scaling vectors and the frozen random matrices may impact the performance of the method.\n3. The study focuses primarily on language and vision models with Transformer architecture, and the applicability of the method to other architectures and domains is not thoroughly explored.\n4. The performance of the method may benefit from additional refinements, such as dynamic parameter budget allocation or different initialization and regularization techniques.\n\nSuggestions for Improvement:\n\n1. Explore the use of adaptive ranks for the frozen random matrices to improve the flexibility of the method.\n2. Conduct a thorough investigation of the impact of initialization strategies on the performance of the method.\n3. Extend the study to other architectures and domains to demonstrate the generalizability of the method.\n4. Consider incorporating dynamic parameter budget allocation or other advanced techniques to further improve the performance of the method.\n\nRecommendation:\n\nThe paper presents a novel and effective parameter-efficient finetuning method for large language models. While there are some limitations and areas for improvement, the results demonstrate the potential of VeRA for reducing the number of trainable parameters while maintaining comparable results to LoRA. I recommend accepting the paper for publication, with the understanding that the authors address the suggestions for improvement and extend the study to other architectures and domains.\n\n$$$",
        "gr_score_with": 0.6722222222222223,
        "gr_score_without": 0.5710227272727273,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 201,
            "green_fraction": 0.5710227272727273,
            "z_score": -1.1095637347150096,
            "p_value": 0.8664064670809999
        },
        "output_with": {
            "num_tokens_scored": 360,
            "num_green_tokens": 242,
            "green_fraction": 0.6722222222222223,
            "z_score": 2.797330370156714,
            "p_value": 0.002576340806054713
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors examine two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis, and (2) bounds that may also depend on the learning algorithm. They prove mathematically that no generalization bound of the first type can be uniformly tight in the overparameterized setting, and establish a trade-off between learning performance and estimability for the second type of bounds.\n\nStrengths:\n\n* The paper provides a rigorous and formal analysis of the notion of uniformly tight generalization bounds.\n* The authors introduce a framework for understanding the limitations of generalization bounds in the overparameterized setting.\n* The paper presents several theorems and lemmas that provide a deeper understanding of the relationships between learning performance, estimability, and generalization bounds.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and theoretical computer science.\n* The authors do not provide a comprehensive review of existing generalization bounds and their limitations, which may make it difficult for readers to understand the significance of their results.\n* The paper's conclusions and implications may be subject to interpretation, and the authors acknowledge that different scholarly positions exist on this matter.\n\nSuggestions for Improvement:\n\n* The authors could provide a more accessible introduction to the mathematical framework and notation used in the paper.\n* The paper could benefit from a more comprehensive review of existing generalization bounds and their limitations, which would provide context for the authors' results.\n* The authors could provide more concrete examples and illustrations to demonstrate the implications of their results and make the paper more engaging for readers.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of machine learning and theoretical computer science by shedding light on the limitations of generalization bounds in the overparameterized setting. The authors' formal analysis and rigorous proofs provide a solid foundation for understanding the relationships between learning performance, estimability, and generalization bounds. However, the paper's assumptions and conclusions may be subject to interpretation, and the authors could benefit from providing a more comprehensive review of existing generalization bounds and their limitations. Overall, the paper is well-written and well-structured, and it is a valuable contribution to the field.\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research:\n\n* Investigate the implications of the authors' results for existing generalization bounds and their limitations.\n* Explore the relationships between learning performance, estimability, and generalization bounds in more detail.\n* Develop new generalization bounds that take into account the limitations and trade-offs identified by the authors.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper studies the concept of generalization bounds in the context of overparameterized neural networks. The authors aim to investigate whether generalization bounds can be uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and population distributions. The paper provides two main contributions: (1) a formal proof that certain types of generalization bounds, such as those that depend solely on the training set, the learned hypothesis, and the hypothesis class, are not uniformly tight in the overparameterized setting; and (2) a trade-off between learning performance and estimability, showing that an algorithm cannot perform well and be certain of it when it does.\n\nStrengths:\n\n1. The paper provides a rigorous formal analysis of the limitations of generalization bounds in the overparameterized setting.\n2. The authors introduce a novel framework for understanding the relationship between estimability and learnability, which is a key contribution to the field.\n3. The paper provides a clear and concise presentation of the main results, making it accessible to a broad audience.\n\nWeaknesses:\n\n1. The paper focuses primarily on the limitations of generalization bounds, which may not provide a complete picture of the complexities involved in generalization.\n2. The paper assumes a specific definition of overparameterization, which may not hold in all cases.\n3. The paper's conclusions are based on a specific set of assumptions, which may not be universally applicable.\n\nSuggestions for Improvement:\n\n1. The paper could benefit from a more detailed discussion of the implications of the trade-off between learning performance and estimability.\n2. The paper could explore alternative definitions of overparameterization to better understand the scope of the results.\n3. The paper could provide more concrete examples or case studies to illustrate the limitations of generalization bounds in practice.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of generalization bounds in overparameterized neural networks. The formal analysis and novel framework introduced in the paper provide a valuable insight into the limitations of generalization bounds. The paper's conclusions are well-supported, and the presentation is clear and concise. However, the paper's focus on limitations and the specific assumptions made may limit its applicability to a broader audience. Overall, the paper is well-written and provides a valuable contribution to the field.\n\nRecommendation for publication: Accept, with minor revisions to address the weaknesses mentioned above.",
        "gr_score_with": 0.6261127596439169,
        "gr_score_without": 0.56353591160221,
        "output_without": {
            "num_tokens_scored": 362,
            "num_green_tokens": 204,
            "green_fraction": 0.56353591160221,
            "z_score": -1.415981102576828,
            "p_value": 0.9216094812980405
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 211,
            "green_fraction": 0.6261127596439169,
            "z_score": 0.9786762561407161,
            "p_value": 0.16386998378842887
        }
    },
    {
        "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
        "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that enables large language model (LLM)-based embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three essential functions: multimodal perception, foundational knowledge base, and skill prediction and planning. The authors construct an extensive dataset of 850K open-world instruction pairs to train Steve-Eye and develop three open-world evaluation benchmarks to validate the model's capability to strategically act and plan.\n\nStrengths:\n\n1. The paper presents a novel approach to multimodal interaction in open-world environments, addressing the limitations of text-only agents.\n2. The proposed model, Steve-Eye, demonstrates improved performance in multimodal perception, foundational knowledge question answering, and skill prediction and planning.\n3. The authors provide a comprehensive evaluation of the model's performance on three open-world benchmarks, including environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of pre-trained LLMs and visual encoders, which may limit the model's ability to generalize to new tasks and environments.\n2. The dataset collection process is labor-intensive and requires significant human annotation effort, which may not be scalable for larger datasets.\n3. The paper does not provide a thorough analysis of the model's limitations and failure cases, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more robust and generalizable pre-trained models that can handle a wide range of tasks and environments.\n2. Explore more efficient and scalable methods for dataset collection, such as active learning or transfer learning.\n3. Conduct a more comprehensive analysis of the model's limitations and failure cases to better understand its robustness and reliability.\n\nRecommendation:\n\nThis paper presents a novel and promising approach to multimodal interaction in open-world environments. While there are some limitations and areas for improvement, the proposed model demonstrates improved performance on three open-world benchmarks. I recommend this paper for publication in a reputable conference, with the understanding that the authors will address the limitations and suggestions for improvement outlined above.\n\nRating: 7.5/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes Steve-Eye, a large multimodal model that combines a pre-trained language model with a visual encoder to enable embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three indispensable functions: multimodal perception, foundational knowledge base, and skill prediction and planning. The paper introduces a semi-automatic strategy to collect an extensive dataset of 850K open-world instruction pairs and develops three open-world evaluation benchmarks to evaluate the model's capability to strategically act and plan.\n\nStrengths:\n1. The paper addresses a significant limitation of existing LLM-based agents, which rely heavily on text input/output and struggle to comprehend the visual richness of open worlds.\n2. The proposed Steve-Eye model integrates a visual encoder with a pre-trained LLM to enable multimodal perception and generate multimodal feedback.\n3. The paper provides a comprehensive evaluation of Steve-Eye on three open-world benchmarks: environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n4. The results demonstrate the effectiveness of Steve-Eye in outperforming existing LLM-based agents and text-only language models.\n\nWeaknesses:\n1. The paper relies heavily on the availability of a pre-trained LLM and a visual encoder, which may not be universally applicable.\n2. The dataset collection process is semi-automatic and relies on ChatGPT to curate questions and generate multimodal description pairs.\n3. The paper does not provide a thorough analysis of the potential biases and limitations of the dataset.\n4. The skill planning results show some degradation when compared to the baselines, indicating that the model still has room for improvement.\n\nSuggestions for Improvement:\n1. Explore the use of other pre-trained models or self-supervised learning methods to reduce the reliance on pre-trained LLMs and visual encoders.\n2. Develop more efficient and scalable methods for collecting and curating multimodal instruction pairs.\n3. Provide a more detailed analysis of the potential biases and limitations of the dataset and explore methods to mitigate these biases.\n4. Investigate the use of transfer learning or few-shot learning to improve the model's performance on new tasks and environments.\n\nRecommendation:\nThe paper proposes a significant improvement over existing LLM-based agents and text-only language models, and the results demonstrate the effectiveness of Steve-Eye on three open-world benchmarks. However, the reliance on pre-trained models and the semi-automatic dataset collection process may limit the model's universal applicability. Further research is needed to explore the potential biases and limitations of the dataset and to develop more efficient and scalable methods for collecting and curating multimodal instruction pairs. With these improvements, Steve-Eye has the potential to become a state-of-the-art model for embodied agents in open worlds.\n$$",
        "gr_score_with": 0.6819338422391857,
        "gr_score_without": 0.5825825825825826,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 194,
            "green_fraction": 0.5825825825825826,
            "z_score": -0.6486086655238339,
            "p_value": 0.7417043235046554
        },
        "output_with": {
            "num_tokens_scored": 393,
            "num_green_tokens": 268,
            "green_fraction": 0.6819338422391857,
            "z_score": 3.315720582249749,
            "p_value": 0.00045703585926396023
        }
    },
    {
        "title": "PERCEPTUAL GROUP TOKENIZER: BUILDING PERCEPTION WITH ITERATIVE GROUPING",
        "abstract": "Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping (Palmer, 2002; Wagemans et al., 2012; Herzog, 2018). Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-training, and interpretability. Specifically, Perceptual Group Tokenizer achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear probe evaluation, establishing a new milestone for this paradigm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT), which entirely relies on perceptual grouping principles to extract visual features and perform self-supervised representation learning. The model consists of a series of grouping operations that iteratively hypothesize the context for pixels or superpixels to refine feature representations. The authors show that PGT achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles.\n2. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks.\n3. The authors provide a thorough analysis of the model's properties, including adaptive computation and high model interpretability.\n4. The paper provides a clear and concise explanation of the model's architecture and training procedure.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision.\n2. The model's performance on other tasks, such as object detection and segmentation, is not evaluated.\n3. The paper does not provide a thorough comparison with other state-of-the-art models, such as ViT and ResNet.\n4. The model's computational cost is relatively high due to the iterative grouping processes.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision.\n2. Evaluate the model's performance on other tasks, such as object detection and segmentation.\n3. Compare the model's performance with other state-of-the-art models, such as ViT and ResNet.\n4. Explore ways to reduce the model's computational cost, such as using more efficient grouping operations.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability. However, the paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision, and the model's performance on other tasks is not evaluated. Therefore, I recommend that the authors provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision, evaluate the model's performance on other tasks, and compare the model's performance with other state-of-the-art models.\n\nScore: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper proposes the Perceptual Group Tokenizer (PGT), a novel visual recognition architecture that entirely relies on perceptual grouping principles for feature extraction and self-supervised representation learning. The PGT model consists of multiple grouping layers that iteratively hypothesize contexts for pixels or superpixels, refining feature representations through soft assignment. The authors demonstrate that PGT achieves competitive performance on the ImageNet-1K self-supervised learning benchmark, outperforming state-of-the-art vision architectures, and exhibits desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The PGT model presents a novel approach to visual recognition, leveraging perceptual grouping principles to extract features and learn representations.\n2. The authors demonstrate the effectiveness of PGT on the ImageNet-1K self-supervised learning benchmark, achieving competitive performance with state-of-the-art vision architectures.\n3. The PGT model exhibits desirable properties such as adaptive computation and high model interpretability, making it an attractive option for visual recognition tasks.\n4. The authors provide a comprehensive evaluation of the PGT model, including comparisons with state-of-the-art vision architectures and analyses of its design choices and properties.\n\nWeaknesses:\n1. The PGT model requires iterative grouping processes, which can be computationally expensive and may limit its applicability to real-time applications.\n2. The authors rely on a specific grouping operation, which may not be optimal for all scenarios, and the design of the grouping operation may be limited by its complexity.\n3. The PGT model may not be as generalizable as other vision architectures, as it relies on specific design choices and properties that may not be applicable to all visual recognition tasks.\n4. The authors do not provide a detailed comparison with other perceptual grouping-based models, limiting the scope of the paper's contribution.\n\nSuggestions for Improvement:\n1. Investigate alternative grouping operations that can reduce computational costs and improve the PGT model's applicability.\n2. Analyze the PGT model's generalizability across different visual recognition tasks and scenarios.\n3. Compare the PGT model with other perceptual grouping-based models to provide a more comprehensive evaluation of its contribution.\n4. Investigate the PGT model's robustness to different types of noise and corruption, and explore techniques to improve its robustness.\n\nRecommendation:\nThis paper presents a novel and effective approach to visual recognition, leveraging perceptual grouping principles to extract features and learn representations. The PGT model demonstrates competitive performance on the ImageNet-1K self-supervised learning benchmark and exhibits desirable properties such as adaptive computation and high model interpretability. However, the PGT model's computational costs and limited generalizability may limit its applicability. Therefore, I recommend that the authors investigate alternative grouping operations, improve the PGT model's generalizability, and provide a more comprehensive comparison with other perceptual grouping-based models.\n\nRating: 7.5/10\n\nRecommendation for Publication: Accept, with minor revisions to address the suggestions for improvement.",
        "gr_score_with": 0.7109375,
        "gr_score_without": 0.5964285714285714,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 167,
            "green_fraction": 0.5964285714285714,
            "z_score": -0.12182748262732698,
            "p_value": 0.548482175962465
        },
        "output_with": {
            "num_tokens_scored": 384,
            "num_green_tokens": 273,
            "green_fraction": 0.7109375,
            "z_score": 4.43767847591857,
            "p_value": 4.546715735351666e-06
        }
    },
    {
        "title": "FOSI: Hybrid First and Second Order Optimization",
        "abstract": "Popular machine learning approaches forgo second-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any base first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We formally analyze FOSI\u2019s convergence and the conditions under which it improves a base optimizer. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods such as Heavy-Ball and Adam, and outperforms second-order methods (K-FAC and L-BFGS).",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that combines a first-order base optimizer with Newton's method to improve the convergence of any base first-order optimizer. FOSI iteratively splits the optimization problem into two quadratic problems on orthogonal subspaces, uses Newton's method to minimize one, and the base optimizer to minimize the other. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems. The paper formally analyzes FOSI's convergence and the conditions under which it improves a base optimizer. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nStrengths:\n\n1. FOSI is a hybrid optimization algorithm that combines the benefits of first-order and second-order methods, making it a robust and efficient optimization technique.\n2. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems.\n3. The paper provides a thorough analysis of FOSI's convergence and the conditions under which it improves a base optimizer.\n4. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nWeaknesses:\n\n1. The algorithm requires the number of Lanczos iterations (m) to be set, which can be a hyperparameter that needs to be tuned.\n2. The paper assumes that the Hessian is symmetric and positive definite, which may not be the case in all optimization problems.\n3. The algorithm requires the use of a base first-order optimizer, which may not be the best choice for all optimization problems.\n4. The paper does not provide a thorough analysis of the computational complexity of FOSI.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically tuning the number of Lanczos iterations (m) and other hyperparameters of FOSI.\n2. Consider relaxing the assumption that the Hessian is symmetric and positive definite to make FOSI more applicable to a wider range of optimization problems.\n3. Investigate the use of other base first-order optimizers, such as stochastic gradient descent, to see if they can be improved by FOSI.\n4. Provide a thorough analysis of the computational complexity of FOSI and compare it to other optimization algorithms.\n\nRecommendation:\n\nFOSI is a promising optimization algorithm that combines the benefits of first-order and second-order methods. While there are some limitations and areas for improvement, the algorithm shows great potential for improving the convergence rate and optimization time of first-order methods and outperforming second-order methods on various deep neural network training tasks. Therefore, I recommend accepting this paper for publication in a top-tier conference or journal.\n\nRating: 8/10\n\nReviewer's Confidence: 9/10\n\nReviewer's Expertise: Optimization algorithms, machine learning, deep learning.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes FOSI, a hybrid optimization method that improves the performance of any base first-order optimizer by incorporating second-order information. FOSI iteratively splits the optimization problem into two quadratic problems on orthogonal subspaces and uses a second-order method to minimize one and the base optimizer to minimize the other. The method is analyzed and shown to converge under common assumptions, and its preconditioner is analyzed to determine the conditions under which it improves the condition number of the problem. Empirical evaluations demonstrate that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods.\n\nStrengths:\n1. FOSI is a hybrid optimization method that combines the strengths of first-order and second-order methods, making it a versatile and effective optimization technique.\n2. The method is analyzed and shown to converge under common assumptions, providing a theoretical foundation for its use.\n3. The preconditioner analysis is a significant contribution, as it provides insight into the conditions under which FOSI improves the condition number of the problem.\n4. Empirical evaluations demonstrate the effectiveness of FOSI in improving the convergence rate and optimization time of first-order methods and outperforming second-order methods.\n\nWeaknesses:\n1. The method requires the use of the Lanczos algorithm, which can be computationally expensive for large-scale optimization problems.\n2. The method requires the estimation of the extreme eigenvalues and eigenvectors of the Hessian, which can be challenging in practice.\n3. The method is sensitive to the choice of hyperparameters, particularly the number of Lanczos iterations and the learning rate.\n\nSuggestions for Improvement:\n1. Developing more efficient methods for estimating the extreme eigenvalues and eigenvectors of the Hessian could improve the computational efficiency of FOSI.\n2. Investigating the use of alternative methods for preconditioning, such as diagonal preconditioning, could provide a more efficient and effective approach.\n3. Developing methods for automatically tuning the hyperparameters of FOSI could improve its performance and robustness.\n\nRecommendation:\nFOSI is a promising hybrid optimization method that has the potential to improve the performance of first-order and second-order methods. Its theoretical analysis and empirical evaluations demonstrate its effectiveness in a range of optimization problems. However, its computational efficiency and sensitivity to hyperparameters are areas that require further investigation. With additional research and development, FOSI could become a widely used and effective optimization technique in a range of applications.",
        "gr_score_with": 0.7088235294117647,
        "gr_score_without": 0.6027397260273972,
        "output_without": {
            "num_tokens_scored": 365,
            "num_green_tokens": 220,
            "green_fraction": 0.6027397260273972,
            "z_score": 0.10702568978892259,
            "p_value": 0.4573842996502203
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 241,
            "green_fraction": 0.7088235294117647,
            "z_score": 4.096136940451229,
            "p_value": 2.1005074363962487e-05
        }
    },
    {
        "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n* The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a promising direction for future research.\n* The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings.\n* The paper provides a comprehensive evaluation of GR-1, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nWeaknesses:\n\n* The paper relies heavily on pre-training on a large-scale video dataset, which may not be feasible for all applications or domains.\n* The authors do not provide a thorough analysis of the trade-offs between pre-training and finetuning, which may be important for practical applications.\n* The paper assumes access to a large-scale video dataset, which may not be available for all researchers or applications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the trade-offs between pre-training and finetuning, including the impact on performance, computational resources, and data requirements.\n* Explore alternative approaches to pre-training, such as using smaller datasets or different types of data, to make the method more widely applicable.\n* Provide more information about the scalability of GR-1, including the computational resources required for training and inference.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to visual robot manipulation, and the authors demonstrate its effectiveness on challenging benchmarks. However, the paper relies heavily on pre-training on a large-scale video dataset, and the trade-offs between pre-training and finetuning are not thoroughly analyzed. To make the method more widely applicable and scalable, the authors should explore alternative approaches to pre-training and provide a more detailed analysis of the trade-offs between pre-training and finetuning. With these improvements, the paper has the potential to make a significant impact in the field of visual robot manipulation.\n\nRating: 7/10\n\nRecommendation for Publication: Accept with revisions.",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThis paper proposes a novel approach to enhance visual robot manipulation learning by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer that takes as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset, and then seamlessly finetuned on robot data to predict actions and future images. The authors perform extensive experiments on both CALVIN benchmark and a real robot to verify the performance of GR-1, showing that it improves state-of-the-art methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\n**Strengths**\n\n1. The paper proposes a novel approach to enhance visual robot manipulation learning by leveraging large-scale video generative pre-training, which shows promising results.\n2. The authors introduce GR-1, a GPT-style transformer that takes as inputs a language instruction, a sequence of observation images, and robot states, and outputs actions and future images in an end-to-end manner, which is a significant contribution to the field.\n3. The paper performs extensive experiments on both CALVIN benchmark and a real robot, providing strong evidence for the effectiveness of GR-1.\n4. The authors demonstrate that GR-1 improves state-of-the-art methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\n**Weaknesses**\n\n1. The paper assumes access to a large-scale video dataset, which may not be available in all scenarios.\n2. The authors do not provide a thorough analysis of the effect of video generative pre-training on the performance of GR-1.\n3. The paper does not provide a clear explanation of how GR-1 generalizes to unseen scenes and objects.\n4. The authors do not provide a thorough comparison with other methods that leverage large-scale video data.\n\n**Suggestions for Improvement**\n\n1. Provide a thorough analysis of the effect of video generative pre-training on the performance of GR-1.\n2. Explain how GR-1 generalizes to unseen scenes and objects.\n3. Provide a thorough comparison with other methods that leverage large-scale video data.\n4. Discuss potential limitations and future work directions.\n\n**Recommendation**\n\nThis paper proposes a novel approach to enhance visual robot manipulation learning by leveraging large-scale video generative pre-training, which shows promising results. However, there are some weaknesses that need to be addressed. With some improvements, this paper has the potential to make a significant contribution to the field.\n\n**Recommendation to Acceptance**\n\nAccept, subject to minor revisions.\n\n$$$",
        "gr_score_with": 0.6543624161073825,
        "gr_score_without": 0.5545722713864307,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 188,
            "green_fraction": 0.5545722713864307,
            "z_score": -1.7071421227822023,
            "p_value": 0.9561021802742227
        },
        "output_with": {
            "num_tokens_scored": 298,
            "num_green_tokens": 195,
            "green_fraction": 0.6543624161073825,
            "z_score": 1.9157453788335144,
            "p_value": 0.02769875664348987
        }
    },
    {
        "title": "DOUBLY ROBUST INSTANCE-REWEIGHTED ADVERSARIAL TRAINING",
        "abstract": "Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which combines distributionally robust optimization (DRO) and bilevel optimization to determine instance weights for adversarial training in a theoretically grounded way. The framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples. The authors also develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem.\n\nStrengths:\n\n* The paper provides a theoretically grounded approach to determine instance weights for adversarial training, which is a significant improvement over existing heuristics-based methods.\n* The proposed framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples.\n* The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem.\n* The paper provides experimental results on standard benchmarks, demonstrating the effectiveness of the proposed framework.\n\nWeaknesses:\n\n* The paper assumes that the attack loss function \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice.\n* The authors use a log-barrier penalty method to penalize the linear constraint into the attack objective, which may not be effective in all cases.\n* The paper does not provide a comprehensive comparison with other existing methods, such as robust overfitting mitigation methods.\n* The experimental results are limited to standard benchmarks and do not provide a thorough analysis of the robustness of the proposed framework against different types of attacks.\n\nSuggestions for Improvement:\n\n* Provide a more comprehensive comparison with other existing methods, including robust overfitting mitigation methods.\n* Analyze the robustness of the proposed framework against different types of attacks, such as white-box and black-box attacks.\n* Provide more experimental results on other benchmarks and datasets to demonstrate the effectiveness of the proposed framework.\n* Consider using more robust optimization techniques, such as robust gradient descent, to improve the stability of the optimization process.\n\nRecommendation:\n\nThe paper provides a theoretically grounded approach to determine instance weights for adversarial training, which is a significant improvement over existing heuristics-based methods. However, the paper assumes that the attack loss function \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice. Additionally, the authors use a log-barrier penalty method to penalize the linear constraint into the attack objective, which may not be effective in all cases. Therefore, I recommend that the authors provide a more comprehensive comparison with other existing methods, analyze the robustness of the proposed framework against different types of attacks, and provide more experimental results on other benchmarks and datasets to demonstrate the effectiveness of the proposed framework.\n\nOverall, the paper provides a promising approach to adversarial training, and with further improvements, it has the potential to become a leading method in the field.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training (AT) framework, which addresses two critical limitations of traditional AT methods: (1) the lack of rigorous theoretical justification/guarantee in determining importance weights, and (2) the severe non-uniform robust performance across the training distribution. The proposed framework uses distributionally robust optimization (DRO) techniques to obtain importance weights and boosts the robustness on the most vulnerable examples. The authors also develop a novel algorithm, Compositional Implicit Differentiation (CID), which exploits the implicit function theorem and keeps track of a running average of the outer level composed function values. The proposed framework and algorithm are evaluated on several balanced and imbalanced image recognition datasets, and the results demonstrate the effectiveness of the proposed approach in improving worst-case robustness and average robust performance.\n\nStrengths:\n\n* The proposed framework and algorithm are well-motivated and provide a rigorous theoretical justification/guarantee for determining importance weights.\n* The use of DRO techniques and bilevel optimization provides a principled approach to instance-reweighted AT.\n* The CID algorithm is novel and provides a solution to the generic compositional bilevel optimization problem.\n* The experiments demonstrate the effectiveness of the proposed approach in improving worst-case robustness and average robust performance.\n\nWeaknesses:\n\n* The paper assumes that the attack objective \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice.\n* The paper does not provide a clear explanation of how the proposed framework and algorithm can be extended to other types of attacks (e.g., white-box attacks).\n* The experiments are limited to image classification problems, and it would be interesting to see the proposed approach applied to other types of problems (e.g., natural language processing).\n\nSuggestions for Improvement:\n\n* Provide more details on how the proposed framework and algorithm can be extended to other types of attacks.\n* Provide more explanations on the assumptions made in the paper and how they can be relaxed.\n* Consider providing more experiments on other types of problems (e.g., natural language processing).\n\nRecommendation:\n\nThe paper proposes a novel and well-motivated approach to instance-reweighted AT, which addresses two critical limitations of traditional AT methods. The proposed framework and algorithm are well-designed and provide a rigorous theoretical justification/guarantee for determining importance weights. The experiments demonstrate the effectiveness of the proposed approach in improving worst-case robustness and average robust performance. I recommend accepting the paper with minor revisions.\n\n$$$",
        "gr_score_with": 0.6873156342182891,
        "gr_score_without": 0.6238244514106583,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 199,
            "green_fraction": 0.6238244514106583,
            "z_score": 0.8687544708569446,
            "p_value": 0.1924907203787498
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 233,
            "green_fraction": 0.6873156342182891,
            "z_score": 3.2817739422101493,
            "p_value": 0.0005157813431805007
        }
    },
    {
        "title": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS",
        "abstract": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models\u2019 (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to automatically generate rich simulation environments and expert demonstrations for robotic policy training. The framework operates in two distinct modes: goal-directed generation, where a target task is given to the LLM, and exploratory generation, where the LLM bootstraps from previous tasks to propose novel tasks. The authors demonstrate that LLMs can generate high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies. They evaluate several LLMs, including finetuned GPTs and Code Llama, on code generation for robotic simulation tasks and show that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities by 25%.\n\nStrengths:\n\n* The paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can augment diverse simulation tasks for robotic policy training.\n* The authors demonstrate the effectiveness of their framework in generating high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies.\n* The paper provides a thorough evaluation of several LLMs on code generation for robotic simulation tasks and shows that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities.\n\nWeaknesses:\n\n* The paper relies heavily on a single LLM model, GPT4, and does not explore the robustness of their framework to different LLM models.\n* The evaluation of the framework is limited to a specific task domain (table-top pick-and-place) and does not consider more complex robotic tasks.\n* The paper does not provide a thorough analysis of the limitations of their framework, such as the potential for hallucinations and lack of grounding in physical and geometric details.\n\nSuggestions for Improvement:\n\n* To improve the robustness of the framework, the authors should explore the use of multiple LLM models and evaluate their framework on different task domains.\n* The authors should provide a more thorough analysis of the limitations of their framework, including the potential for hallucinations and lack of grounding in physical and geometric details.\n* The paper should include a more detailed discussion of the potential applications of their framework, such as generating dexterous and complex robotic tasks.\n\nRecommendation:\n\nThe paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can enhance task-level generalization of robotic policies. While the paper has some limitations, it provides a thorough evaluation of their framework and demonstrates its effectiveness in generating high-quality, achievable, and diverse tasks. Therefore, I recommend this paper for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a framework called GenSim that utilizes large language models (LLMs) to generate simulation tasks for robotic policy learning. The framework operates in two modes: goal-directed and exploratory. In the goal-directed mode, the framework generates tasks to solve a specific target task, while in the exploratory mode, it generates tasks to establish a foundational policy. The authors demonstrate that the generated tasks exhibit high diversity and quality, and that training on these tasks leads to improved task-level generalization. The authors also show that the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%.\n\nStrengths:\n\n* The paper proposes a novel framework for generating simulation tasks using LLMs, which addresses the limitation of existing methods that focus on scene-level diversity.\n* The framework demonstrates high diversity and quality in the generated tasks, which leads to improved task-level generalization.\n* The authors provide a thorough evaluation of the framework, including experiments on task generation, policy generalization, and sim-to-real transfer.\n\nWeaknesses:\n\n* The paper relies heavily on the capabilities of LLMs, and the authors do not provide a clear explanation of how the LLMs are fine-tuned for the task generation task.\n* The evaluation metrics used in the paper are limited, and the authors do not provide a clear analysis of the results.\n* The paper does not provide a clear comparison with existing methods for generating simulation tasks.\n\nSuggestions for Improvement:\n\n* The authors should provide a clear explanation of the fine-tuning process for the LLMs and how it affects the task generation performance.\n* The authors should provide a more thorough analysis of the results, including a discussion of the limitations and potential biases of the evaluation metrics.\n* The authors should provide a clear comparison with existing methods for generating simulation tasks and evaluate the framework on a wider range of tasks.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of robotic policy learning by proposing a novel framework for generating simulation tasks using LLMs. The framework demonstrates high diversity and quality in the generated tasks, and the authors provide a thorough evaluation of the framework. However, the paper relies heavily on the capabilities of LLMs, and the authors do not provide a clear explanation of the fine-tuning process. Therefore, I recommend that the authors provide a clear explanation of the fine-tuning process and provide a more thorough analysis of the results. Additionally, the authors should provide a clear comparison with existing methods for generating simulation tasks and evaluate the framework on a wider range of tasks.\n\nOverall, I believe that this paper has the potential to make a significant impact on the field of robotic policy learning and deserves to be published in a top-tier conference.",
        "gr_score_with": 0.688622754491018,
        "gr_score_without": 0.6228070175438597,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 213,
            "green_fraction": 0.6228070175438597,
            "z_score": 0.8611209496658327,
            "p_value": 0.1945857160050748
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 230,
            "green_fraction": 0.688622754491018,
            "z_score": 3.306244255822386,
            "p_value": 0.0004727781137604289
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. The authors also demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n\nStrengths:\n\n1. The paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n2. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n3. The authors demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n4. The paper provides a comprehensive analysis of the framework's performance and limitations, including the use of pseudo-labels and the Pathway module.\n\nWeaknesses:\n\n1. The paper relies heavily on pseudo-labels, which may not be accurate and can lead to performance loss on certain datasets.\n2. The Pathway module has limitations, including not handling multi-path scenarios, which may lead to suboptimal performance in certain cases.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, which makes it difficult to evaluate the framework's performance in a broader context.\n4. The paper assumes that the input utterance is well-formed and does not handle cases where the utterance is ambiguous or contains negations.\n\nSuggestions for Improvement:\n\n1. The authors should investigate alternative methods for collecting pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. The Pathway module should be modified to handle multi-path scenarios, such as using a graph-based approach to reason about different possibilities.\n3. The paper should provide a more thorough comparison with other state-of-the-art methods, including a detailed analysis of the framework's performance and limitations.\n4. The paper should address the assumption that the input utterance is well-formed and provide a more robust solution for handling ambiguous or negated utterances.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field. However, the paper relies heavily on pseudo-labels, which may not be accurate, and the Pathway module has limitations. To improve the framework, the authors should investigate alternative methods for collecting pseudo-labels and modify the Pathway module to handle multi-path scenarios. Additionally, the paper should provide a more thorough comparison with other state-of-the-art methods and address the assumption that the input utterance is well-formed. Overall, the paper is well-written and provides a comprehensive analysis of the framework's performance and limitations.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, termed CoT3DRef. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The authors demonstrate the effectiveness of their proposed framework on three challenging benchmarks, i.e., Nr3D, Sr3D, and ScanRefer, achieving state-of-the-art results without requiring any additional manual annotations.\n\nStrengths:\n\n1. The proposed framework is highly interpretable, providing insights into how the model reaches its decisions.\n2. The use of a sequence-to-sequence approach allows for the prediction of a chain of anchor objects, which enhances data efficiency and performance.\n3. The authors demonstrate the effectiveness of their proposed framework on multiple challenging benchmarks, achieving state-of-the-art results.\n4. The framework can be easily integrated into existing architectures, making it a versatile solution for 3D visual grounding tasks.\n\nWeaknesses:\n\n1. The accuracy of the pseudo-labels plays a vital role in the overall performance, and there is a significant accuracy gap between automatically collected pseudo-labels and ground-truth data.\n2. The anchor localization block in the pseudo module is tailored to the ScanNet dataset and may need adaptations to be usable on other 3D scene datasets.\n3. The Pathway module does not handle the multi-path scenario, where multiple paths are valid.\n4. The authors do not provide an in-depth analysis of the limitations of the proposed framework and potential future directions for improvement.\n\nSuggestions for Improvement:\n\n1. Investigate methods to improve the accuracy of the pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. Adapt the anchor localization block to be usable on other 3D scene datasets.\n3. Modify the Pathway module to handle the multi-path scenario, where multiple paths are valid.\n4. Provide an in-depth analysis of the limitations of the proposed framework and potential future directions for improvement.\n\nRecommendation:\n\nThe proposed framework, CoT3DRef, is a significant contribution to the field of 3D visual grounding. The use of a sequence-to-sequence approach and the emphasis on interpretability are particularly noteworthy. While there are some limitations to the proposed framework, the authors demonstrate its effectiveness on multiple challenging benchmarks. Therefore, I highly recommend the acceptance of this paper for publication in the conference proceedings.\n\n$$",
        "gr_score_with": 0.5670391061452514,
        "gr_score_without": 0.5489130434782609,
        "output_without": {
            "num_tokens_scored": 368,
            "num_green_tokens": 202,
            "green_fraction": 0.5489130434782609,
            "z_score": -2.000265762589126,
            "p_value": 0.9772642130180088
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 203,
            "green_fraction": 0.5670391061452514,
            "z_score": -1.2728358026835118,
            "p_value": 0.898461843193412
        }
    },
    {
        "title": "REWARD DESIGN FOR JUSTIFIABLE SEQUENTIAL DECISION-MAKING",
        "abstract": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to designing rewards for reinforcement learning (RL) agents that can justify their decisions using supporting evidence. The authors introduce a debate-based reward model, where two argumentative agents take turns providing supporting evidence for two competing decisions. A proxy of a human judge evaluates which decision is better justified based on the proposed evidence. The authors demonstrate the effectiveness of their approach in learning policies for treating septic patients, showing that the debate-based feedback yields policies highly favored by the judge while hardly sacrificing any performance.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence.\n2. The authors provide a thorough and detailed explanation of their approach, including the formal setup, learning framework, and experiments.\n3. The paper presents a comprehensive evaluation of the effectiveness of the debate-based reward model, including quantitative and qualitative results.\n4. The authors provide a clear and concise description of the experimental setup, including the patient cohort, environment, and evaluation metrics.\n\nWeaknesses:\n\n1. The paper assumes a well-defined argument space, which may not be the case in all domains. The authors acknowledge this limitation and suggest potential alternative approaches.\n2. The paper relies on a proxy of a human judge, which may not accurately capture human preferences and biases.\n3. The authors do not provide a thorough discussion of the potential biases and limitations of the debate-based reward model.\n4. The paper assumes a fixed number of turns in the debate game, which may not be realistic in all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed discussion of the potential biases and limitations of the debate-based reward model, including the impact of human biases and preferences.\n2. The paper should include a more thorough evaluation of the effectiveness of the debate-based reward model in different domains and scenarios.\n3. The authors should provide a more detailed explanation of the learning framework and the role of the proxy judge in the debate-based reward model.\n4. The paper should include a more comprehensive discussion of the potential applications and limitations of the debate-based reward model.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence. While there are some limitations and weaknesses to the approach, the paper provides a thorough and detailed evaluation of the effectiveness of the debate-based reward model. I recommend acceptance of the paper for publication, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to reward design for reinforcement learning (RL) by incorporating a debate-based reward model. The proposed method, called \"Justifiable RL,\" allows RL agents to learn decision-making policies that can be justified by supporting evidence. The debate-based reward model is designed as a zero-sum game, where two argumentative agents take turns providing evidence for competing decisions, and a human judge evaluates which decision is better justified. The authors demonstrate the effectiveness of their approach in learning policies for prescribing and justifying treatment decisions for septic patients. They show that the debate-based feedback yields policies that are highly favored by the judge compared to policies obtained solely from environment rewards, while hardly sacrificing any performance.\n\nStrengths:\n\n1. The paper presents a novel and well-motivated approach to reward design for RL.\n2. The authors provide a comprehensive analysis of their method, including both qualitative and quantitative evaluations.\n3. The paper demonstrates the effectiveness of their approach in a real-world problem, specifically in learning policies for prescribing treatment decisions for septic patients.\n4. The authors provide a clear and concise presentation of their method, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that human preferences can be collected and used as a ground truth for evaluating justifiability, which may not always be possible in real-world scenarios.\n2. The authors do not provide a comprehensive analysis of the limitations and challenges of their method, particularly in terms of scalability and applicability to other domains.\n3. The paper relies heavily on synthetic data, which may not accurately represent real-world scenarios.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods for reward design in RL.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive analysis of the limitations and challenges of their method, particularly in terms of scalability and applicability to other domains.\n2. The paper should include a more detailed comparison with other state-of-the-art methods for reward design in RL.\n3. The authors should provide more insight into how their method can be generalized to other domains, particularly in terms of collecting human preferences.\n4. The paper should include more discussion on the potential applications and implications of their method, particularly in terms of decision-making in high-stakes scenarios.\n\nRecommendation:\n\nI recommend this paper for acceptance, but with some revisions to address the weaknesses mentioned above. The paper presents a novel and well-motivated approach to reward design for RL, and the authors provide a comprehensive analysis of their method. However, the paper relies heavily on synthetic data, and the authors do not provide a comprehensive analysis of the limitations and challenges of their method. With some revisions to address these issues, this paper has the potential to make a significant contribution to the field of RL.",
        "gr_score_with": 0.6404494382022472,
        "gr_score_without": 0.5211726384364821,
        "output_without": {
            "num_tokens_scored": 307,
            "num_green_tokens": 160,
            "green_fraction": 0.5211726384364821,
            "z_score": -2.8191224245545126,
            "p_value": 0.99759224260914
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 228,
            "green_fraction": 0.6404494382022472,
            "z_score": 1.5580494980900013,
            "p_value": 0.059610757462976484
        }
    },
    {
        "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
        "abstract": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach named TGeo-KD for learning sample-wise knowledge fusion ratios during knowledge distillation (KD). The authors argue that existing methods for determining the knowledge fusion ratio often rely on heuristic rules or constant values, which may not capture the nuanced dynamics of the knowledge transfer process. To address this limitation, the authors introduce a novel adaptive method that learns a sample-wise knowledge fusion ratio by exploiting both the correctness of the teacher and student, as well as the discrepancy between their predictions. The proposed method, TGeo-KD, leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations.\n\nStrengths:\n\n1. The paper presents a well-structured and clear introduction to the problem of knowledge distillation and the limitations of existing methods.\n2. The proposed approach, TGeo-KD, is innovative and well-motivated, leveraging the trilateral geometry among the signals from the student, teacher, and ground truth.\n3. The authors provide extensive experiments across diverse domains, including image classification, attack detection, and click-through rate prediction, demonstrating the consistent superiority of TGeo-KD over other re-weighting methods.\n4. The paper provides a clear and concise explanation of the proposed method, including the bilevel optimization problem and the intra- and inter-sample geometric relations.\n\nWeaknesses:\n\n1. The paper assumes a relatively simple neural network architecture for learning the knowledge fusion ratio, which may not be optimal for all scenarios.\n2. The authors do not provide a thorough analysis of the computational complexity of the proposed method, which may be a concern for large-scale applications.\n3. The paper focuses primarily on image classification tasks and does not provide a comprehensive evaluation of TGeo-KD on other tasks, such as object detection or segmentation.\n4. The authors do not provide a clear explanation of how the proposed method can be extended to handle multiple teachers or students.\n\nSuggestions for Improvement:\n\n1. The authors could provide a more detailed analysis of the computational complexity of the proposed method and discuss potential optimizations.\n2. The paper could benefit from a more comprehensive evaluation of TGeo-KD on a wider range of tasks and datasets.\n3. The authors could provide a clear explanation of how the proposed method can be extended to handle multiple teachers or students.\n4. The paper could benefit from a more detailed discussion of the limitations of the proposed method and potential areas for future research.\n\nRecommendation:\n\nI highly recommend this paper for publication in a top-tier conference or journal in the field of machine learning. The proposed approach, TGeo-KD, is innovative and well-motivated, and the authors provide extensive experiments demonstrating its consistent superiority over other re-weighting methods. However, the authors should address the limitations and suggestions for improvement mentioned above to further strengthen the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method named TGeo-KD for learning sample-wise knowledge fusion ratios in knowledge distillation (KD). TGeo-KD leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations. The method aims to find the optimal sample-wise ratios that enable the student network to generalize well on test data. TGeo-KD is shown to outperform other re-weighting methods consistently in various tasks, including image classification, attack detection, and click-through rate prediction.\n\nStrengths:\n\n* The paper proposes a novel method that leverages the trilateral geometry among the signals from the student, teacher, and ground truth.\n* TGeo-KD is shown to outperform other re-weighting methods consistently in various tasks.\n* The method is simple to implement and can be employed across various architectures and model sizes.\n* The paper provides extensive experiments to demonstrate the effectiveness of TGeo-KD.\n\nWeaknesses:\n\n* The paper does not provide a clear explanation of how the trilateral geometry is used to learn the knowledge fusion ratio.\n* The paper relies on extensive experiments to demonstrate the effectiveness of TGeo-KD, which may not be sufficient to establish the method's superiority.\n* The paper does not discuss the limitations of TGeo-KD and how it can be improved.\n\nSuggestions for Improvement:\n\n* Provide a clear explanation of how the trilateral geometry is used to learn the knowledge fusion ratio.\n* Conduct more thorough analysis to establish the method's superiority.\n* Discuss the limitations of TGeo-KD and how it can be improved.\n* Consider exploring cross-class relationships to enhance performance.\n\nRecommendation:\n\nThe paper proposes a novel method that shows promising results in various tasks. However, the paper's limitations and the lack of clear explanation of the trilateral geometry used to learn the knowledge fusion ratio are concerns. With further development and improvement, TGeo-KD has the potential to become a leading method in knowledge distillation.\n\nRating: 7/10\n$$$",
        "gr_score_with": 0.7034220532319392,
        "gr_score_without": 0.5855614973262032,
        "output_without": {
            "num_tokens_scored": 374,
            "num_green_tokens": 219,
            "green_fraction": 0.5855614973262032,
            "z_score": -0.569784662988182,
            "p_value": 0.7155881204561335
        },
        "output_with": {
            "num_tokens_scored": 263,
            "num_green_tokens": 185,
            "green_fraction": 0.7034220532319392,
            "z_score": 3.423767039223625,
            "p_value": 0.000308797699049373
        }
    },
    {
        "title": "TAPMO: SHAPE-AWARE MOTION GENERATION OF SKELETON-FREE CHARACTERS",
        "abstract": "Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of meshspecific motions for various characters. Specifically, TapMo comprises two main components Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character\u2019s shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of nonhuman meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters. The project page: https://semanticdh.github.io/TapMo.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. The pipeline consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning to train TapMo with limited ground-truth data. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. The paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters.\n2. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective.\n3. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n4. The authors provide a clear and concise explanation of the proposed method and its components.\n\nWeaknesses:\n\n1. The paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n2. The paper does not provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. The authors should provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. The paper assumes that the text description is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n\nSuggestions for Improvement:\n\n1. Provide more information on how to handle the case where the input mesh is not provided.\n2. Provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. Provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. Provide more information on how to handle the case where the text description is not provided.\n5. Consider providing more information on the robustness of the proposed method to different types of meshes and text descriptions.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods. However, the paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue. Overall, the paper is well-written and provides a clear and concise explanation of the proposed method and its components. I recommend this paper for publication in the conference.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel text-driven animation pipeline, TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. TapMo consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. To train TapMo with limited ground-truth data, the authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning. The authors demonstrate the effectiveness and generalizability of TapMo through extensive experiments, showing that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to text-driven animation for skeleton-free 3D characters, which is a challenging and under-explored problem.\n2. The authors present a comprehensive and well-organized paper, with clear explanations of the methodology and experimental results.\n3. The paper provides a thorough evaluation of the proposed method, with quantitative and qualitative comparisons to baseline methods.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of pre-trained models and datasets, which may limit its applicability and generalizability to other domains and datasets.\n2. The authors do not provide a clear and concise explanation of the Mesh Handle Predictor and Shape-aware Motion Diffusion components, which may make it difficult for readers to understand and replicate the proposed method.\n3. The paper does not provide a clear and concise explanation of the weakly-supervised training strategy, which may make it difficult for readers to understand and replicate the proposed method.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed and concise explanation of the Mesh Handle Predictor and Shape-aware Motion Diffusion components, including the mathematical formulation and implementation details.\n2. Provide a clear and concise explanation of the weakly-supervised training strategy, including the mathematical formulation and implementation details.\n3. Provide more experimental results and comparisons to baseline methods, including a more detailed analysis of the quantitative and qualitative results.\n\nRecommendation:\n\nThe paper is a strong contribution to the field of computer animation and provides a novel and effective approach to text-driven animation for skeleton-free 3D characters. However, the paper relies heavily on pre-trained models and datasets, which may limit its applicability and generalizability to other domains and datasets. Therefore, the paper could be improved by providing a more detailed and concise explanation of the proposed method, as well as more experimental results and comparisons to baseline methods.\n\nOverall, the paper is well-written, well-organized, and provides a clear and concise explanation of the proposed method. However, the paper could be improved by providing more experimental results and comparisons to baseline methods, as well as a more detailed and concise explanation of the proposed method.\n\nGrade: 8/10\n\nReviewer: [Your Name]\nDate: [Current Date]\n$$$",
        "gr_score_with": 0.7740112994350282,
        "gr_score_without": 0.7350993377483444,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 222,
            "green_fraction": 0.7350993377483444,
            "z_score": 4.792534660754035,
            "p_value": 8.234365411616923e-07
        },
        "output_with": {
            "num_tokens_scored": 354,
            "num_green_tokens": 274,
            "green_fraction": 0.7740112994350282,
            "z_score": 6.683197404452329,
            "p_value": 1.1689215578647373e-11
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. The framework consists of four core modules: Environment, Reasoning, Reflection, and Memory. The Reasoning module utilizes the experiences derived from the Memory module and the common-sense knowledge of the LLM to perform decision-making for the current traffic scenario. The Reflection module assesses past decision sequences, categorizing them as either safe or unsafe, and updates the Memory module with refined decisions. The authors conduct extensive experiments to demonstrate the effectiveness of DiLu in accumulating experience, generalization ability, and transformation ability.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to autonomous driving by leveraging LLMs and knowledge-driven capabilities.\n2. The framework is well-structured, and the authors provide a clear explanation of the Reasoning and Reflection modules.\n3. The experiments are thorough, and the authors provide a comprehensive comparison with the state-of-the-art RL-based method.\n4. The paper highlights the importance of generalization and transformation ability in autonomous driving systems.\n\nWeaknesses:\n\n1. The paper lacks a clear explanation of how the LLM is trained and fine-tuned for the autonomous driving task.\n2. The authors do not provide a detailed analysis of the decision-making latency and hallucinations generated by LLMs.\n3. The paper assumes that the LLM has access to a vast amount of common-sense knowledge, which may not be feasible in real-world scenarios.\n4. The authors do not provide a clear comparison with other knowledge-driven approaches in autonomous driving.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the LLM training and fine-tuning process.\n2. Conduct a more thorough analysis of the decision-making latency and hallucinations generated by LLMs.\n3. Consider incorporating other knowledge-driven approaches in autonomous driving for a more comprehensive comparison.\n4. Provide a clear explanation of how the framework can be scaled up for real-world applications.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to autonomous driving, and the authors provide a clear explanation of the framework. However, the paper lacks a clear explanation of the LLM training and fine-tuning process, and the authors do not provide a detailed analysis of the decision-making latency and hallucinations generated by LLMs. With some modifications and improvements, the paper can be a strong contribution to the field of autonomous driving.\n\nRating: 7/10\n\nRecommendation for Publication: Accept after revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel autonomous driving system called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of dataset bias, overfitting, and uninterpretability in data-driven approaches. DiLu leverages large language models (LLMs) with emergent abilities to facilitate reasoning and reflection processes, enabling the system to perform decision-making based on common-sense knowledge and evolve continuously.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the challenges in autonomous driving and the need for a knowledge-driven approach.\n2. The authors propose a novel paradigm that integrates an interactive environment, a driver agent, and a memory component, which is innovative and different from existing approaches.\n3. The use of LLMs with emergent abilities is a significant strength, as they can facilitate reasoning and reflection processes, enabling the system to perform decision-making based on common-sense knowledge.\n4. The authors demonstrate the effectiveness of DiLu in continuous experience accumulation, generalization, and transformation, outperforming state-of-the-art reinforcement learning (RL)-based methods.\n5. The paper provides a clear explanation of the DiLu framework, including the Memory, Reasoning, and Reflection modules, and their interactions.\n\nWeaknesses:\n\n1. The paper assumes that LLMs can be used as a black box, without providing any insight into their underlying mechanisms or limitations.\n2. The authors do not provide a thorough analysis of the decision-making latency of DiLu, which is a significant concern in real-world autonomous driving systems.\n3. The paper does not address the issue of hallucinations generated by LLMs, which can be detrimental to the safety and reliability of autonomous driving systems.\n4. The authors do not provide a comprehensive comparison of DiLu with other knowledge-driven approaches, such as traditional planning-based methods.\n5. The paper relies heavily on simulated experiments, and the authors do not provide any results from real-world experiments.\n\nSuggestions for Improvement:\n\n1. Provide a more in-depth analysis of the LLMs used in DiLu, including their underlying mechanisms, limitations, and potential biases.\n2. Investigate methods to reduce the decision-making latency of DiLu, such as using more efficient LLMs or optimizing the API response times.\n3. Develop strategies to mitigate hallucinations generated by LLMs, such as using more robust decision-making algorithms or incorporating human feedback.\n4. Conduct real-world experiments to evaluate the effectiveness and robustness of DiLu in different environments.\n5. Provide a more comprehensive comparison of DiLu with other knowledge-driven approaches, including traditional planning-based methods.\n\nRecommendation:\n\nThe paper proposes an innovative approach to autonomous driving, leveraging LLMs with emergent abilities to facilitate reasoning and reflection processes. While there are some limitations and areas for improvement, the authors demonstrate the effectiveness of DiLu in continuous experience accumulation, generalization, and transformation, outperforming state-of-the-art RL-based methods. With further development and refinement, DiLu has the potential to become a leading approach in autonomous driving.\n\nOverall, I would recommend this paper for publication in a top-tier conference or journal, with some revisions to address the limitations and suggestions mentioned above.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.6821345707656613,
        "gr_score_without": 0.5754985754985755,
        "output_without": {
            "num_tokens_scored": 351,
            "num_green_tokens": 202,
            "green_fraction": 0.5754985754985755,
            "z_score": -0.9368185817041418,
            "p_value": 0.8255740573038013
        },
        "output_with": {
            "num_tokens_scored": 431,
            "num_green_tokens": 294,
            "green_fraction": 0.6821345707656613,
            "z_score": 3.480830654652643,
            "p_value": 0.00024993071025915324
        }
    },
    {
        "title": "RTFS-NET: RECURRENT TIME-FREQUENCY MOD-",
        "abstract": "Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method called RTFS-Net, which operates in the time-frequency domain. The proposed method utilizes a multi-layered recurrent neural network (RNN) to model and capture the time and frequency dimensions of the audio independently. Additionally, it introduces a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. The experimental evaluations conducted on three widely used datasets demonstrate the superiority of RTFS-Net over state-of-the-art (SOTA) methods, achieving better separation quality and reducing computational complexity by 83% and parameter count by 90%.\n\nStrengths:\n\n1. The proposed method, RTFS-Net, is the first time-frequency domain AVSS method to outperform all contemporary T-domain counterparts.\n2. The method achieves remarkable performance improvements while maintaining a significantly reduced computational complexity and parameter count.\n3. The use of a multi-layered RNN to model and capture the time and frequency dimensions of the audio independently is a novel and effective approach.\n4. The attention-based fusion technique and the new mask separation approach are unique and efficient ways to integrate audio and visual information.\n5. The experimental evaluations conducted on multiple datasets demonstrate the superiority of RTFS-Net over SOTA methods.\n\nWeaknesses:\n\n1. The paper assumes that the audio and video inputs are aligned in time, which may not always be the case in real-world scenarios.\n2. The method relies on a pre-trained video encoder, which may not be available for all applications.\n3. The experimental evaluations were conducted on a limited number of datasets, and it would be beneficial to evaluate the method on a larger variety of datasets.\n4. The paper does not provide a detailed analysis of the computational complexity of the method, which may be a concern for large-scale applications.\n5. The code for RTFS-Net is not provided, which makes it difficult for readers to reproduce the results.\n\nSuggestions for Improvement:\n\n1. Investigate methods to align the audio and video inputs in time, even if they are not perfectly aligned.\n2. Provide a more detailed analysis of the computational complexity of the method and explore ways to reduce it further.\n3. Evaluate the method on a larger variety of datasets to demonstrate its robustness and generalizability.\n4. Provide a more detailed explanation of the attention-based fusion technique and the new mask separation approach.\n5. Provide a more detailed comparison with SOTA methods and highlight the advantages of RTFS-Net over these methods.\n\nRecommendation:\n\nThe paper presents a novel and effective AVSS method that outperforms SOTA methods. However, it has some limitations that need to be addressed. With some improvements, RTFS-Net has the potential to become a leading method in the field of AVSS. I recommend this paper for publication in a reputable conference, and I hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method called RTFS-Net, which operates in the time-frequency domain. The proposed method utilizes a multi-layered recurrent neural network (RNN) to model the time and frequency dimensions of the audio independently. Additionally, it introduces a unique attention-based fusion technique for integrating audio and visual information. The RTFS-Net outperforms the prior state-of-the-art (SOTA) method in both inference speed and separation quality while reducing the number of parameters by 90% and Multiply-ACcumulate (MAC) operations by 83%.\n\nStrengths:\n\n1. The proposed method demonstrates a significant reduction in computational complexity and parameter count while achieving SOTA performance in AVSS.\n2. The use of a multi-layered RNN to model the time and frequency dimensions of the audio independently is a novel approach that can lead to improved separation quality.\n3. The attention-based fusion technique provides a more efficient way to integrate audio and visual information.\n4. The authors provide a thorough evaluation of the proposed method on multiple datasets, including LRS2-2Mix, LRS3-2Mix, and VoxCeleb2-2Mix.\n\nWeaknesses:\n\n1. The proposed method requires a large amount of computational resources to train and test, which might be a limitation for some users.\n2. The evaluation metrics used in the paper (SI-SNRi, SDRi, and PESQ) might not be sufficient to fully capture the performance of the proposed method in all scenarios.\n3. The authors do not provide a thorough analysis of the sensitivity of the proposed method to different hyperparameters and training conditions.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more efficient neural network architectures or pruning techniques to reduce the computational complexity of the proposed method.\n2. Evaluate the proposed method on more challenging datasets or scenarios to demonstrate its robustness and generalizability.\n3. Conduct a more thorough analysis of the sensitivity of the proposed method to different hyperparameters and training conditions to ensure its robustness.\n\nRecommendation:\n\nThe proposed method demonstrates a significant improvement in AVSS performance while reducing computational complexity and parameter count. However, it requires a large amount of computational resources to train and test. Therefore, I recommend the proposed method for publication in a reputable journal or conference, with the caveat that the authors should address the limitations mentioned above.\n\n$$$",
        "gr_score_with": 0.6861111111111111,
        "gr_score_without": 0.6132723112128147,
        "output_without": {
            "num_tokens_scored": 437,
            "num_green_tokens": 268,
            "green_fraction": 0.6132723112128147,
            "z_score": 0.5665442787974554,
            "p_value": 0.2855119207483118
        },
        "output_with": {
            "num_tokens_scored": 360,
            "num_green_tokens": 247,
            "green_fraction": 0.6861111111111111,
            "z_score": 3.335243675307286,
            "p_value": 0.00042612341546993915
        }
    },
    {
        "title": "SOHES: SELF-SUPERVISED OPEN-WORLD HIERARCHICAL ENTITY SEGMENTATION",
        "abstract": "Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that sidesteps the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pretrained self-supervised representation, we produce abundant high-quality pseudolabels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach, called Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), for open-world entity segmentation. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the method generates initial pseudo-labels by clustering visual features from a pre-trained self-supervised representation. In the self-instruction phase, a segmentation model is trained on the initial pseudo-labels to refine the segmentation. Finally, in the self-correction phase, a teacher-student mutual-learning framework is employed to further improve the segmentation model. SOHES achieves state-of-the-art performance on various datasets, including MS-COCO, LVIS, ADE20K, EntitySeg, and SA-1B, and significantly closes the gap between self-supervised and supervised methods.\n\nStrengths:\n\n1. SOHES is a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision.\n2. The method is self-supervised, which means that it can learn from unlabeled data, making it more efficient and cost-effective than traditional supervised learning approaches.\n3. SOHES can learn hierarchical structures of visual entities, which is a unique feature of this approach.\n4. The method achieves state-of-the-art performance on various datasets, including those with complex scenes and diverse classes.\n5. The paper provides a thorough evaluation of the method, including ablation studies and comparisons with state-of-the-art methods.\n\nWeaknesses:\n\n1. The method requires a large amount of unlabeled data, which may not be readily available in all scenarios.\n2. The self-exploration phase can be computationally expensive, which may limit the scalability of the method.\n3. The teacher-student mutual-learning framework used in the self-correction phase may not be suitable for all types of data and may require additional hyperparameter tuning.\n4. The paper does not provide a detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n5. The method may not be suitable for real-time applications due to its computational complexity.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more efficient clustering algorithms in the self-exploration phase to reduce computational complexity.\n2. Explore the use of different teacher-student mutual-learning frameworks to improve the performance of the method.\n3. Provide a more detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n4. Investigate the use of transfer learning to adapt the method to new domains and scenarios.\n5. Provide a more detailed evaluation of the method's performance on real-world datasets and scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision. The method is self-supervised, which makes it more efficient and cost-effective than traditional supervised learning approaches. However, the method requires a large amount of unlabeled data and may not be suitable for real-time applications due to its computational complexity. Overall, the paper is well-written and provides a thorough evaluation of the method, but it requires additional work to address the weaknesses and limitations of the approach.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThe paper presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach for open-world entity segmentation. SOHES consists of three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the model generates initial pseudolabels by clustering visual features based on similarity and locality. In the self-instruction phase, the model trains a segmentation model on the initial pseudo-labels to refine the segmentation. In the self-correction phase, the model employs a teacher-student mutual-learning framework to further reduce noise and adapt the model for open-world segmentation. SOHES achieves state-of-the-art performance in self-supervised open-world segmentation on various datasets, including SA-1B, MS-COCO, LVIS, and ADE20K.\n\n**Strengths**\n\n1. The paper presents a novel approach for open-world entity segmentation that achieves state-of-the-art performance.\n2. The method is self-supervised, which eliminates the need for human annotations and makes it more scalable.\n3. The paper provides a thorough analysis of the method and its components, including the self-exploration, self-instruction, and self-correction phases.\n4. The paper evaluates the method on various datasets and provides a comprehensive comparison with existing methods.\n5. The paper provides a detailed explanation of the teacher-student mutual-learning framework used in the self-correction phase.\n\n**Weaknesses**\n\n1. The paper assumes that the visual features extracted from the pre-trained model are good enough for clustering and pseudo-label generation. However, this might not be the case in all scenarios.\n2. The paper uses a fixed merging threshold for clustering, which might not be optimal for all datasets and scenarios.\n3. The paper uses a simple teacher-student mutual-learning framework, which might not be effective in all cases.\n4. The paper does not provide a thorough analysis of the impact of different hyperparameters on the method's performance.\n5. The paper assumes that the ground-truth mask annotations are available for evaluation. However, in real-world scenarios, this might not be the case.\n\n**Suggestions for Improvement**\n\n1. Investigate the use of different clustering methods and merging thresholds to improve the quality of the initial pseudo-labels.\n2. Explore the use of different teacher-student mutual-learning frameworks to improve the effectiveness of the self-correction phase.\n3. Provide a thorough analysis of the impact of different hyperparameters on the method's performance.\n4. Investigate the use of other visual features or models to improve the quality of the pseudo-labels.\n5. Provide a thorough explanation of how to handle scenarios where ground-truth mask annotations are not available.\n\n**Recommendation**\n\nThe paper presents a novel approach for open-world entity segmentation that achieves state-of-the-art performance. While there are some weaknesses and limitations to the method, it has the potential to be a significant contribution to the field. I recommend that the authors address the weaknesses and limitations mentioned above and provide a thorough analysis of the method's performance and limitations. With some improvements and additional analysis, this paper has the potential to be a top-tier conference paper.",
        "gr_score_with": 0.6269035532994924,
        "gr_score_without": 0.539906103286385,
        "output_without": {
            "num_tokens_scored": 426,
            "num_green_tokens": 230,
            "green_fraction": 0.539906103286385,
            "z_score": -2.5315988425762077,
            "p_value": 0.9943228094719772
        },
        "output_with": {
            "num_tokens_scored": 394,
            "num_green_tokens": 247,
            "green_fraction": 0.6269035532994924,
            "z_score": 1.090251763997573,
            "p_value": 0.137801128426974
        }
    },
    {
        "title": "PRE-TRAINING WITH SYNTHETIC DATA HELPS OFFLINE REINFORCEMENT LEARNING",
        "abstract": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline deep reinforcement learning (DRL) models using synthetic data, rather than language data. The authors show that pre-training Decision Transformer (DT) with synthetic data generated from a one-step Markov Chain can match or even surpass the performance of pre-training with a large language corpus. They also demonstrate that pre-training Conservative Q-Learning (CQL) with synthetic Markov Decision Process (MDP) data can provide a significant performance boost. The results suggest that synthetic data can be a surprisingly effective approach to improve offline DRL performance for both transformer-based and Q-learning-based algorithms.\n\nStrengths:\n\n* The paper provides a clear and concise overview of the related work and the contributions of the paper.\n* The authors conduct extensive ablations to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* The results are presented in a clear and organized manner, with tables and figures that facilitate easy comparison of the different methods.\n* The authors provide theoretical insights into why IID data can still achieve a good performance, which adds depth to the understanding of the results.\n\nWeaknesses:\n\n* The paper assumes that the reader is familiar with the basics of DRL and reinforcement learning, which may make it difficult for non-experts to understand the paper.\n* The authors do not provide a clear explanation of why pre-training with synthetic data is effective, which is a limitation of the paper.\n* The results are based on a limited set of experiments, which may not be representative of all possible scenarios.\n* The paper does not provide a detailed comparison with other pre-training methods, such as pre-training with generic image data or using offline DRL data itself to learn representations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of why pre-training with synthetic data is effective, including a discussion of the potential benefits of using synthetic data for pre-training.\n* Conduct more extensive experiments to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* Provide a more detailed comparison with other pre-training methods, including a discussion of the advantages and disadvantages of each method.\n* Consider including more visualizations to help illustrate the results and facilitate easy comparison of the different methods.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of offline DRL by demonstrating the effectiveness of pre-training with synthetic data. The results are clear and well-presented, and the authors provide theoretical insights into why IID data can still achieve a good performance. However, the paper assumes a high level of prior knowledge and does not provide a clear explanation of why pre-training with synthetic data is effective. Overall, I would recommend this paper for publication, but with the suggestion that the authors provide more explanation and comparison with other pre-training methods.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline Deep Reinforcement Learning (DRL) models with synthetic data, specifically Markov Chain (MC) and Markov Decision Process (MDP) data. The authors demonstrate that pre-training with synthetic data can improve performance for both transformer-based and Q-learning-based offline DRL algorithms, including Decision Transformer (DT) and Conservative Q-Learning (CQL). The results show that synthetic pre-training can outperform pre-training with a large language corpus and achieve consistent performance improvement over the baseline. The authors also provide ablation studies to investigate the robustness of synthetic pre-training over different settings of the synthetic data.\n\nStrengths:\n\n1. The paper presents a simple yet effective synthetic pre-training scheme for offline DRL, challenging the previous view that language pre-training can provide unique benefits for DRL.\n2. The authors demonstrate the effectiveness of synthetic pre-training for both transformer-based and Q-learning-based offline DRL algorithms, including DT and CQL.\n3. The ablation studies provide a comprehensive understanding of the robustness of synthetic pre-training over different settings of the synthetic data.\n4. The paper provides theoretical insights into why IID data can still achieve good performance, showing that the forward dynamics objective is equivalent to finding the state centroids underlying the synthetic dataset.\n\nWeaknesses:\n\n1. The paper focuses primarily on offline DRL and does not explore online DRL scenarios, which may have different requirements and limitations for pre-training.\n2. The synthetic data generation schemes used in the paper may not be applicable to all offline DRL tasks, and it is unclear how these schemes would generalize to other domains.\n3. The paper does not provide a comprehensive analysis of the computational efficiency of synthetic pre-training, although it mentions that it requires less pre-train updates and computation time compared to language pre-training.\n\nSuggestions for Improvement:\n\n1. Explore online DRL scenarios and investigate how synthetic pre-training can be applied to these scenarios.\n2. Investigate the generalizability of synthetic data generation schemes to other offline DRL tasks and domains.\n3. Provide a more comprehensive analysis of the computational efficiency of synthetic pre-training, including a comparison of pre-train updates and computation time for different offline DRL tasks.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to pre-training offline DRL models, challenging the previous view that language pre-training can provide unique benefits for DRL. The results demonstrate the effectiveness of synthetic pre-training for both transformer-based and Q-learning-based offline DRL algorithms, and the ablation studies provide a comprehensive understanding of the robustness of synthetic pre-training over different settings of the synthetic data. The paper provides theoretical insights into why IID data can still achieve good performance, and it highlights the potential of synthetic pre-training for offline DRL tasks. Therefore, I recommend this paper for publication in a top-tier conference in the field of reinforcement learning.\n\n$$$\nNote: The review is written in a standard conference peer review format, and it follows the required structure and length guidelines.",
        "gr_score_with": 0.6781002638522428,
        "gr_score_without": 0.604221635883905,
        "output_without": {
            "num_tokens_scored": 379,
            "num_green_tokens": 229,
            "green_fraction": 0.604221635883905,
            "z_score": 0.16794802492540534,
            "p_value": 0.43331208412535493
        },
        "output_with": {
            "num_tokens_scored": 379,
            "num_green_tokens": 257,
            "green_fraction": 0.6781002638522428,
            "z_score": 3.103785156383316,
            "p_value": 0.0009553100863864003
        }
    },
    {
        "title": "CROSSQ: BATCH NORMALIZATION",
        "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ\u2019s contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces CrossQ, a lightweight algorithm for continuous control tasks that achieves superior performance by removing much of the algorithmic design complexity that was added over the years. CrossQ uses Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low update-to-data (UTD) ratio of 1. The authors present three design choices that constitute CrossQ: removing target networks, using Batch Normalization, and wider critic networks. They provide empirical investigations and hypotheses for CrossQ's success and show that CrossQ matches or surpasses the state-of-the-art algorithms in sample efficiency while being up to 4\u00d7 faster in terms of wallclock time.\n\nStrengths:\n\n* The paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks.\n* The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* The paper includes a comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* The authors provide a clear and concise description of the algorithm and its components.\n\nWeaknesses:\n\n* The paper assumes a good understanding of deep reinforcement learning and its components, which may make it challenging for readers without a background in the field.\n* The authors do not provide a detailed theoretical analysis of the algorithm's performance, which may limit its applicability to other domains.\n* The paper focuses on continuous control tasks, and it is unclear whether CrossQ can be extended to other domains, such as discrete control tasks or multi-agent systems.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* Include a more comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* Provide a detailed theoretical analysis of the algorithm's performance, including a discussion of its convergence properties and stability.\n* Consider extending CrossQ to other domains, such as discrete control tasks or multi-agent systems.\n\nRecommendation:\n\nThe paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks. The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance. While the paper has some limitations, it is a valuable contribution to the field of deep reinforcement learning. I recommend accepting this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces CrossQ, a lightweight algorithm for continuous control tasks that surpasses the current state-of-the-art in sample efficiency while maintaining a low update-to-data (UTD) ratio of 1. The authors challenge the trend of high UTD ratios and demonstrate that a UTD = 1 method can achieve competitive sample efficiency at a much lower computational cost. CrossQ uses network architectural engineering insights from deep learning to accelerate training and removes much of the algorithmic design complexity that was added over the years. The authors present a simpler yet more efficient algorithm that matches or surpasses the state-of-the-art for model-free off-policy RL for continuous control environments with state observations in sample efficiency.\n\nStrengths:\n\n* The authors present a novel and efficient algorithm for continuous control tasks that surpasses the current state-of-the-art in sample efficiency.\n* The paper provides a thorough and systematic investigation of the design choices in CrossQ and their impact on performance.\n* The authors conduct extensive experiments to evaluate the performance of CrossQ and compare it to state-of-the-art methods.\n* The paper provides a clear and concise presentation of the results and ablation studies.\n\nWeaknesses:\n\n* The paper assumes a deep understanding of deep reinforcement learning and continuous control tasks, which may make it challenging for readers without a background in these areas.\n* The authors do not provide a thorough theoretical analysis of the used BatchNorm approach in the context of RL.\n* The paper focuses on continuous control tasks and does not explore the application of CrossQ to other domains.\n* The authors do not provide a clear explanation of why the combination of removing target networks and adding BatchNorm is more effective than removing target networks alone.\n\nSuggestions for Improvement:\n\n* Provide a more detailed and theoretical analysis of the used BatchNorm approach in the context of RL.\n* Explore the application of CrossQ to other domains, such as image-based RL.\n* Provide a clearer explanation of why the combination of removing target networks and adding BatchNorm is more effective than removing target networks alone.\n* Consider including more results from ablation studies to further investigate the impact of different design choices on performance.\n\nRecommendation:\n\nThis paper presents a novel and efficient algorithm for continuous control tasks that surpasses the current state-of-the-art in sample efficiency. The authors conduct extensive experiments to evaluate the performance of CrossQ and compare it to state-of-the-art methods. However, the paper assumes a deep understanding of deep reinforcement learning and continuous control tasks, and a more thorough theoretical analysis of the used BatchNorm approach in the context of RL would be beneficial. I recommend accepting this paper for publication in a top-tier conference on machine learning and reinforcement learning.",
        "gr_score_with": 0.6506024096385542,
        "gr_score_without": 0.5682539682539682,
        "output_without": {
            "num_tokens_scored": 315,
            "num_green_tokens": 179,
            "green_fraction": 0.5682539682539682,
            "z_score": -1.1499375422788927,
            "p_value": 0.8749152015908829
        },
        "output_with": {
            "num_tokens_scored": 332,
            "num_green_tokens": 216,
            "green_fraction": 0.6506024096385542,
            "z_score": 1.8822353290537144,
            "p_value": 0.029902035409769333
        }
    },
    {
        "title": "Never Train from Scratch: FAIR COMPARISON OF LONG- SEQUENCE MODELS REQUIRES DATA-DRIVEN PRIORS",
        "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to evaluate the performance of long-range sequence models by incorporating self-supervised pretraining on the task data itself. The authors argue that training models from scratch using random initialization can lead to gross overestimation of the differences between architectures, and that pretraining with standard denoising objectives using only the downstream task data can lead to dramatic gains across multiple architectures. The paper demonstrates the effectiveness of self-supervised pretraining on the Long Range Arena (LRA) benchmark, a standard benchmark for long sequence modeling, and shows that pretraining leads to large empirical gains over previously-reported performances on LRA across a range of architectures. The authors also analyze the utility of structured parameterizations for State Space Models (SSMs) and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nStrengths:\n\n* The paper presents a novel and efficient approach to evaluate the performance of long-range sequence models.\n* The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark.\n* The paper highlights the importance of accounting for the pretraining stage while designing and evaluating novel architectures.\n* The authors provide a thorough analysis of the utility of structured parameterizations for SSMs and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nWeaknesses:\n\n* The paper assumes that the downstream task data is available, which may not be the case in all scenarios.\n* The authors do not provide a detailed analysis of the computational overhead of self-supervised pretraining.\n* The paper does not provide a thorough comparison with other pretraining methods, such as supervised pretraining or transfer learning.\n* The authors do not provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational overhead of self-supervised pretraining and discuss ways to reduce it.\n* Compare self-supervised pretraining with other pretraining methods, such as supervised pretraining or transfer learning, to demonstrate its effectiveness.\n* Provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n* Discuss the potential limitations of self-supervised pretraining and how to address them.\n\nRecommendation:\n\nThis paper presents a novel and efficient approach to evaluate the performance of long-range sequence models. The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark. While there are some weaknesses and limitations to the paper, it makes a significant contribution to the field and has the potential to impact the design and evaluation of novel architectures. I recommend this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for evaluating the performance of long-range sequence models by incorporating self-supervised pretraining on the task data itself, dubbed as Self-Pretraining (SPT). The authors argue that the conventional practice of training models from scratch using random initialization may lead to underestimation of model capabilities, particularly for long-range sequence tasks. Through a series of experiments, the authors demonstrate that SPT can lead to significant performance gains for various architectures, including Transformers and State Space Models (SSMs), on the Long Range Arena (LRA) benchmark. The results show that SPT can bridge the gap between the performance of complex architectures and simpler ones, and that the learned priors from pretraining can be as effective as manual designs.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the limitations of conventional evaluation practices for long-range sequence models and proposes a novel solution to mitigate this issue.\n2. The experiments demonstrate the efficacy of SPT on various architectures and datasets, including the LRA benchmark, which is a challenging testbed for long-range sequence models.\n3. The results provide insights into the importance of accounting for the pretraining stage while designing and evaluating novel architectures, which is a crucial aspect of deep learning research.\n\nWeaknesses:\n\n1. The paper focuses primarily on the LRA benchmark, which may not be representative of all long-range sequence tasks. It would be beneficial to extend the experiments to other datasets and evaluate the generalizability of the proposed method.\n2. The authors do not provide a detailed analysis of the computational overhead of SPT, which may be a concern for some applications. While they mention that SPT can lead to significant gains even when using the same amount of compute as the trained-from-scratch baseline, a more detailed comparison would be helpful.\n3. The paper relies on a limited set of architectures, including Transformers and SSMs, which may not be representative of all long-range sequence models. It would be beneficial to extend the experiments to other architectures, such as recurrent neural networks (RNNs) or graph neural networks (GNNs).\n\nSuggestions for Improvement:\n\n1. The authors could extend the experiments to other datasets, such as speech recognition, natural language processing, or graph-based tasks, to evaluate the generalizability of the proposed method.\n2. A more detailed analysis of the computational overhead of SPT would be beneficial, including a comparison of the computational resources required for SPT and the trained-from-scratch baseline.\n3. The authors could provide a more detailed comparison of the learned priors from SPT with manual designs, including a analysis of the degree of dependence between sequence elements, as presented in Section 3.6.\n\nRecommendation:\n\nThe paper proposes a novel method for evaluating the performance of long-range sequence models, which is a crucial aspect of deep learning research. The experiments demonstrate the efficacy of SPT on various architectures and datasets, including the LRA benchmark, which is a challenging testbed for long-range sequence models. While the paper has some limitations, such as the focus on a single benchmark and the limited set of architectures, the results provide valuable insights into the importance of accounting for the pretraining stage while designing and evaluating novel architectures. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "gr_score_with": 0.7053140096618358,
        "gr_score_without": 0.5777126099706745,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 197,
            "green_fraction": 0.5777126099706745,
            "z_score": -0.8399216329024694,
            "p_value": 0.7995238362856751
        },
        "output_with": {
            "num_tokens_scored": 414,
            "num_green_tokens": 292,
            "green_fraction": 0.7053140096618358,
            "z_score": 4.374205100574974,
            "p_value": 6.093787683774893e-06
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework combines the strengths of LLMs, which can perform high-level abstract reasoning, and automated reasoners, which can perform precise low-level reasoning. The authors present a formal calculus, LEMUR, that operates over a configuration, which is either a distinguished symbol or a tuple of a program, an assumption, and a list of properties. The calculus performs oracle calls to LLMs to propose new properties and revise them. The authors also present a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n\nStrengths:\n\n1. The paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification.\n2. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners.\n3. The paper presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n4. The authors demonstrate the practicality of the framework by implementing it as a fully automated framework and evaluating its efficiency on standard benchmark sets.\n5. The paper provides a detailed analysis of the limitations of the framework and points to future research directions to extend the framework.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs can generate correct and relevant properties, which may not always be the case.\n2. The paper does not provide a detailed analysis of the limitations of the automated reasoners and how they can be improved.\n3. The paper does not provide a detailed comparison with existing approaches to automated program verification.\n4. The paper assumes that the program verification tasks can be decomposed into smaller, more manageable sub-tasks, which may not always be the case.\n5. The paper does not provide a detailed analysis of the scalability of the framework to large programs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the limitations of the LLMs and how they can be improved.\n2. The authors should provide a detailed comparison with existing approaches to automated program verification.\n3. The authors should provide a detailed analysis of the scalability of the framework to large programs.\n4. The authors should provide a more detailed analysis of the limitations of the automated reasoners and how they can be improved.\n5. The authors should provide a more detailed evaluation of the framework on a larger set of benchmarks.\n\nRecommendation:\n\nThe paper is well-written and presents a novel and innovative framework for automated program verification. The authors provide a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners. The paper also presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure. The authors demonstrate the practicality of the framework by implementing it as a fully automated framework and evaluating its efficiency on standard benchmark sets. However, the paper assumes that the LLMs can generate correct and relevant properties, which may not always be the case. Therefore, the paper requires further improvement to address these limitations.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces LEMUR, a novel framework that combines Large Language Models (LLMs) and automated reasoners to automate program verification tasks. The authors propose a general methodology to combine the power of LLMs and automated reasoners, formally describe this methodology as a set of derivation rules, and prove its soundness. They also instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks. The authors demonstrate the efficiency of LEMUR on standard benchmark sets, including solving hard SV-COMP benchmarks, and show that it can handle programs with more than one loop. They also compare LEMUR with existing end-to-end learning-based verification approaches, such as Code2Inv, and show that it outperforms them on several metrics.\n\nStrengths:\n\n1. Novel framework: LEMUR is the first framework that provides a theoretical foundation for integrating LLMs and automated reasoners, and it demonstrates practical improvements on several benchmark sets.\n2. Soundness: The authors prove the soundness of the LEMUR calculus, which ensures that the framework produces correct results.\n3. Efficiency: LEMUR demonstrates efficiency on several benchmark sets, including solving hard SV-COMP benchmarks, and shows that it can handle programs with more than one loop.\n4. Comparison: The authors compare LEMUR with existing end-to-end learning-based verification approaches, such as Code2Inv, and show that it outperforms them on several metrics.\n\nWeaknesses:\n\n1. Limited scope: The authors primarily focus on imperative languages, and extending LEMUR to functional languages is mentioned as a future research direction.\n2. Performance: While LEMUR demonstrates efficiency on several benchmark sets, its performance may vary depending on the LLM oracle, and it may not be able to offer a significant leap in automatically verifying complex properties on real-world C libraries.\n3. Prompting: The authors rely on GPT-family models, but they mention that developing a prompting language for invariant generation with LLMs is an open challenge, and fine-tuning LLMs for invariant generation tasks may be necessary.\n\nSuggestions for Improvement:\n\n1. Extend LEMUR to functional languages: As mentioned, extending LEMUR to functional languages is a future research direction, but it would be valuable to explore this area further.\n2. Investigate more advanced LLMs: While GPT-family models demonstrate good performance, exploring more advanced LLMs, such as transformers, may lead to improved results.\n3. Develop a prompting language: As mentioned, developing a prompting language for invariant generation with LLMs is an open challenge, but it would be valuable to explore this area further to improve the performance of LEMUR.\n4. Investigate more advanced verification techniques: While LEMUR demonstrates efficiency on several benchmark sets, exploring more advanced verification techniques, such as k-induction, may lead to improved results.\n\nRecommendation:\n\nThis paper introduces a novel framework, LEMUR, that combines Large Language Models (LLMs) and automated reasoners to automate program verification tasks. The authors demonstrate the efficiency of LEMUR on several benchmark sets, including solving hard SV-COMP benchmarks, and show that it can handle programs with more than one loop. While there are some weaknesses, such as limited scope and performance variability, the authors' work demonstrates significant progress in the area of program verification, and it has the potential to be a valuable contribution to the field. Therefore, I recommend this paper for publication.",
        "gr_score_with": 0.6076555023923444,
        "gr_score_without": 0.5164473684210527,
        "output_without": {
            "num_tokens_scored": 304,
            "num_green_tokens": 157,
            "green_fraction": 0.5164473684210527,
            "z_score": -2.9734876595570388,
            "p_value": 0.9985278187700031
        },
        "output_with": {
            "num_tokens_scored": 418,
            "num_green_tokens": 254,
            "green_fraction": 0.6076555023923444,
            "z_score": 0.3196838366931434,
            "p_value": 0.3746040068814522
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents MovingParts, a novel NeRF-based method for dynamic scene reconstruction and part discovery. The authors introduce a hybrid approach that models scene motion under both the Eulerian and Lagrangian views, enabling high-quality dynamic scene reconstruction and automatic rigid part discovery. The method consists of three modules: a canonical module that models the scene geometry and appearance, an Eulerian module that records the motion of particles passing through each location, and a Lagrangian module that tracks the trajectories of specific object particles over time. The authors also introduce a cycle-consistency loss to enforce the consistency between the Eulerian and Lagrangian modules. The method is evaluated on the D-NeRF 360\u00b0 synthetic dataset and a synthetic dataset with ground-truth motion masks, demonstrating its ability to achieve high-quality reconstruction and part discovery.\n\nStrengths:\n\n1. The authors provide a thorough review of the literature on dynamic scene reconstruction and part discovery, highlighting the limitations of previous methods.\n2. The proposed hybrid approach models scene motion under both the Eulerian and Lagrangian views, enabling high-quality dynamic scene reconstruction and automatic rigid part discovery.\n3. The method introduces a cycle-consistency loss to enforce the consistency between the Eulerian and Lagrangian modules, ensuring that the two views are reciprocal.\n4. The authors provide a detailed description of the method's architecture, training procedure, and evaluation metrics.\n5. The method is evaluated on both synthetic and real-world datasets, demonstrating its ability to achieve high-quality reconstruction and part discovery.\n\nWeaknesses:\n\n1. The method requires a large amount of training data, which may not be feasible for all applications.\n2. The authors assume that the scene is piece-wise rigid, which may not hold for all dynamic scenes.\n3. The method relies on a cycle-consistency loss, which may not be effective in all cases.\n4. The authors do not provide a thorough analysis of the method's limitations and potential failures.\n5. The method's scalability to very long sequences is not addressed.\n\nSuggestions for Improvement:\n\n1. The authors could provide a more detailed analysis of the method's limitations and potential failures.\n2. The method's scalability to very long sequences could be addressed through the development of a unified long sequence encoding-decoding scheme.\n3. The authors could provide more insight into the role of the cycle-consistency loss in ensuring the consistency between the Eulerian and Lagrangian modules.\n4. The method's performance on more challenging dynamic scenes, such as those with non-rigid motion or complex geometry, could be evaluated.\n5. The authors could provide more detail on the method's applications in downstream tasks, such as object tracking, structured scene modeling, and editing.\n\nRecommendation:\n\nThe paper presents a novel method for dynamic scene reconstruction and part discovery, which has the potential to achieve high-quality reconstruction and automatic rigid part discovery. However, the method requires a large amount of training data, assumes a piece-wise rigid scene, and relies on a cycle-consistency loss. To address these limitations, the authors could provide a more detailed analysis of the method's limitations and potential failures, develop a unified long sequence encoding-decoding scheme, and provide more insight into the role of the cycle-consistency loss. With these improvements, the method has the potential to achieve state-of-the-art results in dynamic scene reconstruction and part discovery.\n$$$",
        "gr_score_with": 0.7078947368421052,
        "gr_score_without": 0.596551724137931,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 173,
            "green_fraction": 0.596551724137931,
            "z_score": -0.11970297467719694,
            "p_value": 0.5476407780728971
        },
        "output_with": {
            "num_tokens_scored": 380,
            "num_green_tokens": 269,
            "green_fraction": 0.7078947368421052,
            "z_score": 4.2934303399092615,
            "p_value": 8.796676537370974e-06
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks with small initialization. The authors consider a training dataset with well-separated input vectors and provide a careful analysis of the neurons' directional dynamics. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nStrengths:\n\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5, which is a significant improvement over previous work.\n3. The paper provides a quantitative bound on the time it takes for good alignment to be achieved, which is O(logn\u221a\u00b5).\n4. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nWeaknesses:\n\n1. The paper assumes that the training dataset has well-separated input vectors, which may not be a realistic assumption in practice.\n2. The authors use a balanced initialization scheme, which may not be a common initialization scheme in practice.\n3. The paper does not provide a detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n2. The paper should be extended to consider more realistic initialization schemes and training datasets.\n3. The authors should provide more numerical experiments to validate their theoretical findings.\n\nRecommendation:\n\nThis paper provides a significant contribution to the understanding of the dynamics of gradient flow for training two-layer ReLU networks with small initialization. The authors' analysis is thorough and provides a quantitative bound on the time it takes for good alignment to be achieved. However, the paper assumes a balanced initialization scheme and well-separated input vectors, which may not be realistic in practice. Therefore, I recommend that the authors extend their analysis to more realistic initialization schemes and training datasets.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper studies the problem of training a two-layer ReLU network with small initialization using gradient flow. It considers a binary classification task with a dataset where any pair of data points with the same label are positively correlated and any pair with different labels are negatively correlated. The paper provides a complete analysis of the dynamics of gradient flow, showing that during the early phase of training, the neurons in the first layer try to align with either the positive data or the negative data, depending on the corresponding weight on the second layer. It provides a bound on the time it takes for all neurons to achieve good alignment with the input data and shows that after the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank.\n\nStrengths:\n1. The paper provides a complete analysis of the dynamics of gradient flow for a two-layer ReLU network with small initialization.\n2. It provides a bound on the time it takes for all neurons to achieve good alignment with the input data.\n3. The paper shows that after the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank.\n4. The numerical experiment on the MNIST dataset illustrates the theoretical findings.\n\nWeaknesses:\n1. The paper assumes a binary classification task with a dataset where any pair of data points with the same label are positively correlated and any pair with different labels are negatively correlated.\n2. The analysis is based on the assumption that the gradient flow dynamics are approximately decoupled, which may not hold in general.\n3. The paper assumes that the initialization is sufficiently small, which may not be the case in practice.\n4. The analysis is limited to two-layer ReLU networks and may not be applicable to deeper networks or networks with different activation functions.\n\nSuggestions for Improvement:\n1. Consider more general datasets and classification tasks.\n2. Relax the assumption that the gradient flow dynamics are approximately decoupled.\n3. Provide more empirical evidence to support the theoretical findings.\n4. Consider the implications of the analysis for deeper networks or networks with different activation functions.\n\nRecommendation:\nThe paper provides a significant contribution to the understanding of the dynamics of gradient flow for two-layer ReLU networks with small initialization. The analysis provides a bound on the time it takes for all neurons to achieve good alignment with the input data and shows that after the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. However, the analysis is limited to two-layer ReLU networks and assumes a binary classification task with a dataset where any pair of data points with the same label are positively correlated and any pair with different labels are negatively correlated. Therefore, the paper would benefit from more empirical evidence and consideration of more general datasets and classification tasks.\n\nI would recommend this paper for publication in a reputable journal or conference, but with the suggestion that the authors address the limitations and weaknesses mentioned above.",
        "gr_score_with": 0.7133956386292835,
        "gr_score_without": 0.6388059701492538,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 214,
            "green_fraction": 0.6388059701492538,
            "z_score": 1.4499960975600945,
            "p_value": 0.07352980373059434
        },
        "output_with": {
            "num_tokens_scored": 321,
            "num_green_tokens": 229,
            "green_fraction": 0.7133956386292835,
            "z_score": 4.147250961904799,
            "p_value": 1.6824558130998172e-05
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper addresses a significant challenge in smart city applications, namely data scarcity in many cities and regions.\n2. The proposed framework is novel and effective, leveraging pre-training paradigm to achieve effective fine-grained spatio-temporal knowledge transfer across different cities.\n3. The framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The authors provide a comprehensive experimental evaluation, demonstrating the effectiveness of their framework on multiple real-world datasets.\n\nWeaknesses:\n\n1. The paper assumes access to a collection of optimized neural network parameters from source cities, which may not be feasible in practice.\n2. The framework relies on the quality of the prompts, which can be challenging to design and optimize.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed framework.\n4. The authors do not discuss potential limitations and challenges of scaling the framework to larger cities or more complex spatio-temporal prediction tasks.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically generating high-quality prompts, such as leveraging large language models or transfer learning techniques.\n2. Explore ways to reduce the computational complexity of the framework, such as using more efficient diffusion models or approximations.\n3. Conduct a more detailed analysis of the framework's scalability and limitations, including its performance on larger cities or more complex spatio-temporal prediction tasks.\n4. Provide more insight into the interpretability of the framework's generated neural networks and the transferable knowledge they represent.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for spatio-temporal few-shot learning with urban knowledge transfer. While there are some limitations and challenges to be addressed, the framework has the potential to revolutionize smart city applications in data-scarce environments. I recommend accepting the paper for publication in its current form, with the suggestion that the authors address the limitations and challenges mentioned above in future work.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary:**\n\nThe paper proposes a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages pre-training paradigm to achieve effective fine-grained spatio-temporal knowledge transfer across different cities. GPD employs a Transformer-based denoising diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate the effectiveness of GPD on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\n**Strengths:**\n\n1. The paper addresses a critical problem in smart city applications, i.e., spatio-temporal few-shot learning with data scarcity.\n2. The proposed GPD framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n3. The authors provide extensive experimental results on multiple real-world datasets, demonstrating the effectiveness of GPD.\n4. The paper explores various conditioning strategies and prompt selection methods, which is valuable for understanding the limitations and potential improvements of the framework.\n\n**Weaknesses:**\n\n1. The paper assumes that the source cities have abundant data, which may not be the case in many real-world scenarios.\n2. The authors do not provide a thorough analysis of the computational cost and scalability of the GPD framework.\n3. The paper focuses primarily on traffic speed prediction and crowd flow prediction, and it is unclear whether the framework can be extended to other spatio-temporal prediction tasks.\n4. The authors do not provide a clear explanation of the choice of hyperparameters and the sensitivity analysis of the framework.\n\n**Suggestions for Improvement:**\n\n1. Investigate the effectiveness of GPD in scenarios with limited data in source cities.\n2. Provide a more detailed analysis of the computational cost and scalability of the GPD framework.\n3. Explore the extension of the GPD framework to other spatio-temporal prediction tasks.\n4. Perform a thorough sensitivity analysis of the framework to the choice of hyperparameters.\n5. Consider exploring more sophisticated methods for prompt selection, such as leveraging large language models to capture the unique characteristics of cities.\n\n**Recommendation:**\n\nThe paper presents a novel and effective generative pre-training framework for spatio-temporal few-shot learning with urban knowledge transfer. While there are some limitations and areas for improvement, the paper contributes significantly to the field of urban transfer learning and has the potential to revolutionize smart city applications in data-scarce environments. I recommend the paper for publication in a reputable conference or journal.",
        "gr_score_with": 0.6260387811634349,
        "gr_score_without": 0.5913978494623656,
        "output_without": {
            "num_tokens_scored": 372,
            "num_green_tokens": 220,
            "green_fraction": 0.5913978494623656,
            "z_score": -0.33848216686934884,
            "p_value": 0.6325000681806484
        },
        "output_with": {
            "num_tokens_scored": 361,
            "num_green_tokens": 226,
            "green_fraction": 0.6260387811634349,
            "z_score": 1.0100568165096824,
            "p_value": 0.15623403495450683
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of widely used, open-source language models (LLMs) to prompt formatting choices, finding that performance spreads across tasks, models, and number of few-shot examples. They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats, allowing for the efficient exploration of the space. The authors demonstrate that FORMATSPREAD can be used to estimate the performance spread across prompt formatting choices, and that this spread is large regardless of model choice, even when increasing model size, number of few-shots, or using instruction tuning.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of using these models in real-world applications.\n2. The authors propose a novel grammar that defines the space of plausible prompt formats and semantic equivalence between formats, which allows for the efficient exploration of the space.\n3. The paper introduces FORMATSPREAD, an algorithm that can be used to estimate the performance spread across prompt formatting choices, which is a valuable tool for researchers and practitioners.\n4. The authors provide a thorough evaluation of the algorithm on a large set of tasks and models, demonstrating its effectiveness in estimating performance spread.\n\nWeaknesses:\n\n1. The paper assumes that the grammar is manually constructed, which may not be practical for large-scale applications.\n2. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation for large-scale applications.\n3. The paper focuses on classification tasks and does not explore other types of tasks, such as open-ended text generation.\n4. The authors do not provide a detailed analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nSuggestions for Improvement:\n\n1. Develop a more automated method for constructing the grammar, which could be based on machine learning algorithms or other techniques.\n2. Provide a more detailed analysis of the computational cost of FORMATSPREAD and explore ways to reduce it.\n3. Explore the use of FORMATSPREAD on other types of tasks, such as open-ended text generation.\n4. Conduct a more thorough analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of natural language processing and language modeling, and the authors should be commended for their thorough analysis of the sensitivity of LLMs to prompt formatting choices. However, to make the paper more impactful, the authors should address the limitations mentioned above and provide a more detailed analysis of the algorithm's computational cost and robustness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, a tool designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors aim to address the issue of prompt formatting variance, which can significantly influence the performance of large language models (LLMs). They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats. FORMATSPREAD uses Bayesian optimization to identify the expected performance range with low additional computational cost. The authors demonstrate the effectiveness of FORMATSPREAD by evaluating several widely-used open-source LLMs for classification tasks in few-shot learning settings. They find significant performance spread across tasks, with a median spread of 7.5 accuracy points across choices in the model and the number of few-shot examples. They also show that the performance spread is not eliminated by increasing model size, number of few-shot examples, or instruction tuning.\n\nStrengths:\n\n1. The paper provides a thorough analysis of the importance of prompt formatting variance in LLM performance.\n2. The authors propose a well-defined grammar for describing the space of plausible prompt formats.\n3. FORMATSPREAD is a useful tool for efficiently searching the space of prompt formats under a user-specified computational budget.\n4. The authors demonstrate the effectiveness of FORMATSPREAD on several widely-used open-source LLMs.\n5. The paper highlights the need for reporting a range of performance across plausible formats when evaluating LLMs.\n\nWeaknesses:\n\n1. The paper assumes a specific grammar for describing prompt formats, which might not be suitable for all domains or applications.\n2. FORMATSPREAD relies on Bayesian optimization, which can be computationally expensive for large search spaces.\n3. The authors do not provide a detailed comparison with other methods for prompt optimization.\n4. The paper focuses on classification tasks, and it is unclear whether the results generalize to other types of tasks.\n5. The authors do not discuss the potential risks of relying on FORMATSPREAD for prompt optimization, such as overfitting or biased results.\n\nSuggestions for Improvement:\n\n1. Consider incorporating more advanced grammar models or domain-specific grammars to describe prompt formats.\n2. Develop more efficient optimization algorithms for FORMATSPREAD, such as gradient-based methods or hybrid approaches.\n3. Provide a more detailed comparison with other methods for prompt optimization, including gradient-based methods and edit-based enumeration.\n4. Extend the evaluation to other types of tasks, such as text generation or machine translation.\n5. Discuss the potential risks of relying on FORMATSPREAD for prompt optimization and propose mitigation strategies.\n\nRecommendation:\n\nThis paper provides a valuable contribution to the field of prompt optimization for LLMs. The authors propose a well-defined grammar for describing prompt formats and a useful tool for efficiently searching the space of prompt formats under a user-specified computational budget. While the paper has some limitations, it highlights the importance of prompt formatting variance in LLM performance and provides a useful starting point for further research. I recommend this paper for publication in a top-tier conference or journal in the field of natural language processing.",
        "gr_score_with": 0.7815533980582524,
        "gr_score_without": 0.7288135593220338,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 258,
            "green_fraction": 0.7288135593220338,
            "z_score": 4.947348641521393,
            "p_value": 3.761558020668516e-07
        },
        "output_with": {
            "num_tokens_scored": 412,
            "num_green_tokens": 322,
            "green_fraction": 0.7815533980582524,
            "z_score": 7.522422341196502,
            "p_value": 2.6885278490425927e-14
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem induced by a soft DSF. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nStrengths:\n\n* The paper proposes a novel sorting network that learns to sort high-dimensional inputs.\n* The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n* The authors provide a detailed analysis of the error accumulation problem and propose an error-free DSF to resolve it.\n* The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nWeaknesses:\n\n* The paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications.\n* The proposed method may not be effective for sorting low-dimensional inputs or inputs with clear ordering information.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n* The paper assumes that the supervision is provided in the form of permutation matrices, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The paper should discuss the limitations of the proposed method and provide a more detailed analysis of the cases where it may not be effective.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method on diverse sorting benchmarks.\n* The paper should discuss the potential negative societal impact of the proposed method and provide a more detailed analysis of the cases where it may be considered as an unethical use of artificial intelligence.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, the paper requires further research to address the limitations and potential negative societal impact of the proposed method.\n\nOverall, the paper is well-written and provides a novel contribution to the field of sorting networks. However, it requires further research to address the limitations and potential negative societal impact of the proposed method.\n\nRecommendation: Accept\n\n$$$\nNote: The paper requires further research to address the limitations and potential negative societal impact of the proposed method. Therefore, the recommendation is to accept the paper with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to map high-dimensional inputs to ordinal scores using a permutation-equivariant Transformer network with multi-head attention. The authors address the challenge of making the sorting network differentiable by introducing an error-free differentiable swap function (DSF) that does not accumulate errors during sorting operations. The proposed method is evaluated on diverse sorting benchmarks, including multi-digit images and image fragments, and is shown to outperform or be comparable to existing baseline methods.\n\nStrengths:\n\n1. The paper addresses a significant challenge in sorting networks, namely the accumulation of errors during sorting operations, and proposes a novel solution that is both theoretically sound and empirically effective.\n2. The authors demonstrate the versatility of the proposed method on a range of sorting benchmarks, including multi-digit images and image fragments.\n3. The use of a permutation-equivariant Transformer network with multi-head attention is a novel contribution that allows the sorting network to capture long-term dependencies and reduce inductive biases.\n\nWeaknesses:\n\n1. The paper assumes a well-defined ordering of the inputs, which may not be the case in all applications. The authors do not discuss how to handle ambiguous or unordered inputs.\n2. The proposed method requires a significant amount of training data, which may not be available in all applications.\n3. The paper does not provide a thorough comparison with existing methods, particularly in terms of computational complexity and training time.\n\nSuggestions for Improvement:\n\n1. The authors could explore methods to handle ambiguous or unordered inputs, such as using a ranking loss function or incorporating additional supervision signals.\n2. The paper could benefit from a more detailed comparison with existing methods, including a discussion of the computational complexity and training time of the proposed method.\n3. The authors could provide more insights into the behavior of the proposed method, such as analyzing the effect of the balancing hyperparameter or the steepness of the sigmoid function on the sorting performance.\n\nRecommendation:\n\nThe paper is well-written, well-organized, and presents a novel contribution to the field of sorting networks. The proposed method is theoretically sound and empirically effective, and the authors demonstrate its versatility on a range of sorting benchmarks. However, the paper could benefit from a more detailed comparison with existing methods and a discussion of the limitations and challenges of the proposed approach.\n\nOverall, I would recommend this paper for publication, with the hope that the authors will address the suggested improvements to strengthen the paper's contribution.\n\n$$$",
        "gr_score_with": 0.7335243553008596,
        "gr_score_without": 0.6333333333333333,
        "output_without": {
            "num_tokens_scored": 270,
            "num_green_tokens": 171,
            "green_fraction": 0.6333333333333333,
            "z_score": 1.118188718910112,
            "p_value": 0.1317432006662287
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 256,
            "green_fraction": 0.7335243553008596,
            "z_score": 5.0919244518170865,
            "p_value": 1.7722367123766447e-07
        }
    },
    {
        "title": "WEAKER MVI CONDITION: EXTRAGRADIENT METH-",
        "abstract": "This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems. One of these follows the weak Minty variational inequality (weak MVI), which is motivated by relaxing Minty variational inequality (MVI) without compromising convergence guarantee of extragradient algorithm. Existing extragradient-type algorithms involve one exploration step and one update step per iteration. We analyze the algorithms with multiple exploration steps and show that current assumption can be further relaxed when more exploration is introduced. Furthermore, we design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors focus on the weak Minty variational inequality (weak MVI) problem, which is a relaxation of the Minty variational inequality without compromising convergence guarantees. They analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. Furthermore, they design an adaptive algorithm that explores until the optimal improvement is achieved. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases.\n\nStrengths:\n\n1. The paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm.\n2. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced.\n3. The paper designs an adaptive algorithm that explores until the optimal improvement is achieved.\n4. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms.\n5. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which is a significant improvement.\n\nWeaknesses:\n\n1. The paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, which may not be the case in practice.\n2. The authors use a tolerance \u03b52 to prevent the algorithm from incorrect convergence, which may not be necessary in some cases.\n3. The paper does not provide a detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n4. The authors do not provide a theoretical analysis of the convergence rate of the adaptive algorithm.\n5. The paper assumes that the weak MVI condition holds, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n2. Theoretically analyze the convergence rate of the adaptive algorithm.\n3. Relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave.\n4. Provide a more detailed explanation of the tolerance \u03b52 and its role in preventing incorrect convergence.\n5. Provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nRecommendation:\n\nThis paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases. However, the paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, and uses a tolerance \u03b52 to prevent the algorithm from incorrect convergence. Therefore, I recommend that the authors provide a more detailed comparison with other algorithms, theoretically analyze the convergence rate of the adaptive algorithm, relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave, provide a more detailed explanation of the tolerance \u03b52, and provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nOverall, this paper provides a significant contribution to the field of optimization and machine learning, and I recommend that it be published in a reputable journal.\n\nRating: 8/10\n\nRecommendation: Publish",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for solving weak Minty variational inequality (MVI) problems by generalizing the extragradient algorithm. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. They also design an adaptive algorithm that explores until the optimal improvement is achieved. The proposed algorithm effectively tackles cyclic behaviors and provides a bound analysis on the framework of multi-step extrapolation EG+ algorithms. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L and capture past algorithms as special cases.\n\nStrengths:\n\n* The paper proposes a novel framework for solving weak Minty MVI problems by generalizing the extragradient algorithm.\n* The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced.\n* The proposed algorithm effectively tackles cyclic behaviors.\n* The authors provide a bound analysis on the framework of multi-step extrapolation EG+ algorithms.\n\nWeaknesses:\n\n* The paper assumes that the function f(x,y) is smooth (possibly nonconvex-nonconcave), which may not be the case in all practical problems.\n* The authors use a tolerance \u03b52 to recover GDA when the algorithm potentially stagnates due to intractable local environment, which may not be desirable in all cases.\n* The paper does not provide a detailed comparison with other existing methods for solving weak Minty MVI problems.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison with other existing methods for solving weak Minty MVI problems.\n* The authors should provide a more thorough analysis of the assumptions made in the paper, such as the smoothness of the function f(x,y).\n* The authors should consider using a more robust method for recovering GDA when the algorithm potentially stagnates due to intractable local environment.\n\nRecommendation:\n\nThe paper proposes a novel framework for solving weak Minty MVI problems by generalizing the extragradient algorithm. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. The proposed algorithm effectively tackles cyclic behaviors and provides a bound analysis on the framework of multi-step extrapolation EG+ algorithms. However, the paper assumes that the function f(x,y) is smooth (possibly nonconvex-nonconcave), which may not be the case in all practical problems. The authors should provide a more detailed comparison with other existing methods for solving weak Minty MVI problems. Overall, the paper is well-written and provides a novel contribution to the field of optimization.\n\nRating: 8/10\n\nRecommendation for publication: Yes",
        "gr_score_with": 0.5611510791366906,
        "gr_score_without": 0.5267857142857143,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 177,
            "green_fraction": 0.5267857142857143,
            "z_score": -2.739247350209009,
            "p_value": 0.9969209987778734
        },
        "output_with": {
            "num_tokens_scored": 278,
            "num_green_tokens": 156,
            "green_fraction": 0.5611510791366906,
            "z_score": -1.3220338943502388,
            "p_value": 0.9069215677084858
        }
    },
    {
        "title": "TAIL: TASK-SPECIFIC ADAPTERS FOR IMITATION LEARNING WITH LARGE PRETRAINED MODELS",
        "abstract": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly due to data scarcity and computational challenges associated with training or fine-tuning large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to a stream of new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques\u2014e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)\u2014in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained models to new control tasks. The authors explore parameter-efficient fine-tuning (PEFT) techniques, specifically Low-Rank Adaptation (LoRA), and demonstrate their potential in enhancing adaptation efficiency, mitigating catastrophic forgetting, and ensuring robust performance across diverse tasks. The proposed framework is evaluated on the LIBERO robotic manipulation continual learning benchmark, showcasing its effectiveness in adapting to new tasks while preserving performance on previous tasks.\n\nStrengths:\n\n1. The paper proposes a novel framework for efficient adaptation of large pretrained models, addressing the challenges of catastrophic forgetting and overfitting in continual learning scenarios.\n2. The authors provide a comprehensive exploration of PEFT techniques, including LoRA, Bottleneck Adapter, and Prefix Tuning, and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting.\n3. The paper presents extensive experimental results on the LIBERO benchmark, showcasing the effectiveness of TAIL in adapting to new tasks while preserving performance on previous tasks.\n4. The authors provide a thorough analysis of the strengths and limitations of each adaptation strategy, providing valuable insights for future research.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the pretrained model, which may not be optimal for all applications.\n2. The authors do not provide a detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. The paper assumes a fixed dataset for pretraining, which may not be representative of all real-world scenarios.\n4. The authors do not provide a detailed analysis of the robustness of TAIL to noisy or incomplete data.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of varying the architecture of the pretrained model on the performance of TAIL.\n2. Conduct a more detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. Evaluate the robustness of TAIL to noisy or incomplete data.\n4. Explore the application of TAIL to other domains, such as reinforcement learning.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for efficient adaptation of large pretrained models in continual learning scenarios. The authors provide a comprehensive exploration of PEFT techniques and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting. While there are some limitations to the paper, the proposed framework has the potential to significantly impact the field of artificial intelligence and robotics. Therefore, I recommend acceptance of the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained models to new control tasks. TAIL incorporates lightweight adapter modules into pretrained decision-making models and compares efficient adaptation techniques, including parameter-efficient fine-tuning (PEFT) methods, in a continual learning setting. The authors explore the potential of adapters, prefix tuning, and low-rank adaptation (LoRA) in fostering efficient and continual adaptation in large pretrained decision-making models. Experiments demonstrate that TAIL with LoRA achieves the best post-adaptation performance, preserving the original pretrained representations while being resilient against overfitting in the limited-data regime.\n\nStrengths:\n\n* The paper addresses a crucial gap in research on efficient and continual adaptation for pretrained decision models.\n* TAIL is a comprehensive framework that incorporates lightweight adapter modules and compares efficient adaptation techniques in a continual learning setting.\n* The authors provide extensive experiments and comparisons with prevalent parameter-efficient fine-tuning techniques and adaptation baselines.\n* LoRA is shown to be an effective adaptation method, achieving the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning.\n\nWeaknesses:\n\n* The paper assumes that the pretrained model has been pre-trained on a large dataset, which may not be the case in many real-world scenarios.\n* The experiments are limited to a specific benchmark (LIBERO) and may not generalize to other domains or tasks.\n* The paper does not provide a clear comparison with other adaptation methods that are specifically designed for control tasks.\n* The authors do not provide a detailed analysis of the trade-offs between adaptation efficiency, model capacity, and catastrophic forgetting.\n\nSuggestions for Improvement:\n\n* Provide more details on the pre-training process and how it affects the performance of TAIL.\n* Conduct experiments on other benchmarks and domains to demonstrate the generalizability of TAIL.\n* Compare TAIL with other adaptation methods specifically designed for control tasks, such as reinforcement learning-based methods.\n* Provide a more detailed analysis of the trade-offs between adaptation efficiency, model capacity, and catastrophic forgetting.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of efficient and continual adaptation for pretrained decision models. The proposed TAIL framework and LoRA adaptation method show promising results and have the potential to be applied in real-world scenarios. However, further research is needed to address the limitations and weaknesses mentioned above. I recommend this paper for publication in a top-tier conference or journal, with the suggestion that the authors address the above concerns and provide more details on the pre-training process and generalizability of TAIL.\n\n$$$",
        "gr_score_with": 0.6658415841584159,
        "gr_score_without": 0.6330275229357798,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 207,
            "green_fraction": 0.6330275229357798,
            "z_score": 1.2192841339687959,
            "p_value": 0.1113681842136035
        },
        "output_with": {
            "num_tokens_scored": 404,
            "num_green_tokens": 269,
            "green_fraction": 0.6658415841584159,
            "z_score": 2.7015645129414403,
            "p_value": 0.0034507045087517167
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel regularization technique, zero-mean regularization, for spectral contrastive learning (SpeCL). The authors argue that SpeCL has two limitations: (i) it requires the orthogonality of representations between negative pairs, and (ii) it does not account for wrong connections in the positive-pair graph. To address these limitations, they introduce an additive factor, \u03c4, into the term of SpeCL involving negative pairs. This modification is equivalent to regularizing the mean of representations to be zero, referred to as zero-mean regularization. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem and the proposed solution.\n2. The authors provide a thorough theoretical analysis of the benefits of zero-mean regularization in both unsupervised and supervised scenarios.\n3. The paper presents extensive experimental results to verify the effectiveness of zero-mean regularization on various benchmark datasets.\n4. The authors provide a clear explanation of the implications of zero-mean regularization on the structure of the learned representations.\n\nWeaknesses:\n\n1. The paper assumes a specific form of the positive-pair graph, which may not be realistic in all scenarios.\n2. The authors do not provide a clear explanation of how to choose the value of \u03c4 in practice.\n3. The paper does not discuss the computational cost of incorporating zero-mean regularization into the SpeCL algorithm.\n4. The authors do not provide a comparison with other regularization techniques, such as dropout or weight decay.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the assumptions made about the positive-pair graph and how they affect the results.\n2. Discuss the practical implications of choosing the value of \u03c4 and provide guidance on how to select it.\n3. Investigate the computational cost of incorporating zero-mean regularization into the SpeCL algorithm and discuss potential ways to reduce it.\n4. Compare the performance of zero-mean regularization with other regularization techniques, such as dropout or weight decay, to provide a more comprehensive evaluation of its benefits.\n\nRecommendation:\n\nThe paper presents a novel and effective regularization technique for spectral contrastive learning. The theoretical analysis and experimental results demonstrate the benefits of zero-mean regularization in mitigating adverse effects of wrong connections. However, the paper could be improved by providing more detailed explanations of the assumptions made about the positive-pair graph and the practical implications of choosing the value of \u03c4. Additionally, a comparison with other regularization techniques would provide a more comprehensive evaluation of the benefits of zero-mean regularization. Overall, the paper is well-written and presents a significant contribution to the field of contrastive learning.",
        "peer_review_with_watermark": "$$$Paper Summary:\n\nThe paper proposes zero-mean regularization, a modification to the spectral contrastive loss, to alleviate the limitations of spectral contrastive learning. The authors argue that spectral contrastive loss requires orthogonality of representations between negative pairs, which can be relaxed to enhance discriminativeness. Additionally, the pairwise similarities among representations learned by spectral contrastive loss are determined by the connections within the positive-pair graph, which can be incorrect due to noisy views or labels. The zero-mean regularization introduces an additional factor \u03c4 into the term of spectral contrastive loss involving negative pairs, which relaxes the orthogonality constraint and implicitly reduces the positive-pair weights uniformly. The authors provide theoretical investigations in both unsupervised and supervised scenarios, including unsupervised domain adaptation and supervised learning with noisy labels, to demonstrate the effectiveness of zero-mean regularization.\n\nStrengths:\n\n1. The paper proposes a novel modification to spectral contrastive loss, which addresses the limitations of the existing approach.\n2. The authors provide theoretical foundations for the zero-mean regularization, including proof of the benefits in unsupervised domain adaptation and supervised learning with noisy labels.\n3. The experimental results on various benchmark datasets demonstrate the effectiveness of zero-mean regularization in enhancing contrastive learning.\n4. The paper highlights the potential applications of zero-mean regularization in various domains, including self-supervised learning, unsupervised domain adaptation, and learning with noisy labels.\n\nWeaknesses:\n\n1. The paper assumes that the data distribution is known, which may not be the case in practice.\n2. The theoretical analysis assumes a simplified setting, which may not capture the complexities of real-world datasets.\n3. The experimental results focus on a limited set of datasets and tasks, which may not generalize to other domains.\n4. The paper does not provide a clear understanding of the relationship between the hyperparameters \u03c4 and the performance of the algorithm.\n\nSuggestions for Improvement:\n\n1. The authors should provide more details on how to choose the hyperparameters \u03c4 and the trade-off between the benefits of zero-mean regularization and the potential costs of over-regularization.\n2. The theoretical analysis should be extended to more complex settings, including non-linear data distributions and non-simplified models.\n3. The experimental results should be extended to a wider range of datasets and tasks to demonstrate the robustness of the approach.\n4. The paper should provide a clearer understanding of the relationship between the zero-mean regularization and the neural collapse phenomenon.\n\nRecommendation:\n\nThe paper proposes a novel modification to spectral contrastive loss, which addresses the limitations of the existing approach. The theoretical foundations and experimental results demonstrate the effectiveness of zero-mean regularization in enhancing contrastive learning. However, the paper assumes a simplified setting, which may not capture the complexities of real-world datasets. Therefore, I recommend accepting the paper, but with the suggestion to address the limitations mentioned above in future work.$$",
        "gr_score_with": 0.648876404494382,
        "gr_score_without": 0.5612535612535613,
        "output_without": {
            "num_tokens_scored": 351,
            "num_green_tokens": 197,
            "green_fraction": 0.5612535612535613,
            "z_score": -1.4815845619082153,
            "p_value": 0.9307745653465637
        },
        "output_with": {
            "num_tokens_scored": 356,
            "num_green_tokens": 231,
            "green_fraction": 0.648876404494382,
            "z_score": 1.882605607277968,
            "p_value": 0.029876917961561847
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline, RDESIGN, which tackles the challenging task of designing RNA sequences that fold into a specified tertiary structure. The authors argue that existing approaches, primarily developed for protein design, are not directly applicable to RNA due to its structural complexity and data scarcity. To address this, the authors design a comprehensive RNA tertiary structure modeling approach and a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster and sample levels. The proposed method incorporates extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The authors demonstrate the effectiveness of RDESIGN through extensive experiments across standard RNA design benchmarks and generalization ability assessments, providing a reliable pipeline for future research in this area.\n\nStrengths:\n\n1. The paper tackles a critical problem in RNA design, which is the design of RNA sequences that fold into a specified tertiary structure.\n2. The authors propose a novel data-driven RNA design pipeline, RDESIGN, which incorporates a comprehensive RNA tertiary structure modeling approach and a hierarchical data-efficient representation learning framework.\n3. The proposed method demonstrates superior performance compared to existing baseline methods, including sequence-based and tertiary structure-based models.\n4. The authors provide a thorough evaluation of RDESIGN on various RNA design benchmarks, including the Rfam and RNA-Puzzles datasets.\n5. The paper provides a clear and concise explanation of the proposed method and its components, making it easy for readers to understand and replicate the work.\n\nWeaknesses:\n\n1. The paper assumes that the target RNA tertiary structure is known, which might not be the case in real-world applications.\n2. The authors do not provide a thorough evaluation of RDESIGN on real-world RNA design problems, which might limit its practical applications.\n3. The paper does not discuss the limitations of the proposed method, such as its scalability and computational requirements.\n4. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with multiple target structures.\n5. The paper does not provide a thorough comparison with other RNA design methods, such as evolutionary algorithms and machine learning-based approaches.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough evaluation of RDESIGN on real-world RNA design problems, including the design of RNA sequences with multiple target structures.\n2. The paper should discuss the limitations of the proposed method, such as its scalability and computational requirements.\n3. The authors should provide a clear explanation of how the proposed method can be extended to design RNA sequences with multiple target structures.\n4. The paper should provide a thorough comparison with other RNA design methods, such as evolutionary algorithms and machine learning-based approaches.\n5. The authors should provide a clear explanation of the advantages and disadvantages of the proposed method compared to existing RNA design methods.\n\nRecommendation:\n\nIn summary, the paper proposes a novel data-driven RNA design pipeline, RDESIGN, which demonstrates superior performance compared to existing baseline methods. However, the paper assumes that the target RNA tertiary structure is known, which might not be the case in real-world applications. The authors should provide a more thorough evaluation of RDESIGN on real-world RNA design problems and discuss the limitations of the proposed method. With these improvements, the paper can provide a more comprehensive evaluation of RDESIGN and its practical applications.",
        "gr_score_with": 0.7310924369747899,
        "gr_score_without": 0.6612903225806451,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 205,
            "green_fraction": 0.6612903225806451,
            "z_score": 2.2029235923512354,
            "p_value": 0.013800067235067095
        },
        "output_with": {
            "num_tokens_scored": 357,
            "num_green_tokens": 261,
            "green_fraction": 0.7310924369747899,
            "z_score": 5.0561596155618105,
            "p_value": 2.1389166874709688e-07
        }
    },
    {
        "title": "ONE-TO-MANY POLICY TRANSFER",
        "abstract": "We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named Meta-Evolve that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2\u00d7 and one-to-six transfer of agile locomotion policy by 2.4\u00d7 in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers. Supplementary videos available at the project website: https://sites.google.com/view/meta-evolve.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for transferring an expert policy from a source robot to multiple target robots. The method uses continuous robot evolution to efficiently transfer the policy through a tree-structured evolutionary robot sequence. The paper presents a heuristic approach to determine an optimized robot evolution tree and shows that the method can significantly outperform one-to-one policy transfer baselines in terms of simulation cost and training iterations.\n\nStrengths:\n\n* The paper introduces a new research problem of transferring an expert policy from a source robot to multiple target robots, which is an important problem in robotics.\n* The method proposed, Meta-Evolve, is a novel and efficient approach to solve this problem.\n* The paper presents a heuristic approach to determine the robot evolution tree, which is a challenging problem.\n* The experiments conducted on Hand Manipulation Suite tasks and an agile locomotion task show that the method can significantly outperform one-to-one policy transfer baselines.\n\nWeaknesses:\n\n* The paper assumes that the source and target robots can be matched in their kinematic tree topology, which may not always be the case.\n* The method proposed relies on the assumption that the target robots are mutually similar in morphology and kinematics, which may not always be true.\n* The paper does not provide a formal proof of the convergence of the method, which is an important aspect of the method's theoretical validity.\n* The experiments conducted are limited to two tasks and it is unclear whether the method can generalize to other tasks.\n\nSuggestions for Improvement:\n\n* Provide a formal proof of the convergence of the method to establish its theoretical validity.\n* Investigate the assumption that the source and target robots can be matched in their kinematic tree topology and propose a method to relax this assumption.\n* Explore the possibility of using other machine learning techniques, such as meta-learning, to improve the performance of the method.\n* Conduct more extensive experiments on a variety of tasks to demonstrate the method's generalizability.\n\nRecommendation:\n\nThe paper proposes a novel and efficient method for transferring an expert policy from a source robot to multiple target robots. While the method has some limitations, it shows promising results in the experiments conducted. With further development and refinement, the method has the potential to make significant contributions to the field of robotics. Therefore, I recommend the paper for publication in a reputable conference.\n\nRating: 7/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a new method named Meta-Evolve that enables efficient transfer of an expert policy from a source robot to multiple target robots using continuous robot evolution. The proposed method constructs a tree-structured evolution path that interconnects the source and target robots, allowing for sharing of evolution paths among multiple target robots. A heuristic approach is presented to determine the optimal evolution tree, and the method is shown to significantly outperform one-to-one policy transfer baselines in terms of total cost and simulation epochs.\n\nStrengths:\n\n* The proposed method, Meta-Evolve, provides a new and efficient solution to the problem of transferring an expert policy from a source robot to multiple target robots.\n* The method's ability to share evolution paths among multiple target robots is a significant improvement over one-to-one policy transfer baselines.\n* The heuristic approach to determining the optimal evolution tree is practical and efficient.\n* Experimental results demonstrate the effectiveness of the proposed method on two different tasks.\n\nWeaknesses:\n\n* The method's performance may degrade when the target robots are highly dissimilar or when the source and target robots have vastly different morphologies.\n* The heuristic approach to determining the optimal evolution tree may not always yield the optimal solution.\n* The method's dependence on a well-trained expert policy may limit its applicability in scenarios where such a policy is not available.\n\nSuggestions for Improvement:\n\n* Further investigation into the method's performance when dealing with highly dissimilar target robots or vastly different morphologies is necessary.\n* Development of more sophisticated heuristics or optimization techniques to determine the optimal evolution tree would improve the method's performance and robustness.\n* Exploration of alternative approaches that do not rely on a well-trained expert policy would broaden the method's applicability.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of robotics and reinforcement learning, offering a new and efficient solution to a long-standing problem. The proposed method has been experimentally validated on two different tasks and demonstrates promising results. However, further investigation and improvement of the method's performance and robustness are necessary to ensure its widespread adoption.\n\nRating: 8/10\n\nRecommendation for publication: Yes\n\nRecommendation for future work: Yes\n\n$$$",
        "gr_score_with": 0.63125,
        "gr_score_without": 0.4624624624624625,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 154,
            "green_fraction": 0.4624624624624625,
            "z_score": -5.1229736456121655,
            "p_value": 0.9999998496228003
        },
        "output_with": {
            "num_tokens_scored": 320,
            "num_green_tokens": 202,
            "green_fraction": 0.63125,
            "z_score": 1.1412572586102623,
            "p_value": 0.12688144146448055
        }
    },
    {
        "title": "WHAT\u2019S IN MY BIG DATA?",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. The authors apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama, and uncover several surprising and previously undocumented findings about these corpora.\n\nStrengths:\n\n* The paper proposes a novel platform and set of analyses for understanding the contents of large text corpora.\n* The authors provide a comprehensive analysis of ten different corpora used to train popular language models.\n* The paper highlights the importance of understanding the contents of large text corpora and provides a framework for doing so.\n* The authors provide a clear and concise description of the WIMBD platform and its capabilities.\n* The paper includes a thorough analysis of the results, including both quantitative and qualitative findings.\n\nWeaknesses:\n\n* The paper assumes that the reader has a strong background in natural language processing and machine learning.\n* The authors do not provide a detailed description of the Elasticsearch search engine used in WIMBD.\n* The paper does not provide a clear explanation of the hash function used in the counting and searching algorithms.\n* The authors do not provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* The paper does not provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n\nSuggestions for Improvement:\n\n* Provide a more detailed description of the Elasticsearch search engine used in WIMBD.\n* Provide a clear explanation of the hash function used in the counting and searching algorithms.\n* Provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* Provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n* Consider including a more detailed analysis of the results, including a discussion of the implications of the findings.\n\nRecommendation:\n\nThis paper provides a novel and comprehensive framework for understanding the contents of large text corpora. The authors provide a clear and concise description of the WIMBD platform and its capabilities, and the paper includes a thorough analysis of the results. However, the paper assumes a strong background in natural language processing and machine learning, and some of the technical details are not clearly explained. Overall, this paper is a valuable contribution to the field of natural language processing and machine learning, and it has the potential to be a highly influential paper in the field.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a platform called WHAT'S IN MY BIG DATA? (WIMBD) for analyzing large text corpora. WIMBD consists of two components: a search tool and a counting functionality. The search tool allows for programmatic access to search for documents containing a query using an Elasticsearch index, while the counting functionality enables quick iteration over an entire dataset and extraction of relevant information. The authors apply WIMBD to ten different corpora used to train popular language models and perform sixteen analyses across four aspects: high-level statistics, data quality, community-and society-relevant measurements, and cross-data analysis.\n\nStrengths:\n1. The paper proposes a novel platform for analyzing large text corpora, addressing a critical gap in the field of natural language processing.\n2. The authors provide a comprehensive analysis of ten corpora, covering a wide range of aspects, including high-level statistics, data quality, community-and society-relevant measurements, and cross-data analysis.\n3. The paper highlights the importance of understanding the content of large text corpora, which is essential for training high-quality language models.\n4. The authors provide a detailed explanation of the methodology used in the analysis, making it easier for readers to replicate the results.\n\nWeaknesses:\n1. The paper focuses primarily on the analysis of large text corpora, but does not provide a detailed explanation of the theoretical foundations of the platform.\n2. The authors do not discuss the limitations of the platform, such as the scalability and efficiency of the search tool and counting functionality.\n3. The paper relies heavily on the Elasticsearch index, but does not provide a detailed explanation of the index configuration and optimization techniques used.\n4. The authors do not discuss the potential applications of the platform beyond the analysis of large text corpora.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the theoretical foundations of the platform, including the algorithms used in the search tool and counting functionality.\n2. Discuss the limitations of the platform, such as the scalability and efficiency of the search tool and counting functionality.\n3. Provide a detailed explanation of the index configuration and optimization techniques used in the Elasticsearch index.\n4. Discuss the potential applications of the platform beyond the analysis of large text corpora.\n\nRecommendation:\nThe paper proposes a novel platform for analyzing large text corpora, addressing a critical gap in the field of natural language processing. While the paper has some weaknesses, such as the lack of theoretical foundations and discussion of limitations, the authors provide a comprehensive analysis of ten corpora, covering a wide range of aspects. I recommend the paper for publication in a top-tier conference or journal, with some revisions to address the weaknesses mentioned above.\n\n$$$",
        "gr_score_with": 0.7788461538461539,
        "gr_score_without": 0.6720257234726688,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 209,
            "green_fraction": 0.6720257234726688,
            "z_score": 2.592922661717437,
            "p_value": 0.004758208387808328
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 243,
            "green_fraction": 0.7788461538461539,
            "z_score": 6.448545884618023,
            "p_value": 5.6464187654926265e-11
        }
    },
    {
        "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
        "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The model, called the recurrent circuit model, uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with a modulated global signal to sample from the prior at a rate prescribed by the period of the global oscillator. The model can flexibly use sensory and contextual sources of information to encode task-specific posteriors. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n\nStrengths:\n\n1. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions.\n2. The model is flexible and can be used to encode task-specific posteriors using sensory and contextual sources of information.\n3. The authors demonstrate that the model can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n4. The paper provides a thorough analysis of the model's performance and compares it to traditional diffusion models.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the dendritic tree, which may not be biologically realistic.\n2. The model's performance is evaluated on a limited set of tasks and distributions, and it is unclear how well it generalizes to other scenarios.\n3. The paper does not provide a clear explanation of how the model's parameters are learned, which is a critical component of any machine learning model.\n4. The paper's conclusion is somewhat overstated, as the model's performance is not significantly better than traditional diffusion models in all cases.\n\nSuggestions for Improvement:\n\n1. Consider exploring more biologically realistic architectures for the dendritic tree, such as those with variable branching factors or depths.\n2. Evaluate the model's performance on a wider range of tasks and distributions to assess its generalizability.\n3. Provide a more detailed explanation of how the model's parameters are learned, including any optimization algorithms or techniques used.\n4. Be more cautious in the paper's conclusion, as the model's performance is not uniformly better than traditional diffusion models.\n\nRecommendation:\n\nI recommend accepting this paper for publication, subject to the above suggestions for improvement. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions, and the authors demonstrate its flexibility and performance on a range of tasks. However, the paper's assumptions and limitations should be carefully considered and addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The authors adapt several ideas developed in the context of diffusion models to construct a recurrent circuit model that can implicitly represent priors over latent variables and combine them with sensory and contextual sources of information to encode task-specific posteriors. The proposed model uses dendritic nonlinearities, optimized for denoising, and stochastic somatic activity modulated by a global oscillation that determines the effective rate of sampling. The authors demonstrate that this architecture can sample from complex distributions, including nonlinear manifolds embedded in higher dimensional ambient spaces, and multimodal posteriors.\n\nStrengths:\n\n1. The paper provides a novel and biologically-inspired approach to representing complex high-dimensional distributions, which is an important open question in the field of probabilistic brain computation.\n2. The authors demonstrate the ability of the proposed model to sample from complex distributions, including nonlinear manifolds embedded in higher dimensional ambient spaces, and multimodal posteriors.\n3. The paper provides a clear and concise explanation of the proposed model, including its mathematical formulation and experimental results.\n4. The authors discuss the potential neural signatures of the proposed model, which could be tested experimentally.\n\nWeaknesses:\n\n1. The paper relies heavily on the diffusion model framework, which may limit its applicability to other types of distributions.\n2. The authors do not provide a comprehensive comparison of the proposed model with other existing models, such as Markov chain Monte Carlo methods.\n3. The experimental results are limited to a small number of simulations, and it would be beneficial to see more extensive experimental evaluations.\n4. The authors do not provide a clear explanation of how the proposed model could be applied to real-world problems, such as computer vision or natural language processing.\n\nSuggestions for Improvement:\n\n1. The authors could provide a more comprehensive comparison of the proposed model with other existing models, such as Markov chain Monte Carlo methods.\n2. The experimental results could be expanded to include more extensive simulations and real-world applications.\n3. The authors could provide a clearer explanation of how the proposed model could be applied to real-world problems.\n4. The authors could discuss the potential limitations and challenges of the proposed model, such as its computational complexity and the need for large-scale simulations.\n\nRecommendation:\n\nThe paper provides a novel and biologically-inspired approach to representing complex high-dimensional distributions, and the experimental results are promising. However, the paper would benefit from a more comprehensive comparison with other existing models, more extensive experimental evaluations, and a clearer explanation of how the proposed model could be applied to real-world problems. If the authors address these limitations, the paper could be considered for publication in a top-tier journal.",
        "gr_score_with": 0.7211267605633803,
        "gr_score_without": 0.6488095238095238,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 218,
            "green_fraction": 0.6488095238095238,
            "z_score": 1.826456633087005,
            "p_value": 0.03389075395495987
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 256,
            "green_fraction": 0.7211267605633803,
            "z_score": 4.658698283157513,
            "p_value": 1.5910760134489908e-06
        }
    },
    {
        "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
        "abstract": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. The authors leverage a body of work in information theory called partial information decomposition (PID) to identify three sources of unfairness in FL: Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples and provide a framework for quantifying accuracy-fairness trade-offs in FL.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective.\n2. The authors leverage PID to identify three sources of unfairness in FL, which is a novel contribution to the field.\n3. The paper provides a framework for quantifying accuracy-fairness trade-offs in FL, which is a significant contribution to the field.\n4. The authors provide experimental results on synthetic and Adult datasets to support their theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes a binary classification problem, which may not be applicable to all real-world scenarios.\n2. The authors do not provide a clear explanation of how to implement the PID decomposition in practice.\n3. The paper does not provide a comprehensive discussion of the limitations of the PID decomposition.\n4. The authors do not provide a clear explanation of how to choose the parameters of the PID decomposition.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive explanation of how to implement the PID decomposition in practice.\n2. The paper should provide a more detailed discussion of the limitations of the PID decomposition.\n3. The authors should provide a clear explanation of how to choose the parameters of the PID decomposition.\n4. The paper should provide more experimental results on real-world datasets to support the theoretical findings.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of fairness in FL and provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective. However, the paper assumes a binary classification problem and does not provide a clear explanation of how to implement the PID decomposition in practice. Therefore, I recommend that the authors provide a more comprehensive explanation of how to implement the PID decomposition in practice and provide more experimental results on real-world datasets to support the theoretical findings.\n\nRating: 8/10\n\nReviewer's Expertise: I have expertise in machine learning, fairness, and information theory.\n\nReviewer's Conflict of Interest: None.\n\nReviewer's Disclosure: None.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" proposes an information-theoretic approach to understand group fairness trade-offs in Federated Learning (FL). The authors leverage Partial Information Decomposition (PID) to decompose global and local fairness into three sources of unfairness: Unique Disparity, Redundant Disparity, and Masked Disparity. They derive fundamental limits on the trade-offs between global and local fairness, highlighting where they agree or disagree. The authors also introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization framework that systematically explores the trade-offs between accuracy and fairness metrics.\n\nStrengths:\n\n1. The paper provides a comprehensive framework for understanding group fairness trade-offs in FL, which is a crucial aspect of ensuring fairness in machine learning models.\n2. The use of PID to decompose global and local fairness into three sources of unfairness is a novel and interesting approach.\n3. The authors derive fundamental limits on the trade-offs between global and local fairness, which provides valuable insights for practitioners.\n4. The AGLFOP framework provides a systematic way to explore the trade-offs between accuracy and fairness metrics.\n\nWeaknesses:\n\n1. The paper assumes that the sensitive attributes are binary, which may not be the case in real-world applications.\n2. The authors rely heavily on theoretical results, but they lack experimental evaluations to validate their claims.\n3. The paper assumes that the client distribution is fixed, which may not be the case in real-world applications where clients may join or leave the network.\n4. The AGLFOP framework may not be scalable for large datasets, which may limit its practical applications.\n\nSuggestions for Improvement:\n\n1. The authors should provide experimental evaluations to validate their claims and show that their framework works in practice.\n2. The paper should discuss how to handle non-binary sensitive attributes, which is a common scenario in real-world applications.\n3. The authors should discuss how to adapt the AGLFOP framework to changing client distributions, which may occur in real-world applications.\n4. The paper should discuss how to make the AGLFOP framework scalable for large datasets.\n\nRecommendation:\n\nThis paper provides a comprehensive framework for understanding group fairness trade-offs in FL, which is a crucial aspect of ensuring fairness in machine learning models. The use of PID to decompose global and local fairness into three sources of unfairness is a novel and interesting approach. However, the paper lacks experimental evaluations, and the AGLFOP framework may not be scalable for large datasets. I recommend that the authors address these limitations in future work.\n\nI would like to see more experimental evaluations to validate the claims of the paper. The authors should also discuss how to handle non-binary sensitive attributes, adapt the AGLFOP framework to changing client distributions, and make the framework scalable for large datasets.\n\nIn summary, this paper provides a comprehensive framework for understanding group fairness trade-offs in FL, but it lacks experimental evaluations and may not be scalable for large datasets. I recommend that the authors address these limitations in future work.\n\n$$$",
        "gr_score_with": 0.6868131868131868,
        "gr_score_without": 0.603225806451613,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 187,
            "green_fraction": 0.603225806451613,
            "z_score": 0.11610262901818845,
            "p_value": 0.45378560262595213
        },
        "output_with": {
            "num_tokens_scored": 364,
            "num_green_tokens": 250,
            "green_fraction": 0.6868131868131868,
            "z_score": 3.3810634880606245,
            "p_value": 0.0003610292866554042
        }
    },
    {
        "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION",
        "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for incremental robustness certification of deep neural networks (DNNs) using randomized smoothing (RS). The method, called Incremental Randomized Smoothing (IRS), reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples. This approach significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees.\n\nStrengths:\n\n1. The paper addresses a critical limitation of existing RS-based certification practices, which requires expensive certification to be re-run from scratch for each approximate network.\n2. The proposed IRS approach is the first incremental RS-based certification framework that efficiently computes the certified radius of the updated smoothed classifier.\n3. The paper presents an extensive evaluation of the performance of IRS, demonstrating speedups of up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models.\n4. The authors provide a thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, which affects the efficiency of IRS.\n\nWeaknesses:\n\n1. The paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n3. The paper assumes that the user has a larger sample budget, which may not always be the case in real-world scenarios.\n4. The authors do not provide a detailed comparison with other incremental certification methods, such as deterministic certification using formal logic.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of storing and retrieving the certification cache, including a discussion of potential optimization techniques.\n2. The paper should include a more detailed comparison with other incremental certification methods, including deterministic certification using formal logic.\n3. The authors should provide a more thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, including a discussion of potential optimization techniques.\n4. The paper should include a discussion of potential applications of IRS in real-world scenarios, including approximate tuning and shipping certification caches.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and areas for improvement, the proposed IRS approach has the potential to significantly reduce the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. Therefore, I recommend accepting this paper for publication in the conference proceedings.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Incremental Randomized Smoothing (IRS), an approach for efficient and effective probabilistic deep neural network (DNN) certification. IRS leverages the certification guarantees obtained from the smoothed model to certify a smoothed approximated model with very few samples, significantly reducing the computational cost of certification while maintaining strong robustness guarantees. The authors present an extensive evaluation of the performance of IRS, demonstrating its ability to achieve up to 4.1x certification speedup over the standard non-incremental RS baseline on state-of-the-art classification models.\n\nStrengths:\n\n* IRS is the first incremental approach for probabilistic DNN certification, offering a novel and efficient way to certify DNNs.\n* The authors provide a comprehensive evaluation of IRS, demonstrating its ability to achieve significant certification speedups over the baseline approach.\n* IRS is designed to be applicable to various DNN approximations, including quantization and pruning, making it a versatile and widely applicable approach.\n\nWeaknesses:\n\n* The authors assume that the cache stores the top predicted class index, its lower confidence bound, and the seeds for Gaussian corruptions obtained from the RS execution of the original classifier, which may require additional memory and computational resources.\n* The choice of hyperparameters, such as the threshold \u03b3, may require careful tuning and may not be straightforward for all applications.\n* The authors do not provide a comprehensive comparison with other state-of-the-art approaches for DNN certification, which may limit the understanding of the relative strengths and weaknesses of IRS.\n\nSuggestions for Improvement:\n\n* The authors should provide more details about the memory and computational resources required for storing and retrieving the cache, as well as the impact of these requirements on the overall certification process.\n* The authors should provide more guidance on how to choose the hyperparameters, such as \u03b3, and how these choices may impact the performance of IRS.\n* The authors should provide a more comprehensive comparison with other state-of-the-art approaches for DNN certification, including both probabilistic and deterministic methods, to better understand the relative strengths and weaknesses of IRS.\n\nRecommendation:\n\nOverall, the paper presents a novel and efficient approach for probabilistic DNN certification, and the authors provide a comprehensive evaluation of its performance. While there are some limitations and areas for improvement, IRS has the potential to become a widely applicable and effective approach for DNN certification. Therefore, I recommend that the authors consider addressing the suggestions for improvement and provide more details about the implementation and evaluation of IRS.",
        "gr_score_with": 0.7115902964959568,
        "gr_score_without": 0.6034985422740525,
        "output_without": {
            "num_tokens_scored": 343,
            "num_green_tokens": 207,
            "green_fraction": 0.6034985422740525,
            "z_score": 0.13243661015593397,
            "p_value": 0.44731947948908646
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 264,
            "green_fraction": 0.7115902964959568,
            "z_score": 4.387580778985712,
            "p_value": 5.730921483064875e-06
        }
    },
    {
        "title": "SELF-CONSUMING GENERATIVE MODELS GO MAD",
        "abstract": "Seismic advances in generative AI algorithms for imagery, text, and other data types have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates an autophagous (\u201cself-consuming\u201d) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), by analogy to mad cow disease, and show that appreciable MADness arises in just a few generations. Generation t = 1 t = 3 t = 5 t = 7 t = 9 Figure 1: Training generative artificial intelligence (AI) models on synthetic data progressively amplifies artifacts. As AI-synthesized data proliferates in standard datasets and the Internet, future AI models will train on both real and synthetic data, forming autophagous (\u201cself-consuming\u201d) loops. Here we highlight a potential unintended consequence of autophagous training. We trained a sequence of StyleGAN2 (Karras et al., 2019a) models wherein the model at generation t \u2265 2 trains only on data synthesized by the model at generation t\u2212 1. This setup is a fully synthetic loop (Figure 3) without sampling bias (\u03bb = 1). Note how the cross-hatched artifacts (possibly an architectural fingerprint (Karras et al., 2021)) are progressively amplified at each generation. Appendix D has more samples.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of autophagous (self-consuming) loops in generative AI models, where AI-synthesized data is used to train future models, creating a loop of synthetic data being used to train subsequent models. The authors propose three families of autophagous loops: fully synthetic, synthetic augmentation, and fresh data loops. They conduct a thorough analytical and empirical analysis using state-of-the-art generative image models and demonstrate that without enough fresh real data in each generation, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease, a condition they term Model Autophagy Disorder (MAD).\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of autophagous loops, proposing three realistic models that mimic real-world generative model training scenarios.\n2. The authors demonstrate the negative impact of autophagous loops on the quality and diversity of synthetic data, providing empirical evidence of Model Autophagy Disorder (MAD).\n3. The paper highlights the importance of sampling biases in autophagous loops, showing that biased sampling can boost synthetic data quality but at the expense of diversity.\n4. The authors provide a detailed analysis of the fully synthetic loop, showing that unbiased sampling degrades synthetic data quality and diversity, while biased sampling can increase quality but decrease diversity.\n\nWeaknesses:\n\n1. The paper assumes that the generative models used are perfect, which may not be the case in real-world scenarios.\n2. The authors do not provide a comprehensive analysis of the potential benefits of autophagous loops, such as the ability to generate high-quality synthetic data.\n3. The paper focuses primarily on image generation models and does not explore the implications of autophagous loops on other data types, such as text or audio.\n4. The authors do not provide a clear solution to mitigate the negative effects of autophagous loops, such as developing autophagy-aware watermarking techniques.\n\nSuggestions for Improvement:\n\n1. The authors should consider exploring the potential benefits of autophagous loops and provide a more comprehensive analysis of their implications.\n2. The paper should be extended to include a more detailed analysis of the potential effects of autophagous loops on other data types, such as text or audio.\n3. The authors should provide a clear solution to mitigate the negative effects of autophagous loops, such as developing autophagy-aware watermarking techniques.\n4. The paper should be expanded to include a more detailed analysis of the limitations of the proposed models and the potential for future research in this area.\n\nRecommendation:\n\nThe paper provides a comprehensive analysis of autophagous loops and their negative impact on the quality and diversity of synthetic data. While there are some limitations to the paper, the authors' findings are significant and provide a valuable contribution to the field of generative AI. Therefore, I recommend that the paper be accepted for publication in its current form, with the suggested improvements for future research.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper explores the concept of autophagous generative models, where a sequence of generative models are trained on synthetic data produced by previous generations. The authors propose three families of autophagous loops and study their properties using state-of-the-art generative image models. They find that, without enough fresh real data in each generation, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. This condition is termed Model Autophagy Disorder (MAD).\n\nStrengths:\n1. The paper provides a comprehensive study of autophagous generative models, which is a timely and important topic in the field of machine learning.\n2. The authors propose a clear and concise framework for understanding autophagous loops and their properties.\n3. The paper includes a thorough theoretical and empirical analysis, which provides a solid foundation for the results.\n4. The authors discuss the potential implications of autophagous models, including the amplification of biases and artifacts, and the degradation of quality and diversity.\n\nWeaknesses:\n1. The paper assumes that the autophagous loop is a closed system, where the only input is the synthetic data produced by previous generations. This is a simplification, and it is not clear how the results would generalize to more complex scenarios.\n2. The paper does not provide a clear explanation of how the autophagous loop can be controlled or prevented. This is a critical issue, as the degradation of quality and diversity can have significant consequences.\n3. The paper relies heavily on simulations and experiments, which can be limited by the quality and representativeness of the data. More robust and diverse datasets would be needed to confirm the results.\n\nSuggestions for Improvement:\n1. The authors should explore more complex scenarios, such as autophagous loops with multiple inputs or outputs, or loops that involve multiple types of data.\n2. The paper could benefit from a more detailed discussion of the potential applications and implications of autophagous models, including their potential impact on downstream tasks and decision-making processes.\n3. The authors should consider providing more guidance on how to control or prevent autophagous loops, including strategies for identifying and rejecting synthetic data, and techniques for promoting diversity and quality in the generated data.\n\nRecommendation:\nThe paper provides a significant contribution to the field of machine learning, and it highlights the importance of understanding autophagous generative models. The results are compelling, and they have significant implications for the development and deployment of AI systems. However, the paper could benefit from more robust and diverse datasets, as well as a more detailed discussion of the potential applications and implications of autophagous models. With these improvements, the paper would be a major contribution to the field, and it would have a significant impact on the development of AI systems.",
        "gr_score_with": 0.702020202020202,
        "gr_score_without": 0.5871794871794872,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 229,
            "green_fraction": 0.5871794871794872,
            "z_score": -0.5166218051313308,
            "p_value": 0.6972899044337996
        },
        "output_with": {
            "num_tokens_scored": 396,
            "num_green_tokens": 278,
            "green_fraction": 0.702020202020202,
            "z_score": 4.144262145491151,
            "p_value": 1.704547949281294e-05
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method constructs a directed graph to represent unlabeled data and uses a diffusion process to quantify the influence of each candidate subset. A greedy algorithm is introduced to select the subset with maximum influence, which approximates the vast unlabeled data. The paper provides theoretical analysis and empirical evaluations on various benchmarks, demonstrating the superiority of IDEAL over previous state-of-the-art selective annotation baselines.\n\nStrengths:\n\n1. The paper presents a novel approach to selective annotations, addressing the challenges of balancing diversity and representativeness.\n2. The method is end-to-end, unsupervised, and efficient, reducing unnecessary costs at inference time.\n3. The paper provides theoretical analysis, establishing an upper limit for the global optimal solution and demonstrating the reliability of the greedy search algorithm.\n4. Empirical evaluations demonstrate the superiority of IDEAL across various benchmarks, including classification, multi-choice, dialogue, and generation tasks.\n\nWeaknesses:\n\n1. The paper assumes access to a large-scale unlabeled data pool, which may not be feasible in practice.\n2. The method relies on a similarity-based prompt retrieval method, which may not be optimal in all cases.\n3. The paper does not provide a comprehensive comparison with other graph-based methods, such as K-Means and MFL.\n4. The case study on automatic annotation is limited to a small set of datasets and may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for constructing the directed graph, such as using different similarity metrics or graph neural networks.\n2. Evaluate the performance of IDEAL on other datasets and tasks, including those with varying levels of complexity and domain adaptation.\n3. Compare IDEAL with other graph-based methods, such as K-Means and MFL, to demonstrate its superiority.\n4. Provide a more detailed analysis of the time complexity and scalability of IDEAL, especially for large datasets.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach to selective annotations, addressing the challenges of balancing diversity and representativeness. The theoretical analysis and empirical evaluations demonstrate the superiority of IDEAL over previous state-of-the-art selective annotation baselines. While there are some limitations and areas for improvement, the paper provides a valuable contribution to the field of in-context learning and large language models. I recommend acceptance of the paper, with the suggested improvements and additional evaluations to further strengthen the results.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThis paper proposes a novel method, Influence-Driven Selective Annotations to Empower In-Context Learners (IDEAL), to minimize annotation costs while improving the quality of in-context examples in large language models. The method utilizes influence-driven selective annotations to empower in-context learners, where it selects a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. The authors provide theoretical analysis to establish an upper limit for the global optimal solution and demonstrate that the greedy search algorithm selects a subset with influence at least as substantial as a specific proportion of the optimal solution's influence. The experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection.\n\n**Strengths**\n\n1. The proposed method, IDEAL, tackles the problem of minimizing annotation costs while improving the quality of in-context examples in large language models.\n2. The authors provide theoretical analysis to establish an upper limit for the global optimal solution and demonstrate the effectiveness of the greedy search algorithm.\n3. The experiments are comprehensive and include various benchmarks, demonstrating the superiority of the proposed method.\n4. The authors provide a detailed analysis of the time cost of subset selection, highlighting the efficiency of the proposed method.\n5. The proposed method is versatile and can be combined with various prompt retrieval technologies.\n\n**Weaknesses**\n\n1. The method relies on the construction of a directed graph, which may not be feasible for very large datasets.\n2. The influence-driven selective annotations method may not be effective in cases where the relationships between examples are not well-represented in the graph.\n3. The authors do not provide a detailed analysis of the computational complexity of the proposed method.\n4. The experiments are limited to a specific set of benchmarks and language models.\n5. The proposed method may not be effective in cases where the annotation budget is very small.\n\n**Suggestions for Improvement**\n\n1. Investigate the feasibility of the method for very large datasets and explore ways to improve the efficiency of graph construction.\n2. Analyze the effectiveness of the influence-driven selective annotations method in cases where the relationships between examples are not well-represented in the graph.\n3. Provide a detailed analysis of the computational complexity of the proposed method.\n4. Conduct experiments on a broader set of benchmarks and language models.\n5. Investigate the effectiveness of the proposed method in cases where the annotation budget is very small.\n\n**Recommendation**\n\nThis paper proposes a novel method for minimizing annotation costs while improving the quality of in-context examples in large language models. The theoretical analysis and experiments demonstrate the effectiveness of the proposed method. However, the method relies on the construction of a directed graph, which may not be feasible for very large datasets. Further investigation of the method's feasibility and effectiveness in various scenarios is necessary.",
        "gr_score_with": 0.7155963302752294,
        "gr_score_without": 0.5796344647519582,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 222,
            "green_fraction": 0.5796344647519582,
            "z_score": -0.8133715461539449,
            "p_value": 0.7919974660203406
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 234,
            "green_fraction": 0.7155963302752294,
            "z_score": 4.267062769956203,
            "p_value": 9.903172231194924e-06
        }
    },
    {
        "title": "TOWARDS TRANSPARENT TIME SERIES FORECASTING",
        "abstract": "Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in highstakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. In particular, currently used bottom-up approaches that focus on the values of the time series at specific time points (usually regularly spaced) do not provide a holistic understanding of the entire time series. This limits the applicability of ML in many critical areas. To open up these domains for ML, we propose a top-down framework of bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. Applying this framework, we develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. Through a series of experiments, we demonstrate the efficacy and interpretability of our approach, paving the way for more transparent and reliable applications of ML in various domains.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel top-down approach to transparent time series forecasting, which is essential for decision-making systems in high-stakes domains. The authors introduce bi-level transparency, a framework that involves understanding the higher-level trends and the lower-level properties of the predicted time series. They develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the challenges of transparent time series forecasting and the limitations of current bottom-up approaches.\n2. The authors introduce a novel top-down approach to trajectory comprehension, which is more consistent with the natural way people understand trajectories.\n3. The bi-level transparency framework is well-defined and provides a clear understanding of the impact of the input on the trends and properties of the predicted trajectory.\n4. The authors develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, which is accompanied by an interactive visualization tool.\n5. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on real-world and synthetic datasets.\n\nWeaknesses:\n\n1. The paper assumes that the input space is static features, which may not be the case in many real-world applications where the input may contain the previous part of the trajectory or other exogenous time series.\n2. The authors do not provide a thorough comparison of their approach with existing methods for time series forecasting, which may make it difficult to evaluate the novelty and impact of their contribution.\n3. The paper does not provide a detailed analysis of the computational complexity of the bi-level transparency framework and the TIMEVIEW model.\n4. The authors do not discuss the potential limitations and risks of their approach, such as the potential for overfitting or the difficulty of interpreting complex models.\n\nSuggestions for Improvement:\n\n1. The authors should extend their approach to handle dynamic inputs, such as previous parts of the trajectory or other exogenous time series.\n2. The paper should provide a more thorough comparison of their approach with existing methods for time series forecasting, including both transparent and black-box models.\n3. The authors should provide a detailed analysis of the computational complexity of the bi-level transparency framework and the TIMEVIEW model.\n4. The paper should discuss the potential limitations and risks of their approach, such as the potential for overfitting or the difficulty of interpreting complex models.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of transparent time series forecasting by introducing a novel top-down approach and developing a transparent ML model for time series forecasting based on static features. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on real-world and synthetic datasets. However, the paper has some limitations, such as assuming static inputs and not providing a thorough comparison with existing methods. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of their approach to make it more impactful and widely applicable.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel top-down approach to transparent time series forecasting, which is called bi-level transparency. The proposed framework involves understanding the higher-level trends and lower-level properties of the predicted time series. The authors introduce a transparent ML model called TIMEVIEW, which is based on static features and is complemented by an interactive visualization tool. The paper demonstrates the efficacy and interpretability of TIMEVIEW through a series of experiments.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to transparent time series forecasting, which is a significant contribution to the field.\n2. The authors provide a clear and concise explanation of the bi-level transparency framework and its components.\n3. The proposed TIMEVIEW model is well-designed and implemented, and its performance is comparable to that of black-box models.\n4. The paper includes a thorough analysis of the limitations and open challenges of the proposed approach.\n\nWeaknesses:\n\n1. The paper assumes that the input data is a set of static features, which may not be applicable to all time series forecasting scenarios.\n2. The proposed bi-level transparency framework may not be easily extendable to other types of inputs, such as time series or exogenous time series.\n3. The paper does not provide a detailed comparison with other existing methods, which may make it difficult to evaluate the novelty and significance of the proposed approach.\n4. The paper relies heavily on experiments and visualization, which may not be sufficient to fully demonstrate the efficacy and interpretability of TIMEVIEW.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other existing methods to evaluate the novelty and significance of the proposed approach.\n2. The paper should provide a more thorough analysis of the limitations and open challenges of the proposed approach, including potential extensions to other types of inputs.\n3. The authors should provide a more detailed explanation of the composition map and how it is used to achieve bi-level transparency.\n4. The paper should include a more detailed discussion of the ethics and reproducibility of the proposed approach.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to transparent time series forecasting, and its contributions are significant. However, the paper has some limitations and weaknesses that need to be addressed. With some revisions and improvements, this paper has the potential to make a substantial contribution to the field of time series forecasting.\n\nRecommendation: Accept after revisions.\n\n$$",
        "gr_score_with": 0.7272727272727273,
        "gr_score_without": 0.5862068965517241,
        "output_without": {
            "num_tokens_scored": 348,
            "num_green_tokens": 204,
            "green_fraction": 0.5862068965517241,
            "z_score": -0.5250465696237572,
            "p_value": 0.700224591155266
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 224,
            "green_fraction": 0.7272727272727273,
            "z_score": 4.559531331755593,
            "p_value": 2.563395389784076e-06
        }
    },
    {
        "title": "DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional finetuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel decoding strategy, Decoding by Contrasting Layers (DoLa), to reduce hallucinations in large language models (LLMs). The approach exploits the hierarchical encoding of factual knowledge within transformer LLMs by contrasting the logits of higher and lower layers to improve factuality in the decoding process. The method dynamically selects the premature layer based on a distributional distance measure, and the contrasted output is used as the next-word prediction. The authors conduct experiments on multiple tasks, including multiple-choice, open-ended generation, and chain-of-thought reasoning, and demonstrate that DoLa significantly improves truthfulness without external information retrieval or model fine-tuning.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of hallucinations in LLMs and the need for a novel decoding strategy to address this issue.\n2. The proposed method, DoLa, is well-motivated and exploits the hierarchical encoding of factual knowledge within transformer LLMs.\n3. The authors provide a thorough experimental evaluation of DoLa on multiple tasks, including multiple-choice, open-ended generation, and chain-of-thought reasoning.\n4. The results demonstrate that DoLa significantly improves truthfulness without external information retrieval or model fine-tuning.\n\nWeaknesses:\n\n1. The paper assumes that the hierarchical encoding of factual knowledge within transformer LLMs is a reliable and accurate representation of the model's knowledge, which may not always be the case.\n2. The dynamic layer selection strategy used in DoLa may not be suitable for all tasks or datasets, and may require additional hyperparameter tuning.\n3. The paper does not provide a detailed analysis of the computational cost of DoLa, which may be a concern for large-scale applications.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of DoLa and explore ways to optimize it for large-scale applications.\n2. The paper should provide a more thorough evaluation of the robustness of DoLa to different tasks, datasets, and hyperparameter settings.\n3. The authors should explore ways to combine DoLa with other methods, such as reinforcement learning from human feedback or external knowledge retrieval, to address the limitations of the current approach.\n\nRecommendation:\n\nThe paper provides a novel and well-motivated approach to reducing hallucinations in LLMs, and the experimental results demonstrate its effectiveness on multiple tasks. However, the paper assumes a reliable and accurate representation of the model's knowledge, and the dynamic layer selection strategy may require additional hyperparameter tuning. Therefore, I recommend that the authors address these limitations and provide a more detailed analysis of the computational cost of DoLa to make it a more robust and scalable approach.\n\nOverall, the paper provides a significant contribution to the field of NLP and demonstrates the potential of DoLa as a novel decoding strategy for reducing hallucinations in LLMs.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel decoding strategy, Decoding by Contrasting Layers (DoLa), to improve factuality in large language models (LLMs). The approach exploits the hierarchical encoding of factual knowledge within transformer LLMs by contrasting the logits of higher and lower layers to obtain the next-token probability. The proposed method, DoLa, dynamically selects appropriate layers based on token complexity, which allows for efficient inference without requiring additional training or external information retrieval. Experiments on multiple tasks demonstrate that DoLa significantly improves truthfulness and factuality in LLMs, outperforming state-of-the-art methods.\n\nStrengths:\n1. The proposed method, DoLa, is simple and efficient, requiring only a forward pass with early exiting from the same model itself.\n2. DoLa dynamically selects appropriate layers based on token complexity, which allows for efficient inference and avoids the need for training and using smaller LMs in previous methods.\n3. The approach is general and can be applied to any transformer-based LLMs, making it a foundational improvement that could be combined with other methods to address limitations.\n\nWeaknesses:\n1. The method focuses solely on factuality, and its effectiveness in other dimensions, such as reinforcement learning from human feedback, has not been explored.\n2. The approach relies on the model's internal knowledge without using external retrieval modules, which means it cannot correct misinformation acquired during training.\n3. The experiments on tasks requiring arithmetic reasoning show that DoLa may not be effective in all cases, indicating that there may be limitations to its applicability.\n\nSuggestions for Improvement:\n1. Investigate the effectiveness of DoLa in other dimensions, such as reinforcement learning from human feedback, to expand its applicability.\n2. Explore using external retrieval modules to correct misinformation acquired during training, which could improve the reliability of DoLa.\n3. Perform experiments on tasks requiring arithmetic reasoning to better understand its limitations and potential improvements.\n\nRecommendation:\nThe proposed method, DoLa, is a significant improvement over previous methods, demonstrating its effectiveness in reducing hallucinations in LLMs. However, its limitations need to be addressed to expand its applicability. Future work could combine DoLa with other methods to address its limitations, making it a more robust and reliable decoding strategy. The paper's clear writing, thorough experiments, and concise conclusion make it an excellent contribution to the field of natural language processing.\n\n$$$",
        "gr_score_with": 0.7223719676549866,
        "gr_score_without": 0.6219839142091153,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 232,
            "green_fraction": 0.6219839142091153,
            "z_score": 0.8668528402312737,
            "p_value": 0.19301132291192852
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 268,
            "green_fraction": 0.7223719676549866,
            "z_score": 4.811483856122011,
            "p_value": 7.490691879864081e-07
        }
    },
    {
        "title": "DOUBLY ROBUST PROXIMAL CAUSAL LEARNING FOR CONTINUOUS TREATMENTS",
        "abstract": "Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatments can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments for proximal causal learning. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications1.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a kernel-based doubly robust estimator for continuous treatments within the proximal causal framework. The authors address the challenge of estimating causal effects with continuous treatments by integrating a kernel function into the proximal inverse probability weighted estimator. They derive the kernel-based doubly robust estimator and show that it can approximate the influence function of the causal effect. The authors also propose a new approach to efficiently solve the nuisance functions, which is achieved by estimating the policy function and incorporating it into a min-max optimization problem. The convergence analysis of the proposed estimator is provided, and the authors demonstrate the utility of their method using synthetic data and the legalized abortion and crime dataset.\n\nStrengths:\n\n1. The paper addresses a significant challenge in causal inference, which is the estimation of causal effects with continuous treatments.\n2. The proposed kernel-based doubly robust estimator is a significant contribution to the field of causal inference, and it can handle continuous treatments.\n3. The authors provide a comprehensive convergence analysis of the proposed estimator, which is essential for understanding its performance.\n4. The paper demonstrates the utility of the proposed method using synthetic data and real-world data, which provides evidence of its effectiveness.\n\nWeaknesses:\n\n1. The paper assumes that the kernel function is bounded and differentiable, which may not be true for all kernel functions.\n2. The authors do not provide a detailed comparison of their method with existing methods, which makes it difficult to evaluate its performance.\n3. The paper assumes that the policy function can be estimated accurately, which may not be the case in practice.\n4. The authors do not provide a detailed discussion of the limitations of their method and potential future directions.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison of the proposed method with existing methods, including a discussion of their strengths and weaknesses.\n2. Investigate the effect of different kernel functions on the performance of the proposed estimator.\n3. Provide a more detailed discussion of the limitations of the proposed method and potential future directions.\n4. Consider incorporating additional regularization techniques to improve the stability of the proposed estimator.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of causal inference. However, it would benefit from a more detailed comparison with existing methods and a more detailed discussion of the limitations and potential future directions. With these improvements, the paper would be more comprehensive and provide a clearer understanding of the proposed method's performance.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nNote: The paper has the potential to be published in a top-tier journal in the field of causal inference, but it requires further improvements to make it more comprehensive and convincing.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a kernel-based doubly robust (DR) estimator for estimating causal effects of continuous treatments within the proximal causal learning (PCL) framework. The proposed estimator, called Proximal Kernel DR (PKDR), overcomes the challenges of estimating causal effects for continuous treatments by replacing the conventional indicator function with a kernel function. The paper also proposes a new approach for estimating the nuisance function q0, which is a critical component of the PCL framework. The proposed approach estimates the policy function and incorporates it into a min-max optimization problem, which is more efficient than previous methods. The paper provides a comprehensive convergence analysis of the proposed estimator and demonstrates its utility through experiments on synthetic and real-world datasets.\n\nStrengths:\n\n1. The paper addresses a critical challenge in causal inference, namely estimating causal effects of continuous treatments, and proposes a novel solution that is provably consistent and efficient.\n2. The proposed PKDR estimator is a significant improvement over previous DR estimators, which are restricted to binary treatments.\n3. The paper provides a comprehensive convergence analysis of the proposed estimator, which is a major strength of the paper.\n4. The experimental results demonstrate the utility of the proposed estimator in synthetic and real-world datasets.\n\nWeaknesses:\n\n1. The paper assumes that the kernel function is bounded and differentiable, which may not be a realistic assumption in all cases.\n2. The paper does not provide a clear explanation of how to choose the bandwidth parameter, which is critical for the performance of the proposed estimator.\n3. The paper assumes that the nuisance function q0 is uniformly bounded, which may not be a realistic assumption in all cases.\n4. The experimental results are based on a limited number of datasets, and it is not clear whether the proposed estimator will perform well on other datasets.\n\nSuggestions for Improvement:\n\n1. Relax the assumptions on the kernel function and nuisance function q0, and provide a more realistic and flexible approach for estimating causal effects.\n2. Provide a clear and systematic approach for choosing the bandwidth parameter, which is critical for the performance of the proposed estimator.\n3. Conduct more comprehensive experiments on a wider range of datasets, including datasets with different types of treatments and outcomes.\n4. Consider incorporating other types of kernel functions, such as Gaussian or Epanechnikov kernels, and compare their performance.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of causal inference, particularly for estimating causal effects of continuous treatments. The proposed PKDR estimator is a novel and efficient solution that overcomes the challenges of previous DR estimators. However, there are some limitations and areas for improvement, which should be addressed in future work. Overall, I recommend this paper for publication, but with some reservations.\n\nScore: 8/10",
        "gr_score_with": 0.7284595300261096,
        "gr_score_without": 0.6292134831460674,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 224,
            "green_fraction": 0.6292134831460674,
            "z_score": 1.1253080191727118,
            "p_value": 0.13022926637474314
        },
        "output_with": {
            "num_tokens_scored": 383,
            "num_green_tokens": 279,
            "green_fraction": 0.7284595300261096,
            "z_score": 5.13186311799779,
            "p_value": 1.4344405584626618e-07
        }
    },
    {
        "title": "GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS",
        "abstract": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson\u2019s r) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces GeoLLM, a novel method that extracts geospatial knowledge from large language models (LLMs) using auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that their method can effectively unlock this knowledge across various models and tasks. They fine-tune LLMs using prompts constructed with map data and evaluate their performance on a range of geospatial prediction tasks. The results show that GeoLLM outperforms traditional baselines, including k-NN and XGBoost, and even exceeds the performance of satellite-based methods. The authors also observe that the performance of their method scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper presents a novel method for extracting geospatial knowledge from LLMs, which is a significant contribution to the field.\n2. The authors provide a comprehensive evaluation of their method on a range of geospatial prediction tasks, demonstrating its effectiveness and scalability.\n3. The paper highlights the potential of LLMs for geospatial tasks, which is an exciting area of research with many potential applications.\n4. The authors provide a clear and concise explanation of their method and its evaluation, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with LLMs and their capabilities, which may not be the case for all readers.\n2. The evaluation of the method is limited to a few geospatial prediction tasks, and it would be beneficial to see a more comprehensive evaluation on a wider range of tasks.\n3. The paper does not provide a detailed explanation of the map data used in the prompts, which is a crucial component of the method.\n4. The authors do not discuss the potential biases of LLMs in geospatial tasks, which is an important consideration for applications in real-world settings.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the map data used in the prompts and its role in the method.\n2. Evaluate the method on a wider range of geospatial prediction tasks to demonstrate its scalability and effectiveness.\n3. Discuss the potential biases of LLMs in geospatial tasks and how they can be mitigated.\n4. Provide more information on the computational cost of the method and how it can be optimized for real-world applications.\n\nRecommendation:\n\nThe paper presents a novel and effective method for extracting geospatial knowledge from LLMs, which has significant potential for applications in geospatial tasks. While there are some limitations and areas for improvement, the paper provides a clear and concise explanation of the method and its evaluation, making it an excellent contribution to the field. I recommend the paper for publication in a top-tier conference or journal in the field of geospatial AI.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces GeoLLM, a novel method that efficiently extracts geospatial knowledge from large language models (LLMs) by fine-tuning them on prompts constructed with auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that this knowledge can be unlocked using their proposed method. They present experiments across multiple tasks, including population density, economic livelihoods, and housing prices, and show that GeoLLM outperforms traditional baselines and satellite-based methods.\n\nStrengths:\n\n1. The authors propose a novel and effective method for extracting geospatial knowledge from LLMs, which has the potential to revolutionize geospatial tasks.\n2. They provide a comprehensive benchmark that encompasses a wide range of geospatial prediction tasks, making it easier to compare the performance of different methods.\n3. The authors demonstrate the utility of their approach across multiple tasks and datasets, showing that GeoLLM can be applied to various real-world problems.\n4. They provide a detailed analysis of the performance of different models, including LLMs, and show that GeoLLM outperforms traditional baselines and satellite-based methods.\n\nWeaknesses:\n\n1. The authors rely heavily on the performance of GPT-3.5, which may not be widely available or accessible to all researchers.\n2. The paper does not provide a clear explanation of how the map data is used to construct the prompts, which may limit the reproducibility of the results.\n3. The authors do not provide a detailed analysis of the biases of the LLMs that they use, which may be an important consideration for applications that require fairness and transparency.\n4. The paper does not provide a clear explanation of how the proposed method can be scaled up to larger datasets or more complex tasks.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how the map data is used to construct the prompts, and how this can be replicated by other researchers.\n2. Provide a detailed analysis of the biases of the LLMs that are used, and how these biases can be addressed.\n3. Provide a clear explanation of how the proposed method can be scaled up to larger datasets or more complex tasks.\n4. Provide more detailed results and analysis of the performance of the proposed method on different tasks and datasets.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of geospatial tasks and has the potential to revolutionize the way that geospatial data is processed and analyzed. The authors propose a novel and effective method for extracting geospatial knowledge from LLMs, which has been demonstrated to outperform traditional baselines and satellite-based methods. While there are some limitations and areas for improvement, the paper provides a clear and compelling argument for the use of LLMs in geospatial tasks. I recommend that this paper be accepted for publication, with the suggested improvements and recommendations.\n\nRating: 8/10\n\nReview Length: 500-750 words\n\nReviewer Expertise: AI, NLP, Geospatial tasks",
        "gr_score_with": 0.7069408740359897,
        "gr_score_without": 0.6342105263157894,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 241,
            "green_fraction": 0.6342105263157894,
            "z_score": 1.361458696805105,
            "p_value": 0.08668438881698842
        },
        "output_with": {
            "num_tokens_scored": 389,
            "num_green_tokens": 275,
            "green_fraction": 0.7069408740359897,
            "z_score": 4.305574025666049,
            "p_value": 8.327664095106014e-06
        }
    },
    {
        "title": "PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS",
        "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs) that combines two novel components: Relative Importance and Activations (RIA) and Channel Permutation. RIA is a new pruning metric that jointly considers the weight and activations of LLMs, while Channel Permutation is a new approach to maximize the preservation of important weights under N:M sparsity. The authors demonstrate that RIA alone can outperform existing post-training pruning methods on prevalent LLMs, and that Channel Permutation can even outperform the original LLaMA2 70B on zero-shot tasks.\n\nStrengths:\n\n1. The paper proposes two novel and innovative components, RIA and Channel Permutation, which can be combined to further enhance the N:M structured pruned LLMs.\n2. The authors demonstrate the effectiveness of RIA and Channel Permutation through extensive experiments on prominent LLMs like LLaMA, LLaMA2, and OPT across varying model sizes.\n3. The paper provides a thorough analysis of the performance of RIA and Channel Permutation under different sparsity levels, calibration data, and zero-shot performance.\n4. The authors demonstrate the hardware-friendliness of their method, ensuring enhanced inference acceleration.\n\nWeaknesses:\n\n1. The paper assumes that the input and output channels of weight have unique information, which may not always be the case in LLMs.\n2. The authors do not provide a thorough analysis of the computational complexity of their method, which may be a concern for large-scale LLMs.\n3. The paper assumes that the Hungarian algorithm can efficiently solve the linear sum assignment problem, which may not always be the case in practice.\n4. The authors do not provide a clear comparison with other pruning methods that do not require calibration data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational complexity of their method and its implications for large-scale LLMs.\n2. The paper should include a more detailed comparison with other pruning methods that do not require calibration data.\n3. The authors should provide more insights into the assumption that the input and output channels of weight have unique information in LLMs.\n4. The paper should include more experiments to evaluate the robustness of RIA and Channel Permutation under different scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective plug-and-play post-training pruning method for LLMs that combines RIA and Channel Permutation. The authors demonstrate the effectiveness of their method through extensive experiments and provide a thorough analysis of its performance under different sparsity levels, calibration data, and zero-shot performance. However, the paper assumes that the input and output channels of weight have unique information, and does not provide a thorough analysis of the computational complexity of their method. Therefore, the paper is recommended for publication after addressing these concerns.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents a novel plug-and-play post-training pruning method for large language models (LLMs), referred to as PLUG-AND-PLAY. The proposed solution has two key components: Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The authors demonstrate that RIA alone can surpass all existing post-training pruning methods on prevalent LLMs, and when combined with Channel Permutation, it can even outperform the original LLaMA2 70B on zero-shot tasks.\n\nStrengths:\n\n1. The proposed method, PLUG-AND-PLAY, is a plug-and-play solution that does not require any additional fine-tuning or retraining, making it a highly efficient and practical approach.\n2. The Relative Importance and Activations (RIA) metric is a novel and effective pruning metric that jointly considers the weight and activations, leading to better performance compared to existing methods.\n3. The Channel Permutation approach is a new and efficient way to turn LLMs into N:M sparsity, leading to significant performance improvement and inference acceleration.\n4. The authors provide extensive experiments on various LLMs, demonstrating the effectiveness and efficiency of the proposed method.\n5. The paper presents a thorough analysis of the running time and inference acceleration, providing a comprehensive understanding of the proposed method.\n\nWeaknesses:\n\n1. The paper assumes that the input and output channels are independent and identically distributed (i.i.d.), which might not be the case in real-world scenarios.\n2. The Channel Permutation approach relies on a heuristic allocation strategy, which might not be optimal in all cases.\n3. The authors do not provide a thorough analysis of the robustness and generalizability of the proposed method.\n4. The paper does not provide a comparison with other state-of-the-art methods in terms of model size and inference time.\n5. The authors do not provide a clear explanation of how the proposed method can be adapted to other types of neural networks.\n\nSuggestions for Improvement:\n\n1. Investigate the assumption of i.i.d. input and output channels and explore ways to relax this assumption.\n2. Develop a more robust and efficient Channel Permutation approach that can handle varying channel distributions.\n3. Perform a thorough analysis of the robustness and generalizability of the proposed method using various datasets and tasks.\n4. Compare the proposed method with other state-of-the-art methods in terms of model size and inference time.\n5. Explore ways to adapt the proposed method to other types of neural networks.\n\nRecommendation:\n\nThis paper presents a novel and efficient post-training pruning method for large language models. The proposed method, PLUG-AND-PLAY, has shown excellent performance and inference acceleration on various LLMs. However, further investigation is needed to address the limitations and suggestions for improvement mentioned above. With some modifications and extensions, this work has the potential to make a significant impact in the field of neural network pruning and inference acceleration.\n\nOverall, I would recommend this paper for publication in a top-tier conference or journal, with some revisions to address the limitations and suggestions for improvement mentioned above.\n\nScore: 8/10\n$$$",
        "gr_score_with": 0.6206140350877193,
        "gr_score_without": 0.5088757396449705,
        "output_without": {
            "num_tokens_scored": 338,
            "num_green_tokens": 172,
            "green_fraction": 0.5088757396449705,
            "z_score": -3.4195078304467224,
            "p_value": 0.9996863273654122
        },
        "output_with": {
            "num_tokens_scored": 456,
            "num_green_tokens": 283,
            "green_fraction": 0.6206140350877193,
            "z_score": 0.8987471213975019,
            "p_value": 0.18439368580005455
        }
    },
    {
        "title": "TEST: TEXT PROTOTYPE ALIGNED EMBEDDING TO ACTIVATE LLM\u2019S ABILITY FOR TIME SERIES",
        "abstract": "This work summarizes two ways to accomplish Time-Series (TS) tasks in today\u2019s Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TSfor-LLM, where we aim to activate LLM\u2019s ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM\u2019s embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today\u2019s SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM\u2019s ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called TEST (Text-Prototype-Aligned Embedding to Activate LLM's Ability for Time Series) to enable Large Language Models (LLMs) to handle Time-Series (TS) data. The method focuses on the TS-for-LLM paradigm, where TS is converted into a model-friendly representation to enable LLMs to understand TS data. The proposed TEST method tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, and creates soft prompts to make LLM more open to the embeddings. The authors demonstrate the feasibility of TS-for-LLM through theory and experiments on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes.\n\nStrengths:\n\n1. The paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field.\n2. The authors provide a thorough analysis of the TS-for-LLM paradigm and its potential methods, which is a valuable addition to the existing literature.\n3. The experiments demonstrate the effectiveness of TEST on various TS tasks, including classification, forecasting, and representation tasks.\n4. The authors provide a detailed comparison with existing methods, including LLM-QA methods, SFT LLM-for-TS methods, classical TS models, and SOTA TS models.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs used are pre-trained and frozen, which may not be the case in real-world applications where LLMs are fine-tuned or trained from scratch.\n2. The authors do not provide a thorough analysis of the impact of the LLM size and type on the results, which is an important aspect to consider in real-world applications.\n3. The paper assumes that the TS data is available in a format that can be easily tokenized and embedded, which may not be the case in real-world applications where TS data may be noisy or missing.\n4. The authors do not provide a detailed comparison with other TS-for-LLM methods, which would be a valuable addition to the existing literature.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the impact of the LLM size and type on the results, including the effect of different architectures and training procedures.\n2. The authors should provide a more thorough comparison with other TS-for-LLM methods, including a discussion of the strengths and weaknesses of each method.\n3. The authors should provide a more detailed analysis of the TS data used in the experiments, including a discussion of the noise and missing values present in the data.\n4. The authors should provide a more detailed discussion of the potential applications of TEST in real-world scenarios, including a discussion of the potential benefits and limitations of the method.\n\nRecommendation:\n\nThe paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field. The experiments demonstrate the effectiveness of TEST on various TS tasks, including classification, forecasting, and representation tasks. However, the paper assumes that the LLMs used are pre-trained and frozen, which may not be the case in real-world applications. Therefore, the authors should provide a more detailed analysis of the impact of the LLM size and type on the results, including the effect of different architectures and training procedures. Additionally, the authors should provide a more thorough comparison with other TS-for-LLM methods, including a discussion of the strengths and weaknesses of each method.\n\nIn conclusion, the paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field. However, the paper has some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called TEST (Text Embedding Space for Time Series) to enable large language models (LLMs) to handle time series (TS) data without modifying the model itself. TEST is a data-centric approach that converts TS into a model-friendly representation to facilitate the pre-trained LLM's understanding of TS data. The proposed method consists of two key steps: (1) building an encoder to embed TS tokens into a high-dimensional space using instance-wise, feature-wise, and text-prototype-aligned contrast, and (2) creating soft prompts to make the LLM more open to the TS embeddings. Experiments on TS classification, forecasting, and representation tasks demonstrate that the proposed approach can achieve comparable performance to state-of-the-art (SOTA) methods.\n\nStrengths:\n\n1. The paper proposes an innovative approach to enabling LLMs to handle TS data, which is an important problem in time series analysis.\n2. The proposed approach is data-centric, which makes it more efficient and cost-effective compared to model-centric approaches.\n3. The experiments demonstrate the effectiveness of the proposed approach on various TS tasks, including classification, forecasting, and representation.\n4. The paper provides a comprehensive analysis of the strengths and weaknesses of the proposed approach, as well as suggestions for future improvements.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained LLM has already learned the language patterns, which may not be the case in all scenarios.\n2. The proposed approach relies heavily on the quality of the text prototypes used in the text-prototype-aligned contrast, which may not be optimal in all cases.\n3. The paper does not provide a thorough comparison with other SOTA methods, which makes it difficult to evaluate the effectiveness of the proposed approach.\n4. The experiments are limited to a small number of datasets, which may not be representative of all TS tasks.\n\nSuggestions for Improvement:\n\n1. To improve the robustness of the proposed approach, the authors can investigate the use of more robust text prototypes or alternative contrastive loss functions.\n2. To further evaluate the effectiveness of the proposed approach, the authors can conduct experiments on a larger number of datasets and compare the results with other SOTA methods.\n3. To address the limitation of assuming that the pre-trained LLM has already learned the language patterns, the authors can investigate the use of language models that are specifically trained for TS tasks.\n4. To improve the efficiency of the proposed approach, the authors can investigate the use of more efficient contrastive loss functions or alternative optimization algorithms.\n\nRecommendation:\n\nThe proposed approach is an innovative and effective way to enable LLMs to handle TS data. While there are some limitations and areas for improvement, the approach has the potential to make significant contributions to the field of time series analysis. I recommend that the authors continue to develop and refine the proposed approach, and conduct further experiments to evaluate its effectiveness on a larger number of datasets.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.7259615384615384,
        "gr_score_without": 0.6520618556701031,
        "output_without": {
            "num_tokens_scored": 388,
            "num_green_tokens": 253,
            "green_fraction": 0.6520618556701031,
            "z_score": 2.0934763648080104,
            "p_value": 0.018153325885369154
        },
        "output_with": {
            "num_tokens_scored": 416,
            "num_green_tokens": 302,
            "green_fraction": 0.7259615384615384,
            "z_score": 5.244381581532018,
            "p_value": 7.840368880262292e-08
        }
    },
    {
        "title": "SHARING RATIO DECOMPOSITION",
        "abstract": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep learning models. SRD analyzes the model using Pointwise Feature Vectors (PFVs) and decomposes relevance with sharing ratios, accounting for nonlinear interactions between filters. The method is based on the observation of Activation-Pattern-Only Prediction (APOP), which highlights the importance of inactive neurons in model behavior. SRD is shown to outperform other saliency map methods across various metrics, including effectiveness, sophistication, and resilience, particularly in terms of robustness against random noise perturbation and targeted adversarial attacks.\n\nStrengths:\n\n* The paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods.\n* The method is based on a thorough reflection of the model's behavior, which is reflected in its robustness against random noise perturbation and targeted adversarial attacks.\n* The paper provides a clear and well-structured explanation of the method, including its mathematical formulation and experimental results.\n* The authors have conducted a comprehensive analysis, including qualitative and quantitative results, which demonstrate the superiority of SRD over other saliency map methods.\n\nWeaknesses:\n\n* The paper assumes that the sharing ratio can be derived at any point, even during inference, which may not be feasible in practice.\n* The method requires prior knowledge of the sharing ratio, which may not be available in all cases.\n* The paper does not provide a clear explanation of how the sharing ratio is calculated, which may be a limitation of the method.\n* The experimental results are limited to a specific dataset and model architecture, which may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the sharing ratio is calculated and how it is used in the method.\n* Investigate the feasibility of deriving the sharing ratio at any point during inference and provide a clear explanation of the challenges and limitations.\n* Conduct experiments on a wider range of datasets and model architectures to demonstrate the robustness and generalizability of SRD.\n* Provide a more detailed analysis of the APOP phenomenon and its implications for model behavior.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the method and demonstrates its superiority over other saliency map methods. Therefore, I recommend this paper for publication in a reputable conference or journal.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep neural networks. SRD adopts a vector perspective to analyze the model's behavior, considering the intricate nonlinear interactions between filters. It also introduces the concept of Activation-Pattern-Only Prediction (APOP), which highlights the importance of inactive neurons in model behavior. The method decomposes the relevance of each pointwise feature vector (PFV) into the shares of PFVs in its receptive field, providing high-resolution effective receptive fields (ERFs) at any layer. The authors conduct extensive experiments on various state-of-the-art methods and demonstrate that SRD outperforms them in terms of effectiveness, sophistication, and resilience.\n\nStrengths:\n\n1. SRD provides faithful and robust explanations for deep neural networks, which is a significant improvement over existing methods.\n2. The method adopts a vector perspective, which is more comprehensive than conventional neuron-level analyses.\n3. The introduction of APOP highlights the importance of inactive neurons in model behavior, which is an interesting and novel contribution.\n4. The recursive decomposition of PFVs provides high-resolution ERFs at any layer, enabling comprehensive exploration from local to global explanations.\n\nWeaknesses:\n\n1. The paper assumes that the sharing ratio between layers can be derived at any point, which may not be the case in practice.\n2. The method requires prior knowledge of the sharing ratio, which can be challenging to obtain.\n3. The authors do not provide a thorough comparison with other methods that also adopt a vector perspective, such as LayerCAM.\n4. The paper focuses primarily on image classification tasks and does not explore the applicability of SRD to other domains.\n\nSuggestions for Improvement:\n\n1. Provide more details on how to derive the sharing ratio between layers, especially in situations where this information is not readily available.\n2. Investigate the applicability of SRD to other domains, such as natural language processing or recommender systems.\n3. Compare SRD with other methods that adopt a vector perspective, such as LayerCAM, to provide a more comprehensive evaluation.\n4. Provide more examples and visualizations to illustrate the effectiveness and robustness of SRD.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of XAI, providing a novel and comprehensive method for explaining deep neural networks. While there are some limitations and areas for improvement, the authors' efforts to provide faithful and robust explanations are commendable. With further refinement and exploration, SRD has the potential to become a leading method for XAI in various domains. I recommend this paper for publication in a top-tier conference or journal, with the suggestion that the authors address the areas mentioned above for improvement.\n\n$$$",
        "gr_score_with": 0.7220956719817767,
        "gr_score_without": 0.6288659793814433,
        "output_without": {
            "num_tokens_scored": 388,
            "num_green_tokens": 244,
            "green_fraction": 0.6288659793814433,
            "z_score": 1.1608231763901813,
            "p_value": 0.12285690789170911
        },
        "output_with": {
            "num_tokens_scored": 439,
            "num_green_tokens": 317,
            "green_fraction": 0.7220956719817767,
            "z_score": 5.222070155767614,
            "p_value": 8.846695386658829e-08
        }
    },
    {
        "title": "CROSSLOCO: HUMAN MOTION DRIVEN CONTROL OF LEGGED ROBOTS VIA GUIDED UNSUPERVISED REIN-",
        "abstract": "Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents CrossLoco, a guided unsupervised reinforcement learning framework that enables simultaneous learning of human-robot motion correspondence and robot motion control. The framework introduces a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and synthesized robot movements. The authors demonstrate that CrossLoco can translate a large set of human motions for robots, including walking, running, and dancing, and achieve better quantitative results in terms of accuracy, diversity, and user preference compared to baseline methods.\n\nStrengths:\n\n* The paper presents a novel framework for learning human-robot motion correspondence and robot motion control in an unsupervised manner.\n* The cycle-consistency-based correspondence reward term is a key innovation that enables the learning of complex robot skills and human-robot motion correspondence.\n* The authors provide a thorough evaluation of their method, including quantitative and qualitative results, as well as a comparison with baseline methods.\n* The paper explores the potential applications of CrossLoco, including language2text motion synthesis and interactive robot control.\n\nWeaknesses:\n\n* The paper assumes that the human motion dataset is available, which may not be the case in many real-world scenarios.\n* The authors do not provide a clear explanation of how the cycle-consistency-based correspondence reward term is computed, which may make it difficult for readers to reproduce the results.\n* The paper does not discuss the scalability of CrossLoco to more complex scenarios, such as long-horizon human demonstrations or tool manipulation on a legged-manipulation robot.\n* The authors do not provide a clear comparison with other unsupervised reinforcement learning methods that do not use cycle-consistency.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the cycle-consistency-based correspondence reward term is computed, including the specific mathematical formulation and the parameters used.\n* Discuss the scalability of CrossLoco to more complex scenarios, such as long-horizon human demonstrations or tool manipulation on a legged-manipulation robot.\n* Compare CrossLoco with other unsupervised reinforcement learning methods that do not use cycle-consistency to provide a more comprehensive evaluation of the method.\n* Provide more details on the implementation of the framework, including the specific neural network architectures used and the hyperparameters employed.\n\nRecommendation:\n\nThe paper presents a novel and innovative framework for learning human-robot motion correspondence and robot motion control in an unsupervised manner. While there are some limitations and areas for improvement, the paper provides a thorough evaluation of the method and explores its potential applications. I recommend this paper for publication in a top-tier conference in the field of robotics and machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents CrossLoco, a guided unsupervised reinforcement learning framework that enables simultaneous learning of human-robot motion correspondence and robot motion control. The key innovation of CrossLoco is the introduction of a cycle-consistency-based reward term that maximizes the mutual information between human motions and synthesized robot movements. The framework is evaluated on a quadrupedal robot with 12 joints and a human dataset containing 50 motion trajectories. The results demonstrate the effectiveness of CrossLoco in transferring various human motions to a robot with a different morphology and establishing automatic correspondence between humans and robots.\n\nStrengths:\n\n* The paper presents a novel approach to learning human-robot motion correspondence and robot motion control in a guided unsupervised reinforcement learning framework.\n* The cycle-consistency-based reward term is a key innovation that enables the simultaneous learning of human-robot motion correspondence and robot motion control.\n* The evaluation of CrossLoco on a quadrupedal robot with 12 joints and a human dataset containing 50 motion trajectories demonstrates its effectiveness in transferring various human motions to a robot with a different morphology.\n* The paper presents a comprehensive set of experiments and evaluation metrics to assess the performance of CrossLoco.\n\nWeaknesses:\n\n* The paper assumes a fixed dataset of human motions and does not address the issue of how to learn a new set of human motions or adapt to changing environments.\n* The evaluation of CrossLoco is limited to a single type of robot and human dataset, and it is unclear how well the framework would generalize to other types of robots or human datasets.\n* The paper does not provide a clear explanation of how the cycle-consistency-based reward term is computed and how it is used to update the policy and mappers.\n\nSuggestions for Improvement:\n\n* To address the issue of learning a new set of human motions or adapting to changing environments, the authors could consider incorporating online learning or adaptation mechanisms into the CrossLoco framework.\n* To evaluate the generalizability of CrossLoco to other types of robots or human datasets, the authors could consider conducting experiments on a variety of robots and human datasets.\n* To provide a clearer explanation of how the cycle-consistency-based reward term is computed and how it is used to update the policy and mappers, the authors could include additional details in the paper.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to learning human-robot motion correspondence and robot motion control in a guided unsupervised reinforcement learning framework. While there are some limitations and areas for improvement, the paper provides a comprehensive set of experiments and evaluation metrics to assess the performance of CrossLoco. With further refinement and evaluation, CrossLoco has the potential to be a powerful tool for enabling robots to learn and adapt to new situations.",
        "gr_score_with": 0.6687116564417178,
        "gr_score_without": 0.53954802259887,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 191,
            "green_fraction": 0.53954802259887,
            "z_score": -2.321518053251903,
            "p_value": 0.9898705495507638
        },
        "output_with": {
            "num_tokens_scored": 326,
            "num_green_tokens": 218,
            "green_fraction": 0.6687116564417178,
            "z_score": 2.532575016447864,
            "p_value": 0.0056614069459559775
        }
    },
    {
        "title": "A PRECISE CHARACTERIZATION OF SGD STABILITY USING LOSS SURFACE GEOMETRY",
        "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors focus on the linearized dynamics of SGD close to the optimum, and introduce a novel coherence measure \u03c3 to capture the relevant geometric characteristics of the loss surface. They provide a sufficient condition for the linear dynamics to diverge, which relies solely on the values of \u03bb1(H), \u03b7, B, n, and \u03c3. The authors also demonstrate that their sufficient condition is nearly optimal across a natural range of SGD hyperparameters.\n\nStrengths:\n\n* The paper presents a novel coherence measure \u03c3 that summarizes relevant information in the loss surface geometry.\n* The authors provide a precise characterization of the stability of SGD using linearized dynamics.\n* The paper demonstrates that the sufficient condition for divergence is nearly optimal across a natural range of SGD hyperparameters.\n* The authors provide a series of experiments to validate their theoretical results.\n\nWeaknesses:\n\n* The paper assumes that the loss function is additively decomposable, which may not be the case in all scenarios.\n* The authors do not provide a detailed comparison of their results with other existing works in the field.\n* The paper assumes that the learning rate and batch size are fixed, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of their results with other existing works in the field.\n* The paper should be extended to consider more general loss functions and learning rates.\n* The authors should provide a more detailed analysis of the coherence measure \u03c3 and its relation to the stability of SGD.\n\nRecommendation:\n\nThe paper presents a novel and precise characterization of the stability of SGD using loss surface geometry. The authors provide a sufficient condition for divergence that is nearly optimal across a natural range of SGD hyperparameters. The paper also provides a series of experiments to validate their theoretical results. However, the paper assumes that the loss function is additively decomposable and does not provide a detailed comparison of their results with other existing works in the field. Overall, the paper is well-written and provides a significant contribution to the field of optimization.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors focus on linear stability, which is a predictive proxy for sharpness and generalization error in overparameterized neural networks. They introduce a novel coherence measure, \u03c3, that captures the geometric properties of the loss function relevant to linear stability. The sufficient condition for linear instability at an optimum is established, which hinges on this coherence measure and provides a simplified condition for identifying linear instability. The authors also demonstrate that their bound is nearly optimal across a range of hyperparameters and that it aligns with empirical findings on additively decomposable quadratic loss functions.\n\nStrengths:\n\n1. The paper provides a clear and well-structured presentation of the problem and its significance in understanding the stability of SGD.\n2. The authors introduce a novel coherence measure, \u03c3, that captures the geometric properties of the loss function relevant to linear stability, which is a significant contribution to the field.\n3. The sufficient condition for linear instability at an optimum is established, which provides a simplified condition for identifying linear instability.\n4. The authors demonstrate that their bound is nearly optimal across a range of hyperparameters, which adds to the confidence in their result.\n5. The empirical findings on additively decomposable quadratic loss functions are consistent with the theoretical results, which provides strong support for the validity of the coherence measure and the stability condition.\n\nWeaknesses:\n\n1. The paper assumes that the loss function is additively decomposable, which may limit its applicability to other types of loss functions.\n2. The coherence measure, \u03c3, may be difficult to compute in practice, especially for large neural networks, which could limit its utility.\n3. The paper does not provide a detailed comparison with other existing works on linear stability, which could be useful in understanding the significance of the contribution.\n4. The empirical findings are limited to additively decomposable quadratic loss functions, which may not be representative of all types of loss functions.\n5. The paper does not discuss the potential applications of the coherence measure and stability condition in practice, which could be useful in understanding its utility and significance.\n\nSuggestions for Improvement:\n\n1. Consider extending the coherence measure and stability condition to other types of loss functions, such as non-additively decomposable loss functions.\n2. Develop efficient methods for computing the coherence measure, \u03c3, in practice, especially for large neural networks.\n3. Provide a detailed comparison with other existing works on linear stability to understand the significance of the contribution.\n4. Expand the empirical findings to other types of loss functions and neural networks to understand its applicability and utility.\n5. Discuss potential applications of the coherence measure and stability condition in practice, such as in understanding the stability of SGD in real-world scenarios.\n\nRecommendation:\n\nOverall, the paper provides a significant contribution to the field by introducing a novel coherence measure and stability condition for linear stability of SGD. The empirical findings are consistent with the theoretical results, which provides strong support for the validity of the coherence measure and stability condition. However, there are some limitations and areas for improvement, such as extending the coherence measure and stability condition to other types of loss functions and developing efficient methods for computing the coherence measure in practice. With some refinements, this paper has the potential to be a high-quality contribution to the field.\n\nRecommendation for publication:\n\nYes, this paper has the potential to be a high-quality contribution to the field and should be considered for publication in a top-tier conference or journal in the field of machine learning and optimization.",
        "gr_score_with": 0.6871794871794872,
        "gr_score_without": 0.5884615384615385,
        "output_without": {
            "num_tokens_scored": 260,
            "num_green_tokens": 153,
            "green_fraction": 0.5884615384615385,
            "z_score": -0.379622546144228,
            "p_value": 0.647887188917185
        },
        "output_with": {
            "num_tokens_scored": 390,
            "num_green_tokens": 268,
            "green_fraction": 0.6871794871794872,
            "z_score": 3.5144992116412967,
            "p_value": 0.00022029198978675102
        }
    },
    {
        "title": "DON\u2019T TRUST: VERIFY \u2013 GROUNDING LLM QUANTI-",
        "abstract": "Large language models (LLM), such as Google\u2019s Minerva and OpenAI\u2019s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code \u2014 which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting \u2014 the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among multiple informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. DTV leverages the autoformalization capability of LLMs through few-shot prompting to translate informal statements into formal statements, and then uses a formal theorem proving environment to verify the correctness of the formal statements. The authors demonstrate the effectiveness of DTV by evaluating it on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. The results show that DTV consistently outperforms vanilla majority voting, the best previous approach, and leads to improvement across different model sizes from 8B, 62B, and 540B.\n\nStrengths:\n\n* The paper proposes a novel approach to identify the correct answer among multiple informal solution samples generated by LLMs.\n* The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith.\n* The paper provides a clear and concise description of the DTV approach and its implementation details.\n* The authors discuss the limitations of the current approach and provide suggestions for future work.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used for autoformalization have sufficient training data and capabilities to translate informal statements into formal statements accurately.\n* The authors do not provide a thorough analysis of the errors made by the LLMs in translating informal statements into formal statements.\n* The paper relies on the consistency of the formal environment and its automated theorem prover to verify the correctness of the formal statements, which may not always be the case.\n* The authors do not provide a detailed discussion of the computational cost and time complexity of the DTV approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the errors made by the LLMs in translating informal statements into formal statements and discuss ways to mitigate these errors.\n* The paper should include a more detailed discussion of the computational cost and time complexity of the DTV approach and provide suggestions for improving its efficiency.\n* The authors should provide more case studies and examples to demonstrate the effectiveness of DTV on a wider range of quantitative reasoning problems.\n* The paper should include a more detailed discussion of the limitations of the current approach and provide suggestions for future work to overcome these limitations.\n\nRecommendation:\n\nThe paper is well-written and provides a clear and concise description of the DTV approach and its implementation details. The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. However, the paper assumes that the LLMs used for autoformalization have sufficient training data and capabilities to translate informal statements into formal statements accurately, which may not always be the case. Therefore, I recommend that the authors provide a more thorough analysis of the errors made by the LLMs in translating informal statements into formal statements and discuss ways to mitigate these errors. I also recommend that the authors provide more case studies and examples to demonstrate the effectiveness of DTV on a wider range of quantitative reasoning problems.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among many informal solution samples generated by large language models (LLMs) for mathematical quantitative reasoning problems. The approach leverages the autoformalization capability of LLMs to translate informal statements and solutions into formal Isabelle code, which can be verified automatically for internal consistency. The authors demonstrate the effectiveness of DTV on GSM8K, MATH, and MultiArith datasets and show that it outperforms vanilla majority voting, the previously best method to identify correct answers.\n\nStrengths:\n\n1. The paper proposes a novel approach to identifying correct answers in LLMs for mathematical quantitative reasoning problems.\n2. The approach leverages the autoformalization capability of LLMs to translate informal statements and solutions into formal Isabelle code.\n3. The authors demonstrate the effectiveness of DTV on GSM8K, MATH, and MultiArith datasets and show that it outperforms vanilla majority voting.\n4. The approach is lightweight with no training or finetuning required.\n\nWeaknesses:\n\n1. The approach requires the theorem proving environment to support the problem domain in question, which may limit its applicability.\n2. The LLM's capability to translate a theorem written in natural language into a formal statement may be prone to mistakes.\n3. The approach may not be effective for problems involving geometric and probabilistic reasoning.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more effective filters to mitigate the limitations of LLMs in translating theorems into formal statements.\n2. Explore the use of reinforcement learning from human feedback (RLHF) to further finetune LLMs on aligned mathematical formalizations directly.\n3. Investigate the use of other formal theorem proving environments to expand the scope of the approach.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to identifying correct answers in LLMs for mathematical quantitative reasoning problems. The approach has the potential to improve the reliability of LLMs in this domain and is a valuable contribution to the field. However, the limitations of the approach need to be addressed to make it more effective and applicable to a wider range of problems.\n\n$$$",
        "gr_score_with": 0.7025316455696202,
        "gr_score_without": 0.5919003115264797,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 190,
            "green_fraction": 0.5919003115264797,
            "z_score": -0.29604890296848735,
            "p_value": 0.6164036324083084
        },
        "output_with": {
            "num_tokens_scored": 316,
            "num_green_tokens": 222,
            "green_fraction": 0.7025316455696202,
            "z_score": 3.720615717131424,
            "p_value": 9.936882649676153e-05
        }
    },
    {
        "title": "PERTURBATION-BASED GNN EXPLAINERS THROUGH",
        "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a comprehensive benchmarking study on perturbation-based explainability methods for Graph Neural Networks (GNNs). The authors evaluate and compare a wide range of explainability techniques, including factual and counterfactual reasoners, and identify their strengths and limitations. The study surfaces new insights that have not been studied in existing benchmarking literature, including the stability to noise and variational factors and the feasibility of counterfactual recourses. The authors also provide a meticulously curated, publicly accessible code base for the benchmarking study.\n\nStrengths:\n\n* The paper provides a comprehensive evaluation of perturbation-based explainability methods for GNNs, including both factual and counterfactual reasoners.\n* The study surfaces new insights that have not been studied in existing benchmarking literature, including the stability to noise and variational factors and the feasibility of counterfactual recourses.\n* The authors provide a meticulously curated, publicly accessible code base for the benchmarking study.\n\nWeaknesses:\n\n* The paper focuses primarily on perturbation-based explainability methods, and it would be beneficial to include other types of explainability methods in the study.\n* The evaluation metrics used in the study are limited to sufficiency, stability, and feasibility, and it would be beneficial to include other metrics, such as interpretability and reliability.\n* The paper does not provide a clear explanation of how the explainability methods were implemented and evaluated, which makes it difficult to replicate the results.\n\nSuggestions for Improvement:\n\n* Include other types of explainability methods in the study, such as model-based and hybrid methods.\n* Use a more comprehensive set of evaluation metrics, including interpretability and reliability.\n* Provide a clear explanation of how the explainability methods were implemented and evaluated, including the code and datasets used.\n* Consider using more advanced evaluation metrics, such as the accuracy of the explanations and the robustness of the explanations to noise and variability.\n\nRecommendation:\n\nThe paper provides a comprehensive evaluation of perturbation-based explainability methods for GNNs, and it surfaces new insights that have not been studied in existing benchmarking literature. However, the study has some limitations, including the focus on perturbation-based methods and the limited evaluation metrics used. Therefore, I recommend that the authors consider including other types of explainability methods in the study and using a more comprehensive set of evaluation metrics. Additionally, the authors should provide a clear explanation of how the explainability methods were implemented and evaluated, including the code and datasets used. With these improvements, the paper will provide a more comprehensive and reliable evaluation of explainability methods for GNNs.\n\nRating: 7/10\n\nThis paper provides a comprehensive evaluation of perturbation-based explainability methods for GNNs, and it surfaces new insights that have not been studied in existing benchmarking literature. However, the study has some limitations, including the focus on perturbation-based methods and the limited evaluation metrics used. With some improvements, the paper will provide a more comprehensive and reliable evaluation of explainability methods for GNNs.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a comprehensive benchmarking study on perturbation-based GNN explainers for factual and counterfactual reasoning. The authors evaluate the performance of 11 state-of-the-art methods across 8 datasets for node classification and 7 datasets for graph classification. The study focuses on the stability of explanations against topological noise, model parameters, and different GNN architectures. The authors also investigate the feasibility of counterfactual explanations in molecular graphs. The results show that RCEXPLAINER consistently outperforms other methods in terms of efficacy and stability to noise and variational factors. The study highlights the importance of considering stability as a desirable factor in GNN explainers.\n\nStrengths:\n\n* The paper presents a comprehensive benchmarking study on perturbation-based GNN explainers.\n* The authors evaluate the performance of 11 state-of-the-art methods across 8 datasets for node classification and 7 datasets for graph classification.\n* The study focuses on the stability of explanations against topological noise, model parameters, and different GNN architectures.\n* The authors investigate the feasibility of counterfactual explanations in molecular graphs.\n\nWeaknesses:\n\n* The paper assumes that the GNN model is fixed and does not consider the impact of different GNN architectures on the performance of the explainers.\n* The study only considers perturbation-based GNN explainers and does not evaluate other types of explainers.\n* The authors do not provide a detailed analysis of the limitations of the current GNN explainers and potential future research directions.\n\nSuggestions for Improvement:\n\n* Consider evaluating the performance of GNN explainers on different GNN architectures and exploring the impact of architecture changes on the performance of the explainers.\n* Investigate the performance of other types of GNN explainers, such as gradient-based or decomposition-based explainers.\n* Provide a more detailed analysis of the limitations of the current GNN explainers and potential future research directions.\n\nRecommendation:\n\nThe paper presents a comprehensive benchmarking study on perturbation-based GNN explainers and provides valuable insights into the performance of different methods. However, the study has some limitations, such as assuming a fixed GNN architecture and only considering perturbation-based explainers. To improve the study, the authors should consider evaluating the performance of GNN explainers on different GNN architectures and exploring the impact of architecture changes on the performance of the explainers.",
        "gr_score_with": 0.65748031496063,
        "gr_score_without": 0.5631399317406144,
        "output_without": {
            "num_tokens_scored": 293,
            "num_green_tokens": 165,
            "green_fraction": 0.5631399317406144,
            "z_score": -1.2877405021578556,
            "p_value": 0.9010818451649367
        },
        "output_with": {
            "num_tokens_scored": 254,
            "num_green_tokens": 167,
            "green_fraction": 0.65748031496063,
            "z_score": 1.8701001908389219,
            "p_value": 0.030734952999385893
        }
    },
    {
        "title": "SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-ofthe-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces SWE-bench, a novel benchmark for evaluating the capabilities of language models (LMs) in software engineering tasks. SWE-bench consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The benchmark tasks require LMs to edit codebases to address specific issues, often involving complex reasoning, context processing, and code editing. The paper evaluates several state-of-the-art LMs, including proprietary models and fine-tuned models, and finds that they struggle to solve even the simplest issues, with the best-performing model, Claude 2, able to resolve only 1.96% of the issues.\n\nStrengths:\n\n1. Novel benchmark: SWE-bench is a unique and challenging benchmark that pushes the limits of LMs in software engineering tasks.\n2. Real-world relevance: The benchmark is based on real-world GitHub issues and pull requests, making it highly relevant to software engineering practice.\n3. Comprehensive evaluation: The paper evaluates multiple LMs, including proprietary and fine-tuned models, providing a comprehensive assessment of their capabilities.\n4. Detailed analysis: The paper provides a thorough analysis of the results, including a discussion of the difficulties faced by LMs in software engineering tasks.\n\nWeaknesses:\n\n1. Limited scope: The paper focuses on Python repositories, which may limit the generalizability of the results to other programming languages.\n2. Evaluation metrics: The paper uses a single metric, percentage of issues resolved, which may not capture the full range of LM capabilities.\n3. Lack of human evaluation: The paper does not include human evaluation of the LM-generated patches, which could provide additional insights into their quality and effectiveness.\n4. Limited discussion of future work: The paper does not discuss potential future directions for research, such as improving LM performance or exploring new software engineering tasks.\n\nSuggestions for Improvement:\n\n1. Expand the scope of the benchmark: Consider including repositories from other programming languages to increase the generalizability of the results.\n2. Develop additional evaluation metrics: Consider using metrics such as patch quality, code maintainability, or developer productivity to provide a more comprehensive assessment of LM capabilities.\n3. Include human evaluation: Conduct human evaluation of the LM-generated patches to provide additional insights into their quality and effectiveness.\n4. Discuss future work: Provide a discussion of potential future directions for research, such as improving LM performance or exploring new software engineering tasks.\n\nRecommendation:\n\nThe paper presents a novel and challenging benchmark for evaluating the capabilities of LMs in software engineering tasks. While there are some limitations to the paper, the comprehensive evaluation and detailed analysis make it a valuable contribution to the field. I recommend the paper for publication in a top-tier conference or journal, with the suggestion to address the limitations mentioned above.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents SWE-bench, a new benchmark for evaluating language models (LMs) in software engineering settings. SWE-bench is a realistic evaluation framework that uses real-world software engineering problems drawn from GitHub issues and corresponding pull requests across 12 popular Python repositories. The benchmark requires models to generate patches to resolve issues in software codebases. The authors fine-tune their own model, SWE-Llama, and compare its performance to state-of-the-art proprietary models like Claude 2. The results show that all models struggle to resolve issues, with Claude 2 performing best but still only able to resolve 1.96% of issues. The authors provide several analyses of model performance and difficulties, including the impact of retrieval mechanisms and context length on model performance. They also present a qualitative analysis of SWE-Llama generations and highlight several trends and challenges in model performance.\n\nStrengths:\n\n* The paper presents a new and challenging benchmark for LMs in software engineering settings.\n* The authors provide a detailed and realistic evaluation framework that uses real-world software engineering problems.\n* The paper includes several analyses of model performance and difficulties, which provides valuable insights into the challenges of LMs in software engineering.\n* The authors present a fine-tuned model, SWE-Llama, and compare its performance to state-of-the-art proprietary models.\n\nWeaknesses:\n\n* The paper focuses primarily on the evaluation of LMs and does not explore potential applications or implications of SWE-bench.\n* The paper assumes that models will be fine-tuned for software engineering tasks, but it does not provide a clear explanation of how to fine-tune models for this domain.\n* The paper does not provide a detailed analysis of the challenges and difficulties of software engineering tasks for LMs.\n* The paper assumes that models will be able to handle complex software engineering tasks, but it does not provide a clear explanation of how to handle these complexities.\n\nSuggestions for Improvement:\n\n* The paper should provide a more detailed analysis of the challenges and difficulties of software engineering tasks for LMs.\n* The paper should provide a clear explanation of how to fine-tune models for software engineering tasks.\n* The paper should explore potential applications and implications of SWE-bench.\n* The paper should provide a more detailed analysis of the strengths and weaknesses of SWE-bench.\n\nRecommendation:\n\nThe paper presents a new and challenging benchmark for LMs in software engineering settings. The authors provide a detailed and realistic evaluation framework and several analyses of model performance and difficulties. The paper has several strengths and suggests several potential applications and implications of SWE-bench. However, it also has some weaknesses and suggestions for improvement. Overall, the paper is a valuable contribution to the field of natural language processing and software engineering.",
        "gr_score_with": 0.722397476340694,
        "gr_score_without": 0.5760598503740648,
        "output_without": {
            "num_tokens_scored": 401,
            "num_green_tokens": 231,
            "green_fraction": 0.5760598503740648,
            "z_score": -0.9783803138275606,
            "p_value": 0.8360568694970838
        },
        "output_with": {
            "num_tokens_scored": 317,
            "num_green_tokens": 229,
            "green_fraction": 0.722397476340694,
            "z_score": 4.4484859775352925,
            "p_value": 4.32388468096037e-06
        }
    },
    {
        "title": "HYPO: HYPERSPHERICAL OUT-OF-DISTRIBUTION GENERALIZATION",
        "abstract": "Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles\u2014ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at https://github.com/deeplearning-wisc/hypo.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, HYPO (HYPerspherical OOD generalization), for out-of-distribution (OOD) generalization. The framework is designed to learn domain-invariant representations in a hyperspherical space, where samples from the same class across different training domains are closely aligned with their class prototypes, while different class prototypes are maximally separated. The authors provide theoretical justifications on how their prototypical learning objective improves the OOD generalization bound and demonstrate the effectiveness of HYPO on challenging OOD benchmarks.\n\nStrengths:\n\n1. The paper provides a comprehensive and well-structured approach to OOD generalization, including theoretical justifications and empirical evaluations.\n2. The authors propose a novel framework, HYPO, which is designed to learn domain-invariant representations in a hyperspherical space.\n3. The paper provides a thorough analysis of the OOD generalization problem, including the definition of intra-class variation and inter-class separation.\n4. The authors provide a theoretical understanding of how their loss function reduces the OOD generalization error.\n\nWeaknesses:\n\n1. The paper assumes that the class prototypes are known, which may not be the case in practice.\n2. The authors do not provide a detailed analysis of the computational complexity of their algorithm.\n3. The paper assumes that the training data is i.i.d., which may not be the case in practice.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods for OOD generalization.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of their algorithm.\n2. The paper should include a more detailed comparison with other state-of-the-art methods for OOD generalization.\n3. The authors should provide a more detailed explanation of how they handle the case where the class prototypes are not known.\n4. The paper should include more detailed results on the robustness of HYPO to different types of corruption and domain shift.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to OOD generalization, and the authors provide a thorough analysis of the OOD generalization problem. The paper is well-structured and easy to follow, and the authors provide a clear explanation of their method. However, the paper assumes that the class prototypes are known, which may not be the case in practice, and the authors do not provide a detailed analysis of the computational complexity of their algorithm. Therefore, I recommend that the paper be accepted for publication with minor revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel framework, HYPO, for out-of-distribution (OOD) generalization, which learns domain-invariant representations in a hyperspherical space. The authors provide theoretical justifications on how their prototypical learning objective improves the OOD generalization bound. They demonstrate that their approach outperforms competitive baselines on challenging OOD benchmarks.\n\nStrengths:\n\n1. The authors provide a clear and well-structured paper that presents a new algorithm for OOD generalization, which is a challenging problem in machine learning.\n2. The proposed algorithm, HYPO, is theoretically justified, providing a provable understanding of how it reduces the OOD generalization error.\n3. The authors demonstrate the effectiveness of their approach on several challenging OOD benchmarks, including CIFAR-10, PACS, Office-Home, and VLCS.\n\nWeaknesses:\n\n1. The paper assumes that the training data is available from a finite set of environments, which may not be realistic in many real-world scenarios.\n2. The authors do not provide a thorough comparison with other state-of-the-art methods for OOD generalization, which makes it difficult to evaluate the significance of their results.\n3. The theoretical analysis assumes that the neural network is a linear model, which may not be realistic in many deep learning scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough comparison with other state-of-the-art methods for OOD generalization, including a discussion of their strengths and weaknesses.\n2. The paper should be made more realistic by assuming that the training data is available from a large set of environments, or by providing a method for learning from a small set of environments.\n3. The authors should provide a more thorough analysis of the theoretical results, including a discussion of the assumptions made and the limitations of the approach.\n\nRecommendation:\n\nThe paper presents a new algorithm for OOD generalization that is theoretically justified and demonstrates good performance on challenging benchmarks. However, the paper assumes a realistic scenario, and the authors should provide a more thorough comparison with other state-of-the-art methods. With some revisions, the paper could be a significant contribution to the field of OOD generalization.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.6310679611650486,
        "gr_score_without": 0.5340136054421769,
        "output_without": {
            "num_tokens_scored": 294,
            "num_green_tokens": 157,
            "green_fraction": 0.5340136054421769,
            "z_score": -2.3093555731463598,
            "p_value": 0.9895380709098626
        },
        "output_with": {
            "num_tokens_scored": 309,
            "num_green_tokens": 195,
            "green_fraction": 0.6310679611650486,
            "z_score": 1.1149385096360613,
            "p_value": 0.1324383864247472
        }
    },
    {
        "title": "DEEP TEMPORAL GRAPH CLUSTERING",
        "abstract": "Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and static graph clustering from several levels. To verify the superiority of the proposed framework TGC, we conduct extensive experiments. The experimental results show that temporal graph clustering enables more flexibility in finding a balance between time and space requirements, and our framework can effectively improve the performance of existing temporal graph learning methods. The code is released: https://github.com/MGitHubL/ Deep-Temporal-Graph-Clustering.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a general framework for deep Temporal Graph Clustering (TGC), which introduces two deep clustering modules to suit the interaction sequence-based batch-processing pattern of temporal graphs. The authors discuss the differences between temporal graph clustering and static graph clustering from several levels, including intuition, complexity, data, and experiment. To verify the superiority of the proposed framework TGC, the authors conduct extensive experiments and show that temporal graph clustering enables more flexibility in finding a balance between time and space requirements.\n\nStrengths:\n1. The paper provides a comprehensive introduction to temporal graph clustering and discusses its differences with static graph clustering.\n2. The proposed framework TGC is general and can be easily applied to different temporal graph methods.\n3. The authors conduct extensive experiments to demonstrate the effectiveness of TGC on existing temporal graph learning methods.\n4. The paper highlights the flexibility of temporal graph clustering in finding a balance between time and space requirements.\n\nWeaknesses:\n1. The paper lacks a thorough analysis of the complexity of the proposed framework TGC.\n2. The authors do not provide a detailed comparison of the performance of TGC with other state-of-the-art methods.\n3. The paper assumes that the interaction sequence-based batch-processing pattern is the most suitable approach for temporal graph clustering, without considering other possible approaches.\n4. The authors do not discuss the limitations of the proposed framework TGC and potential areas for future research.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the complexity of the proposed framework TGC and compare it with other state-of-the-art methods.\n2. Conduct a more thorough comparison of the performance of TGC with other state-of-the-art methods.\n3. Consider other possible approaches for temporal graph clustering and compare them with the proposed framework TGC.\n4. Discuss the limitations of the proposed framework TGC and potential areas for future research.\n\nRecommendation:\nThe paper proposes a general framework for deep Temporal Graph Clustering (TGC) and conducts extensive experiments to demonstrate its effectiveness. However, the paper lacks a thorough analysis of the complexity of the proposed framework and a detailed comparison of its performance with other state-of-the-art methods. Therefore, I recommend that the authors revise the paper to address these issues and provide a more comprehensive evaluation of the proposed framework. With these revisions, the paper has the potential to make a significant contribution to the field of temporal graph clustering.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a general framework for deep Temporal Graph Clustering (TGC), which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. The authors discuss the differences between temporal graph clustering and static graph clustering from several levels, including intuition, complexity, data, and experiments. They propose a simple general framework TGC, which includes two deep clustering modules: a temporal module for time information mining and a clustering module for node clustering. The authors conduct extensive experiments to verify the superiority of the proposed framework TGC and demonstrate its effectiveness on existing temporal graph learning methods.\n\nStrengths:\n\n* The paper proposes a novel framework for deep Temporal Graph Clustering, which is a significant contribution to the field of graph clustering.\n* The authors provide a comprehensive discussion on the differences between temporal graph clustering and static graph clustering, which is helpful for understanding the challenges and opportunities of temporal graph clustering.\n* The paper includes a thorough experimental evaluation, which demonstrates the effectiveness of the proposed framework TGC on various datasets and methods.\n\nWeaknesses:\n\n* The paper assumes that the temporal graph is represented as an interaction sequence, which may not be the case in all scenarios. The authors should discuss the limitations of this assumption and potential extensions to more general graph representations.\n* The paper uses a simple baseline method HTNE for comparison, which may not be the best choice for evaluating the performance of TGC. The authors should consider using more advanced methods as baselines for comparison.\n* The paper does not provide a clear explanation of how the clustering module is adapted to the batch-processing pattern of temporal graphs. The authors should provide more details on this aspect.\n\nSuggestions for Improvement:\n\n* The authors should provide more discussions on the limitations of the proposed framework TGC and potential extensions to more general graph representations.\n* The authors should consider using more advanced methods as baselines for comparison and provide a more thorough experimental evaluation.\n* The authors should provide more details on how the clustering module is adapted to the batch-processing pattern of temporal graphs.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of graph clustering, and the proposed framework TGC is a valuable tool for clustering nodes in temporal graphs. However, the paper has some limitations, and the authors should address these issues in future work. Overall, the paper is well-written, and the authors provide a thorough experimental evaluation. I recommend this paper for publication in a reputable journal or conference.\n\nRating: 8/10",
        "gr_score_with": 0.7318611987381703,
        "gr_score_without": 0.6514084507042254,
        "output_without": {
            "num_tokens_scored": 284,
            "num_green_tokens": 185,
            "green_fraction": 0.6514084507042254,
            "z_score": 1.7685882573922898,
            "p_value": 0.038481305900076306
        },
        "output_with": {
            "num_tokens_scored": 317,
            "num_green_tokens": 232,
            "green_fraction": 0.7318611987381703,
            "z_score": 4.792427933114379,
            "p_value": 8.238748455896943e-07
        }
    },
    {
        "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
        "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which builds upon the previous work of COMBINER (Guo et al., 2023). RECOMBINER addresses several limitations of COMBINER, including the use of factorized priors and posterior approximations that lack flexibility, the inability to adapt to local deviations from global patterns in the data, and the sensitivity to modeling choices and the variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization for neural network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The proposed method, RECOMBINER, is evaluated on various data modalities, including image, audio, video, and 3D protein structure data, and demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\nStrengths:\n\n1. The paper proposes several novel and non-trivial extensions to the previous work of COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices.\n2. The authors provide a comprehensive evaluation of the proposed method on various data modalities, including image, audio, video, and 3D protein structure data.\n3. The paper demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n4. The authors provide a clear and concise explanation of the proposed method and its extensions, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The encoding time complexity of RECOMBINER is high, which may limit its practical application.\n2. The paper does not provide a detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n3. The authors do not provide a clear explanation of how the proposed method can be extended to other data modalities.\n4. The paper does not discuss the potential applications of RECOMBINER in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n2. The paper should provide a clear explanation of how the proposed method can be extended to other data modalities.\n3. The authors should discuss the potential applications of RECOMBINER in real-world scenarios.\n4. The paper should provide a more detailed analysis of the encoding time complexity of RECOMBINER and propose potential solutions to reduce it.\n\nRecommendation:\n\nThe paper proposes a novel and competitive data compression method called RECOMBINER, which addresses several limitations of the previous work of COMBINER. The authors provide a comprehensive evaluation of the proposed method on various data modalities and demonstrate competitive results with the best INR-based methods. However, the encoding time complexity of RECOMBINER is high, which may limit its practical application. Therefore, I recommend that the authors provide a more detailed comparison with other state-of-the-art compression methods, discuss the potential applications of RECOMBINER in real-world scenarios, and propose potential solutions to reduce the encoding time complexity.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which is an extension of COMBINER, a recent data compression method that uses Bayesian Implicit Neural Representations (INR). RECOMBINER addresses several limitations of COMBINER, including its use of factorized priors and posterior approximations that lack flexibility, its inability to effectively adapt to local deviations from global patterns in the data, and its sensitivity to modeling choices and variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization of the INR weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The method is evaluated on several data modalities, including image, audio, video, and 3D protein structure data, and shows strong performance across all modalities.\n\nStrengths:\n\n1. The paper proposes a novel and effective data compression method that addresses several limitations of existing methods.\n2. The authors provide a comprehensive evaluation of their method on several data modalities, including image, audio, video, and 3D protein structure data.\n3. The paper provides a clear and well-structured presentation of the method and its evaluation.\n4. The authors provide a detailed analysis of the results and discuss the implications of their findings.\n\nWeaknesses:\n\n1. The paper assumes that the data can be represented as a continuous function, which may not be the case in some applications.\n2. The method relies on a specific choice of INR architecture, which may not be optimal for all data modalities.\n3. The paper does not provide a detailed analysis of the computational complexity of the method, which may be a concern for large-scale applications.\n4. The authors do not provide a comparison with other state-of-the-art data compression methods.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the method and discuss strategies for reducing it.\n2. The paper should include a comparison with other state-of-the-art data compression methods to provide a more comprehensive evaluation of RECOMBINER.\n3. The authors should provide a more detailed analysis of the limitations of the method and discuss potential avenues for future work.\n4. The paper should include a more detailed discussion of the implications of the results and their potential applications.\n\nRecommendation:\n\nThe paper proposes a novel and effective data compression method that addresses several limitations of existing methods. The authors provide a comprehensive evaluation of their method on several data modalities and provide a clear and well-structured presentation of the method and its evaluation. While there are some limitations and concerns, the paper is well-written and provides a significant contribution to the field of data compression. I recommend accepting the paper for publication in a reputable conference or journal.",
        "gr_score_with": 0.7089337175792507,
        "gr_score_without": 0.663768115942029,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 229,
            "green_fraction": 0.663768115942029,
            "z_score": 2.41790105843453,
            "p_value": 0.007805160385365116
        },
        "output_with": {
            "num_tokens_scored": 347,
            "num_green_tokens": 246,
            "green_fraction": 0.7089337175792507,
            "z_score": 4.142278029747442,
            "p_value": 1.719365604942112e-05
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, they propose S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling.\n\nStrengths:\n\n* The paper proposes a unified sampling framework (USF) that systematically studies the optional strategies for solver searching, which is a novel contribution to the field of diffusion probabilistic models.\n* The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, which is a significant insight.\n* The proposed framework allows free choices of solver strategy at each step, which provides a high degree of flexibility.\n* The authors propose a predictor-based search method (S3) that automatically optimizes the solver schedule, which is a useful tool for practitioners.\n\nWeaknesses:\n\n* The paper assumes that the reader has a good understanding of diffusion probabilistic models and their applications, which may not be the case for all readers.\n* The authors do not provide a detailed comparison with other state-of-the-art methods, which makes it difficult to evaluate the effectiveness of their proposed framework.\n* The paper does not provide a clear explanation of how the predictor-based search method (S3) works, which may make it difficult for readers to understand and replicate the results.\n* The authors do not provide a detailed analysis of the computational cost of their proposed framework, which may be a concern for practitioners who need to implement the framework in practice.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of the diffusion probabilistic models and their applications to make the paper more accessible to a wider audience.\n* The authors should provide a more detailed comparison with other state-of-the-art methods to evaluate the effectiveness of their proposed framework.\n* The authors should provide a clear explanation of how the predictor-based search method (S3) works, including the details of the predictor and the search process.\n* The authors should provide a detailed analysis of the computational cost of their proposed framework, including the time and memory requirements.\n\nRecommendation:\n\nThe paper proposes a novel and useful framework for diffusion probabilistic models, and the authors provide a clear explanation of their proposed framework. However, the paper assumes a good understanding of the topic, and the authors do not provide a detailed comparison with other state-of-the-art methods. Therefore, I recommend that the authors provide a more detailed explanation of the diffusion probabilistic models and their applications, and provide a more detailed comparison with other state-of-the-art methods. With these improvements, the paper would be more accessible to a wider audience and would provide a more comprehensive evaluation of the effectiveness of their proposed framework.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs), which aims to systematically study the impact of different solving strategies on the sampling process. The framework splits the solving process of one step into independent decisions of several components, including the choice of timestep, starting point, prediction type, order of Taylor expansion, derivative estimation method, and usage of ODE correctors. Based on this framework, the authors reveal that the quality and efficiency of training-free samplers can be further improved by designing appropriate solver schedules, which means different solving strategies assigned to each timestep. A predictor-based search method, S3, is also proposed to automatically optimize the solver schedule to get a better time-quality trade-off of sampling. Experiments show that S3 can find outstanding solver schedules which outperform the state-of-the-art sampling methods on various datasets.\n\nStrengths:\n\n* The paper proposes a unified sampling framework for systematical studying of solving strategies of DPMs, which provides a new perspective for the community to understand the impact of different solving strategies on the sampling process.\n* The framework allows different solving strategies in each timestep, making the sampler more flexible and potentially leading to better performance.\n* The predictor-based search method, S3, is proposed to automatically optimize the solver schedule, which can significantly reduce the search overhead and make the search process more efficient.\n* Experiments show that S3 can achieve much higher sample quality than baselines, especially with very few steps.\n\nWeaknesses:\n\n* The paper assumes that the underlying diffusion process is well-specified, which might not be the case in practice. The authors should provide more discussion on how to handle misspecified models.\n* The framework requires the choice of several hyperparameters, such as the order of Taylor expansion, derivative estimation method, and usage of ODE correctors. The choice of these hyperparameters might be challenging, and the authors should provide more guidance on how to select them.\n* The paper focuses on the sampling process, but the impact of the solver schedule on the downstream tasks, such as image generation, should be further explored.\n* The search space of the solver schedule is extremely large, and the authors should provide more discussion on how to efficiently search for the optimal solver schedule.\n\nSuggestions for Improvement:\n\n* Provide more discussion on how to handle misspecified models, including the impact of model misspecification on the sampling process and the downstream tasks.\n* Provide more guidance on how to select the hyperparameters, such as the order of Taylor expansion, derivative estimation method, and usage of ODE correctors.\n* Explore the impact of the solver schedule on the downstream tasks, such as image generation, and provide more experiments to demonstrate its effectiveness.\n* Investigate more efficient search methods for the solver schedule, such as using surrogate models or gradient-based optimization.\n\nRecommendation:\n\nThe paper proposes a unified sampling framework for systematical studying of solving strategies of DPMs, which provides a new perspective for the community to understand the impact of different solving strategies on the sampling process. The predictor-based search method, S3, is also proposed to automatically optimize the solver schedule, which can significantly reduce the search overhead and make the search process more efficient. Experiments show that S3 can achieve much higher sample quality than baselines, especially with very few steps. However, the paper assumes that the underlying diffusion process is well-specified, which might not be the case in practice. The choice of several hyperparameters, such as the order of Taylor expansion, derivative estimation method, and usage of ODE correctors, might be challenging. Therefore, I recommend that the authors provide more discussion on how to handle misspecified models, provide more guidance on how to select the hyperparameters, and explore the impact of the solver schedule on the downstream tasks.",
        "gr_score_with": 0.6851385390428212,
        "gr_score_without": 0.6133720930232558,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 211,
            "green_fraction": 0.6133720930232558,
            "z_score": 0.5064353956664477,
            "p_value": 0.3062755158784186
        },
        "output_with": {
            "num_tokens_scored": 397,
            "num_green_tokens": 272,
            "green_fraction": 0.6851385390428212,
            "z_score": 3.4628911640740143,
            "p_value": 0.00026720225504474473
        }
    }
]