[
    {
        "title": "REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS",
        "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to learning robust world models in the reward-free setting. The authors address the problem of generating curricula for training robust agents without access to a reward function during exploration. They introduce WAKER, an algorithm for actively sampling environments for exploration during reward-free training based on the estimated error of the world model in each environment. The authors demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.\n\nStrengths:\n\n1. The paper addresses a novel problem in the field of reinforcement learning, specifically the generation of curricula for training robust agents in the reward-free setting.\n2. The authors provide a clear and concise definition of the problem and a thorough analysis of the related work.\n3. The proposed algorithm, WAKER, is well-motivated and easy to understand, and the authors provide a detailed explanation of its components and how they work together.\n4. The experimental results are comprehensive and demonstrate the effectiveness of WAKER in improving robustness, efficiency, and generalisation.\n\nWeaknesses:\n\n1. The paper assumes that the world model learns a suitable representation model, which may not always be the case in practice.\n2. The authors rely on an intrinsically motivated policy to adequately explore the state-action space across a range of environments, which may pose a challenge for scalability to more complex environments.\n3. The paper does not provide a clear explanation of how the world model is updated during the training process, and how the error estimates are used to bias sampling towards more uncertain environments.\n4. The experimental results are limited to a few domains and tasks, and it is not clear how WAKER would perform in more complex or diverse environments.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the world model is updated during the training process, and how the error estimates are used to bias sampling towards more uncertain environments.\n2. Investigate the use of function approximation to estimate uncertainty throughout large parameter spaces, to improve scalability to more complex environments.\n3. Provide more experimental results to demonstrate the effectiveness of WAKER in a wider range of domains and tasks.\n4. Discuss the potential applications of WAKER in more complex or diverse environments, and how it could be adapted to handle such scenarios.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of reinforcement learning by proposing a novel approach to learning robust world models in the reward-free setting. The authors provide a clear and concise definition of the problem, a thorough analysis of the related work, and a well-motivated and easy-to-understand algorithm. The experimental results demonstrate the effectiveness of WAKER in improving robustness, efficiency, and generalisation. However, there are some limitations and weaknesses that need to be addressed in future work. Overall, I recommend accepting the paper for publication, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces the problem of learning robust world models from reward-free exploration. It presents WAKER, an algorithm that selects environments for data collection based on the estimated error of the world model. WAKER is designed to train robust world models without access to a reward function during exploration, and it achieves state-of-the-art performance in several pixel-based continuous control domains. The paper also introduces the Minimax World Model Error problem, which optimizes an upper bound on the Reward-Free Minimax Regret objective. WAKER is shown to improve robustness and generalization to out-of-distribution environments without requiring expert domain knowledge.\n\nStrengths:\n\n* The paper introduces a new problem and algorithm, WAKER, which achieves state-of-the-art performance in several pixel-based continuous control domains.\n* WAKER is designed to train robust world models without access to a reward function during exploration, which is a significant contribution.\n* The paper provides a clear and concise description of the algorithm and its theoretical foundations.\n* The experiments demonstrate the effectiveness of WAKER in improving robustness and generalization to out-of-distribution environments.\n\nWeaknesses:\n\n* The paper assumes that the representation model learnt by the world model is Markovian, which may not always be the case.\n* The paper assumes that the true latent dynamics function exists, which may not always be the case.\n* The paper uses a discrete buffer to estimate uncertainty, which may not be scalable to more complex environments.\n* The paper relies on an intrinsically motivated policy to explore the state-action space, which may pose a challenge for scalability to more complex environments.\n\nSuggestions for Improvement:\n\n* Investigate the assumption of Markovian representation model and explore alternative approaches.\n* Investigate the existence of the true latent dynamics function and explore alternative approaches.\n* Explore the use of function approximation to estimate uncertainty throughout large parameter spaces.\n* Investigate the use of WAKER for reward-free pretraining, followed by task-specific fine-tuning.\n\nRecommendation:\n\nI recommend the paper for acceptance. The paper introduces a new problem and algorithm, WAKER, which achieves state-of-the-art performance in several pixel-based continuous control domains. The paper provides a clear and concise description of the algorithm and its theoretical foundations, and the experiments demonstrate the effectiveness of WAKER in improving robustness and generalization to out-of-distribution environments. However, the paper assumes several strong assumptions, and the scalability of the approach to more complex environments remains an open question.\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\n\nPaper Summary:\n\nThe paper introduces the problem of learning robust world models from reward-free exploration. It presents WAKER, an algorithm that selects environments for data collection based on the estimated error of the world model. WAKER is designed to train robust world models without access to a reward function during exploration, and it achieves state-of-the-art performance in several pixel-based continuous control domains. The paper also introduces the Minimax World Model Error problem, which optimizes an upper bound on the Reward-Free Minimax Regret objective. WAKER is shown to improve robustness and generalization to out-of-distribution environments without requiring expert domain knowledge.\n\nStrengths:\n\n* The paper introduces a new problem and algorithm, WAKER, which achieves state-of-the-art performance in several pixel-based continuous control domains.\n* WAKER is designed to train robust world models without access to a reward function during exploration, which is a significant contribution.\n* The paper provides a clear and concise description of the algorithm and its theoretical foundations.\n* The experiments demonstrate the effectiveness of WAKER in improving robustness and generalization to out-of-distribution environments.\n\nWeaknesses:\n\n* The paper assumes that the representation model learnt by the world model is Markovian, which may not always be the case.\n* The paper assumes that the true latent dynamics function exists, which may not always be the case.\n* The paper uses a discrete buffer to estimate uncertainty, which may not be scalable to more complex environments.\n* The paper relies on an intrinsically motivated policy to explore the state-action space, which may pose a challenge for scalability to more complex environments.\n\nSuggestions for Improvement:\n\n* Investigate the assumption of Markovian representation model and explore alternative approaches.\n* Investigate the existence of the true latent dynamics function and explore alternative approaches.\n* Explore the use of function approximation to estimate uncertainty throughout large parameter spaces.\n* Investigate the use of WAKER for reward-free pretraining, followed by task-specific fine-tuning.\n\nRecommendation:\n\nI recommend the paper for acceptance. The paper introduces a new problem and algorithm, WAKER, which achieves state-of-the-art performance in several pixel-based continuous control domains. The paper provides a clear and concise description of the algorithm and its theoretical foundations, and the experiments demonstrate the effectiveness of WAKER in improving robustness and generalization to out-of-distribution environments. However, the paper assumes several strong assumptions, and the scalability of the approach to more complex environments remains an open question.\n\n$$$",
        "gr_score_with": 0.6303630363036303,
        "output_with_attack": {
            "num_tokens_scored": 303,
            "num_green_tokens": 191,
            "green_fraction": 0.6303630363036303,
            "z_score": 8.16588234629995,
            "p_value": 1.5954708119208302e-16
        },
        "output_with": {
            "num_tokens_scored": 303,
            "num_green_tokens": 191,
            "green_fraction": 0.6303630363036303,
            "z_score": 8.16588234629995,
            "p_value": 1.5954708119208302e-16
        },
        "output_without": {
            "num_tokens_scored": 384,
            "num_green_tokens": 192,
            "green_fraction": 0.5,
            "z_score": 3.9793445894722748,
            "p_value": 3.455275827868095e-05
        }
    },
    {
        "title": "TAG2TEXT: GUIDING VISION-LANGUAGE MODEL VIA IMAGE TAGGING",
        "abstract": "This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with an off-the-shelf detector with limited performance, our approach explicitly learns an image tagger using tags parsed from image-paired text and thus provides a strong semantic guidance to vision-language models. In this way, Tag2Text can utilize largescale annotation-free image tags in accordance with image-text pairs, and provides more diverse tag categories beyond objects. As a result, Tag2Text demonstrates the ability of a foundational image tagging model, with superior zero-shot performance even comparable to fully supervised models. Moreover, by leveraging the tagging guidance, Tag2Text effectively enhances the performance of visionlanguage models on both generation-based and alignment-based tasks. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art results with similar model sizes and data scales, demonstrating the efficacy of the proposed tagging guidance. Codes, demo and pre-trained models are available at https://github.com/xinyu1205/recognize-anything.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Tag2Text, a vision-language pre-training (VLP) framework that introduces image tagging into vision-language models to guide the learning of visual-linguistic features. Tag2Text utilizes largescale image-text pairs and automatically parses text into image tags, which serve as ground-truth labels for image tag recognition tasks. The framework encompasses a novel scheme of image-tag-text generation, enabling the model to effectively regulate the content and quality of the generated text with the guidance of recognized tags. Tag2Text demonstrates superior zero-shot recognition capabilities, even comparable to full supervision manners of ML-Decoder, and outperforms other SOTA vision-language models in common category recognition.\n\nStrengths:\n\n1. The paper presents a novel approach to vision-language pre-training by introducing image tagging into vision-language models.\n2. The framework utilizes largescale image-text pairs and automatically parses text into image tags, which serve as ground-truth labels for image tag recognition tasks.\n3. The paper demonstrates the effectiveness of tagging guidance in enhancing the performance and controllability of vision-language models.\n4. The framework encompasses a novel scheme of image-tag-text generation, enabling the model to effectively regulate the content and quality of the generated text with the guidance of recognized tags.\n5. The paper presents a comprehensive evaluation of Tag2Text on a wide range of vision-language tasks, including image tagging, image captioning, and image-text retrieval.\n\nWeaknesses:\n\n1. The paper relies heavily on largescale image-text pairs, which may not be available for all datasets or applications.\n2. The framework requires a significant amount of computational resources and time for pre-training and fine-tuning.\n3. The paper does not provide a clear explanation of how the model handles noisy or ambiguous tags parsed from the texts.\n4. The evaluation of Tag2Text on image-text retrieval tasks is limited to two benchmarks, which may not be representative of all possible applications.\n5. The paper does not provide a clear comparison of the performance of Tag2Text with other state-of-the-art models on all tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the model handles noisy or ambiguous tags parsed from the texts.\n2. The authors should provide a clear comparison of the performance of Tag2Text with other state-of-the-art models on all tasks.\n3. The authors should evaluate Tag2Text on a wider range of vision-language tasks and datasets to demonstrate its robustness and generalizability.\n4. The authors should provide more details on the implementation of the framework, including the choice of hyperparameters and the computational resources required.\n5. The authors should provide a clear explanation of how the model can be adapted to different applications and datasets.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to vision-language pre-training, and the framework demonstrates superior performance on a wide range of vision-language tasks. However, the paper has some limitations, including the reliance on largescale image-text pairs and the requirement of significant computational resources and time for pre-training and fine-tuning. With some modifications and improvements, the framework has the potential to become a state-of-the-art model for vision-language tasks. Therefore, I recommend this paper for publication in a top-tier conference or journal.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel vision-language pre-training (VLP) framework, named Tag2Text, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. The framework utilizes large-scale annotation-free image tags parsed from image-text pairs, enabling the model to effectively leverage tagging guidance for both generation-based and alignment-based tasks. The proposed approach, Tag2Text, demonstrates state-of-the-art performance on a wide range of downstream benchmarks, including image captioning, image-text retrieval, and multi-label recognition. The framework's effectiveness is attributed to its ability to learn a foundational image tagging model, which provides a strong semantic guidance to vision-language models.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to VLP, which introduces image tagging into vision-language models.\n2. The framework demonstrates state-of-the-art performance on a wide range of downstream benchmarks.\n3. The proposed approach learns a foundational image tagging model, which provides a strong semantic guidance to vision-language models.\n4. The framework effectively leverages tagging guidance for both generation-based and alignment-based tasks.\n5. The paper provides a comprehensive evaluation of the framework's performance on multiple benchmarks and tasks.\n\nWeaknesses:\n\n1. The paper relies on large-scale image-text pairs for pre-training, which may not be available for all applications.\n2. The framework assumes that image-text pairs are available, which may not be the case for all tasks.\n3. The paper does not provide a detailed analysis of the framework's performance on tasks with limited image-text pairs.\n4. The framework's performance may degrade if the quality of the image-text pairs is poor.\n5. The paper does not provide a detailed comparison of the framework's performance with other state-of-the-art VLP frameworks.\n\nSuggestions for Improvement:\n\n1. Investigate the framework's performance on tasks with limited image-text pairs.\n2. Develop a framework that can learn image tagging from image-text pairs with varying quality.\n3. Provide a detailed comparison of the framework's performance with other state-of-the-art VLP frameworks.\n4. Investigate the framework's performance on tasks with multiple languages and cultures.\n5. Develop a framework that can learn image tagging from image-text pairs with varying levels of noise and corruption.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to VLP, which introduces image tagging into vision-language models. The framework demonstrates state-of-the-art performance on a wide range of downstream benchmarks and effectively leverages tagging guidance for both generation-based and alignment-based tasks. However, the framework relies on large-scale image-text pairs for pre-training, which may not be available for all applications. Overall, the paper is well-written, and the proposed framework is a valuable contribution to the VLP community. The framework has the potential to improve the performance of vision-language models on a wide range of tasks and applications.\n\nRecommendation for Publication:\n\nThe paper is recommended for publication in a top-tier conference in the field of computer vision and natural language processing. The paper's contributions, evaluation, and results are all of high quality, and the framework has the potential to significantly impact the VLP community. However, the paper may benefit from additional evaluation and comparison with other state-of-the-art VLP frameworks to further establish its significance and impact.",
        "peer_review_with_watermark_attack": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel vision-language pre-training (VLP) framework, named Tag2Text, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. The framework utilizes large-scale annotation-free image tags parsed from image-text pairs, enabling the model to effectively leverage tagging guidance for both generation-based and alignment-based tasks. The proposed approach, Tag2Text, demonstrates state-of-the-art performance on a wide range of downstream benchmarks, including image captioning, image-text retrieval, and multi-label recognition. The framework's effectiveness is attributed to its ability to learn a foundational image tagging model, which provides a strong semantic guidance to vision-language models.\n\nStrengths:\n\n1. The paper proposes a novel and effective approach to VLP, which introduces image tagging into vision-language models.\n2. The framework demonstrates state-of-the-art performance on a wide range of downstream benchmarks.\n3. The proposed approach learns a foundational image tagging model, which provides a strong semantic guidance to vision-language models.\n4. The framework effectively leverages tagging guidance for both generation-based and alignment-based tasks.\n5. The paper provides a comprehensive evaluation of the framework's performance on multiple benchmarks and tasks.\n\nWeaknesses:\n\n1. The paper relies on large-scale image-text pairs for pre-training, which may not be available for all applications.\n2. The framework assumes that image-text pairs are available, which may not be the case for all tasks.\n3. The paper does not provide a detailed analysis of the framework's performance on tasks with limited image-text pairs.\n4. The framework's performance may degrade if the quality of the image-text pairs is poor.\n5. The paper does not provide a detailed comparison of the framework's performance with other state-of-the-art VLP frameworks.\n\nSuggestions for Improvement:\n\n1. Investigate the framework's performance on tasks with limited image-text pairs.\n2. Develop a framework that can learn image tagging from image-text pairs with varying quality.\n3. Provide a detailed comparison of the framework's performance with other state-of-the-art VLP frameworks.\n4. Investigate the framework's performance on tasks with multiple languages and cultures.\n5. Develop a framework that can learn image tagging from image-text pairs with varying levels of noise and corruption.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to VLP, which introduces image tagging into vision-language models. The framework demonstrates state-of-the-art performance on a wide range of downstream benchmarks and effectively leverages tagging guidance for both generation-based and alignment-based tasks. However, the framework relies on large-scale image-text pairs for pre-training, which may not be available for all applications. Overall, the paper is well-written, and the proposed framework is a valuable contribution to the VLP community. The framework has the potential to improve the performance of vision-language models on a wide range of tasks and applications.\n\nRecommendation for Publication:\n\nThe paper is recommended for publication in a top-tier conference in the field of computer vision and natural language processing. The paper's contributions, evaluation, and results are all of high quality, and the framework has the potential to significantly impact the VLP community. However, the paper may benefit from additional evaluation and comparison with other state-of-the-art VLP frameworks to further establish its significance and impact.",
        "gr_score_with": 0.6183844011142061,
        "output_with_attack": {
            "num_tokens_scored": 359,
            "num_green_tokens": 222,
            "green_fraction": 0.6183844011142061,
            "z_score": 8.42564971385346,
            "p_value": 1.7937697125760612e-17
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 222,
            "green_fraction": 0.6183844011142061,
            "z_score": 8.42564971385346,
            "p_value": 1.7937697125760612e-17
        },
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 220,
            "green_fraction": 0.5583756345177665,
            "z_score": 6.395909087821364,
            "p_value": 7.979747169885688e-11
        }
    },
    {
        "title": "GNERP: GAUSSIAN-GUIDED NEURAL RECONSTRUC-",
        "abstract": "Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)\u2013based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel method for reconstructing the detailed geometry of reflective scenes using a Gaussian-based representation of normals and polarization priors. The method, called GNeRP, extends the geometry representation from scalar Signed Distance Functions (SDFs) to Gaussian fields of normals, which captures high-frequency details and separates complex geometry from specular reflection. The method is supervised by polarization priors, which reveal information about surface normals, and a Degree of Polarization (DoP) based reweighting strategy is introduced to alleviate noise issues in polarization priors.\n\nStrengths:\n1. The paper presents a novel approach for reconstructing reflective scenes, which is a challenging problem in computer vision.\n2. The Gaussian-based representation of normals is a more informative representation of geometry than scalar SDFs, and it captures high-frequency details.\n3. The use of polarization priors as supervision is a novel and effective way to guide the learning of geometry behind specular reflection.\n4. The DoP reweighing strategy is a useful technique for alleviating noise issues in polarization priors.\n\nWeaknesses:\n1. The paper relies heavily on the assumption that the incident light is unpolarized, which may not be true in all cases.\n2. The method requires a large amount of data, including radiance images and aligned polarization priors, which may be difficult to obtain.\n3. The method is computationally expensive, requiring a significant amount of time and resources to train.\n4. The paper does not provide a thorough evaluation of the method's performance on a wide range of scenes and datasets.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the limitations of the method and potential areas for improvement.\n2. The authors should provide a more thorough evaluation of the method's performance on a wide range of scenes and datasets.\n3. The authors should consider using more robust and efficient methods for estimating polarization priors.\n4. The authors should consider using more advanced techniques for alleviating noise issues in polarization priors.\n\nRecommendation:\nThe paper presents a novel and effective approach for reconstructing reflective scenes, and it has the potential to make a significant impact in the field of computer vision. However, the method has some limitations and areas for improvement, and the authors should address these issues in future work. I recommend that the paper be accepted for publication, but with the caveat that the authors should provide a more thorough evaluation of the method's performance and address the limitations and areas for improvement.\n\nRating: 8/10",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel method, GNeRP, for reconstructing the detailed geometry of reflective scenes. The method uses a Gaussian-based representation of normals, supervised by polarization priors, to capture more details than existing methods. A Degree of Polarization (DoP) based reweighting strategy is introduced to alleviate noise issues of polarization priors. The method is evaluated on a new, challenging multi-view dataset, PolRef, and compared with state-of-the-art neural 3D reconstruction methods.\n\nStrengths:\n\n* The proposed Gaussian-based representation of normals is more informative than existing methods, capturing both low-frequency orientation and high-frequency details.\n* The use of polarization priors as supervision improves the accuracy of geometry reconstruction, especially in reflective scenes.\n* The DoP reweighing strategy effectively alleviates noise issues of polarization priors, making the method more robust.\n\nWeaknesses:\n\n* The method requires polarization priors as input, which may not be available in all scenarios.\n* The reweighing strategy may not work well in cases where the DoP is very low or very high, which could lead to biased results.\n\nSuggestions for Improvement:\n\n* Investigate alternative methods for obtaining polarization priors, such as using machine learning-based methods or incorporating polarization information from other sources.\n* Further analyze the robustness of the reweighing strategy in cases with low or high DoP values, and propose modifications or adjustments to improve performance.\n\nRecommendation:\n\nThe proposed method, GNeRP, is a significant advancement in neural 3D reconstruction, particularly for reflective scenes. Its ability to capture detailed geometry and robust performance make it a strong candidate for various applications, including computer vision, graphics, and robotics. However, further investigation and refinement of the method, as suggested above, would be beneficial to ensure its widespread adoption.\n\nOverall, the paper presents a well-written, clear, and concise account of the method, including its strengths, weaknesses, and suggestions for improvement. The experimental results and comparisons with state-of-the-art methods demonstrate the method's effectiveness, making it a valuable contribution to the field of neural 3D reconstruction.",
        "peer_review_with_watermark_attack": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel method, GNeRP, for reconstructing the detailed geometry of reflective scenes. The method uses a Gaussian-based representation of normals, supervised by polarization priors, to capture more details than existing methods. A Degree of Polarization (DoP) based reweighting strategy is introduced to alleviate noise issues of polarization priors. The method is evaluated on a new, challenging multi-view dataset, PolRef, and compared with state-of-the-art neural 3D reconstruction methods.\n\nStrengths:\n\n* The proposed Gaussian-based representation of normals is more informative than existing methods, capturing both low-frequency orientation and high-frequency details.\n* The use of polarization priors as supervision improves the accuracy of geometry reconstruction, especially in reflective scenes.\n* The DoP reweighing strategy effectively alleviates noise issues of polarization priors, making the method more robust.\n\nWeaknesses:\n\n* The method requires polarization priors as input, which may not be available in all scenarios.\n* The reweighing strategy may not work well in cases where the DoP is very low or very high, which could lead to biased results.\n\nSuggestions for Improvement:\n\n* Investigate alternative methods for obtaining polarization priors, such as using machine learning-based methods or incorporating polarization information from other sources.\n* Further analyze the robustness of the reweighing strategy in cases with low or high DoP values, and propose modifications or adjustments to improve performance.\n\nRecommendation:\n\nThe proposed method, GNeRP, is a significant advancement in neural 3D reconstruction, particularly for reflective scenes. Its ability to capture detailed geometry and robust performance make it a strong candidate for various applications, including computer vision, graphics, and robotics. However, further investigation and refinement of the method, as suggested above, would be beneficial to ensure its widespread adoption.\n\nOverall, the paper presents a well-written, clear, and concise account of the method, including its strengths, weaknesses, and suggestions for improvement. The experimental results and comparisons with state-of-the-art methods demonstrate the method's effectiveness, making it a valuable contribution to the field of neural 3D reconstruction.",
        "gr_score_with": 0.5730994152046783,
        "output_with_attack": {
            "num_tokens_scored": 342,
            "num_green_tokens": 196,
            "green_fraction": 0.5730994152046783,
            "z_score": 6.511775546533351,
            "p_value": 3.7133819310240305e-11
        },
        "output_with": {
            "num_tokens_scored": 342,
            "num_green_tokens": 196,
            "green_fraction": 0.5730994152046783,
            "z_score": 6.511775546533351,
            "p_value": 3.7133819310240305e-11
        },
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 174,
            "green_fraction": 0.5058139534883721,
            "z_score": 3.9839922853030934,
            "p_value": 3.3883536711042865e-05
        }
    },
    {
        "title": "VERGENCE AND GENERALIZATION FOR EMBODIED AI",
        "abstract": "Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent\u2019s focus from task-relevant visual cues. Inspired by selective attention in humans\u2014the process through which people filter their perception based on their experiences, knowledge, and the task at hand\u2014we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. Code is available on the project page.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to embodied artificial intelligence (AI) by introducing a parameter-efficient codebook module to filter visual stimuli for embodied AI agents. The codebook module is trained jointly with the overall policy to optimize task rewards and acts as a task-conditioned selective filter over the visual observation. The authors demonstrate the effectiveness of their approach by achieving state-of-the-art performances on object goal navigation and object displacement tasks across 5 benchmarks. They also show that the filtered representations produced by the codebook generalize better and converge faster when adapted to other simulation environments.\n\nStrengths:\n\n* The paper presents a novel approach to embodied AI that draws inspiration from selective attention in humans.\n* The codebook module is parameter-efficient and can be trained jointly with the overall policy, making it a computationally efficient solution.\n* The authors demonstrate the effectiveness of their approach on multiple benchmarks and show that it outperforms state-of-the-art baselines.\n* The qualitative and quantitative analyses presented in the paper provide strong evidence for the benefits of the codebook module.\n\nWeaknesses:\n\n* The paper assumes that the visual encoder is frozen and does not explore the possibility of jointly training the visual encoder and the codebook module.\n* The authors do not provide a detailed analysis of the computational cost of the codebook module and how it compares to other approaches.\n* The paper assumes that the task reward function is provided, but does not explore the possibility of learning the task reward function jointly with the codebook module.\n\nSuggestions for Improvement:\n\n* Investigate the possibility of jointly training the visual encoder and the codebook module to see if it leads to better performance.\n* Provide a more detailed analysis of the computational cost of the codebook module and how it compares to other approaches.\n* Explore the possibility of learning the task reward function jointly with the codebook module to see if it leads to better performance.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to embodied AI that draws inspiration from selective attention in humans. The codebook module is parameter-efficient and can be trained jointly with the overall policy, making it a computationally efficient solution. The authors demonstrate the effectiveness of their approach on multiple benchmarks and show that it outperforms state-of-the-art baselines. However, there are some limitations and areas for improvement that need to be addressed. Overall, I recommend this paper for publication in a top-tier conference in the field of artificial intelligence.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to improve the performance of embodied AI agents by introducing a parameter-efficient codebook module that filters out task-irrelevant visual information. The codebook module is trained jointly with the overall policy to optimize task rewards, allowing the agent to focus on task-relevant visual cues. The proposed approach is evaluated on several navigation benchmarks and a manipulation benchmark, demonstrating significant improvements in success rate, episode length, and smoothness of trajectories. The codebook bottleneck is also shown to be more generalizable and adaptable to new visual domains without exhaustive finetuning. The qualitative and quantitative analyses reveal that the codebook captures more task-relevant information, resulting in more effective exploration strategies.\n\nStrengths:\n\n* The proposed codebook module is a novel and effective approach to improve the performance of embodied AI agents.\n* The codebook bottleneck is shown to be more generalizable and adaptable to new visual domains without exhaustive finetuning.\n* The qualitative and quantitative analyses provide strong evidence that the codebook captures more task-relevant information, resulting in more effective exploration strategies.\n* The paper demonstrates significant improvements in success rate, episode length, and smoothness of trajectories on several navigation benchmarks and a manipulation benchmark.\n\nWeaknesses:\n\n* The paper assumes that the codebook module can be trained jointly with the overall policy to optimize task rewards, which may not be the case in all scenarios.\n* The paper does not provide a thorough analysis of the codebook's performance in more complex environments or with more challenging tasks.\n* The paper assumes that the codebook bottleneck is effective in all cases, which may not be the case in situations where the task-relevant information is not well-represented in the codebook.\n\nSuggestions for Improvement:\n\n* The paper should provide a more thorough analysis of the codebook's performance in more complex environments or with more challenging tasks.\n* The paper should provide more detailed information on the training process and the codebook's performance in different scenarios.\n* The paper should explore the possibility of using the codebook bottleneck in other reinforcement learning settings, such as in partially observable environments.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to improve the performance of embodied AI agents. The codebook bottleneck is shown to be more generalizable and adaptable to new visual domains without exhaustive finetuning, and the qualitative and quantitative analyses provide strong evidence that the codebook captures more task-relevant information, resulting in more effective exploration strategies. The paper demonstrates significant improvements in success rate, episode length, and smoothness of trajectories on several navigation benchmarks and a manipulation benchmark. However, the paper assumes that the codebook module can be trained jointly with the overall policy to optimize task rewards, which may not be the case in all scenarios. Therefore, the paper should provide a more thorough analysis of the codebook's performance in more complex environments or with more challenging tasks. Based on the strengths and weaknesses of the paper, I recommend that the paper be accepted for publication in the conference, but with the suggestion that the authors provide more detailed information on the training process and the codebook's performance in different scenarios. $$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to improve the performance of embodied AI agents by introducing a parameter-efficient codebook module that filters out task-irrelevant visual information. The codebook module is trained jointly with the overall policy to optimize task rewards, allowing the agent to focus on task-relevant visual cues. The proposed approach is evaluated on several navigation benchmarks and a manipulation benchmark, demonstrating significant improvements in success rate, episode length, and smoothness of trajectories. The codebook bottleneck is also shown to be more generalizable and adaptable to new visual domains without exhaustive finetuning. The qualitative and quantitative analyses reveal that the codebook captures more task-relevant information, resulting in more effective exploration strategies.\n\nStrengths:\n\n* The proposed codebook module is a novel and effective approach to improve the performance of embodied AI agents.\n* The codebook bottleneck is shown to be more generalizable and adaptable to new visual domains without exhaustive finetuning.\n* The qualitative and quantitative analyses provide strong evidence that the codebook captures more task-relevant information, resulting in more effective exploration strategies.\n* The paper demonstrates significant improvements in success rate, episode length, and smoothness of trajectories on several navigation benchmarks and a manipulation benchmark.\n\nWeaknesses:\n\n* The paper assumes that the codebook module can be trained jointly with the overall policy to optimize task rewards, which may not be the case in all scenarios.\n* The paper does not provide a thorough analysis of the codebook's performance in more complex environments or with more challenging tasks.\n* The paper assumes that the codebook bottleneck is effective in all cases, which may not be the case in situations where the task-relevant information is not well-represented in the codebook.\n\nSuggestions for Improvement:\n\n* The paper should provide a more thorough analysis of the codebook's performance in more complex environments or with more challenging tasks.\n* The paper should provide more detailed information on the training process and the codebook's performance in different scenarios.\n* The paper should explore the possibility of using the codebook bottleneck in other reinforcement learning settings, such as in partially observable environments.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to improve the performance of embodied AI agents. The codebook bottleneck is shown to be more generalizable and adaptable to new visual domains without exhaustive finetuning, and the qualitative and quantitative analyses provide strong evidence that the codebook captures more task-relevant information, resulting in more effective exploration strategies. The paper demonstrates significant improvements in success rate, episode length, and smoothness of trajectories on several navigation benchmarks and a manipulation benchmark. However, the paper assumes that the codebook module can be trained jointly with the overall policy to optimize task rewards, which may not be the case in all scenarios. Therefore, the paper should provide a more thorough analysis of the codebook's performance in more complex environments or with more challenging tasks. Based on the strengths and weaknesses of the paper, I recommend that the paper be accepted for publication in the conference, but with the suggestion that the authors provide more detailed information on the training process and the codebook's performance in different scenarios. $$",
        "gr_score_with": 0.5432525951557093,
        "output_with_attack": {
            "num_tokens_scored": 289,
            "num_green_tokens": 157,
            "green_fraction": 0.5432525951557093,
            "z_score": 4.954227497426869,
            "p_value": 3.6309102979701833e-07
        },
        "output_with": {
            "num_tokens_scored": 289,
            "num_green_tokens": 157,
            "green_fraction": 0.5432525951557093,
            "z_score": 4.954227497426869,
            "p_value": 3.6309102979701833e-07
        },
        "output_without": {
            "num_tokens_scored": 288,
            "num_green_tokens": 135,
            "green_fraction": 0.46875,
            "z_score": 2.365293205001912,
            "p_value": 0.009007900736344082
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. The authors also demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n\nStrengths:\n\n1. The paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n2. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n3. The authors demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n4. The paper provides a comprehensive analysis of the framework's performance and limitations, including the use of pseudo-labels and the Pathway module.\n\nWeaknesses:\n\n1. The paper relies heavily on pseudo-labels, which may not be accurate and can lead to performance loss on certain datasets.\n2. The Pathway module has limitations, including not handling multi-path scenarios, which may lead to suboptimal performance in certain cases.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, which makes it difficult to evaluate the framework's performance in a broader context.\n4. The paper assumes that the input utterance is well-formed and does not handle cases where the utterance is ambiguous or contains negations.\n\nSuggestions for Improvement:\n\n1. The authors should investigate alternative methods for collecting pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. The Pathway module should be modified to handle multi-path scenarios, such as using a graph-based approach to reason about different possibilities.\n3. The paper should provide a more thorough comparison with other state-of-the-art methods, including a detailed analysis of the framework's performance and limitations.\n4. The paper should address the assumption that the input utterance is well-formed and provide a more robust solution for handling ambiguous or negated utterances.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field. However, the paper relies heavily on pseudo-labels, which may not be accurate, and the Pathway module has limitations. To improve the framework, the authors should investigate alternative methods for collecting pseudo-labels and modify the Pathway module to handle multi-path scenarios. Additionally, the paper should provide a more thorough comparison with other state-of-the-art methods and address the assumption that the input utterance is well-formed. Overall, the paper is well-written and provides a comprehensive analysis of the framework's performance and limitations.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\n\nThe paper proposes CoT3DRef, a novel and interpretable framework for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. The authors demonstrate that this sequential approach enhances interpretability and improves overall performance and data efficiency. They also show that the framework can be easily integrated into other architectures and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n\nStrengths:\n\n1. The authors propose a novel and interpretable framework for 3D visual grounding, which formulates the problem as a sequence-to-sequence task.\n2. The framework predicts a chain of anchor objects, which enhances interpretability and improves overall performance and data efficiency.\n3. The authors demonstrate that the framework can be easily integrated into other architectures and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n4. The framework does not rely on any additional manual annotations, instead leveraging automatic rule-based methods, syntactic parsing, and in-context learning to collect pseudo-labels for the anchor objects.\n\nWeaknesses:\n\n1. The accuracy of the pseudo-labels plays a vital role in the overall performance, and the authors acknowledge a significant accuracy gap between automatically collected pseudo-labels and ground-truth data, contributing to the performance loss observed on the Nr3D dataset.\n2. The pseudo module limitations hinder achieving more gain on Nr3D, and the authors encourage future efforts to enhance the pseudo module performance.\n3. The Pathway module limitations do not handle the multi-path scenario, where multiple paths are valid, and the authors suggest building a graph that reasons the different possibilities to handle this limitation.\n\nSuggestions for Improvement:\n\n1. The authors could further investigate the pseudo module limitations and design methods to enhance its performance, especially on the Nr3D dataset.\n2. They could also explore other methods to collect pseudo-labels for the anchor objects, such as using other natural language processing techniques or machine learning models.\n3. The Pathway module limitations could be addressed by building a graph that reasons the different possibilities, as suggested by the authors.\n4. The authors could also conduct more extensive experiments to evaluate the performance of the framework on other datasets and scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which formulates the problem as a sequence-to-sequence task and predicts a chain of anchor objects. The framework shows promising results and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. However, the pseudo module limitations and Pathway module limitations need to be addressed to further improve the performance of the framework. Therefore, I recommend the paper for publication, with the understanding that the authors will address the limitations and improve the framework in future work.\n\n$$",
        "peer_review_with_watermark_attack": "$$\n\nPaper Summary:\n\nThe paper proposes CoT3DRef, a novel and interpretable framework for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. The authors demonstrate that this sequential approach enhances interpretability and improves overall performance and data efficiency. They also show that the framework can be easily integrated into other architectures and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n\nStrengths:\n\n1. The authors propose a novel and interpretable framework for 3D visual grounding, which formulates the problem as a sequence-to-sequence task.\n2. The framework predicts a chain of anchor objects, which enhances interpretability and improves overall performance and data efficiency.\n3. The authors demonstrate that the framework can be easily integrated into other architectures and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n4. The framework does not rely on any additional manual annotations, instead leveraging automatic rule-based methods, syntactic parsing, and in-context learning to collect pseudo-labels for the anchor objects.\n\nWeaknesses:\n\n1. The accuracy of the pseudo-labels plays a vital role in the overall performance, and the authors acknowledge a significant accuracy gap between automatically collected pseudo-labels and ground-truth data, contributing to the performance loss observed on the Nr3D dataset.\n2. The pseudo module limitations hinder achieving more gain on Nr3D, and the authors encourage future efforts to enhance the pseudo module performance.\n3. The Pathway module limitations do not handle the multi-path scenario, where multiple paths are valid, and the authors suggest building a graph that reasons the different possibilities to handle this limitation.\n\nSuggestions for Improvement:\n\n1. The authors could further investigate the pseudo module limitations and design methods to enhance its performance, especially on the Nr3D dataset.\n2. They could also explore other methods to collect pseudo-labels for the anchor objects, such as using other natural language processing techniques or machine learning models.\n3. The Pathway module limitations could be addressed by building a graph that reasons the different possibilities, as suggested by the authors.\n4. The authors could also conduct more extensive experiments to evaluate the performance of the framework on other datasets and scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which formulates the problem as a sequence-to-sequence task and predicts a chain of anchor objects. The framework shows promising results and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. However, the pseudo module limitations and Pathway module limitations need to be addressed to further improve the performance of the framework. Therefore, I recommend the paper for publication, with the understanding that the authors will address the limitations and improve the framework in future work.\n\n$$",
        "gr_score_with": 0.6233062330623306,
        "output_with_attack": {
            "num_tokens_scored": 369,
            "num_green_tokens": 230,
            "green_fraction": 0.6233062330623306,
            "z_score": 8.729801570981724,
            "p_value": 1.2755991062544462e-18
        },
        "output_with": {
            "num_tokens_scored": 369,
            "num_green_tokens": 230,
            "green_fraction": 0.6233062330623306,
            "z_score": 8.729801570981724,
            "p_value": 1.2755991062544462e-18
        },
        "output_without": {
            "num_tokens_scored": 368,
            "num_green_tokens": 213,
            "green_fraction": 0.5788043478260869,
            "z_score": 6.975812043071693,
            "p_value": 1.520545401946817e-12
        }
    },
    {
        "title": "SDXL: IMPROVING LATENT DIFFUSION MODELS FOR HIGH-RESOLUTION IMAGE SYNTHESIS",
        "abstract": "We present Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder. Further, we design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. To ensure highest quality results, we also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL improves dramatically over previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators such as Midjourney (Holz, 2023).",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image synthesis that significantly improves upon previous versions of Stable Diffusion. SDXL leverages a three times larger UNet backbone, multiple novel conditioning schemes, and a refinement model to achieve high-quality results. The authors demonstrate that SDXL outperforms previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. Additionally, SDXL is an open model that achieves competitive performance with black-box image generation models, addressing concerns about the opacity of their architecture.\n\nStrengths:\n\n1. The paper presents a thorough analysis of improvements to the foundation model Stable Diffusion, leading to significant improvements in synthesized image quality, prompt adherence, and composition.\n2. The authors propose multiple novel conditioning schemes, including size-conditioning, crop-conditioning, and multi-aspect training, which are effective in improving the performance of the model.\n3. The paper presents a refinement model that improves sample quality, which is a significant contribution to the field of text-to-image synthesis.\n4. The authors demonstrate the multimodal processing capabilities of SDXL, allowing it to process both text prompts and input images.\n\nWeaknesses:\n\n1. The paper relies heavily on user studies to evaluate the performance of SDXL, which may not be a robust or objective measure of performance.\n2. The authors do not provide a clear explanation of how the refinement model improves sample quality, and more detailed analysis of this component would be beneficial.\n3. The paper does not provide a thorough comparison of SDXL with other state-of-the-art models, which makes it difficult to evaluate the significance of the improvements presented.\n4. The authors mention the need for further work on decreasing the compute needed for inference, but do not provide a clear plan for addressing this challenge.\n\nSuggestions for Improvement:\n\n1. Provide more detailed analysis of the refinement model and its impact on sample quality.\n2. Include a more thorough comparison of SDXL with other state-of-the-art models to evaluate the significance of the improvements presented.\n3. Address the challenge of decreasing the compute needed for inference, and provide a clear plan for addressing this challenge.\n4. Consider using more robust and objective evaluation metrics, such as FID and IS, to evaluate the performance of SDXL.\n\nRecommendation:\n\nThis paper presents a significant improvement to the foundation model Stable Diffusion, and the proposed novel conditioning schemes and refinement model are effective in improving the performance of the model. However, the paper relies heavily on user studies, and more detailed analysis of the refinement model and a thorough comparison with other state-of-the-art models would be beneficial. Overall, this paper is a valuable contribution to the field of text-to-image synthesis, and the authors' work has the potential to impact the development of future models.\n\nRating: 7/10\n\nRecommendation for Publication: Accept",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image synthesis that outperforms previous versions of Stable Diffusion. SDXL features a 3 times larger UNet backbone, multiple novel conditioning schemes, and a refinement model that improves the visual fidelity of samples. The authors demonstrate that SDXL achieves competitive performance with black-box state-of-the-art image generators and release an open model that achieves competitive performance.\n\nStrengths:\n\n1. The authors demonstrate significant improvements in synthesized image quality, prompt adherence, and composition compared to previous versions of Stable Diffusion.\n2. The paper introduces multiple novel conditioning schemes, including size- and crop-conditioning, which can be used independently or in combination to extend any model.\n3. The authors demonstrate the effectiveness of multi-aspect training, which allows the model to handle multiple aspect-ratios simultaneously.\n4. The paper introduces a refinement model that improves the visual fidelity of samples, which is demonstrated to be effective in user studies.\n\nWeaknesses:\n\n1. The paper relies heavily on user studies, which may not be representative of the broader community's experience with the model.\n2. The authors do not provide a clear comparison with other state-of-the-art models, and the performance metrics used (FID and CLIP scores) do not capture the full range of improvements made by SDXL.\n3. The paper does not provide a clear explanation of how the model is scaled to larger sizes, which is a significant improvement over previous versions of Stable Diffusion.\n\nSuggestions for Improvement:\n\n1. Provide a more comprehensive comparison with other state-of-the-art models, including a clear evaluation of their strengths and weaknesses.\n2. Use a more diverse set of metrics to evaluate the model's performance, including metrics that capture the full range of improvements made by SDXL.\n3. Provide a clear explanation of how the model is scaled to larger sizes, including a detailed analysis of the hyperparameters used.\n\nRecommendation:\n\nThis paper is a significant improvement over previous versions of Stable Diffusion, and the authors demonstrate a clear understanding of the strengths and weaknesses of their model. However, the paper relies heavily on user studies, and a more comprehensive comparison with other state-of-the-art models is needed to fully evaluate the model's performance. I recommend that the authors address these limitations in future work.\n\nOverall, I recommend that this paper be accepted, but with the above suggestions for improvement.",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper introduces Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image synthesis that outperforms previous versions of Stable Diffusion. SDXL features a 3 times larger UNet backbone, multiple novel conditioning schemes, and a refinement model that improves the visual fidelity of samples. The authors demonstrate that SDXL achieves competitive performance with black-box state-of-the-art image generators and release an open model that achieves competitive performance.\n\nStrengths:\n\n1. The authors demonstrate significant improvements in synthesized image quality, prompt adherence, and composition compared to previous versions of Stable Diffusion.\n2. The paper introduces multiple novel conditioning schemes, including size- and crop-conditioning, which can be used independently or in combination to extend any model.\n3. The authors demonstrate the effectiveness of multi-aspect training, which allows the model to handle multiple aspect-ratios simultaneously.\n4. The paper introduces a refinement model that improves the visual fidelity of samples, which is demonstrated to be effective in user studies.\n\nWeaknesses:\n\n1. The paper relies heavily on user studies, which may not be representative of the broader community's experience with the model.\n2. The authors do not provide a clear comparison with other state-of-the-art models, and the performance metrics used (FID and CLIP scores) do not capture the full range of improvements made by SDXL.\n3. The paper does not provide a clear explanation of how the model is scaled to larger sizes, which is a significant improvement over previous versions of Stable Diffusion.\n\nSuggestions for Improvement:\n\n1. Provide a more comprehensive comparison with other state-of-the-art models, including a clear evaluation of their strengths and weaknesses.\n2. Use a more diverse set of metrics to evaluate the model's performance, including metrics that capture the full range of improvements made by SDXL.\n3. Provide a clear explanation of how the model is scaled to larger sizes, including a detailed analysis of the hyperparameters used.\n\nRecommendation:\n\nThis paper is a significant improvement over previous versions of Stable Diffusion, and the authors demonstrate a clear understanding of the strengths and weaknesses of their model. However, the paper relies heavily on user studies, and a more comprehensive comparison with other state-of-the-art models is needed to fully evaluate the model's performance. I recommend that the authors address these limitations in future work.\n\nOverall, I recommend that this paper be accepted, but with the above suggestions for improvement.",
        "gr_score_with": 0.5957446808510638,
        "output_with_attack": {
            "num_tokens_scored": 329,
            "num_green_tokens": 196,
            "green_fraction": 0.5957446808510638,
            "z_score": 7.224120182882696,
            "p_value": 2.521781971755503e-13
        },
        "output_with": {
            "num_tokens_scored": 329,
            "num_green_tokens": 196,
            "green_fraction": 0.5957446808510638,
            "z_score": 7.224120182882696,
            "p_value": 2.521781971755503e-13
        },
        "output_without": {
            "num_tokens_scored": 384,
            "num_green_tokens": 185,
            "green_fraction": 0.4817708333333333,
            "z_score": 3.2467828781769366,
            "p_value": 0.0005835869707065684
        }
    },
    {
        "title": "MOTION GUIDANCE: DIFFUSION-BASED IMAGE EDIT-",
        "abstract": "Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel technique called motion guidance, which enables users to specify dense and complex motion fields to edit images. The method works by steering the diffusion sampling process using an off-the-shelf optical flow network, while also encouraging the generated image to be visually similar to the source image. The authors demonstrate that their technique can handle a wide range of complex motion fields, including translations, rotations, homographies, stretching, deformations, and even flow fields extracted from a video.\n\nStrengths:\n\n1. The paper proposes a novel and effective technique for motion-based image editing, which can handle complex motion fields and produce high-quality edits.\n2. The method is simple, zero-shot, and does not require any training or explicit constraints on the diffusion network architecture.\n3. The authors provide a comprehensive evaluation of their method, including qualitative and quantitative results, and comparisons with state-of-the-art baselines.\n4. The paper explores the possibility of repurposing other low-level computer vision models for image generation tasks through diffusion guidance, which is a promising area of research.\n\nWeaknesses:\n\n1. The paper relies on the performance of the off-the-shelf optical flow network, which may not be optimal for all cases.\n2. The method may be susceptible to various weaknesses, such as slow sampling speed and limitations of the optical flow method.\n3. The paper does not provide a thorough analysis of the trade-offs between the flow loss and the CLIP similarity metrics.\n4. The evaluation of the method is limited to a specific set of datasets and target flows, which may not be representative of all possible scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more advanced optical flow networks or other motion estimation models to improve the performance of the method.\n2. Explore the possibility of using other guidance signals, such as classifier guidance or text-based guidance, to improve the flexibility and robustness of the method.\n3. Provide a more thorough analysis of the trade-offs between the flow loss and the CLIP similarity metrics, including a deeper understanding of the underlying factors that influence these metrics.\n4. Evaluate the method on a more diverse set of datasets and target flows to better understand its limitations and potential applications.\n\nRecommendation:\n\nThe paper presents a novel and effective technique for motion-based image editing, which has the potential to improve the flexibility and robustness of image manipulation models. While the method has some limitations, it is a promising area of research that deserves further exploration. I recommend the paper for publication in a top-tier conference, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$ \n\nPaper Summary:\n\nThe paper proposes a zero-shot image editing technique called motion guidance, which uses an off-the-shelf optical flow network to guide the diffusion sampling process. This allows users to edit images by specifying a dense, possibly complex, flow field indicating where each pixel should move. The proposed method does not require any training, text inputs, or specific diffusion architecture constraints. It can handle complex motions, such as translations, rotations, homographies, stretching, deformations, and flow fields extracted from videos. The authors demonstrate the effectiveness of their method on both real and generated images, achieving high-quality edits and diverse flows. They also provide qualitative and quantitative results, comparing their method with state-of-the-art image editing techniques, such as InstructPix2Pix, Forward Warp with SDEdit, and DragGAN. \n\nStrengths:\n\n1. The proposed method is simple, zero-shot, and does not require any training, text inputs, or specific diffusion architecture constraints.\n2. It can handle complex motions, such as translations, rotations, homographies, stretching, deformations, and flow fields extracted from videos.\n3. The authors provide qualitative and quantitative results, demonstrating the effectiveness of their method on both real and generated images.\n4. They compare their method with state-of-the-art image editing techniques, such as InstructPix2Pix, Forward Warp with SDEdit, and DragGAN.\n5. The proposed method can handle diverse images, including non-photorealistic images such as paintings or sketches.\n\nWeaknesses:\n\n1. The proposed method inherits the deficiencies of diffusion models and guidance-based methods, such as slow sampling speed.\n2. It inherits the limitations of the optical flow method, and certain target flows may not be possible.\n3. The authors do not provide a comprehensive comparison with all state-of-the-art image editing techniques.\n4. They do not provide a detailed discussion of the hyperparameters and their effects on the proposed method.\n5. The proposed method may not be suitable for all types of images, especially those with complex and ambiguous motions.\n\nSuggestions for Improvement:\n\n1. Investigate ways to improve the sampling speed of the proposed method.\n2. Develop a more comprehensive comparison with all state-of-the-art image editing techniques.\n3. Provide a detailed discussion of the hyperparameters and their effects on the proposed method.\n4. Investigate ways to handle ambiguous and complex motions.\n5. Develop a more robust and flexible optical flow network to handle diverse images.\n\nRecommendation:\n\nThe proposed method is a significant contribution to the field of image editing, and it has the potential to be widely adopted. However, further research is needed to address the weaknesses and suggestions mentioned above. With some modifications and improvements, the proposed method can be even more effective and robust. \n\nOverall, the paper is well-structured, and the authors provide a clear and concise explanation of their method. The proposed technique has the potential to be widely adopted, and it can be a valuable tool for image editing applications.",
        "peer_review_with_watermark_attack": "$$$ \n\nPaper Summary:\n\nThe paper proposes a zero-shot image editing technique called motion guidance, which uses an off-the-shelf optical flow network to guide the diffusion sampling process. This allows users to edit images by specifying a dense, possibly complex, flow field indicating where each pixel should move. The proposed method does not require any training, text inputs, or specific diffusion architecture constraints. It can handle complex motions, such as translations, rotations, homographies, stretching, deformations, and flow fields extracted from videos. The authors demonstrate the effectiveness of their method on both real and generated images, achieving high-quality edits and diverse flows. They also provide qualitative and quantitative results, comparing their method with state-of-the-art image editing techniques, such as InstructPix2Pix, Forward Warp with SDEdit, and DragGAN. \n\nStrengths:\n\n1. The proposed method is simple, zero-shot, and does not require any training, text inputs, or specific diffusion architecture constraints.\n2. It can handle complex motions, such as translations, rotations, homographies, stretching, deformations, and flow fields extracted from videos.\n3. The authors provide qualitative and quantitative results, demonstrating the effectiveness of their method on both real and generated images.\n4. They compare their method with state-of-the-art image editing techniques, such as InstructPix2Pix, Forward Warp with SDEdit, and DragGAN.\n5. The proposed method can handle diverse images, including non-photorealistic images such as paintings or sketches.\n\nWeaknesses:\n\n1. The proposed method inherits the deficiencies of diffusion models and guidance-based methods, such as slow sampling speed.\n2. It inherits the limitations of the optical flow method, and certain target flows may not be possible.\n3. The authors do not provide a comprehensive comparison with all state-of-the-art image editing techniques.\n4. They do not provide a detailed discussion of the hyperparameters and their effects on the proposed method.\n5. The proposed method may not be suitable for all types of images, especially those with complex and ambiguous motions.\n\nSuggestions for Improvement:\n\n1. Investigate ways to improve the sampling speed of the proposed method.\n2. Develop a more comprehensive comparison with all state-of-the-art image editing techniques.\n3. Provide a detailed discussion of the hyperparameters and their effects on the proposed method.\n4. Investigate ways to handle ambiguous and complex motions.\n5. Develop a more robust and flexible optical flow network to handle diverse images.\n\nRecommendation:\n\nThe proposed method is a significant contribution to the field of image editing, and it has the potential to be widely adopted. However, further research is needed to address the weaknesses and suggestions mentioned above. With some modifications and improvements, the proposed method can be even more effective and robust. \n\nOverall, the paper is well-structured, and the authors provide a clear and concise explanation of their method. The proposed technique has the potential to be widely adopted, and it can be a valuable tool for image editing applications.",
        "gr_score_with": 0.6631578947368421,
        "output_with_attack": {
            "num_tokens_scored": 380,
            "num_green_tokens": 252,
            "green_fraction": 0.6631578947368421,
            "z_score": 10.443616075673265,
            "p_value": 7.836226087112018e-26
        },
        "output_with": {
            "num_tokens_scored": 380,
            "num_green_tokens": 252,
            "green_fraction": 0.6631578947368421,
            "z_score": 10.443616075673265,
            "p_value": 7.836226087112018e-26
        },
        "output_without": {
            "num_tokens_scored": 396,
            "num_green_tokens": 207,
            "green_fraction": 0.5227272727272727,
            "z_score": 4.958384326283548,
            "p_value": 3.5540920337486353e-07
        }
    },
    {
        "title": "METHOD WITH OPTIMAL ADAPTATION TO INEXACTNESS",
        "abstract": "We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, which occurs typically in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key setting. We further introduce a tensor generalization for stochastic higher-order derivatives. When the oracles are non-stochastic, the proposed tensor algorithm matches the global convergence of Nesterov Accelerated Tensor method. Both algorithms allow for approximate solutions of their auxiliary subproblems with verifiable conditions on the accuracy of the solution.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel stochastic accelerated second-order algorithm for convex and strongly convex optimization problems. The proposed algorithm, Accelerated Stochastic Cubic Newton, achieves optimal convergence rates in both gradient and Hessian inexactness. The authors also introduce a tensor generalization of second-order methods for stochastic high-order derivatives. The paper establishes a lower bound for stochastic second-order optimization and proves the algorithm's convergence rates. Additionally, the authors provide a restarted version of the algorithm for strongly convex minimization, which exhibits a linear convergence rate.\n\nStrengths:\n\n1. The paper presents a novel stochastic accelerated second-order algorithm that achieves optimal convergence rates in both gradient and Hessian inexactness.\n2. The authors introduce a tensor generalization of second-order methods for stochastic high-order derivatives, which is a significant contribution to the field.\n3. The paper establishes a lower bound for stochastic second-order optimization, which provides a fundamental understanding of the problem.\n4. The authors provide a restarted version of the algorithm for strongly convex minimization, which exhibits a linear convergence rate.\n\nWeaknesses:\n\n1. The paper assumes that the function f(x) is convex and has L1-Lipschitz-continuous gradient and L2-Lipschitz-continuous Hessian, which may not be the case in all optimization problems.\n2. The algorithm involves a subproblem that necessitates an additional subroutine to find its solution, which may be computationally expensive.\n3. The paper does not provide a detailed analysis of the computational complexity of the algorithm.\n4. The authors do not provide a comparison with other existing algorithms for stochastic second-order optimization.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the algorithm and compare it with other existing algorithms.\n2. The paper should provide a more general framework for stochastic second-order optimization, rather than focusing on a specific algorithm.\n3. The authors should provide more experimental results to demonstrate the effectiveness of the algorithm in different scenarios.\n4. The paper should discuss the potential applications of the algorithm in real-world optimization problems.\n\nRecommendation:\n\nThe paper presents a novel stochastic accelerated second-order algorithm that achieves optimal convergence rates in both gradient and Hessian inexactness. The authors also introduce a tensor generalization of second-order methods for stochastic high-order derivatives, which is a significant contribution to the field. However, the paper assumes a specific form of the function f(x) and does not provide a detailed analysis of the computational complexity of the algorithm. Therefore, I recommend that the authors provide more experimental results and a more detailed analysis of the computational complexity of the algorithm to make it more convincing and applicable to real-world optimization problems.\n\nOverall, the paper is well-written and provides a significant contribution to the field of stochastic second-order optimization. However, it requires more experimental results and a more detailed analysis of the computational complexity of the algorithm to make it more convincing and applicable to real-world optimization problems.\n\nRating: 8/10\n\nRecommendation: Accept after revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents an accelerated stochastic second-order method for convex optimization, dubbed as Accelerated Stochastic Cubic Regularized Newton's method. The proposed method is robust to both gradient and Hessian inexactness, which is typical in machine learning applications. The authors establish a theoretical lower bound for inexact second-order methods and prove that their algorithm achieves optimal convergence in both gradient and Hessian inexactness. Additionally, they introduce a tensor generalization of second-order methods for stochastic high-order derivatives. The authors also propose a restarted version of their algorithm for strongly convex minimization, which exhibits a linear rate. The paper presents numerical experiments on logistic regression problems, demonstrating the efficiency of their proposed methods.\n\nStrengths:\n\n* The paper presents a comprehensive treatment of stochastic second-order methods, including theoretical lower bounds and optimal convergence rates.\n* The authors introduce a tensor generalization of second-order methods, which is an interesting extension of their work.\n* The paper presents a restarted version of their algorithm for strongly convex minimization, which is a useful extension of their work.\n* The authors provide numerical experiments on logistic regression problems, which demonstrate the efficiency of their proposed methods.\n\nWeaknesses:\n\n* The paper assumes that the function f(x) is convex and sufficiently smooth, which might not be the case in all applications.\n* The authors do not provide a detailed comparison of their method with other existing methods for stochastic second-order optimization.\n* The paper does not discuss the computational complexity of their method, which is an important aspect of optimization methods.\n* The authors do not provide a detailed analysis of the convergence rate of their method in the case of strongly convex minimization.\n\nSuggestions for Improvement:\n\n* The authors could provide a more detailed comparison of their method with other existing methods for stochastic second-order optimization, including a discussion of their relative advantages and disadvantages.\n* The paper could benefit from a more detailed analysis of the convergence rate of their method in the case of strongly convex minimization.\n* The authors could discuss the computational complexity of their method, including the number of iterations required to achieve a desired accuracy.\n* The paper could benefit from a more detailed discussion of the assumptions required for their method, including the smoothness of the function f(x) and the properties of the stochastic gradient and Hessian oracles.\n\nRecommendation:\n\nThis paper presents an interesting and comprehensive treatment of stochastic second-order methods, including theoretical lower bounds and optimal convergence rates. The authors introduce a tensor generalization of second-order methods, which is an interesting extension of their work. The paper presents numerical experiments on logistic regression problems, which demonstrate the efficiency of their proposed methods. However, the paper could benefit from a more detailed comparison of their method with other existing methods for stochastic second-order optimization, a more detailed analysis of the convergence rate of their method in the case of strongly convex minimization, and a discussion of the computational complexity of their method. Therefore, I recommend that this paper be accepted for publication, subject to revisions that address these issues.",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThis paper presents an accelerated stochastic second-order method for convex optimization, dubbed as Accelerated Stochastic Cubic Regularized Newton's method. The proposed method is robust to both gradient and Hessian inexactness, which is typical in machine learning applications. The authors establish a theoretical lower bound for inexact second-order methods and prove that their algorithm achieves optimal convergence in both gradient and Hessian inexactness. Additionally, they introduce a tensor generalization of second-order methods for stochastic high-order derivatives. The authors also propose a restarted version of their algorithm for strongly convex minimization, which exhibits a linear rate. The paper presents numerical experiments on logistic regression problems, demonstrating the efficiency of their proposed methods.\n\nStrengths:\n\n* The paper presents a comprehensive treatment of stochastic second-order methods, including theoretical lower bounds and optimal convergence rates.\n* The authors introduce a tensor generalization of second-order methods, which is an interesting extension of their work.\n* The paper presents a restarted version of their algorithm for strongly convex minimization, which is a useful extension of their work.\n* The authors provide numerical experiments on logistic regression problems, which demonstrate the efficiency of their proposed methods.\n\nWeaknesses:\n\n* The paper assumes that the function f(x) is convex and sufficiently smooth, which might not be the case in all applications.\n* The authors do not provide a detailed comparison of their method with other existing methods for stochastic second-order optimization.\n* The paper does not discuss the computational complexity of their method, which is an important aspect of optimization methods.\n* The authors do not provide a detailed analysis of the convergence rate of their method in the case of strongly convex minimization.\n\nSuggestions for Improvement:\n\n* The authors could provide a more detailed comparison of their method with other existing methods for stochastic second-order optimization, including a discussion of their relative advantages and disadvantages.\n* The paper could benefit from a more detailed analysis of the convergence rate of their method in the case of strongly convex minimization.\n* The authors could discuss the computational complexity of their method, including the number of iterations required to achieve a desired accuracy.\n* The paper could benefit from a more detailed discussion of the assumptions required for their method, including the smoothness of the function f(x) and the properties of the stochastic gradient and Hessian oracles.\n\nRecommendation:\n\nThis paper presents an interesting and comprehensive treatment of stochastic second-order methods, including theoretical lower bounds and optimal convergence rates. The authors introduce a tensor generalization of second-order methods, which is an interesting extension of their work. The paper presents numerical experiments on logistic regression problems, which demonstrate the efficiency of their proposed methods. However, the paper could benefit from a more detailed comparison of their method with other existing methods for stochastic second-order optimization, a more detailed analysis of the convergence rate of their method in the case of strongly convex minimization, and a discussion of the computational complexity of their method. Therefore, I recommend that this paper be accepted for publication, subject to revisions that address these issues.",
        "gr_score_with": 0.6123778501628665,
        "output_with_attack": {
            "num_tokens_scored": 307,
            "num_green_tokens": 188,
            "green_fraction": 0.6123778501628665,
            "z_score": 7.5807366481584815,
            "p_value": 1.717992086698213e-14
        },
        "output_with": {
            "num_tokens_scored": 307,
            "num_green_tokens": 188,
            "green_fraction": 0.6123778501628665,
            "z_score": 7.5807366481584815,
            "p_value": 1.717992086698213e-14
        },
        "output_without": {
            "num_tokens_scored": 309,
            "num_green_tokens": 173,
            "green_fraction": 0.5598705501618123,
            "z_score": 5.721638620042663,
            "p_value": 5.275076386412202e-09
        }
    },
    {
        "title": "HOW OVER-PARAMETERIZATION SLOWS DOWN GRA-",
        "abstract": "This paper rigorously shows how over-parameterization dramatically changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from nearisotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where M\u2217 \u2208 Rn\u00d7n is a positive semi-definite unknown matrix of rank r \u226a n, and one uses a symmetric parameterization XX\u22a4 to learn M\u2217. Here, X \u2208 Rn\u00d7k with k > r is the factor matrix. We give a novel \u03a9 ( 1/T 2 ) lower bound of randomly initialized GD for the over-parameterized case (k > r) where T is the number of iterations. This is in stark contrast to the exact-parameterization scenario (k = r) where the convergence rate is exp (\u2212\u03a9 (T )). Next, we study asymmetric setting where M\u2217 \u2208 Rn1\u00d7n2 is the unknown matrix of rank r \u226a min{n1, n2}, and one uses an asymmetric parameterization FG\u22a4 to learn M\u2217 where F \u2208 Rn1\u00d7k and G \u2208 Rn2\u00d7k. Building on prior work, we give a global exact convergence result of randomly initialized GD for the exact-parameterization case (k = r) with an exp (\u2212\u03a9 (T )) rate. Furthermore, we give the first global exact convergence result for the over-parameterization case (k > r) with an exp ( \u2212\u03a9 ( \u03b1T )) rate where \u03b1 is the initialization scale. This linear convergence result in the over-parameterization case is especially significant because one can apply the asymmetric parameterization to the symmetric setting to speed up from \u03a9 ( 1/T 2 ) to linear convergence. Therefore, we identify a surprising phenomenon: asymmetric parameterization can exponentially speed up convergence. Equally surprising is our analysis that highlights the importance of imbalance between F and G. This is in sharp contrast to prior works which emphasize balance. We further give an example showing the dependency on \u03b1 in the convergence rate is unavoidable in the worst case. On the other hand, we propose a novel method that only modifies one step of GD and obtains a convergence rate independent of \u03b1, recovering the rate in the exact-parameterization case. We provide empirical studies to verify our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper investigates the behavior of gradient descent (GD) in the context of matrix sensing problems, specifically in symmetric and asymmetric settings. The authors rigorously show how over-parameterization affects the convergence of GD, leading to a surprising phenomenon: asymmetric parameterization can exponentially speed up convergence. They provide a novel \u03a9 (1/T^2) lower bound for the symmetric setting with symmetric over-parameterization and a global exact convergence result for the asymmetric setting with an exponential convergence rate. Furthermore, they propose a simple algorithm that modifies one step of GD, obtaining a convergence rate independent of the initialization scale.\n\nStrengths:\n\n1. The paper provides a rigorous analysis of the behavior of GD in matrix sensing problems, shedding light on the impact of over-parameterization.\n2. The authors' use of asymmetric parameterization to speed up convergence is a novel and significant contribution.\n3. The paper introduces a simple algorithm that eliminates the dependency on the initialization scale, making it a valuable contribution to the field.\n4. The authors provide empirical studies to verify their theoretical findings, adding credibility to their results.\n\nWeaknesses:\n\n1. The paper's focus on matrix sensing problems may limit its generalizability to other areas, such as deep learning.\n2. The authors assume a specific initialization scale, which may not be realistic in all scenarios.\n3. The paper's complexity and length may make it challenging for non-experts to follow.\n4. The authors could provide more insight into the practical implications of their findings and how they can be applied in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. Consider exploring the generalizability of the authors' findings to other areas, such as deep learning.\n2. Provide more realistic initialization scales and discuss the implications of their assumptions.\n3. Simplify the paper's language and structure to make it more accessible to non-experts.\n4. Provide more practical insights into the applications of the authors' findings and how they can be used in real-world scenarios.\n\nRecommendation:\n\nThis paper is a significant contribution to the field of matrix sensing and gradient descent. Its rigorous analysis and novel findings provide valuable insights into the behavior of GD in over-parameterized settings. While there are some limitations and areas for improvement, the paper's strengths and contributions make it a worthwhile read for experts in the field. I recommend this paper for publication in a reputable conference or journal, with the suggestion that the authors address the areas mentioned above.\n\n$$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper explores the impact of over-parameterization on the convergence behaviors of gradient descent (GD) for matrix sensing problems. The authors focus on two settings: symmetric matrix sensing with symmetric parameterization and asymmetric matrix sensing with asymmetric parameterization. They derive a novel \u2126(1/T^2) lower bound for the symmetric setting with over-parameterization and a global exact convergence result for randomly initialized GD with an exponential convergence rate for the asymmetric setting. Furthermore, they show that the imbalance between the factor matrices F and G is crucial for achieving linear convergence.\n\nStrengths:\n- The paper provides a rigorous and systematic investigation of the impact of over-parameterization on GD convergence behaviors for matrix sensing problems.\n- The authors derive novel results, including a \u2126(1/T^2) lower bound for the symmetric setting with over-parameterization and a global exact convergence result for randomly initialized GD with an exponential convergence rate for the asymmetric setting.\n- The paper highlights the importance of imbalance between F and G for achieving linear convergence.\n\nWeaknesses:\n- The paper assumes that the Restricted Isometry Property (RIP) holds for the measurement operator A, which may not be true in practice.\n- The authors rely on a specific initialization scheme, and it is unclear whether their results hold for other initialization schemes.\n- The paper assumes that the singular values of M* are known, which may not be the case in practice.\n\nSuggestions for Improvement:\n- The authors should explore ways to relax the RIP assumption and derive results for more general measurement operators.\n- The paper should investigate whether the results hold for other initialization schemes, such as uniform initialization or initialization with a known distribution.\n- The authors should explore ways to estimate the singular values of M* from the measurements, which can be used to improve the convergence rate.\n\nRecommendation:\nOverall, this paper provides a significant contribution to the understanding of over-parameterization and its impact on GD convergence behaviors for matrix sensing problems. The results highlight the importance of imbalance between F and G for achieving linear convergence. To improve the paper, the authors should explore ways to relax the RIP assumption, investigate the robustness of their results to different initialization schemes, and explore ways to estimate the singular values of M* from the measurements. With these improvements, this paper will be an even more significant contribution to the field.",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\nThis paper explores the impact of over-parameterization on the convergence behaviors of gradient descent (GD) for matrix sensing problems. The authors focus on two settings: symmetric matrix sensing with symmetric parameterization and asymmetric matrix sensing with asymmetric parameterization. They derive a novel \u2126(1/T^2) lower bound for the symmetric setting with over-parameterization and a global exact convergence result for randomly initialized GD with an exponential convergence rate for the asymmetric setting. Furthermore, they show that the imbalance between the factor matrices F and G is crucial for achieving linear convergence.\n\nStrengths:\n- The paper provides a rigorous and systematic investigation of the impact of over-parameterization on GD convergence behaviors for matrix sensing problems.\n- The authors derive novel results, including a \u2126(1/T^2) lower bound for the symmetric setting with over-parameterization and a global exact convergence result for randomly initialized GD with an exponential convergence rate for the asymmetric setting.\n- The paper highlights the importance of imbalance between F and G for achieving linear convergence.\n\nWeaknesses:\n- The paper assumes that the Restricted Isometry Property (RIP) holds for the measurement operator A, which may not be true in practice.\n- The authors rely on a specific initialization scheme, and it is unclear whether their results hold for other initialization schemes.\n- The paper assumes that the singular values of M* are known, which may not be the case in practice.\n\nSuggestions for Improvement:\n- The authors should explore ways to relax the RIP assumption and derive results for more general measurement operators.\n- The paper should investigate whether the results hold for other initialization schemes, such as uniform initialization or initialization with a known distribution.\n- The authors should explore ways to estimate the singular values of M* from the measurements, which can be used to improve the convergence rate.\n\nRecommendation:\nOverall, this paper provides a significant contribution to the understanding of over-parameterization and its impact on GD convergence behaviors for matrix sensing problems. The results highlight the importance of imbalance between F and G for achieving linear convergence. To improve the paper, the authors should explore ways to relax the RIP assumption, investigate the robustness of their results to different initialization schemes, and explore ways to estimate the singular values of M* from the measurements. With these improvements, this paper will be an even more significant contribution to the field.",
        "gr_score_with": 0.6478405315614618,
        "output_with_attack": {
            "num_tokens_scored": 301,
            "num_green_tokens": 195,
            "green_fraction": 0.6478405315614618,
            "z_score": 8.761973375640524,
            "p_value": 9.59307987259109e-19
        },
        "output_with": {
            "num_tokens_scored": 301,
            "num_green_tokens": 195,
            "green_fraction": 0.6478405315614618,
            "z_score": 8.761973375640524,
            "p_value": 9.59307987259109e-19
        },
        "output_without": {
            "num_tokens_scored": 367,
            "num_green_tokens": 166,
            "green_fraction": 0.45231607629427795,
            "z_score": 2.0303479053278326,
            "p_value": 0.021160593979352
        }
    },
    {
        "title": "MODERNTCN: A MODERN PURE CONVOLUTION STRUCTURE FOR GENERAL TIME SERIES ANALYSIS",
        "abstract": "Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel time series analysis model called ModernTCN, which is a modern pure convolution structure for general time series analysis. The authors aim to bring convolution back to the arena of time series analysis by modernizing the traditional TCN and conducting time series related modifications to make it more suitable for time series tasks. The proposed model, ModernTCN, achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection.\n\nStrengths:\n1. The paper presents a novel approach to time series analysis by modernizing the traditional TCN and incorporating time series related modifications.\n2. The authors demonstrate the excellent task generality of ModernTCN, achieving consistent state-of-the-art performance on multiple mainstream time series analysis tasks.\n3. The paper provides a comprehensive comparison of performance and efficiency with other state-of-the-art models, highlighting the superiority of ModernTCN in both aspects.\n\nWeaknesses:\n1. The paper assumes that the input time series is already preprocessed and does not provide information on how to handle missing values or outliers.\n2. The authors do not provide a detailed explanation of the hyperparameter tuning process and the choice of hyperparameters.\n3. The paper focuses primarily on the performance of ModernTCN on five mainstream time series analysis tasks and does not explore its application in other areas, such as anomaly detection or classification.\n\nSuggestions for Improvement:\n1. Provide more details on how to handle missing values or outliers in the input time series.\n2. Include a more comprehensive hyperparameter tuning process and explanation of the choice of hyperparameters.\n3. Explore the application of ModernTCN in other areas, such as anomaly detection or classification, and provide a more detailed analysis of its performance in these tasks.\n\nRecommendation:\nThe paper presents a novel and effective approach to time series analysis, and the proposed model, ModernTCN, demonstrates excellent task generality and performance. However, the paper could be improved by providing more details on handling missing values or outliers, hyperparameter tuning, and exploring the application of ModernTCN in other areas. Overall, the paper is well-written, and the authors provide a clear and concise explanation of their approach and results. Therefore, I recommend this paper for publication in the conference.\n\nScore: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes ModernTCN, a modern pure convolution structure for general time series analysis. ModernTCN modernizes the traditional TCN and conducts time series related modifications to make it more suitable for time series tasks. The authors aim to bring convolution back to the arena of time series analysis, which has received less attention in recent years. ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection, while maintaining the efficiency advantage of convolution-based models.\n\nStrengths:\n1. The paper presents a novel approach to time series analysis by modernizing the traditional TCN and conducting time series related modifications to make it more suitable for time series tasks.\n2. The authors demonstrate the excellent task generality of ModernTCN, achieving consistent state-of-the-art performance on five mainstream time series analysis tasks.\n3. ModernTCN maintains the efficiency advantage of convolution-based models, providing a better balance of efficiency and performance.\n4. The paper provides a comprehensive comparison of the performance and efficiency of ModernTCN with state-of-the-art Transformer-based and MLP-based models, as well as convolution-based models.\n\nWeaknesses:\n1. The paper assumes that the time series data has a fixed length, which may not be the case in many real-world applications.\n2. The authors do not provide a detailed analysis of the time complexity of ModernTCN, which may be a concern for large-scale time series analysis tasks.\n3. The paper does not discuss the interpretability of ModernTCN, which is an important aspect of time series analysis.\n\nSuggestions for Improvement:\n1. The authors should consider incorporating dynamic length time series data into ModernTCN, which would make it more generalizable to real-world applications.\n2. A detailed analysis of the time complexity of ModernTCN would be helpful to understand its scalability for large-scale time series analysis tasks.\n3. The authors should discuss the interpretability of ModernTCN, which would be useful for understanding the underlying patterns and relationships in the time series data.\n\nRecommendation:\nThe paper presents a novel approach to time series analysis, and the authors demonstrate the excellent task generality of ModernTCN. However, the paper assumes fixed length time series data, and the time complexity of ModernTCN is not thoroughly analyzed. To make ModernTCN more generalizable and scalable, the authors should address these concerns. With some revisions, the paper has the potential to be accepted in top-tier conferences in the field of time series analysis.",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\nThe paper proposes ModernTCN, a modern pure convolution structure for general time series analysis. ModernTCN modernizes the traditional TCN and conducts time series related modifications to make it more suitable for time series tasks. The authors aim to bring convolution back to the arena of time series analysis, which has received less attention in recent years. ModernTCN achieves consistent state-of-the-art performance on five mainstream time series analysis tasks, including long-term and short-term forecasting, imputation, classification, and anomaly detection, while maintaining the efficiency advantage of convolution-based models.\n\nStrengths:\n1. The paper presents a novel approach to time series analysis by modernizing the traditional TCN and conducting time series related modifications to make it more suitable for time series tasks.\n2. The authors demonstrate the excellent task generality of ModernTCN, achieving consistent state-of-the-art performance on five mainstream time series analysis tasks.\n3. ModernTCN maintains the efficiency advantage of convolution-based models, providing a better balance of efficiency and performance.\n4. The paper provides a comprehensive comparison of the performance and efficiency of ModernTCN with state-of-the-art Transformer-based and MLP-based models, as well as convolution-based models.\n\nWeaknesses:\n1. The paper assumes that the time series data has a fixed length, which may not be the case in many real-world applications.\n2. The authors do not provide a detailed analysis of the time complexity of ModernTCN, which may be a concern for large-scale time series analysis tasks.\n3. The paper does not discuss the interpretability of ModernTCN, which is an important aspect of time series analysis.\n\nSuggestions for Improvement:\n1. The authors should consider incorporating dynamic length time series data into ModernTCN, which would make it more generalizable to real-world applications.\n2. A detailed analysis of the time complexity of ModernTCN would be helpful to understand its scalability for large-scale time series analysis tasks.\n3. The authors should discuss the interpretability of ModernTCN, which would be useful for understanding the underlying patterns and relationships in the time series data.\n\nRecommendation:\nThe paper presents a novel approach to time series analysis, and the authors demonstrate the excellent task generality of ModernTCN. However, the paper assumes fixed length time series data, and the time complexity of ModernTCN is not thoroughly analyzed. To make ModernTCN more generalizable and scalable, the authors should address these concerns. With some revisions, the paper has the potential to be accepted in top-tier conferences in the field of time series analysis.",
        "gr_score_with": 0.6602564102564102,
        "output_with_attack": {
            "num_tokens_scored": 312,
            "num_green_tokens": 206,
            "green_fraction": 0.6602564102564102,
            "z_score": 9.371030809101516,
            "p_value": 3.59137811010177e-21
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 206,
            "green_fraction": 0.6602564102564102,
            "z_score": 9.371030809101516,
            "p_value": 3.59137811010177e-21
        },
        "output_without": {
            "num_tokens_scored": 312,
            "num_green_tokens": 176,
            "green_fraction": 0.5641025641025641,
            "z_score": 5.904611041643469,
            "p_value": 1.7673986024647748e-09
        }
    },
    {
        "title": "VONET: UNSUPERVISED VIDEO OBJECT LEARNING WITH PARALLEL U-NET ATTENTION",
        "abstract": "Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents VONet, a novel approach for unsupervised video object learning. VONet is inspired by MONet, a scene decomposition model for single images, but extends it to handle video sequences. The key innovation in VONet is its parallel attention process, which generates attention masks for all slots simultaneously from a U-Net. Additionally, VONet incorporates an object-wise sequential VAE framework to achieve temporal consistency of objects between adjacent video frames. The paper also presents an expressive transformer-based decoder to empower the model to derive object representations from complex video scenes. The effectiveness of VONet is demonstrated through extensive evaluations on five MOVI datasets, where it outperforms several strong baselines.\n\nStrengths:\n\n* The paper presents a novel and innovative approach to unsupervised video object learning.\n* The parallel attention process is a significant improvement over the recurrent attention mechanism used in MONet.\n* The object-wise sequential VAE framework is a well-designed solution to the temporal consistency problem.\n* The use of an expressive transformer-based decoder is a good choice for handling complex video scenes.\n* The paper provides a comprehensive evaluation of VONet on five MOVI datasets, which demonstrates its effectiveness.\n\nWeaknesses:\n\n* The paper assumes that the number of slots is known in advance, which may not be the case in practice.\n* The parallel attention process may not be scalable to very large numbers of slots.\n* The object-wise sequential VAE framework may not be able to capture complex temporal relationships between objects.\n* The use of a U-Net architecture for attention mask generation may not be the best choice for all applications.\n* The paper does not provide a clear explanation of how the context vectors are acquired.\n\nSuggestions for Improvement:\n\n* The paper could benefit from a more detailed explanation of how the context vectors are acquired.\n* The authors could investigate the scalability of the parallel attention process to very large numbers of slots.\n* The object-wise sequential VAE framework could be improved by incorporating more advanced temporal modeling techniques.\n* The use of a U-Net architecture for attention mask generation could be replaced with a more flexible architecture.\n* The paper could provide more detailed results on the performance of VONet on more challenging datasets.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to unsupervised video object learning, which outperforms several strong baselines. However, there are some limitations and areas for improvement. With some modifications and additional experiments, the paper could be significantly strengthened. I recommend accepting the paper with minor revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces VONet, an unsupervised video object learning method that generates high-quality object representations from video frames. VONet is inspired by MONet, a scene decomposition model for single images, and extends it to video object learning using a parallel attention process. The parallel attention network generates attention masks for all slots simultaneously, reducing the inference time compared to traditional recurrent attention methods. Additionally, VONet incorporates an object-wise sequential VAE framework to achieve temporal consistency of object representations across consecutive video frames. The method is evaluated on five MOVI datasets, outperforming several state-of-the-art baselines in terms of FG-ARI and mIoU metrics.\n\nStrengths:\n\n1. VONet's parallel attention process is efficient and effective, reducing the inference time compared to traditional recurrent attention methods.\n2. The object-wise sequential VAE framework is novel and effective in achieving temporal consistency of object representations across consecutive video frames.\n3. VONet outperforms several state-of-the-art baselines on five MOVI datasets, demonstrating its effectiveness in generating high-quality object representations.\n4. The method is easy to train and requires minimal supervision, making it suitable for unsupervised video object learning.\n\nWeaknesses:\n\n1. VONet's performance is highly dependent on the quality of the context vectors, which may be challenging to obtain, especially in scenarios with complex video content.\n2. The method requires a large number of parameters, which may lead to overfitting if not properly regularized.\n3. VONet's ability to handle complex video scenes is limited, and it may struggle to segment objects with varying sizes and shapes.\n4. The method's failure modes, such as over-segmentation and incomplete object understanding, need to be addressed in future work.\n\nSuggestions for Improvement:\n\n1. Develop more effective methods for obtaining high-quality context vectors, such as using pre-trained models or incorporating additional supervision signals.\n2. Explore regularization techniques to reduce overfitting and improve the robustness of VONet.\n3. Investigate ways to improve VONet's ability to handle complex video scenes, such as using more advanced attention mechanisms or incorporating additional prior knowledge.\n4. Address the failure modes of VONet, such as over-segmentation and incomplete object understanding, by developing new loss functions or incorporating additional supervision signals.\n\nRecommendation:\n\nVONet is a state-of-the-art approach for unsupervised video object learning, and its parallel attention process and object-wise sequential VAE framework are novel and effective. However, its performance is highly dependent on the quality of the context vectors, and its ability to handle complex video scenes is limited. To address these limitations, researchers should focus on developing more effective methods for obtaining high-quality context vectors, exploring regularization techniques, and investigating ways to improve VONet's ability to handle complex video scenes. Additionally, addressing the failure modes of VONet is crucial to improve its robustness and reliability.\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper introduces VONet, an unsupervised video object learning method that generates high-quality object representations from video frames. VONet is inspired by MONet, a scene decomposition model for single images, and extends it to video object learning using a parallel attention process. The parallel attention network generates attention masks for all slots simultaneously, reducing the inference time compared to traditional recurrent attention methods. Additionally, VONet incorporates an object-wise sequential VAE framework to achieve temporal consistency of object representations across consecutive video frames. The method is evaluated on five MOVI datasets, outperforming several state-of-the-art baselines in terms of FG-ARI and mIoU metrics.\n\nStrengths:\n\n1. VONet's parallel attention process is efficient and effective, reducing the inference time compared to traditional recurrent attention methods.\n2. The object-wise sequential VAE framework is novel and effective in achieving temporal consistency of object representations across consecutive video frames.\n3. VONet outperforms several state-of-the-art baselines on five MOVI datasets, demonstrating its effectiveness in generating high-quality object representations.\n4. The method is easy to train and requires minimal supervision, making it suitable for unsupervised video object learning.\n\nWeaknesses:\n\n1. VONet's performance is highly dependent on the quality of the context vectors, which may be challenging to obtain, especially in scenarios with complex video content.\n2. The method requires a large number of parameters, which may lead to overfitting if not properly regularized.\n3. VONet's ability to handle complex video scenes is limited, and it may struggle to segment objects with varying sizes and shapes.\n4. The method's failure modes, such as over-segmentation and incomplete object understanding, need to be addressed in future work.\n\nSuggestions for Improvement:\n\n1. Develop more effective methods for obtaining high-quality context vectors, such as using pre-trained models or incorporating additional supervision signals.\n2. Explore regularization techniques to reduce overfitting and improve the robustness of VONet.\n3. Investigate ways to improve VONet's ability to handle complex video scenes, such as using more advanced attention mechanisms or incorporating additional prior knowledge.\n4. Address the failure modes of VONet, such as over-segmentation and incomplete object understanding, by developing new loss functions or incorporating additional supervision signals.\n\nRecommendation:\n\nVONet is a state-of-the-art approach for unsupervised video object learning, and its parallel attention process and object-wise sequential VAE framework are novel and effective. However, its performance is highly dependent on the quality of the context vectors, and its ability to handle complex video scenes is limited. To address these limitations, researchers should focus on developing more effective methods for obtaining high-quality context vectors, exploring regularization techniques, and investigating ways to improve VONet's ability to handle complex video scenes. Additionally, addressing the failure modes of VONet is crucial to improve its robustness and reliability.\n\n$$$",
        "gr_score_with": 0.578125,
        "output_with_attack": {
            "num_tokens_scored": 384,
            "num_green_tokens": 222,
            "green_fraction": 0.578125,
            "z_score": 7.1127408612387,
            "p_value": 5.688037984823037e-13
        },
        "output_with": {
            "num_tokens_scored": 384,
            "num_green_tokens": 222,
            "green_fraction": 0.578125,
            "z_score": 7.1127408612387,
            "p_value": 5.688037984823037e-13
        },
        "output_without": {
            "num_tokens_scored": 343,
            "num_green_tokens": 162,
            "green_fraction": 0.47230320699708456,
            "z_score": 2.7222623274964604,
            "p_value": 0.0032418326211383812
        }
    },
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential limitations and challenges of applying the concept bottleneck layer to more complex and realistic settings.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the training process for the concept bottleneck layer, including the loss functions and hyperparameters used.\n2. Discuss the potential limitations and challenges of applying the concept bottleneck layer to more complex and realistic settings, such as text or 3D data.\n3. Provide more qualitative and quantitative evaluations of the concept bottleneck layer, including its performance on more challenging datasets and tasks.\n4. Consider incorporating more visualizations and illustrations to help readers understand the concept bottleneck layer and its applications.\n\nRecommendation:\n\nThis paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. While the paper has some limitations, it provides a comprehensive evaluation of CBGMs and demonstrates their effectiveness and robustness. I recommend this paper to readers interested in generative modeling, interpretability, and controllability.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThis paper proposes concept bottleneck generative models (CBGMs), a type of generative model where one of its internal layers\u2014a concept bottleneck (CB) layer\u2014is constrained to map from input representations to human-understandable features. The CB layer is model-agnostic and can be applied to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. The CB layer is trained using a concept loss and an orthogonality loss, and is shown to improve the steerability and interpretability of generative models.\n\nStrengths:\n\n* The concept bottleneck layer is a novel and effective way to improve the interpretability and steerability of generative models.\n* The CB layer is model-agnostic and can be applied to different types of generative models.\n* The proposed method is shown to outperform existing conditional generation approaches in terms of steerability.\n* The CB layer can be used to debug and interpret generative models, making them easier to understand and use.\n\nWeaknesses:\n\n* The CB layer requires pre-defined concepts, which may not always be available or comprehensive.\n* The method requires a large amount of labeled training data, which can be time-consuming and expensive to obtain.\n* The CB layer may not be effective for all types of generative models, and further research is needed to explore its applicability to other models.\n* The method may not be scalable to very large models, and further research is needed to explore its applicability to large-scale models.\n\nSuggestions for Improvement:\n\n* Further research is needed to explore the applicability of the CB layer to other types of generative models.\n* Methods to alleviate the need for pre-defined concepts, such as using unsupervised or weakly-supervised learning, should be explored.\n* Techniques to reduce the amount of labeled training data required, such as using transfer learning or few-shot learning, should be explored.\n* Methods to improve the scalability of the CB layer to very large models should be explored.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of generative models by proposing a novel and effective method to improve their interpretability and steerability. The CB layer is model-agnostic and can be applied to different types of generative models, making it a valuable tool for researchers and practitioners. However, further research is needed to address the limitations and challenges associated with the method. Overall, I recommend this paper for publication in a top-tier conference or journal.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\n\nPaper Summary:\n\nThis paper proposes concept bottleneck generative models (CBGMs), a type of generative model where one of its internal layers\u2014a concept bottleneck (CB) layer\u2014is constrained to map from input representations to human-understandable features. The CB layer is model-agnostic and can be applied to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. The CB layer is trained using a concept loss and an orthogonality loss, and is shown to improve the steerability and interpretability of generative models.\n\nStrengths:\n\n* The concept bottleneck layer is a novel and effective way to improve the interpretability and steerability of generative models.\n* The CB layer is model-agnostic and can be applied to different types of generative models.\n* The proposed method is shown to outperform existing conditional generation approaches in terms of steerability.\n* The CB layer can be used to debug and interpret generative models, making them easier to understand and use.\n\nWeaknesses:\n\n* The CB layer requires pre-defined concepts, which may not always be available or comprehensive.\n* The method requires a large amount of labeled training data, which can be time-consuming and expensive to obtain.\n* The CB layer may not be effective for all types of generative models, and further research is needed to explore its applicability to other models.\n* The method may not be scalable to very large models, and further research is needed to explore its applicability to large-scale models.\n\nSuggestions for Improvement:\n\n* Further research is needed to explore the applicability of the CB layer to other types of generative models.\n* Methods to alleviate the need for pre-defined concepts, such as using unsupervised or weakly-supervised learning, should be explored.\n* Techniques to reduce the amount of labeled training data required, such as using transfer learning or few-shot learning, should be explored.\n* Methods to improve the scalability of the CB layer to very large models should be explored.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of generative models by proposing a novel and effective method to improve their interpretability and steerability. The CB layer is model-agnostic and can be applied to different types of generative models, making it a valuable tool for researchers and practitioners. However, further research is needed to address the limitations and challenges associated with the method. Overall, I recommend this paper for publication in a top-tier conference or journal.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.6017964071856288,
        "output_with_attack": {
            "num_tokens_scored": 334,
            "num_green_tokens": 201,
            "green_fraction": 0.6017964071856288,
            "z_score": 7.51018414220703,
            "p_value": 2.952212523747646e-14
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 201,
            "green_fraction": 0.6017964071856288,
            "z_score": 7.51018414220703,
            "p_value": 2.952212523747646e-14
        },
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 209,
            "green_fraction": 0.5372750642673522,
            "z_score": 5.507858478946322,
            "p_value": 1.81612609719316e-08
        }
    },
    {
        "title": "TRANSFORMER-VQ: LINEAR-TIME TRANSFORMERS",
        "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmaxbased dense self-attention in linear time. Transformer-VQ\u2019s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: https://github.com/transformer-vq/transformer_vq k1 k2 k3 k4 k5 7\u2192 VQ \u2248 k\u03021 k\u03022 k\u03023 k\u03024 k\u03025 Figure 1: Schematic of the VQ-Attention approximation. The colorful and blank boxes depict the keys and attention weights, respectively. The keys on the right have been vector-quantized. Since the green keys k2, k5 map to the same code, they have the same attention weights in this attention head.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time. This is made possible through a combination of vector-quantized keys, localized positional biases, and compressive cache that can be attended to efficiently, while yielding the same results as an uncompressed variable-length cache. The authors propose a new attention mechanism that uses vector-quantization to reduce the complexity of self-attention, and demonstrate the effectiveness of Transformer-VQ on several long-range autoregressive modeling tasks.\n\nStrengths:\n\n* The paper presents a novel attention mechanism that can compute dense self-attention in linear time, which is a significant improvement over the quadratic time complexity of standard transformers.\n* The authors provide a thorough analysis of the proposed mechanism and demonstrate its effectiveness on several long-range autoregressive modeling tasks.\n* The paper presents a clear and concise description of the Transformer-VQ model and its components, making it easy to follow and understand.\n* The authors provide a comprehensive comparison with existing methods, including kernelizable attention and compressive transformers.\n\nWeaknesses:\n\n* The paper assumes familiarity with transformers and attention mechanisms, which may make it difficult for readers who are not familiar with these concepts to understand the paper.\n* The authors do not provide a clear explanation of the theoretical guarantees of the proposed mechanism, which may make it difficult to assess its reliability and robustness.\n* The paper does not provide a thorough analysis of the computational complexity of the proposed mechanism, which may make it difficult to assess its scalability and efficiency.\n* The authors do not provide a clear explanation of the hyperparameter tuning process, which may make it difficult to reproduce the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the theoretical guarantees of the proposed mechanism, including its reliability and robustness.\n* Provide a more thorough analysis of the computational complexity of the proposed mechanism, including its scalability and efficiency.\n* Provide a clear explanation of the hyperparameter tuning process, including the choice of hyperparameters and the impact of hyperparameter values on the results.\n* Provide more experimental results, including results on additional datasets and tasks, to demonstrate the effectiveness and robustness of the proposed mechanism.\n\nRecommendation:\n\nThe paper presents a novel attention mechanism that can compute dense self-attention in linear time, which is a significant improvement over the quadratic time complexity of standard transformers. The authors provide a thorough analysis of the proposed mechanism and demonstrate its effectiveness on several long-range autoregressive modeling tasks. However, the paper assumes familiarity with transformers and attention mechanisms, and does not provide a clear explanation of the theoretical guarantees and computational complexity of the proposed mechanism. Therefore, I recommend that the authors provide a more detailed explanation of the theoretical guarantees and computational complexity of the proposed mechanism, and provide more experimental results to demonstrate its effectiveness and robustness.\n\nOverall, the paper is well-written and presents a novel and effective attention mechanism. However, it requires further development and experimentation to make it more robust and reliable.\n\nRating: 7/10\n\nRecommendation for publication: Yes, but with revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nTransformer-VQ is a decoder-only transformer that can compute dense self-attention in linear time with respect to sequence length. It achieves this efficiency through a combination of vector-quantized keys, localized positional biases, and compressive cache. The authors propose a new attention mechanism, VQ-Attention, which uses vector-quantization to reduce the dimensionality of the keys, and a caching mechanism to reduce the computational cost of attention. They demonstrate the effectiveness of Transformer-VQ on several long-range autoregressive modeling tasks, including Enwik8, PG-19, and ImageNet64.\n\nStrengths:\n1. The authors propose a new attention mechanism, VQ-Attention, which is efficient and can handle long sequences.\n2. The use of vector-quantization and caching mechanisms allows for significant reductions in computational cost and memory usage.\n3. The authors demonstrate the effectiveness of Transformer-VQ on several challenging tasks, including Enwik8, PG-19, and ImageNet64.\n4. The codebook size ablation study shows that increasing the codebook size can improve the prediction quality, but at the expense of additional wall time.\n\nWeaknesses:\n1. The authors assume familiarity with transformers and vector-quantization, which may not be familiar to all readers.\n2. The paper could benefit from more experimental results, particularly on more challenging tasks or with larger models.\n3. The authors do not provide a clear explanation of why VQ-Attention is more efficient than other attention mechanisms, and why it is suitable for long-range autoregressive modeling tasks.\n4. The authors assume that the codebook size will not grow indefinitely, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Provide more experimental results, particularly on more challenging tasks or with larger models.\n2. Provide a more detailed explanation of why VQ-Attention is more efficient than other attention mechanisms, and why it is suitable for long-range autoregressive modeling tasks.\n3. Discuss the potential limitations and challenges of using VQ-Attention, particularly with respect to codebook size and computational cost.\n4. Consider including more discussion on the relationship between VQ-Attention and other attention mechanisms, and how VQ-Attention can be applied to other tasks.\n\nRecommendation:\nThis paper presents a new attention mechanism, VQ-Attention, which is efficient and can handle long sequences. The authors demonstrate the effectiveness of Transformer-VQ on several challenging tasks, and provide a clear explanation of the benefits and limitations of VQ-Attention. However, the paper could benefit from more experimental results and a more detailed explanation of why VQ-Attention is more efficient than other attention mechanisms. With some additional work, this paper could be a strong contender for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\nTransformer-VQ is a decoder-only transformer that can compute dense self-attention in linear time with respect to sequence length. It achieves this efficiency through a combination of vector-quantized keys, localized positional biases, and compressive cache. The authors propose a new attention mechanism, VQ-Attention, which uses vector-quantization to reduce the dimensionality of the keys, and a caching mechanism to reduce the computational cost of attention. They demonstrate the effectiveness of Transformer-VQ on several long-range autoregressive modeling tasks, including Enwik8, PG-19, and ImageNet64.\n\nStrengths:\n1. The authors propose a new attention mechanism, VQ-Attention, which is efficient and can handle long sequences.\n2. The use of vector-quantization and caching mechanisms allows for significant reductions in computational cost and memory usage.\n3. The authors demonstrate the effectiveness of Transformer-VQ on several challenging tasks, including Enwik8, PG-19, and ImageNet64.\n4. The codebook size ablation study shows that increasing the codebook size can improve the prediction quality, but at the expense of additional wall time.\n\nWeaknesses:\n1. The authors assume familiarity with transformers and vector-quantization, which may not be familiar to all readers.\n2. The paper could benefit from more experimental results, particularly on more challenging tasks or with larger models.\n3. The authors do not provide a clear explanation of why VQ-Attention is more efficient than other attention mechanisms, and why it is suitable for long-range autoregressive modeling tasks.\n4. The authors assume that the codebook size will not grow indefinitely, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Provide more experimental results, particularly on more challenging tasks or with larger models.\n2. Provide a more detailed explanation of why VQ-Attention is more efficient than other attention mechanisms, and why it is suitable for long-range autoregressive modeling tasks.\n3. Discuss the potential limitations and challenges of using VQ-Attention, particularly with respect to codebook size and computational cost.\n4. Consider including more discussion on the relationship between VQ-Attention and other attention mechanisms, and how VQ-Attention can be applied to other tasks.\n\nRecommendation:\nThis paper presents a new attention mechanism, VQ-Attention, which is efficient and can handle long sequences. The authors demonstrate the effectiveness of Transformer-VQ on several challenging tasks, and provide a clear explanation of the benefits and limitations of VQ-Attention. However, the paper could benefit from more experimental results and a more detailed explanation of why VQ-Attention is more efficient than other attention mechanisms. With some additional work, this paper could be a strong contender for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.6297376093294461,
        "output_with_attack": {
            "num_tokens_scored": 343,
            "num_green_tokens": 216,
            "green_fraction": 0.6297376093294461,
            "z_score": 8.664219421479523,
            "p_value": 2.2730852274926406e-18
        },
        "output_with": {
            "num_tokens_scored": 343,
            "num_green_tokens": 216,
            "green_fraction": 0.6297376093294461,
            "z_score": 8.664219421479523,
            "p_value": 2.2730852274926406e-18
        },
        "output_without": {
            "num_tokens_scored": 337,
            "num_green_tokens": 176,
            "green_fraction": 0.5222551928783383,
            "z_score": 4.561343724172428,
            "p_value": 2.541364902914147e-06
        }
    },
    {
        "title": "SHARING RATIO DECOMPOSITION",
        "abstract": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to faithfully reflect the model's inference process and provide robust explanations. The method adopts a vector perspective to consider the intricate nonlinear interactions between filters and introduces an interesting observation termed Activation-Pattern-Only Prediction (APOP), which highlights the importance of inactive neurons. SRD decomposes a Pointwise Feature Vector (PFV) into shares of PFVs in its receptive field, providing a high-resolution Effective Receptive Field (ERF) at any layer. The authors demonstrate the effectiveness of SRD in providing robust and trustworthy explanations, outperforming other state-of-the-art methods across various metrics.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to explainability, which reflects the model's inference process faithfully and provides robust explanations.\n2. The authors introduce a vector perspective to consider the nonlinear interactions between filters, which marks a departure from conventional neuron-level analyses.\n3. The Activation-Pattern-Only Prediction (APOP) observation highlights the importance of inactive neurons, which has been overlooked so far by conventional approaches.\n4. The method provides a high-resolution Effective Receptive Field (ERF) at any layer, enabling comprehensive exploration from local to global explanations.\n5. The authors demonstrate the effectiveness of SRD in providing robust and trustworthy explanations, outperforming other state-of-the-art methods across various metrics.\n\nWeaknesses:\n\n1. The paper assumes that the sharing ratio between layers can be derived, which may not always be the case in practice.\n2. The method requires prior knowledge of the sharing ratio, which may not be available in all scenarios.\n3. The authors do not provide a clear explanation of how the sharing ratio is calculated, which may lead to confusion.\n4. The paper focuses primarily on image classification tasks and does not explore other applications of SRD.\n5. The authors do not provide a thorough analysis of the computational complexity of SRD, which may be a limitation in practice.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how the sharing ratio is calculated and how it is derived in practice.\n2. Explore other applications of SRD beyond image classification tasks.\n3. Analyze the computational complexity of SRD and provide recommendations for optimizing its performance.\n4. Provide a more detailed comparison with other state-of-the-art methods, including a discussion of their strengths and weaknesses.\n5. Consider incorporating additional metrics or evaluation protocols to assess the robustness and trustworthiness of SRD.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to explainability, which reflects the model's inference process faithfully and provides robust explanations. While there are some limitations and areas for improvement, the authors demonstrate the effectiveness of SRD in providing robust and trustworthy explanations, outperforming other state-of-the-art methods across various metrics. Therefore, I recommend accepting this paper for publication, with the understanding that the authors will address the suggested improvements in future revisions.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep learning models. SRD decomposes the Pointwise Feature Vector (PFV) into shares of PFVs in its receptive field, allowing for the recursive decomposition of PFVs and providing a high-resolution Effective Receptive Field (ERF) at any layer. The method is centered on faithfully representing the model's inference process, relying solely on model-generated information, and refraining from any form of correction. SRD surpasses other state-of-the-art methods in meeting the desiderata of explanations, including localization, complexity, faithfulness, and robustness.\n\nStrengths:\n1. SRD provides a novel and comprehensive method for explaining deep learning models, incorporating both local and global explanations.\n2. The method is centered on faithfully representing the model's inference process, which is essential for providing robust and trustworthy explanations.\n3. SRD accounts for nonlinear interactions between filters, which is crucial for understanding the behavior of deep learning models.\n4. The method is able to derive high-resolution ERFs at any layer, allowing for comprehensive exploration from local to global explanations.\n\nWeaknesses:\n1. The paper assumes that the sharing ratio between layers can be derived at any point, which is not always the case in practice.\n2. The method requires prior knowledge of the sharing ratio, which can be difficult to obtain, especially for complex models.\n3. The paper does not provide a clear explanation of how the method can be adapted to different network architectures, which can limit its applicability.\n\nSuggestions for Improvement:\n1. Provide more detailed information on how to derive the sharing ratio between layers, especially for complex models.\n2. Investigate the possibility of estimating the sharing ratio using the model's parameters and activations, which can be more practical and generalizable.\n3. Provide more examples and experiments to demonstrate the method's applicability and robustness on different network architectures.\n\nRecommendation:\nThis paper presents a novel and comprehensive method for explaining deep learning models, which is centered on faithfully representing the model's inference process. The method is able to derive high-resolution ERFs at any layer, allowing for comprehensive exploration from local to global explanations. While there are some limitations and areas for improvement, the method shows promising results and can be a valuable tool for understanding the behavior of deep learning models. Therefore, I recommend this paper for publication. $$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\nThis paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep learning models. SRD decomposes the Pointwise Feature Vector (PFV) into shares of PFVs in its receptive field, allowing for the recursive decomposition of PFVs and providing a high-resolution Effective Receptive Field (ERF) at any layer. The method is centered on faithfully representing the model's inference process, relying solely on model-generated information, and refraining from any form of correction. SRD surpasses other state-of-the-art methods in meeting the desiderata of explanations, including localization, complexity, faithfulness, and robustness.\n\nStrengths:\n1. SRD provides a novel and comprehensive method for explaining deep learning models, incorporating both local and global explanations.\n2. The method is centered on faithfully representing the model's inference process, which is essential for providing robust and trustworthy explanations.\n3. SRD accounts for nonlinear interactions between filters, which is crucial for understanding the behavior of deep learning models.\n4. The method is able to derive high-resolution ERFs at any layer, allowing for comprehensive exploration from local to global explanations.\n\nWeaknesses:\n1. The paper assumes that the sharing ratio between layers can be derived at any point, which is not always the case in practice.\n2. The method requires prior knowledge of the sharing ratio, which can be difficult to obtain, especially for complex models.\n3. The paper does not provide a clear explanation of how the method can be adapted to different network architectures, which can limit its applicability.\n\nSuggestions for Improvement:\n1. Provide more detailed information on how to derive the sharing ratio between layers, especially for complex models.\n2. Investigate the possibility of estimating the sharing ratio using the model's parameters and activations, which can be more practical and generalizable.\n3. Provide more examples and experiments to demonstrate the method's applicability and robustness on different network architectures.\n\nRecommendation:\nThis paper presents a novel and comprehensive method for explaining deep learning models, which is centered on faithfully representing the model's inference process. The method is able to derive high-resolution ERFs at any layer, allowing for comprehensive exploration from local to global explanations. While there are some limitations and areas for improvement, the method shows promising results and can be a valuable tool for understanding the behavior of deep learning models. Therefore, I recommend this paper for publication. $$",
        "gr_score_with": 0.6685552407932012,
        "output_with_attack": {
            "num_tokens_scored": 353,
            "num_green_tokens": 236,
            "green_fraction": 0.6685552407932012,
            "z_score": 10.279009855669802,
            "p_value": 4.381156322249153e-25
        },
        "output_with": {
            "num_tokens_scored": 353,
            "num_green_tokens": 236,
            "green_fraction": 0.6685552407932012,
            "z_score": 10.279009855669802,
            "p_value": 4.381156322249153e-25
        },
        "output_without": {
            "num_tokens_scored": 397,
            "num_green_tokens": 222,
            "green_fraction": 0.5591939546599496,
            "z_score": 6.453826238549557,
            "p_value": 5.453042664090972e-11
        }
    },
    {
        "title": "DON\u2019T TRUST: VERIFY \u2013 GROUNDING LLM QUANTI-",
        "abstract": "Large language models (LLM), such as Google\u2019s Minerva and OpenAI\u2019s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code \u2014 which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting \u2014 the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among multiple informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. DTV leverages the autoformalization capability of LLMs through few-shot prompting to translate informal statements into formal statements, and then uses a formal theorem proving environment to verify the correctness of the formal statements. The authors demonstrate the effectiveness of DTV by evaluating it on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. The results show that DTV consistently outperforms vanilla majority voting, the best previous approach, and leads to improvement across different model sizes from 8B, 62B, and 540B.\n\nStrengths:\n\n* The paper proposes a novel approach to identify the correct answer among multiple informal solution samples generated by LLMs.\n* The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith.\n* The results show that DTV consistently outperforms vanilla majority voting, the best previous approach.\n* The authors provide a detailed explanation of the DTV approach and its components.\n* The paper discusses the limitations of the approach and potential future directions.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used for autoformalization are capable of generating accurate formal statements, which may not always be the case.\n* The authors do not provide a detailed analysis of the errors made by the LLMs in autoformalization.\n* The paper does not discuss the scalability of the DTV approach to larger datasets or more complex problems.\n* The authors do not provide a comparison with other approaches that use formal theorem proving environments for verification.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the errors made by the LLMs in autoformalization and discuss potential strategies to improve the accuracy of formal statements.\n* The paper should include a comparison with other approaches that use formal theorem proving environments for verification.\n* The authors should discuss the scalability of the DTV approach to larger datasets or more complex problems.\n* The paper should provide more case studies to demonstrate the effectiveness of DTV on a wider range of problems.\n\nRecommendation:\n\nThe paper proposes a novel approach to identify the correct answer among multiple informal solution samples generated by LLMs. The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. The results show that DTV consistently outperforms vanilla majority voting, the best previous approach. However, the paper assumes that the LLMs used for autoformalization are capable of generating accurate formal statements, which may not always be the case. Therefore, the authors should provide a more detailed analysis of the errors made by the LLMs in autoformalization and discuss potential strategies to improve the accuracy of formal statements.\n\nOverall, the paper is well-written and provides a clear explanation of the DTV approach and its components. The results are convincing, and the authors provide a detailed discussion of the limitations of the approach and potential future directions. However, the paper could benefit from a more detailed analysis of the errors made by the LLMs in autoformalization and a comparison with other approaches that use formal theorem proving environments for verification.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a method called \"Don't Trust: Verify\" (DTV) to identify the correct answer among many informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. DTV leverages the autoformalization capability of LLMs to translate informal mathematical statements into formal Isabelle code, which can be verified automatically for internal consistency. The method involves generating a formal statement and solution, verifying the solution, and filtering out unfaithful statement formalizations. The authors evaluate DTV on GSM8K, MATH, and MultiArith datasets and demonstrate that it outperforms vanilla majority voting, the previously best method to identify correct answers, by more than 12% on GSM8K.\n\nStrengths:\n\n1. The paper presents a novel approach to improving the accuracy of LLMs in quantitative reasoning problems.\n2. The method is lightweight, requiring no training or finetuning, and can be applied to different model sizes.\n3. The authors provide a comprehensive evaluation of DTV on various datasets and models, demonstrating its effectiveness and scalability.\n\nWeaknesses:\n\n1. The method relies on the autoformalization capability of LLMs, which may lead to subtle mistakes in formal statement translation.\n2. The approach requires a formal theorem proving environment to support the problem domain, limiting its applicability to specific domains.\n3. The authors acknowledge that the method is not perfect and may lead to false positives or false negatives in certain cases.\n\nSuggestions for Improvement:\n\n1. Investigate more effective filters to mitigate the risk of subtle mistakes in formal statement translation.\n2. Explore ways to employ reinforcement learning from human feedback (RLHF) to finetune language models on aligned mathematical formalizations directly.\n3. Develop a method to predict whether a particular problem belongs to a domain well-suited for DTV, enabling its applicability to a broader range of problems.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of natural language processing and quantitative reasoning, demonstrating the potential of autoformalization to improve the accuracy of LLMs. The proposed method, Don't Trust: Verify, has shown impressive results on various datasets and models, outperforming the previously best approach. However, it is essential to address the limitations and weaknesses of the method to make it more robust and applicable to a broader range of problems.\n\nRating: 8/10\n\nRecommendation: Accept",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper presents a method called \"Don't Trust: Verify\" (DTV) to identify the correct answer among many informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. DTV leverages the autoformalization capability of LLMs to translate informal mathematical statements into formal Isabelle code, which can be verified automatically for internal consistency. The method involves generating a formal statement and solution, verifying the solution, and filtering out unfaithful statement formalizations. The authors evaluate DTV on GSM8K, MATH, and MultiArith datasets and demonstrate that it outperforms vanilla majority voting, the previously best method to identify correct answers, by more than 12% on GSM8K.\n\nStrengths:\n\n1. The paper presents a novel approach to improving the accuracy of LLMs in quantitative reasoning problems.\n2. The method is lightweight, requiring no training or finetuning, and can be applied to different model sizes.\n3. The authors provide a comprehensive evaluation of DTV on various datasets and models, demonstrating its effectiveness and scalability.\n\nWeaknesses:\n\n1. The method relies on the autoformalization capability of LLMs, which may lead to subtle mistakes in formal statement translation.\n2. The approach requires a formal theorem proving environment to support the problem domain, limiting its applicability to specific domains.\n3. The authors acknowledge that the method is not perfect and may lead to false positives or false negatives in certain cases.\n\nSuggestions for Improvement:\n\n1. Investigate more effective filters to mitigate the risk of subtle mistakes in formal statement translation.\n2. Explore ways to employ reinforcement learning from human feedback (RLHF) to finetune language models on aligned mathematical formalizations directly.\n3. Develop a method to predict whether a particular problem belongs to a domain well-suited for DTV, enabling its applicability to a broader range of problems.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of natural language processing and quantitative reasoning, demonstrating the potential of autoformalization to improve the accuracy of LLMs. The proposed method, Don't Trust: Verify, has shown impressive results on various datasets and models, outperforming the previously best approach. However, it is essential to address the limitations and weaknesses of the method to make it more robust and applicable to a broader range of problems.\n\nRating: 8/10\n\nRecommendation: Accept",
        "gr_score_with": 0.5885416666666666,
        "output_with_attack": {
            "num_tokens_scored": 384,
            "num_green_tokens": 226,
            "green_fraction": 0.5885416666666666,
            "z_score": 7.518937109037744,
            "p_value": 2.7611669809398016e-14
        },
        "output_with": {
            "num_tokens_scored": 384,
            "num_green_tokens": 226,
            "green_fraction": 0.5885416666666666,
            "z_score": 7.518937109037744,
            "p_value": 2.7611669809398016e-14
        },
        "output_without": {
            "num_tokens_scored": 323,
            "num_green_tokens": 151,
            "green_fraction": 0.4674922600619195,
            "z_score": 2.456120308319326,
            "p_value": 0.007022306488994532
        }
    },
    {
        "title": "CURRICULUM REINFORCEMENT LEARNING FOR QUANTUM ARCHITECTURE SEARCH UNDER HARD-",
        "abstract": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a curriculum-based reinforcement learning quantum architecture search (CRLQAS) algorithm designed to tackle the challenges of deploying variational quantum algorithms (VQAs) in realistic noisy quantum environments. The proposed algorithm incorporates several novel features, including a 3D architecture encoding, illegal actions to prevent invalid sequences of gates, a random halting scheme to steer the agent to find shorter circuits, and a novel variant of simultaneous perturbation stochastic approximation (SPSA) algorithm. The authors also introduce an optimized simulator for the algorithm, which significantly improves computational efficiency in simulating noisy quantum circuits. Numerical experiments demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.\n\nStrengths:\n\n1. The paper presents a novel and comprehensive approach to quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments.\n2. The proposed algorithm incorporates several novel features, including a 3D architecture encoding, illegal actions, and a random halting scheme, which are designed to improve the efficiency and effectiveness of the algorithm.\n3. The authors provide a detailed analysis of the performance of CRLQAS in both noiseless and noisy environments, which demonstrates its superiority over existing QAS algorithms.\n4. The paper also introduces an optimized simulator for the algorithm, which significantly improves computational efficiency in simulating noisy quantum circuits.\n\nWeaknesses:\n\n1. The paper assumes a fixed set of quantum gates, which may limit the generality of the proposed algorithm.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed algorithm, which may be a concern for large-scale quantum systems.\n3. The paper assumes a perfect noise model, which may not be realistic in practice.\n4. The authors do not provide a detailed comparison with other QAS algorithms, which may be necessary to fully evaluate the performance of CRLQAS.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed algorithm, which may be necessary to fully evaluate its performance in large-scale quantum systems.\n2. The paper should include a more detailed comparison with other QAS algorithms, which may be necessary to fully evaluate the performance of CRLQAS.\n3. The authors should provide a more detailed analysis of the noise model used in the paper, which may be necessary to fully evaluate the performance of the proposed algorithm in realistic noisy quantum environments.\n4. The paper should include a more detailed discussion of the potential applications of the proposed algorithm, which may be necessary to fully evaluate its potential impact.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments. The proposed algorithm incorporates several novel features, including a 3D architecture encoding, illegal actions, and a random halting scheme, which are designed to improve the efficiency and effectiveness of the algorithm. The paper also introduces an optimized simulator for the algorithm, which significantly improves computational efficiency in simulating noisy quantum circuits. However, the paper assumes a fixed set of quantum gates, which may limit the generality of the proposed algorithm, and does not provide a detailed analysis of the computational complexity of the proposed algorithm. Therefore, the paper requires further refinement and evaluation before it can be considered a major contribution to the field of quantum architecture search.\n\nOverall, the paper is well-written and provides a comprehensive overview of the proposed algorithm. However, it requires further refinement and evaluation before it can be considered a major contribution to the field of quantum architecture search.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces a curriculum-based reinforcement learning quantum architecture search (CRLQAS) algorithm designed to tackle the challenges of deploying variational quantum algorithms (VQAs) in realistic noisy quantum environments. The proposed algorithm incorporates a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of simultaneous perturbation stochastic approximation (SPSA) for faster convergence. Numerical experiments on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across noiseless and noisy environments for various molecules. The authors also develop an optimized simulator for CRLQAS, employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis, which improves computational efficiency by up to six-fold.\n\nStrengths:\n\n* The paper introduces a novel curriculum-based reinforcement learning algorithm for quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments.\n* The proposed algorithm incorporates several key components, including a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of SPSA, which enhance the efficiency and robustness of the algorithm.\n* Numerical experiments demonstrate the effectiveness of CRLQAS in outperforming existing QAS algorithms across noiseless and noisy environments for various molecules.\n* The authors provide a detailed analysis of the computational efficiency of the optimized simulator, which shows a significant improvement in computational efficiency by up to six-fold.\n\nWeaknesses:\n\n* The paper assumes a noiseless environment for the quantum circuit simulator, which may not be representative of real-world quantum devices.\n* The authors do not provide a detailed analysis of the impact of noise on the algorithm's performance, which is a critical aspect of deploying VQAs in realistic noisy quantum environments.\n* The paper focuses on a limited number of molecules, and it is unclear how the algorithm would perform on more complex molecules or larger system sizes.\n* The authors do not provide a detailed comparison of the algorithm's performance with other state-of-the-art QAS algorithms, which would be essential for a comprehensive evaluation of the algorithm's effectiveness.\n\nSuggestions for Improvement:\n\n* The authors should provide a detailed analysis of the impact of noise on the algorithm's performance, including the effects of noise on the cost function landscape and the algorithm's ability to learn from noisy data.\n* The paper should be expanded to include a more comprehensive evaluation of the algorithm's performance on a wider range of molecules and system sizes.\n* The authors should provide a detailed comparison of the algorithm's performance with other state-of-the-art QAS algorithms, including a discussion of the algorithm's strengths and weaknesses in relation to these existing methods.\n* The paper should be rewritten to provide a clearer and more concise explanation of the algorithm's key components and how they interact to enhance the efficiency and robustness of the algorithm.\n\nRecommendation:\n\nThis paper introduces a novel curriculum-based reinforcement learning algorithm for quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments. The proposed algorithm incorporates several key components, including a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of SPSA, which enhance the efficiency and robustness of the algorithm. Numerical experiments demonstrate the effectiveness of CRLQAS in outperforming existing QAS algorithms across noiseless and noisy environments for various molecules. However, the paper assumes a noiseless environment for the quantum circuit simulator, and the authors do not provide a detailed analysis of the impact of noise on the algorithm's performance. Therefore, the paper needs to be expanded to include a more comprehensive evaluation of the algorithm's performance on a wider range of molecules and system sizes, as well as a detailed comparison with other state-of-the-art QAS algorithms. Overall, the paper has the potential to make a significant contribution to the field of quantum computing, but it requires further refinement and expansion to be a truly impactful paper.\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThis paper introduces a curriculum-based reinforcement learning quantum architecture search (CRLQAS) algorithm designed to tackle the challenges of deploying variational quantum algorithms (VQAs) in realistic noisy quantum environments. The proposed algorithm incorporates a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of simultaneous perturbation stochastic approximation (SPSA) for faster convergence. Numerical experiments on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across noiseless and noisy environments for various molecules. The authors also develop an optimized simulator for CRLQAS, employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis, which improves computational efficiency by up to six-fold.\n\nStrengths:\n\n* The paper introduces a novel curriculum-based reinforcement learning algorithm for quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments.\n* The proposed algorithm incorporates several key components, including a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of SPSA, which enhance the efficiency and robustness of the algorithm.\n* Numerical experiments demonstrate the effectiveness of CRLQAS in outperforming existing QAS algorithms across noiseless and noisy environments for various molecules.\n* The authors provide a detailed analysis of the computational efficiency of the optimized simulator, which shows a significant improvement in computational efficiency by up to six-fold.\n\nWeaknesses:\n\n* The paper assumes a noiseless environment for the quantum circuit simulator, which may not be representative of real-world quantum devices.\n* The authors do not provide a detailed analysis of the impact of noise on the algorithm's performance, which is a critical aspect of deploying VQAs in realistic noisy quantum environments.\n* The paper focuses on a limited number of molecules, and it is unclear how the algorithm would perform on more complex molecules or larger system sizes.\n* The authors do not provide a detailed comparison of the algorithm's performance with other state-of-the-art QAS algorithms, which would be essential for a comprehensive evaluation of the algorithm's effectiveness.\n\nSuggestions for Improvement:\n\n* The authors should provide a detailed analysis of the impact of noise on the algorithm's performance, including the effects of noise on the cost function landscape and the algorithm's ability to learn from noisy data.\n* The paper should be expanded to include a more comprehensive evaluation of the algorithm's performance on a wider range of molecules and system sizes.\n* The authors should provide a detailed comparison of the algorithm's performance with other state-of-the-art QAS algorithms, including a discussion of the algorithm's strengths and weaknesses in relation to these existing methods.\n* The paper should be rewritten to provide a clearer and more concise explanation of the algorithm's key components and how they interact to enhance the efficiency and robustness of the algorithm.\n\nRecommendation:\n\nThis paper introduces a novel curriculum-based reinforcement learning algorithm for quantum architecture search, which addresses the challenges of deploying VQAs in realistic noisy quantum environments. The proposed algorithm incorporates several key components, including a 3D architecture encoding, illegal actions, a random halting scheme, and a novel variant of SPSA, which enhance the efficiency and robustness of the algorithm. Numerical experiments demonstrate the effectiveness of CRLQAS in outperforming existing QAS algorithms across noiseless and noisy environments for various molecules. However, the paper assumes a noiseless environment for the quantum circuit simulator, and the authors do not provide a detailed analysis of the impact of noise on the algorithm's performance. Therefore, the paper needs to be expanded to include a more comprehensive evaluation of the algorithm's performance on a wider range of molecules and system sizes, as well as a detailed comparison with other state-of-the-art QAS algorithms. Overall, the paper has the potential to make a significant contribution to the field of quantum computing, but it requires further refinement and expansion to be a truly impactful paper.\n\n$$$",
        "gr_score_with": 0.696078431372549,
        "output_with_attack": {
            "num_tokens_scored": 408,
            "num_green_tokens": 284,
            "green_fraction": 0.696078431372549,
            "z_score": 12.185030382445134,
            "p_value": 1.8675705287383847e-34
        },
        "output_with": {
            "num_tokens_scored": 408,
            "num_green_tokens": 284,
            "green_fraction": 0.696078431372549,
            "z_score": 12.185030382445134,
            "p_value": 1.8675705287383847e-34
        },
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 202,
            "green_fraction": 0.617737003058104,
            "z_score": 8.017478174045666,
            "p_value": 5.396912789608711e-16
        }
    },
    {
        "title": "BEND: BENCHMARKING DNA LANGUAGE MODELS ON BIOLOGICALLY MEANINGFUL TASKS",
        "abstract": "The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features. BEND is available at https://github.com/frederikkemarin/BEND.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. The authors aim to provide a standardized set of tasks that measure the ability of LMs to capture the intricacies of genomic data, and to help advance this nascent field. The BEND benchmark consists of seven curated tasks and datasets, probing understanding of different DNA functional elements over a variety of length scales. The authors evaluate the performance of DNA LMs from six different sources on these tasks, and find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the challenges and limitations of DNA language modeling, and highlights the need for a standardized benchmark to evaluate the performance of these models.\n2. The BEND benchmark is well-curated and includes a diverse set of tasks and datasets that probe understanding of different DNA functional elements over a variety of length scales.\n3. The authors provide a thorough evaluation of the performance of DNA LMs from six different sources on these tasks, and identify key strengths and weaknesses of each model.\n4. The paper highlights the potential of DNA LMs to capture long-range dependencies and improve performance on certain tasks, such as gene finding.\n\nWeaknesses:\n\n1. The paper focuses primarily on the performance of DNA LMs on a limited set of tasks, and does not explore the potential applications of these models in other areas, such as protein structure prediction or genome assembly.\n2. The authors do not provide a detailed analysis of the features learned by the DNA LMs during pre-training, which could provide valuable insights into the strengths and limitations of these models.\n3. The paper assumes that the performance of DNA LMs on the BEND benchmark will generalize to other organisms, which may not be the case.\n\nSuggestions for Improvement:\n\n1. Expand the BEND benchmark to include a more diverse set of tasks and datasets that probe understanding of different DNA functional elements over a variety of length scales.\n2. Explore the potential applications of DNA LMs in other areas, such as protein structure prediction or genome assembly.\n3. Provide a more detailed analysis of the features learned by the DNA LMs during pre-training, and explore the potential implications of these findings for the development of more effective DNA LMs.\n4. Consider including a more diverse set of models in the evaluation, such as models that use different architectures or training objectives.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of DNA language modeling, and highlights the potential of DNA LMs to capture long-range dependencies and improve performance on certain tasks. However, to fully realize the potential of these models, it is essential to expand the BEND benchmark to include a more diverse set of tasks and datasets, and to explore the potential applications of these models in other areas. Additionally, a more detailed analysis of the features learned by the DNA LMs during pre-training would provide valuable insights into the strengths and limitations of these models.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper presents BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. The authors evaluate six DNA language models (DNAs LMs) from different sources and compare their performance with expert methods on various tasks. The results show that DNA LMs approach expert method performance on some tasks, but only capture limited information about long-range features.\n\nStrengths:\n\n1. BEND is a comprehensive benchmark for DNA language models, featuring a diverse set of tasks that cover various length scales and DNA properties.\n2. The authors provide a detailed analysis of the performance of six DNA LMs on these tasks, providing insights into the strengths and limitations of these models.\n3. The comparison with expert methods on various tasks is a key strength of the paper, allowing for a comprehensive evaluation of DNA LMs.\n\nWeaknesses:\n\n1. The paper assumes that DNA LMs can generalize across different organisms, which may not be the case. Future work should investigate the generalization power of DNA LMs across diverse organisms.\n2. The paper only benchmarks DNA LMs, while other representation models that follow a different paradigm may also be useful for evaluating DNA representation learning. Future work should investigate these alternative models.\n3. The paper's results may not be generalizable to other types of genomic data or organisms, as it is focused on the human genome.\n\nSuggestions for Improvement:\n\n1. Future work should investigate the generalization power of DNA LMs across diverse organisms, by extending the benchmark to other, diverse organisms.\n2. Alternative representation models that follow a different paradigm should be evaluated, to provide a more comprehensive understanding of DNA representation learning.\n3. The paper's results should be replicated on other types of genomic data or organisms, to provide a more generalizable understanding of DNA LMs.\n\nRecommendation:\n\nThe paper presents a comprehensive benchmark for DNA language models, providing insights into the strengths and limitations of these models. While there are some limitations, the paper is a significant contribution to the field of DNA representation learning, and its results provide a valuable benchmark for future research. Therefore, I recommend that the paper be published in its current form, with the understanding that future work should investigate the generalization power of DNA LMs across diverse organisms and evaluate alternative representation models.\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\n\nPaper Summary:\n\nThe paper presents BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. The authors evaluate six DNA language models (DNAs LMs) from different sources and compare their performance with expert methods on various tasks. The results show that DNA LMs approach expert method performance on some tasks, but only capture limited information about long-range features.\n\nStrengths:\n\n1. BEND is a comprehensive benchmark for DNA language models, featuring a diverse set of tasks that cover various length scales and DNA properties.\n2. The authors provide a detailed analysis of the performance of six DNA LMs on these tasks, providing insights into the strengths and limitations of these models.\n3. The comparison with expert methods on various tasks is a key strength of the paper, allowing for a comprehensive evaluation of DNA LMs.\n\nWeaknesses:\n\n1. The paper assumes that DNA LMs can generalize across different organisms, which may not be the case. Future work should investigate the generalization power of DNA LMs across diverse organisms.\n2. The paper only benchmarks DNA LMs, while other representation models that follow a different paradigm may also be useful for evaluating DNA representation learning. Future work should investigate these alternative models.\n3. The paper's results may not be generalizable to other types of genomic data or organisms, as it is focused on the human genome.\n\nSuggestions for Improvement:\n\n1. Future work should investigate the generalization power of DNA LMs across diverse organisms, by extending the benchmark to other, diverse organisms.\n2. Alternative representation models that follow a different paradigm should be evaluated, to provide a more comprehensive understanding of DNA representation learning.\n3. The paper's results should be replicated on other types of genomic data or organisms, to provide a more generalizable understanding of DNA LMs.\n\nRecommendation:\n\nThe paper presents a comprehensive benchmark for DNA language models, providing insights into the strengths and limitations of these models. While there are some limitations, the paper is a significant contribution to the field of DNA representation learning, and its results provide a valuable benchmark for future research. Therefore, I recommend that the paper be published in its current form, with the understanding that future work should investigate the generalization power of DNA LMs across diverse organisms and evaluate alternative representation models.\n\n$$$",
        "gr_score_with": 0.5612903225806452,
        "output_with_attack": {
            "num_tokens_scored": 310,
            "num_green_tokens": 174,
            "green_fraction": 0.5612903225806452,
            "z_score": 5.774137852474569,
            "p_value": 3.867404704845273e-09
        },
        "output_with": {
            "num_tokens_scored": 310,
            "num_green_tokens": 174,
            "green_fraction": 0.5612903225806452,
            "z_score": 5.774137852474569,
            "p_value": 3.867404704845273e-09
        },
        "output_without": {
            "num_tokens_scored": 349,
            "num_green_tokens": 162,
            "green_fraction": 0.46418338108882523,
            "z_score": 2.424468063459392,
            "p_value": 0.007665415848957859
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem induced by a soft DSF. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nStrengths:\n\n* The paper proposes a novel sorting network that learns to sort high-dimensional inputs.\n* The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n* The authors provide a detailed analysis of the error accumulation problem and propose an error-free DSF to resolve it.\n* The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nWeaknesses:\n\n* The paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications.\n* The proposed method may not be effective for sorting low-dimensional inputs or inputs with clear ordering information.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n* The paper assumes that the supervision is provided in the form of permutation matrices, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The paper should discuss the limitations of the proposed method and provide examples of real-world applications where it may not be effective.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method on a wider range of benchmarks.\n* The paper should discuss the potential negative societal impact of the proposed method, such as the potential for bias in the sorting process.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.\n\nOverall, the paper is well-written and provides a clear and concise introduction to the problem of sorting high-dimensional inputs. The proposed method is novel and effective, and the paper provides a detailed analysis of the error accumulation problem and proposes an error-free DSF to resolve it. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to differentiable sorting networks, which is a crucial component in neural sorting networks. The authors define a softening error, which is a difference between the original and smoothed values, and propose an error-free differentiable swap function (DSF) that resolves the error accumulation problem. They also introduce a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs. The proposed method is tested on sorting multi-digit images and image fragments, and the results show that it outperforms or is comparable to existing methods.\n\nStrengths:\n\n1. The paper presents a novel approach to differentiable sorting networks, which is a crucial component in neural sorting networks.\n2. The authors define a softening error, which is a clear and well-defined measure of the error accumulation problem.\n3. The proposed error-free DSF is a significant improvement over existing methods, as it achieves a zero error and holds a non-decreasing condition and differentiability.\n4. The permutation-equivariant Transformer network with multi-head attention is a powerful model that captures dependency between high-dimensional inputs.\n5. The paper presents a comprehensive evaluation of the proposed method on sorting multi-digit images and image fragments.\n\nWeaknesses:\n\n1. The paper assumes that the input data is well-structured and has clear ordering information, which may not be the case in real-world applications.\n2. The proposed method may not be suitable for sorting ambiguous data that contains implicitly ordinal information.\n3. The paper does not provide a clear comparison with existing methods that are specifically designed for sorting high-dimensional data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a clearer comparison with existing methods that are specifically designed for sorting high-dimensional data.\n2. The paper should discuss the potential negative societal impact of neural sorting networks, particularly in the context of sorting controversial high-dimensional data.\n3. The authors should provide a more detailed evaluation of the proposed method on sorting ambiguous data that contains implicitly ordinal information.\n\nRecommendation:\n\nThe paper presents a novel and significant contribution to the field of differentiable sorting networks. The proposed method is well-evaluated, and the results are promising. However, the paper should be revised to address the weaknesses mentioned above. With these revisions, the paper will be a strong contribution to the field.\n\nOverall, the paper is well-written, and the authors have made a significant contribution to the field of differentiable sorting networks. The proposed method is a significant improvement over existing methods, and the paper presents a comprehensive evaluation of the method. With revisions to address the weaknesses, the paper will be a strong contribution to the field.",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to differentiable sorting networks, which is a crucial component in neural sorting networks. The authors define a softening error, which is a difference between the original and smoothed values, and propose an error-free differentiable swap function (DSF) that resolves the error accumulation problem. They also introduce a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs. The proposed method is tested on sorting multi-digit images and image fragments, and the results show that it outperforms or is comparable to existing methods.\n\nStrengths:\n\n1. The paper presents a novel approach to differentiable sorting networks, which is a crucial component in neural sorting networks.\n2. The authors define a softening error, which is a clear and well-defined measure of the error accumulation problem.\n3. The proposed error-free DSF is a significant improvement over existing methods, as it achieves a zero error and holds a non-decreasing condition and differentiability.\n4. The permutation-equivariant Transformer network with multi-head attention is a powerful model that captures dependency between high-dimensional inputs.\n5. The paper presents a comprehensive evaluation of the proposed method on sorting multi-digit images and image fragments.\n\nWeaknesses:\n\n1. The paper assumes that the input data is well-structured and has clear ordering information, which may not be the case in real-world applications.\n2. The proposed method may not be suitable for sorting ambiguous data that contains implicitly ordinal information.\n3. The paper does not provide a clear comparison with existing methods that are specifically designed for sorting high-dimensional data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a clearer comparison with existing methods that are specifically designed for sorting high-dimensional data.\n2. The paper should discuss the potential negative societal impact of neural sorting networks, particularly in the context of sorting controversial high-dimensional data.\n3. The authors should provide a more detailed evaluation of the proposed method on sorting ambiguous data that contains implicitly ordinal information.\n\nRecommendation:\n\nThe paper presents a novel and significant contribution to the field of differentiable sorting networks. The proposed method is well-evaluated, and the results are promising. However, the paper should be revised to address the weaknesses mentioned above. With these revisions, the paper will be a strong contribution to the field.\n\nOverall, the paper is well-written, and the authors have made a significant contribution to the field of differentiable sorting networks. The proposed method is a significant improvement over existing methods, and the paper presents a comprehensive evaluation of the method. With revisions to address the weaknesses, the paper will be a strong contribution to the field.",
        "gr_score_with": 0.6496815286624203,
        "output_with_attack": {
            "num_tokens_scored": 314,
            "num_green_tokens": 204,
            "green_fraction": 0.6496815286624203,
            "z_score": 9.015454057325579,
            "p_value": 9.802829494638109e-20
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 204,
            "green_fraction": 0.6496815286624203,
            "z_score": 9.015454057325579,
            "p_value": 9.802829494638109e-20
        },
        "output_without": {
            "num_tokens_scored": 269,
            "num_green_tokens": 144,
            "green_fraction": 0.5353159851301115,
            "z_score": 4.516274366847243,
            "p_value": 3.1468554927737775e-06
        }
    },
    {
        "title": "MAXIMUM ENTROPY MODEL CORRECTION",
        "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model\u2019s next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna\u2019s convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes the MaxEnt Model Correction (MoCo) framework to alleviate model errors in model-based reinforcement learning (MBRL). The framework uses maximum entropy density estimation to correct the next-state distributions needed for planning from the approximate model P\u0302 towards the true one in P. The authors introduce the MaxEnt MoCo algorithm, which modifies the planning algorithm to use the corrected distribution P\u0304 instead of P\u0302. The paper provides theoretical analysis and convergence results for both finite and continuous MDPs, and also introduces a sample-based variant, MoCoDyna, which extends the Dyna framework.\n\nStrengths:\n\n* The paper provides a thorough theoretical analysis of the MaxEnt MoCo framework, including convergence results and error bounds.\n* The authors introduce a new algorithm, MoCoVI, which iteratively finds increasingly better basis functions to improve the accuracy of the value function.\n* The paper also introduces a sample-based variant, MoCoDyna, which extends the Dyna framework and can be used in scenarios where only samples from the true dynamics P are available.\n* The authors provide empirical results to demonstrate the effectiveness of the MoCo framework in reducing model errors and improving convergence rates.\n\nWeaknesses:\n\n* The paper assumes access to the true dynamics P, which may not be feasible in all scenarios.\n* The authors use a lazy computation approach to calculate P\u0304(\u00b7|x, a) only when needed, which may lead to increased computational complexity.\n* The paper does not provide a thorough comparison with other model-based reinforcement learning algorithms.\n* The authors assume that the basis functions can be chosen arbitrarily, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n* Provide a more thorough comparison with other model-based reinforcement learning algorithms.\n* Investigate the use of deep learning models to learn the basis functions.\n* Provide more empirical results to demonstrate the effectiveness of the MoCo framework in different scenarios.\n* Investigate the use of other divergences, such as the Kullback-Leibler divergence, to correct the next-state distributions.\n\nRecommendation:\n\nThe paper provides a thorough theoretical analysis of the MaxEnt MoCo framework and introduces new algorithms, MoCoVI and MoCoDyna, which can be used to alleviate model errors in MBRL. The empirical results demonstrate the effectiveness of the MoCo framework in reducing model errors and improving convergence rates. However, the paper assumes access to the true dynamics P, which may not be feasible in all scenarios. Therefore, the paper can be recommended for publication in a conference or journal that focuses on reinforcement learning and artificial intelligence, but with the caveat that the authors should address the limitations and assumptions of the paper.\n\nOverall, the paper provides a valuable contribution to the field of reinforcement learning and artificial intelligence, and can be used as a starting point for further research in this area.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\nRecommendation for Citation: Yes",
        "peer_review_with_watermark": "$$$ Paper Summary:\nThe paper presents the Maximum Entropy Model Correction (MaxEnt MoCo) algorithm, which aims to reduce the adverse impact of model error in model-based reinforcement learning (MBRL). The algorithm uses Maximum Entropy density estimation to correct the next-state distributions needed for planning from the approximate model P\u0302 towards the true one in P. The authors provide a thorough theoretical analysis, including convergence results, and demonstrate the effectiveness of MaxEnt MoCo in both exact and approximate query settings. They also introduce Model Correcting Value Iteration (MoCoVI), a variant of Value Iteration, and Model Correcting Dyna (MoCoDyna), a sample-based variant of Dyna.\n\nStrengths:\n1. The paper provides a thorough and well-structured analysis of the MaxEnt MoCo algorithm, including its theoretical foundations and convergence results.\n2. The authors demonstrate the effectiveness of MaxEnt MoCo in both exact and approximate query settings, showcasing its robustness and efficiency.\n3. The paper introduces two novel variants of MBRL algorithms: MoCoVI and MoCoDyna, which leverage the MaxEnt MoCo framework to achieve faster convergence and better performance.\n4. The authors provide a clear and concise presentation of the algorithms, making it easy to understand and implement them.\n\nWeaknesses:\n1. The paper assumes access to the true dynamics P, which may not be feasible in real-world applications. The authors should discuss potential alternatives or relaxations of this assumption.\n2. The convergence results are dependent on the choice of basis functions, which may not be well-suited for all problems. The authors should provide more guidance on how to select these functions effectively.\n3. The paper focuses primarily on theoretical aspects and provides limited empirical evaluation. The authors should provide more comprehensive experiments and comparisons with state-of-the-art algorithms.\n\nSuggestions for Improvement:\n1. Provide more discussion on how to select the basis functions effectively and how to handle cases where these functions are not well-suited for the problem at hand.\n2. Consider relaxing the assumption of access to the true dynamics P and discuss potential alternatives or workarounds.\n3. Provide more comprehensive empirical evaluation, including comparisons with state-of-the-art algorithms and a broader range of experiments.\n\nRecommendation:\nThis paper presents a well-structured and thorough analysis of the MaxEnt MoCo algorithm, which demonstrates its effectiveness in reducing model error in MBRL. While there are some limitations and areas for improvement, the paper provides valuable insights and contributions to the field. I recommend acceptance of this paper, with the suggested improvements and additional empirical evaluation.",
        "peer_review_with_watermark_attack": "$$$ Paper Summary:\nThe paper presents the Maximum Entropy Model Correction (MaxEnt MoCo) algorithm, which aims to reduce the adverse impact of model error in model-based reinforcement learning (MBRL). The algorithm uses Maximum Entropy density estimation to correct the next-state distributions needed for planning from the approximate model P\u0302 towards the true one in P. The authors provide a thorough theoretical analysis, including convergence results, and demonstrate the effectiveness of MaxEnt MoCo in both exact and approximate query settings. They also introduce Model Correcting Value Iteration (MoCoVI), a variant of Value Iteration, and Model Correcting Dyna (MoCoDyna), a sample-based variant of Dyna.\n\nStrengths:\n1. The paper provides a thorough and well-structured analysis of the MaxEnt MoCo algorithm, including its theoretical foundations and convergence results.\n2. The authors demonstrate the effectiveness of MaxEnt MoCo in both exact and approximate query settings, showcasing its robustness and efficiency.\n3. The paper introduces two novel variants of MBRL algorithms: MoCoVI and MoCoDyna, which leverage the MaxEnt MoCo framework to achieve faster convergence and better performance.\n4. The authors provide a clear and concise presentation of the algorithms, making it easy to understand and implement them.\n\nWeaknesses:\n1. The paper assumes access to the true dynamics P, which may not be feasible in real-world applications. The authors should discuss potential alternatives or relaxations of this assumption.\n2. The convergence results are dependent on the choice of basis functions, which may not be well-suited for all problems. The authors should provide more guidance on how to select these functions effectively.\n3. The paper focuses primarily on theoretical aspects and provides limited empirical evaluation. The authors should provide more comprehensive experiments and comparisons with state-of-the-art algorithms.\n\nSuggestions for Improvement:\n1. Provide more discussion on how to select the basis functions effectively and how to handle cases where these functions are not well-suited for the problem at hand.\n2. Consider relaxing the assumption of access to the true dynamics P and discuss potential alternatives or workarounds.\n3. Provide more comprehensive empirical evaluation, including comparisons with state-of-the-art algorithms and a broader range of experiments.\n\nRecommendation:\nThis paper presents a well-structured and thorough analysis of the MaxEnt MoCo algorithm, which demonstrates its effectiveness in reducing model error in MBRL. While there are some limitations and areas for improvement, the paper provides valuable insights and contributions to the field. I recommend acceptance of this paper, with the suggested improvements and additional empirical evaluation.",
        "gr_score_with": 0.6473684210526316,
        "output_with_attack": {
            "num_tokens_scored": 380,
            "num_green_tokens": 246,
            "green_fraction": 0.6473684210526316,
            "z_score": 9.82336267559796,
            "p_value": 4.465676878263829e-23
        },
        "output_with": {
            "num_tokens_scored": 380,
            "num_green_tokens": 246,
            "green_fraction": 0.6473684210526316,
            "z_score": 9.82336267559796,
            "p_value": 4.465676878263829e-23
        },
        "output_without": {
            "num_tokens_scored": 389,
            "num_green_tokens": 187,
            "green_fraction": 0.480719794344473,
            "z_score": 3.2310607205520254,
            "p_value": 0.0006166586832267937
        }
    },
    {
        "title": "CADE AND CONTINUOUS SCENE RECONSTRUCTION FROM FOVEATED VISUAL SAMPLING",
        "abstract": "High accuracy, low latency and high energy efficiency represent a set of conflicting goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduced camera frame rates, and increased data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, and saccade mechanism of the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from a sequence of foveal-peripheral views, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our model demonstrates a 5% higher image recognition accuracy compared to state-of-theart foveal-peripheral based vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches. Code is available at Github.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel framework for energy-efficient and low-cost sensing and processing in artificial vision systems. Inspired by the foveal-peripheral sampling mechanism of the human visual system, the proposed framework employs a foveal-peripheral vision-inspired image sampling incorporating saccadic control to reduce the amount of data required from the camera. The incoming stream of foveal and peripheral inputs is processed for scene prediction and reconstruction, where the missing pixels are filled-in to form a smooth and semantically consistent image. The scene reconstruction model is trained using self-supervised learning, and the saccade controller is trained using actor-critic reinforcement learning. The proposed framework demonstrates a significant improvement in image recognition accuracy and requires 70% less pixel usage compared to state-of-the-art foveal-peripheral based vision systems.\n\nStrengths:\n\n1. The proposed framework is inspired by the foveal-peripheral sampling mechanism of the human visual system, which is a unique and innovative approach.\n2. The framework employs a self-supervised learning method for scene reconstruction, which is efficient and effective.\n3. The saccade controller is trained using actor-critic reinforcement learning, which allows for efficient and effective decision-making.\n4. The proposed framework demonstrates a significant improvement in image recognition accuracy and requires 70% less pixel usage compared to state-of-the-art foveal-peripheral based vision systems.\n5. The framework is highly transferable and data-efficient, making it a valuable tool for a wide range of applications.\n\nWeaknesses:\n\n1. The paper assumes a high-resolution foveal view and a low-resolution peripheral view, which may not be realistic for all applications.\n2. The framework requires a significant amount of computational resources for training and inference.\n3. The paper does not provide a detailed analysis of the computational complexity of the framework.\n4. The framework may not be suitable for applications where high-speed processing is required.\n5. The paper does not provide a detailed evaluation of the framework's performance on a wide range of datasets.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more realistic foveal-peripheral sampling mechanisms, such as those that take into account the non-uniform distribution of cones and rods in the retina.\n2. Explore the use of more efficient self-supervised learning methods, such as those that use contrastive learning or other forms of self-supervision.\n3. Investigate the use of more efficient reinforcement learning methods, such as those that use policy gradient methods or other forms of reinforcement learning.\n4. Provide a more detailed analysis of the computational complexity of the framework, including a comparison to other state-of-the-art methods.\n5. Evaluate the framework's performance on a wider range of datasets, including those with varying levels of complexity and resolution.\n\nRecommendation:\n\nThe proposed framework is a valuable contribution to the field of computer vision, and it demonstrates a significant improvement in image recognition accuracy and efficiency. However, there are several areas for improvement, including the use of more realistic foveal-peripheral sampling mechanisms and the exploration of more efficient self-supervised and reinforcement learning methods. With further development and refinement, the framework has the potential to become a leading tool for a wide range of applications in computer vision.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for energy-efficient image sensing and processing, inspired by the foveal-peripheral sampling mechanism of the human visual system. The framework integrates foveal-peripheral sampling, saccade control, and continuous scene reconstruction to achieve high accuracy, low latency, and high energy efficiency. The proposed system reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. The authors demonstrate the effectiveness of their model using the GTSRB dataset and the ImageNet dataset, achieving 5% higher image recognition accuracy compared to state-of-the-art foveal-peripheral based vision systems.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to image sensing and processing, inspired by the human visual system.\n2. The framework integrates multiple components, including foveal-peripheral sampling, saccade control, and continuous scene reconstruction, to achieve high accuracy, low latency, and high energy efficiency.\n3. The authors demonstrate the effectiveness of their model using two large-scale datasets, GTSRB and ImageNet, and achieve state-of-the-art performance.\n4. The framework is energy-efficient, reducing the required input pixels by over 90% per frame.\n5. The authors provide a comprehensive evaluation of their model, including analysis of the impact of foveal-peripheral sampling, scene reconstruction, and saccade control.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of reinforcement learning, which can be complex and difficult to implement.\n2. The evaluation of the model is limited to two datasets, GTSRB and ImageNet, and it is not clear how well the model would perform on other datasets.\n3. The paper does not provide a detailed analysis of the computational cost of the model, which could be a limitation in certain applications.\n4. The authors assume that the saccade model is highly transferable, but it is not clear how well this assumption holds in practice.\n5. The paper does not provide a comprehensive analysis of the potential biases in the model, such as bias towards certain classes or objects.\n\nSuggestions for Improvement:\n\n1. The authors could provide more details on the implementation of the reinforcement learning algorithm, including the choice of hyperparameters and the training procedure.\n2. The evaluation of the model could be expanded to include more datasets, such as CIFAR-10 or SVHN.\n3. The authors could provide a more detailed analysis of the computational cost of the model, including the number of floating-point operations required.\n4. The authors could provide more evidence for the transferability of the saccade model, such as experiments on different datasets or domains.\n5. The authors could provide a more comprehensive analysis of the potential biases in the model, including sensitivity analysis and fairness evaluation.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to image sensing and processing, inspired by the human visual system. The framework integrates multiple components, including foveal-peripheral sampling, saccade control, and continuous scene reconstruction, to achieve high accuracy, low latency, and high energy efficiency. While there are some limitations to the paper, including the reliance on reinforcement learning and the limited evaluation, the results are promising and warrant further investigation. I recommend that the authors continue to develop and refine their model, addressing the weaknesses and suggestions for improvement listed above. With further development, this model has the potential to make significant contributions to the field of computer vision.",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for energy-efficient image sensing and processing, inspired by the foveal-peripheral sampling mechanism of the human visual system. The framework integrates foveal-peripheral sampling, saccade control, and continuous scene reconstruction to achieve high accuracy, low latency, and high energy efficiency. The proposed system reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. The authors demonstrate the effectiveness of their model using the GTSRB dataset and the ImageNet dataset, achieving 5% higher image recognition accuracy compared to state-of-the-art foveal-peripheral based vision systems.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to image sensing and processing, inspired by the human visual system.\n2. The framework integrates multiple components, including foveal-peripheral sampling, saccade control, and continuous scene reconstruction, to achieve high accuracy, low latency, and high energy efficiency.\n3. The authors demonstrate the effectiveness of their model using two large-scale datasets, GTSRB and ImageNet, and achieve state-of-the-art performance.\n4. The framework is energy-efficient, reducing the required input pixels by over 90% per frame.\n5. The authors provide a comprehensive evaluation of their model, including analysis of the impact of foveal-peripheral sampling, scene reconstruction, and saccade control.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of reinforcement learning, which can be complex and difficult to implement.\n2. The evaluation of the model is limited to two datasets, GTSRB and ImageNet, and it is not clear how well the model would perform on other datasets.\n3. The paper does not provide a detailed analysis of the computational cost of the model, which could be a limitation in certain applications.\n4. The authors assume that the saccade model is highly transferable, but it is not clear how well this assumption holds in practice.\n5. The paper does not provide a comprehensive analysis of the potential biases in the model, such as bias towards certain classes or objects.\n\nSuggestions for Improvement:\n\n1. The authors could provide more details on the implementation of the reinforcement learning algorithm, including the choice of hyperparameters and the training procedure.\n2. The evaluation of the model could be expanded to include more datasets, such as CIFAR-10 or SVHN.\n3. The authors could provide a more detailed analysis of the computational cost of the model, including the number of floating-point operations required.\n4. The authors could provide more evidence for the transferability of the saccade model, such as experiments on different datasets or domains.\n5. The authors could provide a more comprehensive analysis of the potential biases in the model, including sensitivity analysis and fairness evaluation.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to image sensing and processing, inspired by the human visual system. The framework integrates multiple components, including foveal-peripheral sampling, saccade control, and continuous scene reconstruction, to achieve high accuracy, low latency, and high energy efficiency. While there are some limitations to the paper, including the reliance on reinforcement learning and the limited evaluation, the results are promising and warrant further investigation. I recommend that the authors continue to develop and refine their model, addressing the weaknesses and suggestions for improvement listed above. With further development, this model has the potential to make significant contributions to the field of computer vision.",
        "gr_score_with": 0.5941320293398533,
        "output_with_attack": {
            "num_tokens_scored": 409,
            "num_green_tokens": 243,
            "green_fraction": 0.5941320293398533,
            "z_score": 7.997516501665636,
            "p_value": 6.347688256157237e-16
        },
        "output_with": {
            "num_tokens_scored": 409,
            "num_green_tokens": 243,
            "green_fraction": 0.5941320293398533,
            "z_score": 7.997516501665636,
            "p_value": 6.347688256157237e-16
        },
        "output_without": {
            "num_tokens_scored": 411,
            "num_green_tokens": 202,
            "green_fraction": 0.49148418491484186,
            "z_score": 3.769891590974183,
            "p_value": 8.165923643855465e-05
        }
    },
    {
        "title": "DISSECTING SAMPLE HARDNESS: A FINE-GRAINED ANALYSIS OF HARDNESS CHARACTERIZATION METHODS FOR DATA-CENTRIC AI",
        "abstract": "Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \u201chard\u201d samples. However, there is a lack of consensus regarding the definition and evaluation of \u201chardness\u201d. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a fine-grained taxonomy of hardness types and a comprehensive benchmarking framework called H-CAT for evaluating Hardness Characterization Methods (HCMs). The authors identify three broad types of hardness: Mislabeling, OoD/Outlier, and Atypical, and propose a systematic definition of different dimensions of hardness. The H-CAT framework supports comprehensive and quantitative benchmarking of HCMs across various hardness types and can easily be extended to new HCMs, hardness types, and datasets. The authors evaluate 13 different HCMs across 8 hardness types using H-CAT and provide insights into the capabilities of different HCMs when dealing with different hardness types.\n\nStrengths:\n\n1. The paper provides a comprehensive and systematic taxonomy of hardness types, which is a significant contribution to the field.\n2. The proposed H-CAT framework is a robust and flexible benchmarking tool that can be used to evaluate HCMs across various hardness types.\n3. The authors provide a thorough evaluation of 13 different HCMs across 8 hardness types, which is a significant contribution to the field.\n4. The paper provides practical tips and takeaways for practitioners, such as selecting an HCM based on the hardness type and using confidence as a general-purpose tool.\n\nWeaknesses:\n\n1. The paper focuses primarily on image datasets and does not provide a comprehensive evaluation of HCMs on tabular datasets.\n2. The authors do not provide a detailed discussion on the limitations of the H-CAT framework and how it can be improved.\n3. The paper does not provide a clear explanation of the methodology used to evaluate the HCMs and how the results were obtained.\n4. The authors do not provide a comparison of the performance of HCMs with other methods, such as traditional machine learning methods.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive evaluation of HCMs on tabular datasets to make the results more generalizable.\n2. The authors should provide a detailed discussion on the limitations of the H-CAT framework and how it can be improved.\n3. The authors should provide a clear explanation of the methodology used to evaluate the HCMs and how the results were obtained.\n4. The authors should compare the performance of HCMs with other methods, such as traditional machine learning methods.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of data-centric AI and provides a comprehensive and systematic taxonomy of hardness types and a robust and flexible benchmarking tool for evaluating HCMs. However, the paper has some limitations, such as the focus on image datasets and the lack of a detailed discussion on the limitations of the H-CAT framework. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of HCMs on tabular datasets to make the results more generalizable.\n\nOverall, the paper is well-written, and the authors provide a clear and concise explanation of the taxonomy and the H-CAT framework. The results are also well-presented, and the authors provide practical tips and takeaways for practitioners. However, the paper could benefit from a more comprehensive evaluation of HCMs on tabular datasets and a detailed discussion on the limitations of the H-CAT framework.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes the Hardness Characterization Analysis Toolkit (H-CAT), a comprehensive benchmarking framework for hardness characterization in data-centric AI. The authors aim to bridge the gap in the current literature, where Hardness Characterization Methods (HCMs) have been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance. The paper proposes a fine-grained taxonomy of hardness types and evaluates 13 different HCMs across various hardness types using H-CAT. The results of the comprehensive evaluation uncover the strengths and weaknesses of different HCMs, providing practical tips to guide HCM selection and future development.\n\nStrengths:\n\n1. The paper proposes a comprehensive framework for hardness characterization, filling the gap in the current literature.\n2. The authors provide a fine-grained taxonomy of hardness types, which enables the design and selection of HCMs tailored to specific hardness challenges.\n3. The paper evaluates 13 different HCMs across various hardness types using H-CAT, providing a comprehensive evaluation of the methods.\n\nWeaknesses:\n\n1. The paper relies on synthetic data, which may not accurately represent real-world data.\n2. The evaluation of HCMs is limited to specific types of hardness and may not generalize to other types of hardness.\n3. The paper does not provide a clear explanation of the underlying mechanisms of HCMs and how they relate to the hardness taxonomy.\n\nSuggestions for Improvement:\n\n1. The authors should consider using real-world data to evaluate HCMs and provide a more comprehensive evaluation of the methods.\n2. The paper should provide a clear explanation of the underlying mechanisms of HCMs and how they relate to the hardness taxonomy.\n3. The authors should consider evaluating HCMs on additional types of hardness to provide a more comprehensive evaluation of the methods.\n\nRecommendation:\n\nThis paper provides a comprehensive framework for hardness characterization and evaluates 13 different HCMs across various hardness types using H-CAT. While the paper has some limitations, the results of the evaluation provide valuable insights into the strengths and weaknesses of different HCMs. The paper is a significant contribution to the field of data-centric AI and provides a useful tool for practitioners to select and evaluate HCMs. However, the authors should consider addressing the limitations of the paper to provide a more comprehensive evaluation of HCMs.\n\nRating: 8/10\n\nRecommendation: Accept with minor revisions.\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThis paper proposes the Hardness Characterization Analysis Toolkit (H-CAT), a comprehensive benchmarking framework for hardness characterization in data-centric AI. The authors aim to bridge the gap in the current literature, where Hardness Characterization Methods (HCMs) have been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance. The paper proposes a fine-grained taxonomy of hardness types and evaluates 13 different HCMs across various hardness types using H-CAT. The results of the comprehensive evaluation uncover the strengths and weaknesses of different HCMs, providing practical tips to guide HCM selection and future development.\n\nStrengths:\n\n1. The paper proposes a comprehensive framework for hardness characterization, filling the gap in the current literature.\n2. The authors provide a fine-grained taxonomy of hardness types, which enables the design and selection of HCMs tailored to specific hardness challenges.\n3. The paper evaluates 13 different HCMs across various hardness types using H-CAT, providing a comprehensive evaluation of the methods.\n\nWeaknesses:\n\n1. The paper relies on synthetic data, which may not accurately represent real-world data.\n2. The evaluation of HCMs is limited to specific types of hardness and may not generalize to other types of hardness.\n3. The paper does not provide a clear explanation of the underlying mechanisms of HCMs and how they relate to the hardness taxonomy.\n\nSuggestions for Improvement:\n\n1. The authors should consider using real-world data to evaluate HCMs and provide a more comprehensive evaluation of the methods.\n2. The paper should provide a clear explanation of the underlying mechanisms of HCMs and how they relate to the hardness taxonomy.\n3. The authors should consider evaluating HCMs on additional types of hardness to provide a more comprehensive evaluation of the methods.\n\nRecommendation:\n\nThis paper provides a comprehensive framework for hardness characterization and evaluates 13 different HCMs across various hardness types using H-CAT. While the paper has some limitations, the results of the evaluation provide valuable insights into the strengths and weaknesses of different HCMs. The paper is a significant contribution to the field of data-centric AI and provides a useful tool for practitioners to select and evaluate HCMs. However, the authors should consider addressing the limitations of the paper to provide a more comprehensive evaluation of HCMs.\n\nRating: 8/10\n\nRecommendation: Accept with minor revisions.\n\n$$$",
        "gr_score_with": 0.7439446366782007,
        "output_with_attack": {
            "num_tokens_scored": 289,
            "num_green_tokens": 215,
            "green_fraction": 0.7439446366782007,
            "z_score": 11.909107493711353,
            "p_value": 5.3053447757996004e-33
        },
        "output_with": {
            "num_tokens_scored": 289,
            "num_green_tokens": 215,
            "green_fraction": 0.7439446366782007,
            "z_score": 11.909107493711353,
            "p_value": 5.3053447757996004e-33
        },
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 221,
            "green_fraction": 0.6636636636636637,
            "z_score": 9.79399750132954,
            "p_value": 5.97383422065309e-23
        }
    },
    {
        "title": "ADAPTING LARGE LANGUAGE MODELS VIA",
        "abstract": "We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension\u2014practice after reading improves the ability to answer questions based on the learned knowledge\u2014we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model\u2019s performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of adapting large language models (LLMs) through continued pre-training on domain-specific corpora. The authors propose a novel method for transforming raw corpora into reading comprehension texts, which enables the model to acquire domain knowledge while enhancing its prompting ability. The approach is demonstrated to be effective across three different domains: biomedicine, finance, and law. The authors also show that the reading comprehension texts can improve the model's performance on general LLM benchmarks, suggesting potential for developing a general language model across more domains.\n\nStrengths:\n\n1. The paper presents a novel and effective approach for adapting LLMs through continued pre-training on domain-specific corpora.\n2. The method is simple to implement and can be applied to any pre-training corpora, making it highly scalable.\n3. The experiments demonstrate the effectiveness of the approach across three different domains, including biomedicine, finance, and law.\n4. The reading comprehension texts are shown to improve the model's performance on general LLM benchmarks, suggesting potential for developing a general language model across more domains.\n\nWeaknesses:\n\n1. The paper assumes that the raw corpora are available and of high quality, which may not always be the case.\n2. The approach relies on the quality of the comprehension tasks, which may not be well-suited for all domains or tasks.\n3. The paper does not provide a thorough analysis of the computational and data requirements of the approach, which may be a limitation for large-scale applications.\n4. The paper assumes that the general language model is a strong baseline, which may not always be the case.\n\nSuggestions for Improvement:\n\n1. Provide more details on the quality and availability of the raw corpora, as well as the comprehension tasks.\n2. Conduct a more thorough analysis of the computational and data requirements of the approach.\n3. Explore the use of more advanced comprehension tasks or techniques to improve the model's performance.\n4. Compare the approach with other adaptation methods, such as fine-tuning or retrieval-augmented prompting.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for adapting LLMs through continued pre-training on domain-specific corpora. While there are some limitations and areas for improvement, the approach has the potential to improve the performance of LLMs on specialized tasks and domains. I recommend the paper for publication in a top-tier conference or journal in the field of natural language processing.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the use of domain-adaptive pre-training for large language models (LLMs) and proposes a method to transform raw corpora into reading comprehension texts. The authors find that continued pre-training on domain-specific raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability. To address this, they design a simple method to mine intrinsic tasks from raw corpora and create reading comprehension texts. The model is then fine-tuned on these texts, along with general instructions, to enhance prompting ability. Experiments in three domains (biomedicine, finance, and law) show that the proposed method consistently improves model performance on domain-specific tasks.\n\nStrengths:\n\n* The paper provides a clear understanding of the limitations of domain-adaptive pre-training for LLMs and proposes a novel solution to address the issue.\n* The design of reading comprehension texts is well-motivated and scalable, making it applicable to various domains.\n* The experiments demonstrate the effectiveness of the proposed method in improving model performance on domain-specific tasks.\n\nWeaknesses:\n\n* The paper assumes that the raw corpora are readily available, which may not be the case in many scenarios. The authors do not provide a clear plan for handling noisy or incomplete corpora.\n* The evaluation metrics used are limited, and the authors do not provide a comprehensive analysis of the model's performance on different tasks.\n* The ablation studies are limited, and the authors do not provide a clear explanation for the effectiveness of different comprehension task types.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed plan for handling noisy or incomplete corpora, including techniques for data preprocessing and cleaning.\n* The evaluation metrics should be expanded to include more comprehensive metrics, such as ROUGE score, BLEU score, and human evaluation.\n* The ablation studies should be more extensive, including a detailed analysis of the effectiveness of different comprehension task types.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of LLMs, proposing a novel method for adapting large language models to specific domains. While there are some limitations, the proposed method shows promising results, and the design is well-motivated and scalable. I recommend the paper for publication, with the suggestion that the authors address the limitations mentioned above.\n\n$$$",
        "peer_review_with_watermark_attack": "$$$\nPaper Summary:\n\nThe paper explores the use of domain-adaptive pre-training for large language models (LLMs) and proposes a method to transform raw corpora into reading comprehension texts. The authors find that continued pre-training on domain-specific raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability. To address this, they design a simple method to mine intrinsic tasks from raw corpora and create reading comprehension texts. The model is then fine-tuned on these texts, along with general instructions, to enhance prompting ability. Experiments in three domains (biomedicine, finance, and law) show that the proposed method consistently improves model performance on domain-specific tasks.\n\nStrengths:\n\n* The paper provides a clear understanding of the limitations of domain-adaptive pre-training for LLMs and proposes a novel solution to address the issue.\n* The design of reading comprehension texts is well-motivated and scalable, making it applicable to various domains.\n* The experiments demonstrate the effectiveness of the proposed method in improving model performance on domain-specific tasks.\n\nWeaknesses:\n\n* The paper assumes that the raw corpora are readily available, which may not be the case in many scenarios. The authors do not provide a clear plan for handling noisy or incomplete corpora.\n* The evaluation metrics used are limited, and the authors do not provide a comprehensive analysis of the model's performance on different tasks.\n* The ablation studies are limited, and the authors do not provide a clear explanation for the effectiveness of different comprehension task types.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed plan for handling noisy or incomplete corpora, including techniques for data preprocessing and cleaning.\n* The evaluation metrics should be expanded to include more comprehensive metrics, such as ROUGE score, BLEU score, and human evaluation.\n* The ablation studies should be more extensive, including a detailed analysis of the effectiveness of different comprehension task types.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of LLMs, proposing a novel method for adapting large language models to specific domains. While there are some limitations, the proposed method shows promising results, and the design is well-motivated and scalable. I recommend the paper for publication, with the suggestion that the authors address the limitations mentioned above.\n\n$$$",
        "gr_score_with": 0.6437125748502994,
        "output_with_attack": {
            "num_tokens_scored": 334,
            "num_green_tokens": 215,
            "green_fraction": 0.6437125748502994,
            "z_score": 9.074862635037457,
            "p_value": 5.691049053920678e-20
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 215,
            "green_fraction": 0.6437125748502994,
            "z_score": 9.074862635037457,
            "p_value": 5.691049053920678e-20
        },
        "output_without": {
            "num_tokens_scored": 329,
            "num_green_tokens": 193,
            "green_fraction": 0.5866261398176292,
            "z_score": 6.8934293023332325,
            "p_value": 2.723159628772031e-12
        }
    }
]