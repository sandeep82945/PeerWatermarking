{
    "eCGpNGDeNu.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, world models, reward-free exploration, agent robustness, minimax regret, algorithm WAKER, data collection, deep reinforcement learning, Markov Decision Process (MDP), reward-free Markov Decision Process (MDP), Partially Observable Markov Decision Process (POMDP), Underspecified POMDP (UPOMDP), latent state representation, model-based RL, predictive model, exploration policy, domain randomisation (DR), Unsupervised Environment Design (UED), reward function, generalist agents, latent space, latent dynamics model, exploration policy πexpl, total variation distance, conditional value at risk (CVaR), out-of-distribution (OOD) environments, ensemble of neural networks, intrinsic motivation, function approximation, DreamerV2.",
    "x6u2BQ7xcq.pdf.json": "Tag2Text, vision language pre-training, VLP, image tagging, vision-language models, visual-linguistic features, semantic guidance, zero-shot performance, generation-based tasks, alignment-based tasks, multimodal representation, transformer-based models, contrastive learning, generative learning, triplet image-tag-text, text semantic parsing, image-text pairs, tagging guidance, image captioning, image-text retrieval, object tags, detector-free models, image features, model parameters, running time, image recognition, multi-label image recognition, Binary Cross-Entropy loss, transformer encoder-decoder, Language Modeling Loss, Image-Text Contrastive Loss, Image-Text Matching Loss, tagging capability, COCO, OpenImages, alignment supervision, feature similarity, tagging benchmarks, tag recognition capability, dataset settings, semantic alignments, image-tag recognition decoder, visual spatial features, robust alignment loss function, image-tag-text generation, tag guidance indicators, model convergence, tag category system, image-to-text retrieval, text-to-image retrieval, state-of-the-art results, efficient end-to-end pre-training.",
    "pTN8dV2pL8.pdf.json": "Gaussian-based representation, normals, polarization priors, neural radiance field (NeRF), Multi-View Stereo (MVS), Signed Distance Function (SDF), 3D shapes, reflective scenes, specular reflection, light rays, Bidirectional Reflectance Distribution Functions (BRDFs), neural networks, Micro-facet BRDF, polarization information, Degree of Polarization (DoP), Gaussian, volume rendering, spatial neural radiance fields, implicit representation, mean square error (MSE) loss, Chamfer Distance, 3D Gaussian, polarimetric information, GNeRP, polarimetric neural 3D reconstruction, anisotropic normals distribution, Euclidean geometry, Stokes vector, volume density, reconstruction accuracy, noise issues, framework, object masks, curvature, machine learning, optimization, geometry representation, dataset, feature extraction, Chamfer Distance metric, 3D printing, autonomous driving, Computer Aided Design (CAD), eigenvalues, anisotropic distribution, occlusion loss, relief printing, reflective objects, Gnome scene, Blackvase, Owl.",
    "kC5nZDU5zf.pdf.json": "self-attention, parameter-efficient, visual backbones, CLIP, task-conditioned bottleneck, learnable codebook, object goal navigation, object displacement, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, ManipulaTHOR, Habitat, target object recognition, cognitive psychology, neuroscience, inattentional blindness, top-down selective attention, bottom-up processing, latent codes, convex combination, Proximal Policy Optimization (PPO), codebook collapse, dropout, linear probing, Grad-Cam, nearest neighbor retrieval, DINOv2, representation-agnostic, m-VOLE, success rate, episode length, curvature, task-specific representations, action embedding, goal embedding, visual encoder, recurrent neural network (RNN), actor-critic",
    "ORUiqcLpV6.pdf.json": "3D visual grounding, Chain-of-Thoughts, CoT3DRef, interpretable framework, sequence-to-sequence, anchors, referring task, machine learning, performance gains, data-efficient, Nr3D, Sr3D, Scanrefer, SOTA (state-of-the-art), BERT, transformer architecture, visual encoder, language encoder, multi-modal fusion, attention mechanisms, pseudo-label generator, referring loss, classification loss, gradient descent, object proposals, bounding boxes, spatial relations, pathway module, softmax function, masked self-attention, language classification head, object localization, natural language processing (NLP), deep learning, autonomous driving, virtual reality, robotics, graph-based models, synthetic utterances, ground-truth labels, object classes, human perception system, visualization techniques, knowledge distillation, reasoning process, data annotations, Achlioptas, Chen, Bakr, Yang, MVT, LAR, SAT, ViL, GAN.",
    "di52zR8xgf.pdf.json": "Stable Diffusion XL (SDXL), latent diffusion model, text-to-image synthesis, UNet backbone, attention blocks, text encoder, conditioning schemes, image-to-image technique, deep generative modeling, 3D classification, controllable image editing, image personalization, synthetic data augmentation, graphical user interface, fMRI brain scans, diffusion architecture, self-attention, transformer-based architectures, LDMs (latent diffusion models), model convergence, noising-denoising process, OpenCLIP ViT-bigG, CLIP ViT-L, multi-aspect training, size-conditioning, crop-conditioning, Fourier feature encoding, discrete-time diffusion schedule, reconstruction metrics, autoencoder (AE), exponential moving average (EMA), black-box models, visual fidelity, FID (Fréchet Inception Distance), IS (Inception Score), multimodal processing, byte-level tokenizers, hyperparameter tuning, knowledge distillation, progressive distillation, offset-noise, image guidance, multi-aspect resolution, conditioning parameters, generative models.",
    "WIAO4vbnNV.pdf.json": "diffusion models, image generation, motion guidance, zero-shot technique, optical flow network, guidance loss, sampling process, flow field, motion fields, guidance function, image editing, neural network, lossy compression, latent space, CLIP similarity, backward warp, denoising, recursive denoising, e2 e, image manipulation, cross-attention, hyperparameters, hyperparameter tuning, translations, rotations, deformations, homographies, SDEdit, RAFT, classifier-free guidance, image structure, occlusions, edit mask, soft optimization, classification loss, CLIP embedding distance, ImageNet classifier guidance, DDPM, DDIM, generative models, synthetic images, motion transfer, quantitative experiments, qualitative experiments, probabilistic models, encoder, decoder, backpropagation, noise estimate, fidelity of motion, data point, user-provided flow, sampling steps, gradient descent, ambient occlusion, motion prompts, Bansal et al., Dhariwal & Nichol, Chen et al., Pan et al., Shi et al., Mou et al., Epstein et al., Hertz et al., Tumanyan et al., Brooks et al., Gu & Davis, Karras et al., Rombach et al.",
    "otU31x3fus.pdf.json": "accelerated stochastic second-order method, gradient inexactness, Hessian inexactness, optimal convergence, tensor generalization, stochastic higher-order derivatives, convex optimization, L2-Lipschitz-continuous Hessian, stochastic gradients, unbiased stochastic gradient, stochastic Hessian, first-order methods, stochastic approximation, Cubic Regularized Newton (CRN) method, Nesterov Accelerated Tensor method, convergence rate, 2-inexact Hessian, stochastic adaptive second-order method, residual bounds, second-order methods, strong convexity, higher-order minimization, Taylor approximation, stochastic optimization, mini-batched stochastic gradient, mini-batched stochastic Hessian, loss function, model convergence, error rates, auxiliary subproblems, accuracy criteria, algorithm efficiency, sample size, lower bounds, tensor norms, p-th order derivatives, dynamic precision level.",
    "xGvPKAiOhq.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, matrix sensing, low-rank matrix recovery, positive semi-definite matrix, symmetric parameterization, asymmetric parameterization, convergence behaviors, exact-parameterization, over-parameterization, global exact convergence, convergence rate, initialization scale, loss function, empirical studies, Restricted Isometry Property (RIP), Frobenius norm, nuclear norm minimization, spectral method, matrix factorization, gradient descent update rule, matrix factorization loss, initialization-dependent linear convergence, model convergence, imbalance between F and G, variance scaling, exponential convergence, technical insight, numerical constants, empirical observations.",
    "vpJMJerXHU.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, effective receptive field (ERF), convolutional neural networks (CNN), time series analysis, ModernTCN, Transformer-based methods, MLP-based models, temporal dependency, multivariate time series, causal convolution, TCN, TimesNet, MICN, SCINet, long-term forecasting, short-term forecasting, imputation, classification, anomaly detection, patchify variable-independent embedding, DWConv, ConvFFN, cross-variable dependency, kernel size, state-of-the-art, architecture design.",
    "qCyhvr0GG8.pdf.json": "self-attention, U-Net, VONet, MONet, parallel attention, temporal consistency, sequential VAE, object-wise sequential VAE, KLD, attention masks, feature map, object-centric representations, slot attention, CNN, dataset, MOVI, transformer-based decoder, background region, object tracking, hyperparameters, sampling, pixel-wise softmax, mixture of components, gradient descent, deep learning, loss function, non-linear optimization.",
    "L9U5MJJleF.pdf.json": "concept bottleneck layer, generative model, concept loss, orthogonality loss, model agnostic, generative adversarial networks, variational autoencoders, diffusion models, steering generative models, pre-concept bottleneck, post-concept bottleneck, human-understandable features, concept bottleneck generative models, CBGMs, loss functions, Kullback-Leibler divergence, steerability, interpretability, debugging, concept embedding layer, sampling, task loss, binary crossentropy loss, latent variable, generative model families, hyperparameters, steerability accuracy, Fréchet inception distance (FID), unknown concepts, known concepts, disentangled representations, lower dimensions, latent vector, concept classifiers, concept probability scores, concept embeddings, qualitative models, classifier-free guidance, conditional generation, model editing, discrete concepts.",
    "oDdzXQzP2F.pdf.json": "Transformer-VQ, dense self-attention, linear time, vector-quantized keys, softmax, Enwik8, PG-19, ImageNet64, model convergence, attention mechanism, autoregressive modeling, hyperparameter tuning, VQ-Attention, LayerNorm, elements-wise product, backpropagation-compatible VQ scheme, codebook, bits-per-byte (BPB), perplexity (ppl), Gaussian Attention Unit (GAU), causal masking, Kronecker delta function, commit loss coefficient, autoregressive modeling, vector quantization (VQ), VQ shortcode, dimensionality reduction, memory-efficient attention, FlashAttention, kernelizable attention, clustering attention, compressive transformers, token contiguity, sequence length, training loss, cross-attention, training updates, attention computations, codebook size, attention dropout, element-wise nonlinearity, training latency, training throughput.",
    "U7VW3KBm34.pdf.json": "Pointwise Feature Vector (PFV), Effective Receptive Field (ERF), Sharing Ratio Decomposition (SRD), eXplainable AI (XAI), Activation-Pattern-Only Prediction (APOP), adversarial attacks, deep learning, nonlinear interactions, saliency maps, Class Activation Mapping (CAM), Grad-CAM, Grad-CAM++, LayerCAM, ResNet50, VGG16, backpropagation, gradient-based methods, LRP, neural networks, model inference, model-generated information, vector perspective, faithfulness, robustness, localization, complexity, fidelity, Sparseness, Stability, Structural Similarity Index Measure (SSIM), Pearson Correlation Coefficient (PCC), Pointing Game, Attribution Localization, Gaussian noise, model behavior",
    "V5tdi14ple.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, large language models (LLM), Google’s Minerva, OpenAI’s GPT, GSM8K, MATH, MultiArith, automatic formalization, Heuristic, formal theorem proving, Isabelle, few-shot prompting, chain-of-thought prompting, automated theorem prover (ATP), natural language processing, model convergence, loss function, autoformalization, formal statement, formal solution, statement translation, informal solutions, Don’t Trust: Verify (DTV), multi-sample consistency, programming-based calculators, statistical reasoning, quantitative reasoning, reinforcement learning from human feedback (RLHF), Coq, Lean, Sledgehammer, number theory, Prealgebra, Algebra, confidence interval, model sizes",
    "rINBD8jPoP.pdf.json": "variational quantum algorithms, VQAs, parameter optimization, quantum architecture search, QAS, curriculum-based reinforcement learning, CRLQAS, 3D architecture encoding, environment dynamics, episode halting scheme, simultaneous perturbation stochastic approximation, SPSA, quantum cost function, Pauli-transfer matrix, Pauli-Liouville basis, quantum chemistry, quantum processing units, Noisy Intermediate-Scale Quantum, NISQ, parameterized quantum circuit, PQC, variational quantum eigensolver, VQE, chemical Hamiltonian, Hilbert space, reinforcement learning, RL, ground state energy, quantum gate, Controlled-NOT, CNOT, rotation gates, RX, RY, RZ, cost function, barren plateaus, classical optimizer, Adam, noise effects, finite sampling noise, tensor-based binary circuit encoding, hyperparameter tuning.",
    "uKB4cFNQFg.pdf.json": "self-supervised learning, DNA language modeling, BEND, Benchmark for DNA language models, genomic DNA, masked language modeling (MLM), protein language models (pLMs), long-range features, embeddings, genomic annotation, natural language processing (NLP), eukaryotic DNA, histone proteins, nucleosomes, chromatin fibers, gene expression, RNA molecules, protein-coding genes, introns, exons, untranslated regions (UTRs), transcription start site (TSS), regulatory regions, enhancers, silencers, insulators, DNA methylation, zero-shot prediction, dilated convolution, transformer-based models, Nucleotide Transformer (NT), Genomic Pretrained Network (GPN), FloraBERT, DNABERT, DNABERT-2, HyenaDNA, BigBird, Byte-Pair Encoding (BPE), gene finding, enhancer annotation, chromatin accessibility prediction, histone modification prediction, CpG methylation prediction, variant effect prediction, DeepBind, DeepSEA, Basset, Enformer, genomic benchmarks, AUROC, multidimensional classification, multi-task training, performance metrics, long-range dependencies.",
    "RLSWbk9kPw.pdf.json": "softening error, differentiable swap function, error-free swap function, permutation matrix, neural sorting network, high-dimensional input, ordinal variable, differentiability, neural network, multi-head attention, permutation-equivariant Transformer, sorting networks, dataset, bubble sort, insertion sort, quick sort, scaling dot-product attention, Deep Sets, instance-wise CNNs, zero-shot learning, gradient-based learning, loss function, permutation-invariant network, Cauchy function, optimal monotonic function, sigmoid function, accem, accew, Frobenius norm, hard permutation matrix, soft permutation matrix, sequencing model, error accumulation problem, ordinal information, differentiable sorting algorithms, straight-through estimator, training data, hyperparameter tuning, model capacity.",
    "kNpSUN0uCc.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, reinforcement learning, MaxEnt Model Correction, MoCo, Maximum Entropy density estimation, Model Correcting Value Iteration, MoCoVI, sample-based variant, MoCoDyna, model-free algorithms, model-based algorithms, Markov Decision Process, approximate model, dynamics P, Bellman operator, next-state expectations, world model, Maximum Likelihood Estimate, KL divergence, maximum likelihood estimation, policy evaluation, control problem, basis functions, density estimation, Gibbs distributions, stochastic approximation, regularization, sample-based setting, empirical averages, action-value functions, discount factor, V πPE, V ∗, V̂ π, optimization objective, function approximation, dual problem, Policy Evaluation problem, error measure, model error, expected return, convergence rate, performance guarantees, sample efficiency, dynamic programming, MDP, empirical distributions, value function, uncertainty quantification.",
    "lOwkOIUJtx.pdf.json": "foveal-peripheral sampling, saccade mechanism, active scene reconstruction, image classification, image detection, reinforcement learning, GTSRB dataset, ImageNet dataset, model accuracy, loss function, self-supervised learning, mean square error, structural similarity, hybrid loss, optical sensing, foveal view, peripheral view, visual field, computational load, data efficiency, actor-critic reinforcement learning, complexity reduction, adversarial robustness, occlusion robustness, image recognition, data communication, encoding mechanism, scene prediction, classification accuracy, neural system, human visual system, attention model, saccade control policy, predictive reconstruction module, ConvLSTM, patch sampling, training data, vision systems, biological visual systems, low-latency processing, energy efficiency.",
    "icTZCUbtD6.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, Hardness Characterization Methods (HCMs), Hardness Characterization Analysis Toolkit (H-CAT), data-centric AI, model performance, sample learnability, model convergence, classification problem, training dataset, sample hardness, mislabeling, out-of-distribution (OoD), atypical samples, learning dynamics, statistical metrics, distance-based methods, prototype similarity, sample selection, uncertainty quantification, gradient descent, performance evaluation, benchmark framework, error estimation, Cleanlab, Data-IQ, Data Maps, Prototypicality, GraNd, RHO-Loss, ensemble methods, Cross-Entropy Loss, executable framework, empirical benchmarking, metrics computation, sample scores, qualitative evaluation, quantitative evaluation, robustness, empirical analysis, adversarial robustness, multiple data modalities, evaluation metrics, precision-recall curve (AUPRC), receiver operating characteristic curve (AUROC), dimensional assessment, data characterization, effective sample acquisition, noise transition matrix, statistical comparison, diversity, empirical results, backbone models, extensibility in models, experimental setups, statistical significance, taxonomy of hardness types.",
    "y886UXPEZ0.pdf.json": "domain-adaptive pre-training, large language models (LLMs), reading comprehension, domain-specific corpora, prompting ability, domain knowledge, model performance, computational requirements, fine-tuning, supervised datasets, knowledge probing, zero-shot performance, natural language understanding, task diversity, generating sentences, inference, commonsense reasoning, machine learning techniques, AdaptLLM, PubMed Abstracts, FreeLaw Opinions, ConvFinQA, MedAlpaca, BloombergGPT, LLaMA, regex-based patterns, domain-specific knowledge, scaling, input-output patterns, general instructions, knowledge transfer, prompt design, intrinsic tasks, mixed data ratios, evaluation metrics, ablation studies.",
    "mCOBKZmrzD.pdf.json": "Equivariant Transformers, Equiformer, computational complexity, higher-degree representations, eSCN convolutions, attention re-normalization, separable S activation, separable layer normalization, EquiformerV2, state-of-the-art methods, OC20 dataset, forces, energies, speed-accuracy trade-offs, DFT calculations, adsorption energies, data efficiency, QM9 dataset, OC22 dataset, equivariant graph neural networks (GNNs), tensor products, Clebsch-Gordan coefficients, hyperparameter, angular resolution, message passing, equivariant features, Euclidean symmetries, permutation symmetries, depth-wise tensor products, SOp3q convolutions, SOp2q linear operations, attention weights, feed forward networks, Gaussian radial basis functions, radial embeddings, structure relaxations.",
    "mutJBk3ILg.pdf.json": "self-supervised learning, SSL, spurious features, spurious correlations, image augmentations, empirical risk minimization, core features, representation learning, Late-layer Transformation-based View Generation, LATETVG, data augmentation, contrastive learning, SimSiam, SimCLR, worst-group performance, dataset imbalance, subgroup connectivity, causal inference, importance weighting, dataset re-sampling, group distributionally robust optimization, contrastive objective, invariance, predictive tasks, unlabeled datasets, binary classification, feature pruning, linear classifiers, negative samples, augmentation graph, spectral contrastive loss, InfoNCE loss, marginalized probability, core-spurious relationships, multimodal images, large-scale datasets, group information, representation space, temporal regularization, balance in training datasets.",
    "tUtGjQEDd4.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, Critically damped Langevin Dynamics, Bridge Matching, Stochastic Optimal Control, Ordinary Differential Equations, Stochastic Differential Equations, generative modeling, sampling complexity, sampling efficiency, data prediction, image generation, diffusion models, Stochastic Differential Equation, prior distribution, linear SDE, neural network, score function, time reversal, empirical performance, sample efficiency, Brownian Bridge, optimal transport, normalizing flow, phase space dynamics, Acceleration Generative Modeling, sampling-hop, Number of Function Evaluations, Stochastic Bridge problem, Monte Carlo sampling, scaling dynamics, Gaussian distributions, covariance matrix, generative models, Fréchet Inception Distance, deterministic sampling, conditional generation, image datasets, Momentum dynamics, trajectory inference, data distribution, numerical simulation, Cholesky decomposition, diffusion coefficient, optimal control, dynamics system, momentum systems, trajectory enhancement, model convergence, loss function.",
    "kmn0BhQk7p.pdf.json": "self-attention, privacy, large language models (LLMs), inference capabilities, memorized training data, personal attributes, dataset, Reddit profiles, top-1 accuracy, top-3 accuracy, privacy-invasive chatbots, text anonymization, model alignment, adversarial interaction, author profiling, PersonalReddit dataset, personal identifiable information (PII), dataset size, gradient descent, log-linear relationship, zero-shot attribute inference, adversary models, pre-trained models, generative models, human performance, model risks, training data memorization, adversarial setting, malicious chatbots, ethical considerations, alignment methods.",
    "s56xikpD92.pdf.json": "self-attention, backdoor attacks, deep neural networks, DNNs, post-development defenses, backdoor functionality, backdoor expert model, finetuning, mislabeled clean samples, attack success rate, clean accuracy, CIFAR10, GTSRB, ImageNet, ResNet, VGG, MobileNetV2, Vision Transformer, backdoor input detectors, ensemble strategy, BaDExpert, loss function, data poisoning, weights tampering, model inference, classifier, Neyman-Pearson lemma, AUROC, adversarial attacks, model architectures, backdoor samples, adaptive attacks, model convergence, trigger patterns, gradient updates.",
    "s90VIdza2K.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, stochastic optimization, empirical risk minimization, f-divergence measures, f-FERM, theoretical convergence, fairness-accuracy tradeoffs, distribution shift, distributionally robust optimization, lp norms, stochastic gradient descent, adversarial attacks, statistical independence, pre-processing methods, post-processing methods, in-processing methods, covariance, Rényi correlation, χ2 divergence, L∞ distance, Maximum Mean Discrepancy (MMD), fairness constraints, regularization terms, neural networks, KL-divergence, Jensen-Shannon divergence, total variation distance, demographic parity, equalized odds, equality of opportunity, unbiased gradient estimators, min-max optimization, Legendre-Fenchel transformation, stochastic first-order algorithms, empirical risk minimization (ERM), neural network training, softmax layer, distributionally robust fair empirical risk minimization, semi-stochastic algorithms, ℓp norms, ℓ∞ norms",
    "XwiA1nDahv.pdf.json": "smoothECE, Expected Calibration Error, ECE, Binned ECE, calibration measures, reliability diagrams, miscalibration, binning, RBF kernel, hyperparameter-free, kernel regression, Gaussian kernel, Nadaraya-Watson, histogram regression, calibration function, calibration distance, Wasserstein distance, consistent calibration measure, Laplace Kernel Calibration Error, MMCE, empirical distribution, residuals, reliability diagram, non-parametric smoothing, smoothing parameter, statistical consistency, probabilistic predictors, machine learning, deep learning, confidence calibration, ImageNet, solar flares, daily rain forecasts, synthetic dataset, bootstrapping, uncertainty quantification",
    "vI95kcLAoU.pdf.json": "self-attention, vision transformers, SKIPAT, multi-head self-attention (MSA), transformer architecture, image classification, semantic segmentation, image denoising, video denoising, computational efficiency, parametric function, transformer backbones, quadratic complexity, Centered Kernel Alignment (CKA), depth-wise convolutions (DwC), ImageNet-1K, ADE20K, SIDD, ResNeXt, SoTA (state-of-the-art), token sampling, model generalization, throughput, FLOPs, image tokenization, class-token, hybrid architectures, MobileViT, convolution layers, object detection, parameter efficiency, Jaccard similarity, CorLoc, ECA (efficient channel attention), GeLU activation, performance gains, simplicity, scalability.",
    "MzjiMxlWab.pdf.json": "latent interest units, behavioral data, user preferences, VAE framework, item characteristics, granularity, user interests, compositionality, prototype-based representation learning, binding mechanism, bi-directional binding block, Collaborative Filtering (CF), preference learning, multi-interest user modeling, item grouping, Variational AutoEncoder (VAE), non-linear probabilistic modeling, multinomial likelihood, information-theoretic regularization, FACETVAE, cross-entropy loss, Kullback-Leibler divergence, micro-disentanglement, similarity, Gumbel-Softmax, cosine similarity, high-level user interests, low-level user interests, item-adopted items, clustering objective, binding problem, object-centric learning, preference representation, unstructured preference learning, item representation, dataset names, MovieLens-1M, CiteULike-a, Yelp, Empirical analysis, multi-faceted item space structure, F matrices, J clusters, assignment score, low-level interest representation, homologous item characteristics, user-item interaction, clustering, assignment matrix, recommendation accuracy, data-driven manner.",
    "WsRHpHH4s0.pdf.json": "self-attention, Blockwise RingAttention, feedforward, memory-efficient, context length, Transformers, architecture design, hyperparameter tuning, language modeling, reinforcement learning, large context Transformers, GPT-3.5, GPT-4, MPT, Claude, softmax, attention computation, blockwise computation, blockwise attention, linear transformations, ReLU activation, blockwise parallel transformers, BPT, model convergence, memory demand, sequence length, hidden size, FLOPs, batch size, tensor parallelism, data parallelism, pipeline parallelism, sequence parallelism, FSDP, model flops utilization, ExoRL, behavior cloning, attention matrix, simulation models, attention scores, outputs, token processing, context size, parallel computations, ring topology, shared memory, scalable AI models.",
    "pCEgna6Qco.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, fine-tuning, in-context learning, format specialization, Prompt Tuning with MOdel Tuning (ProMoT), low data regime, natural language processing (NLP), transformer, large language models (LLMs), mT5, RTE, few-shot learning, multi-task training, catastrophic forgetting, parameter-efficient methods, soft prompt, supervised performance, generalization, task-specific parameters, NLI, summarization, classification, generative tasks, WMT14, TriviaQA, web_questions, semantic content, formats, loss function, accuracy, BLEU score, Rouge-2 score, prompt engineering, task formats, hyper-parameters, soft trainable prompts, model convergence, out-of-domain evaluation tasks, empirical evidence, language pairs.",
    "QQ6RgKYiQq.pdf.json": "MovingParts, NeRF, dynamic scene reconstruction, part discovery, fluid simulation, motion cue, Eulerian view, Lagrangian view, cycle-consistency, rigid motions, trajectory tracking, motion grouping, neural radiance fields, depth-based rendering, surface appearance, canonical space, particle motion, geometry representation, motion extraction, feature volume, multi-view capture, real-world scenes, scene editing, object tracking, 3D shape deformation, rigid body transformation, Mean Squared Error (MSE), total variation loss, Intersection over Union (IoU), transparent rendering, dynamic scenes, light-weight multilayer perceptrons (MLPs), synthetic dataset, robotic manipulation sequences, structure modeling, temporal consistency, dynamic NeRF, part segmentation.",
    "fsW7wJGLBd.pdf.json": "Large Language Models (LLMs), prompt injection attacks, dataset, Tensor Trust, prompt extraction, prompt hijacking, benchmark, adversarial examples, instruction-following LLMs, model convergence, defense prompts, human-generated attacks, GPT 3.5 Turbo, robustness, Hijacking Robustness Rate (HRR), Defense Validity (DV), prompt extraction robustness, Extraction Robustness Rate (ERR), LDA topic modeling, model-specific adversarial tokens, jailbreaking, attack strategies, complex instructions, string comparison task, syntax manipulation, human judgment, access code, attack prompt, opening defense, closing defense, Turing test, malicious user, cybersecurity risks, reinforcement learning from human feedback (RLHF), qualitative analysis, model vulnerabilities, textual adversarial attacks, multi-step attack strategies, filtering techniques, safety features, external knowledge, instruction fine-tuning.",
    "k1wlmtPGLq.pdf.json": "SNNs, batch normalization, Temporal Accumulated Batch Normalization, TAB, Temporal Covariate Shifts, TCS, energy-efficient computing, neuromorphic hardware, Loihi, TrueNorth, spike-triggered asynchronous currents, accumulated membrane potential, internal covariate shift, ICS, ANN-to-SNN conversion, surrogate gradients back-propagation, LIF model, discrete LIF model, spiking neurons, membrane potential dynamics, differential equations, temporal dependencies, spiking activation function, temporal accumulated statistics, CIFAR-10, CIFAR-100, DVS-CIFAR10, ResNet, VGG, AdamW optimizer, weight decay, Heaviside step function, temporal dynamics, Riemann–Stieltjes integral, temporal distribution, current input model, online learning.",
    "uELjxVbrqG.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, face recognition, feature representation space, orthogonal decomposition, sub-features, face distinguishability, knowledge distillation, intra-class incoherence constraint (IIC), SphereFace, CosFace, ArcFace, Curricularface, MagFace, AdaFace, gradient descent, triplet loss, contrastive loss, pair loss, angular loss, adaptive loss functions, data uncertainty learning, probabilistic face embeddings (PFE), sphere confidence face (SCF), Kullback-Leibler Divergence, margin-based methods, feature embeddings, ResNet50, ResNet100, LFW, CFP-FP, CPLFW, AgeDB, CALFW, Vggface2, IJB-C, SGD, metric-learning-based loss functions, uncertainty modeling, intra-class irrelevance, feature dissimilarity, embeddings, average accuracy, feature extraction network, teacher network, student network, cosine similarity.",
    "c5pwL0Soay.pdf.json": "self-attention, unsupervised pre-training, reinforcement learning, exploratory data, downstream tasks, pure exploration, mutual information skill learning, Metric-Aware Abstraction, METRA, state space, latent space, temporal distances, Markov decision process, Euclidean distance, Wasserstein distance, temporal distance metric, skill discovery, diverse behaviors, state coverage, KL divergence, generative autoregressive pre-training, contrastive representation learning, policies, world models, exploratory data, high-dimensional environments, pixel-based environments, skill policy, trajectory autoencoders, exploration bonuses, zero-shot goal-reaching, latent-conditioned policy, reward function, compact latent metric space, intrinsic dimensionality, skill latent vectors, Lipschitz constant, skill dynamics model, policy coverage, state coverage, goal-conditioned tasks, unsupervised skill discovery, ICM, RND, Plan2Explore, LBS, APS, DIAYN, DADS, CIC, LSD, LEXA, pixel-based Quadruped, pixel-based Humanoid, complex environments, locomotion skills, continuous skills, discrete skills.",
    "rpH9FcCEV6.pdf.json": "Proximal Initialization Attack, PIA, Membership Inference Attack, MIA, diffusion models, generative models, image generation, audio generation, query-based methods, SOTA, discrete-time diffusion models, continuous-time diffusion models, GANs, VAEs, privacy risks, privacy leaks, data reconstruction, training loss, Gaussian noise, DDIM, DDPM, AUC, True Positive Rate, TPR, False Positive Rate, FPR, text-to-speech, TTS, CIFAR10, CIFAR100, TinyImageNet, COCO2017, Laion5B, LJSpeech, VCTK, LibriTTS, Grad-TTS, DiffWave, FastDiff, stochastic differential equation, SDE, loss function, ℓp-norm, model robustness, marginal distribution, Gaussian distribution.",
    "WKuimaBj4I.pdf.json": "principal-agent problems, outcome-dependent payment scheme, contract, optimal contract, algorithm, learning optimal contracts, hidden-action models, meta-actions, best-response actions, principal’s expected reward, cumulative utility, regret bound, discover-and-cover algorithm, empirical distribution, Kullback-Leibler divergence, approximate best-response regions, polynomial sample complexity, algorithmic game theory, action-Oracle, approximate halfspace, sampling distributions, stochastic outcomes, best response regions, incentive compatibility, individual rationality, contract theory, regret optimization, crowdsourcing platforms, blockchain-based contracts, healthcare applications, Θ(T), Õ(√m · T^1/(2m+1)), Õ(mn · T^(4/5)), PAC-learning framework, finitely many actions, utility maximization, expected utility, decision-making process, polynomial time algorithm.",
    "qz3mcn99cu.pdf.json": "Lipschitz-based methods, certifiably robust, neural networks, adversarial attacks, robustness, network capacity, data, Lipschitz constraints, state-of-the-art, underfitting, performance gains, Lipschitz-based certification methods, verified robust accuracy (VRA), Randomized Smoothing (RS), Lipschitz bounds, deterministic certification, ImageNet, model capacity, self-attention, hyperparameter design, architecture choice, data pipeline, DDPM, Cholesky-orthogonalized residual dense layers, residual dense layers, Lipschitz constant, Lipschitz control, GloRo regularization, Cayley transformation, matrix exponential, layer-wise orthogonal training, almost orthogonal layer, SDP-based Lipschitz layer, sandwich layer, Cholesky-based orthogonalization, spatial MLP, generative data augmentation, classification model, ResNeXt101, CIFAR-10, CIFAR-100, Tiny-ImageNet, empirical robustness, architectural innovations, 1-Lipschitz activation functions, dense layers, convolutional layers, regularization, orthogonal transformation, gradient signal, overfitting, easier classification, hard samples, decision boundary, statistical analysis.",
    "kxgSlyirUZ.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, COLLIE, constrained text generation, language understanding, logical reasoning, semantic planning, natural language processing, large language models, instruction tuning, compositional constraints, dataset construction, automatic extraction, grammar-based framework, performance evaluation, language models (LLMs), multimodal constraints, problem solving, code generation, dataset COLLIE-v1, Wikipedia, CC-News, Project Gutenberg, grammatical constraints, counting constraints, position constraints, constraint structures, extraction algorithm, natural language instructions, success rate, control codes, functional constraints, parallel constraints, modular design, NLP community, automated feedback, constraint satisfaction rate, logical composition, performance consistency, counting levels, positional constraints.",
    "UCfz492fM8.pdf.json": "Human Motion Driven Control (HMDC), robot motions, kinematics, dynamics, motion retargeting, unsupervised learning, CrossLoco, guided unsupervised reinforcement learning framework, cycle-consistency-based reward, mutual information, robot skills, action commands, Proximal Policy Optimization (PPO), robot-robot correspondence, robot-to-human Mapper (R2H-Mapper), Human-to-Robot Mapper (H2R-Mapper), Gaussian distribution, loss function, diverse human motions, locomotion, quadrupedal robot, Algorithim 1, average correspondence reward (ACR), motion diversity (DIV), average root tracking reward (RTR), Language2Robot, Interactive Robot Control, Learned Motion Matching (LMM), Human Motion Diffusion Model (MDM), simulation, Isaac Gym, task-oriented methods, Generative Adversarial Networks (GAN), skill discovery, cycle-consistency loss, muscle dynamics, Markov Decision Process, non-linear optimization, motion dynamics, root tracking reward, torque penalty, joint limits penalty, robot morphology, motor commands.",
    "vN9fpfqoP1.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, LLaMA-2, CDVAE, DFT, density functional theory, atomistic data, energy above hull, parameter efficient fine tuning (PEFT), symmetric crystal structures, multi-task curriculum, translation augmentations, Materials Project, metastable materials, convolutional neural networks (CNN), generative models, discrete and continuous data, translation invariance, molecular datasets, Gaussian diffusion, crystal diffusion variational autoencoder (CDVAE), batch size, sampling hyperparameters, in-context learning, language modeling, Increase in Perplexity under Transformation (IPT), perplexity, tokenization, byte pair encoding (BPE), crystal string formatting, structural validity, compositional validity, energy above hull (Ehull), stable materials, stochastic prompts, element identity, atomic positions, crystal properties, ML potentials, deep learning, LSTM, SVM, GAN, parameter-efficient instruction tuning, chemical composition, data representation, model scale, atomic coordinates, model convergence, training examples, learning patterns, chemical formulas",
    "d4uL2MSe0z.pdf.json": "self-attention, transformer, Reinforcement Learning, weight sharing, training parameters, regularization technique, baseline transformer model, perplexity, memory consumption, architecture design, model convergence, learning rate optimization, layer dropping, Q-function, dynamic layer tying, Neural Architecture Search (NAS), Differentiable Architecture Search, Q-learning, CNN, RNN, MLP, proximal policy optimization (PPO), gradient descent, action vector, state vector, negative perplexity score, Bellman equation, hyperparameter tuning, GPT-2, BERT, WikiText-2, WikiText-103, LAMBADA, 1 Billion Words, algorithm, model performance, training dynamics, parameter-efficient fine-tuning (PEFT), LoRA, learning rate, action space, training epochs, action selection, exploration probability, low-rank updates, GPU hours, memory optimization, dynamic weight tying, model pruning.",
    "OZitfSXpdT.pdf.json": "knowledge distillation, KD, soft supervision, hard supervision, knowledge fusion ratio, adaptive method, sample-wise knowledge fusion ratio, intra-sample trilateral geometry, inter-sample relations, teacher's global average prediction, neural network, bilevel optimization, model convergence, student prediction, teacher prediction, ground truth, Kullback–Leibler divergence, cross-entropy loss, convex combination, alpha, ResNet, ResNet-34, ResNet-20, ResNet-32, ResNet-110, ResNet-18, ImageNet, CIFAR-100, ADA-KD, WLS-KD, RW-KD, FitNet, ST discrepancy, geometric relations, empirical results, sample-wise ratio, attack detection, click-through rate prediction, temperature, diversity of models, outliers, prediction discrepancies, statistical significance, attention transfer, spatial attention transfer, channel attention transfer, contrastive representation, hyperparameter tuning",
    "qHGgNyQk31.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, text-conditioned video prediction (TVP), sample and computation-efficient model, Seer, pretrained text-to-image (T2I), stable diffusion, U-Net, language conditioning, spatial-temporal attention, Frame Sequential Text Decomposer, temporally aligned sub-instructions, high-fidelity video frames, instruction alignment, long-horizon planning, text-to-video prediction (TVP), diffusion models, Gaussian diffusion process, classifier-free guidance, latent diffusion models (LDM), autoregressive generation, temporal attention, cross-attention, semantic information, Fréchet Video Distance (FVD), Kernel Video Distance (KVD), Something Something V2 (SSv2), EpicKitchens-100, Bridgedata, generative models, VQ-VAE, Transformer, DALL-E, CLIP, Markov Chain, KL-regularized autoencoder, text-video datasets, multi-task generation, video manipulation, video foundation models, causal attention mechanism, window attention, 3D network, inflated 3D U-Net, fast sampler DDIM.",
    "MREQ0k6qvD.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, functional connectivity, state-switching generalized linear models, hidden Markov models, HMM-GLMs, biological plausibility, Gaussian HMM-GLM, one-hot HMM-GLM, Gumbel-Softmax, anatomical connectome, dynamic functional connectivity, statistical methods, information-theoretic methods, cross-correlogram, mutual information, Granger causality, transfer entropy, generalized linear methods, Hawkes process, Poisson log-likelihood, non-linear function, latent variable, adjacency matrix, strength matrix, categorical distribution, Gaussian prior, temperature hyperparameter, inferential statistics, Bayesian inference, Baum-Welch algorithm, maximum likelihood estimation, adjacency accuracy, log-likelihood estimation, predictive likelihood, real neural data, synthetic dataset, PFC-6 dataset, barrel cortex dataset, neural interactions, firing patterns, model convergence, code repository, spike train data, interaction patterns, hidden states.",
    "RIu5lyNXjT.pdf.json": "self-attention, prompt design, prompt formatting, large language models (LLMs), few-shot settings, instruction tuning, model sensitivity, FORMATSPREAD, performance reporting, accuracy points, LLaMA-2-13B, empirical evaluation, prompt engineering, semantic equivalence, multi-arm bandit, Thompson sampling, Upper Confidence Bound (UCB), Bayesian optimization, prompt embedding, spurious biases, continuous prompt embeddings, grammar definition, task performance spread, classification tasks, Super-NaturalInstructions, model architecture, model performance variance, accuracy distribution, degree of dissimilarity, format identification, task evaluation metrics, input-output formatting, metric correlation, contextual restrictions, computational budget, model comparisons, prompt construction, format performance spread, performance interval, statistical significance, confidence intervals, variance analysis, score distribution, evaluation datasets.",
    "NnYaYVODyV.pdf.json": "Perceptual Group Tokenizer, perceptual grouping, self-supervised representation learning, visual recognition architecture, ImageNet-1K, feature detection, deep learning, multi-grouping operation, grouping operations, grouping tokens, contextual pixels, self-attention, gradient descent, representation learning, convolutional neural networks (CNN), Vision Transformer (ViT), adaptive computation, object-centric representation, biological vision system, contrastive representation learning, BYOL, DINO, masked image modeling, pixel space tokenization, parameter tuning, Gaussian distribution, loss function, token embeddings, attention matrix, mean IoU, computational efficiency, token visualization, over-complete, iterative binding process, high-order information exchange, OOD (out-of-distribution) generalization.",
    "cPgh4gWZlz.pdf.json": "chain-of-knowledge (CoK), large language models (LLMs), grounding information, factual rationales, hallucination, reasoning preparation, dynamic knowledge adapting, answer consolidation, knowledge-intensive, knowledge domains, adaptive query generator (AQG), SPARQL, SQL, unstructured data, structured knowledge sources, Wikidata, retrieval system, Verify-and-Edit (VE), chain-of-thought (CoT), self-consistency, fine-tuned model, Llama-2, LoRA, error propagation, factual accuracy, knowledge sources, knowledge domains, factual domain, medical domain, physics, biology, intermediate rationales, reasoning generation, knowledge domain selection, query generation, query execution, few-shot chain-of-thought (CoT), ProgramFC, retrieval-augmented methods, MMLU, FEVER, HotpotQA, MedMCQA, ReAct, knowledge-intensive tasks, augmentation, progressive knowledge adapting, factual correctness.",
    "rkplYfqUr0.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, discriminative prompting, GEN-Z, generative prompting, multivariate, LM likelihood, contextual information, text classification, in-context learning, ICL, generative classification, probabilistic classifier, Bayesian inference, zero-shot inference, model robustness, prompt variations, label descriptions, generative classification framework, language model, sentiment classification, domain-aware classification, personalization, natural language descriptions, author information, subject information, contextualization, few-shot learning, generative models, discriminative models, retrieval-based methods, structured prompting, label prediction probability, generative setup, training examples, demonstration selection, text source, domain context, demographic information, representation learning, semantic text classification, probabilistic framework, aggregation strategy, macro-F1 score, label verbalizer, hate speech detection, empowerment prediction, politeness prediction, model performance, contextual variables, human biases, social variables, generative zero-shot classification, robustness, task performance, label variability, model families, context variables, open-source models, performance variance, assumption of independence, training data, personalized predictions, context-aware models.",
    "ixP76Y33y1.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, ResNet, BERT, ImageNet, MNIST, architecture design, model convergence, loss function, LSTM, CNN, SVM, GAN, thermodynamic equilibrium, quantum entanglement, protein folding, generalization error, intrinsic dimension, label sharpness, adversarial robustness, neural networks, binary classification, Lipschitz constant, Poisson process, scaling law, dataset intrinsic dimension, learned representation intrinsic dimension, maximum likelihood estimation, nearest neighbor distance, adversarial attack, vulnerability to attack, representation learning, robustness, generalization scaling behavior, metric space",
    "mqVgBbNCm9.pdf.json": "Skeleton-of-Thought (SoT), large language models (LLMs), sequential decoding, batched decoding, inference efficiency, data-centric optimization, attention operation, transformer architecture, model size, GPU memory, point-expanding, speed-up, Skeleton Prompt Template, Point-Expanding Prompt Template, Vicuna-80, WizardLM, LLaMA, GPT-4, ChatGPT-3.5, neural architecture, memory access, computational graph, speculative decoding, non-autoregressive generation (NAG), Chain of Thought (CoT), Graph of Thoughts, efficiency analysis, Net Win Rates, FastChat, LLMZoo, quantization, model compression, multi-query attention, dynamic mixture-of-experts, loss function, performance evaluation, response coherence, prompt engineering.",
    "lJkOCMP2aW.pdf.json": "self-attention, multi-scale Transformer, adaptive pathways, temporal resolution, temporal distance, multi-scale division, patches, dual attention, local details, global correlations, time series forecasting, Pathformer, empirical design, trend decomposition, seasonality decomposition, Fourier transform, temporal dynamics, multi-scale modeling, Instance Norm, Adaptive Multi-Scale Blocks, Mean Absolute Error (MAE), Mean Squared Error (MSE), statistical modeling, autoregressive methods, convolution, DeepAR, CNN, RNN, GNN, Transformer, Informer, Autoformer, FEDformer, Scaleformer, Pyraformer, PatchTST, trend patterns, periodic patterns, time series analytics, transfer learning, data-adaptive routing, dimensional alignment, weighted aggregation, noise terms, computational resources, feature extraction, parameter tuning.",
    "hmv1LpNfXa.pdf.json": "self-attention, message-passing, graph neural networks (GNNs), polynomial expressivity, quadratic complexity, linear GTs, graph topology, node features, local-to-global attention, polynomial functions, permutation equivariance, polynomial coefficients, attention scores, homophilic datasets, heterophilic datasets, state-of-the-art (SOTA), graph transformers (GTs), positional encoding (PE), structural encoding (SE), Laplacian eigenpairs, GAT attention scheme, polynomials, Weierstrass theorem, polynomial networks, local attention, global attention, polynomial expressivity, kernel trick, mini-batch training, ReLU activation, node classification datasets, polynomial-expressive model, attention matrix, hyperparameter tuning, equivariant attention models, large-scale graphs, graph learning frameworks (PyG, DGL), attention mechanism, node representations, equidistant attention, polynomial approximation.",
    "apA6SSXx2e.pdf.json": "Topological Concentration, Approximated Topological Concentration, link prediction, Graph Neural Networks, node embeddings, local topology, LP performance, topological metrics, degree, subgraph density, message-passing, Topological Distribution Shift, node-centric evaluation, correlation, high-degree nodes, low-degree nodes, Gaussian embeddings, latent space, network dynamics, data augmentation, supervised learning, SOTA performance, empirical findings, graph convolutions, computational complexity, normalization term, neighborhood information, common neighbors, clustering coefficient, edge reweighting, variational inference, relational data, GCN, SAGE, LightGCN, social networks, recommendation systems.",
    "hNhwSmtXRh.pdf.json": "Lemur, Lemur-Chat, language models, natural language, coding capabilities, language agents, pretraining, instruction fine-tuning, open-source models, Llama-2-70B, The Stack, agent benchmarks, tool usage, programming languages, ground execution, interaction, reasoning, planning, autonomous problem solvers, multi-agent systems, MMLU, BBH, GSM8K, HumanEval, MBPP, Spider, MultiPL-E, DS-1000, Python, SQL, Bash, Perl, RefinedWeb, Redpajama, CommonCrawl, Wikipedia, Books, ArXiv, StackExchange, DM Mathematics, TPUv4-512, sequence packing, Open Assistant, Orca data, ShareGPT, Evol-CodeAlpaca, feedback adherence, Partial Observable Markov Decision Process, MINT dataset, GPT-4, self-debug, environmental feedback, tool-augmented reasoning, action sequences, model adaptability, symmetry between text and code, interactive environments, problem-solving, database queries, cybersecurity knowledge, natural language instructions, symbolic programming, language reasoning, agent evaluation, transfer learning, program synthesis, continuous pre-training, instruction tuning, agent evaluations, LLMs, autonomous operation.",
    "fcqWJ8JgMR.pdf.json": "Data-Free Knowledge Distillation (DFKD), Out-of-Domain Knowledge Distillation (OOD-KD), AuG-KD, MobileNet, EfficientNet, ShuffleNet, ResNet, CLIP, Knowledge Distillation (KD), Vanilla KD, Independent and Identically Distributed Hypothesis (IID Hypothesis), Domain Shift, Anchor-Based Mixup Generative Knowledge Distillation, uncertainty-guided, sample-specific anchor, mixup learning, generator, latent variable, activation maps, output logits, Cross Entropy (CE), Information Entropy (H), MosiacKD, Adversarial Knowledge Distillation, Cross-Modal Knowledge Distillation, Data-Free Learning Module, Anchor Learning Module, Mixup Learning Module, class-specific mask, Energy Score, spurious correlations, normalization, uncertainty metric, domain-specific information learning, teacher model, student model, joint distribution P(X,Y).",
    "W2d3LZbhhI.pdf.json": "diffusion probabilistic models, DPMs, ordinary differential equation, ODE, function evaluations, NFE, unified sampling framework, USF, truncation error, exponential integral formulation, sample quality, predictor-based search method, solver schedule, S3, high-order solvers, CIFAR-10, CelebA, ImageNet, LSUN-Bedroom, fast sampling, training-free samplers, reverse diffusion SDEs, forward diffusion stochastic differential equations, SDEs, numerical accuracy, DE solver, high-order numerical methods, signal-to-noise ratio, SNR, noise prediction network, data prediction network, Taylor expansion, derivative estimation method, corrector, low-order solvers, DDIM, PNDM, DPM-Solver, UniPC, AutoML, hyperparameters, solver strategies, gradient descent, performance predictor, search space, model acceleration, FID, multistage search process, neural inference, unfished model, random search, performance metrics.",
    "q3KNrmW6Ql.pdf.json": "Graph Neural Networks, GNNs, fairness-aware, adversarial attacks, G-FairAttack, prediction utility, algorithmic fairness, demographic groups, adversarial training, regularization, edge rewiring, prediction bias, sensitive attributes, utility loss function, cross-entropy loss, demographic parity loss, mutual information loss, Wasserstein distance loss, surrogate model, non-gradient attack algorithm, optimization problem, bilevel optimization, gray-box attack, fairness poisoning attack, fairness evasion attack, softmax function, utility variation budget, time complexity, kernel density estimation, total variation loss, influence attack, fairness metrics, GCN, FairGNN, EDITS, utility constraints.",
    "ozX92bu8VA.pdf.json": "self-attention, multi-layer perception (MLP), LAyer-SElective Rank reduction (LASER), predictive performance, question-answering, Transformer architecture, over-parameterized, pruning strategies, denoising procedure, singular value decomposition (SVD), rank reduction, low-rank approximations, natural language processing (NLP), weight matrices, loss function, accuracy, perplexity, GPT-J, CounterFact, PILE, reinforcement learning, image classification, ViT, decision Transformer, SGD, higher-ordered components, lower-ordered components, singular values, model compression, model distillation, model robustness, top-k accuracy, log loss, parameter reduction, semantic type, high-frequency tokens, attention probabilities, training data, generalization, mathematical modeling",
    "oXjnwQLcTA.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, goal-conditioned reinforcement learning, GCRL, occupancy matching, pseudo-reward, offline datasets, sparse rewards, mixture-distribution matching, SMORe, Bellman-regularized contrastive procedure, density modeling, imitation learning, max-entropy GCRL, Markov Decision Process, state-action-goal occupancy distribution, Q-function, density-ratio, Hindsight Experience Replay (HER), f-divergence, KL-divergence, dual optimization, contrastive RL, Generalized Contrastive Learning, robot manipulation, locomotion tasks, unnormalized densities, transition probability function, discounted return, empirical sampling, expectile regression, action conditioning, environment stochasticity, distribution matching, regression-based methods, actor-critic methods, behavior cloning, high-dimensional observations.",
    "w3YZ9MSlBu.pdf.json": "self-supervised learning (SSL), Music undERstanding model, large-scale self-supervised Training (MERT), masked language modelling (MLM), Residual Vector Quantisation Variational AutoEncoder (RVQ-VAE), Constant-Q Transform (CQT), music information retrieval (MIR), music tagging, beat tracking, music transcription, source separation, acoustic music, pre-trained language models (PLMs), Mel-frequency cepstral coefficients (MFCCs), Noise Contrastive Estimation (NCE), multi-task learning, transformer, acoustic teacher, musical teacher, deep music features, feature extraction, k-means, dimensionality reduction, reconstruction loss, pitch detection, genre classification, emotional score regression, vocal technique detection, singer identification, acoustic model pre-training, audio representation, data augmentation, learning rates, parameter sizes, distributed training, training stability, gradient clipping, attention relaxation techniques, pre-layer normalisation (Pre-LN), post-layer normalisation (Post-LN), contrastive learning, generative pre-trained model, audio data formats.",
    "c0chJTSbci.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, SuSIE, image editing diffusion model, InstructPix2Pix, goal-conditioned policy, Internet-scale pre-training, visual understanding, semantic knowledge, novel objects, language command, robotic control tasks, semantic scene augmentation, VLMs, subgoal synthesis, generative models, action-conditioned dynamics models, model-based reinforcement learning, UU Neural Network, Denoising Diffusion Probabilistic Models, gradient descent, BridgeData V2, CALVIN, RC2, prefix-action separation, visual planning, task accomplishment, semantic information, multi-task learning.",
    "WNkW0cOwiz.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, diffusion models, stochastic differential equations, generative models, infinite Lipschitz property, Lipschitz constants, training processes, inference processes, E-TSDM, DDIM, DPM-Solver, Fréchet Inception Distance, noise prediction, v-prediction, Markovian forward process, Gaussian noise, noise schedule, stochastic differential equation (SDE), score function, conditional distribution, empirical results, unconditional generation, conditional generation, fast sampling, image synthesis, numerical stability, estimation error, performance metrics, peak signal-to-noise ratio (PSNR), sampling capability, Lipschitz singularities, network architecture.",
    "rAHcTCMaLc.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, maximum entropy reinforcement learning, MaxEnt RL, Energy-Based Model, entropy estimation, Stein Soft Actor-Critic, S2AC, Stein Variational Gradient Descent, SVGD, SQL, Soft Actor-Critic, SAC, KL-divergence, variational inference, Gaussian, multimodal action space, Q-values, algorithmic efficiency, particle dynamics, stochastic policy, Bellman loss, isotropic Gaussian, continuous action spaces, change-of-variable formula, coupled dynamics, normalizing flow, weak scaling, robust reinforcement learning, Gibbs densities, MCMC, Hamiltonian Monte Carlo, stochastic gradient Langevin dynamics, entropy term, action samples, exploratory behavior, reward function, variational distribution, EBM sampling, particle-based Bayesian inference, action distribution, closed-form entropy estimate, high-dimensional action spaces, backward compatibility, multimodal policy, optimal solution, empirical evaluation, benchmark, training stability.",
    "gEwKAZZmSw.pdf.json": "variance-controlled adaptive sampling, VCAS, stochastic gradient, backpropagation, activation gradient, weight gradient, importance sampling, fine-grained sampling, sample ratio, computational cost, hyperparameter tuning, convergence, layerwise importance sampling, leverage score sampling, variance control, data pruning, architecture pruning, mixed precision training, quantization, stochastic optimization, online batch selection, meta-learning methods, loss-based methods, gradient norm-based methods, approximated stochastic gradient, cost-effective, low-variance approximation, computational overhead, data dimension, token dimension, model parameters, deep learning, BERT-base, ViT-large, MNLI, ImageNet-1k, FLOPs reduction, empirical tuning, gradient norm, random Bernoulli vector, variance estimation, self-adaptation, gradient variance, task-specific sampling, model convergence, computational resources, training trajectory.",
    "n6mLhaBahJ.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, HAZARD, large language models (LLMs), reinforcement learning (RL), Monte Carlo tree search (MCTS), iGibson, Habitat, SAPIEN, VirtualHome, AI2THOR, ThreeDWorld (TDW), search and rescue (SAR), embodied AI, decision-making, dynamic environments, disaster scenarios, agent-driven interactions, environment-driven changes, object distance, temperature, water level, buoyancy, drag force, A* algorithm, qualitative evaluation, quantitative evaluation metrics, performance evaluation, agent action space, perception model, fire, flood, wind, simulation platforms, agent exploration, object rescue, semantic segmentation, object value, waterproof capability, ignition point, susceptibility to wind, spatial reasoning, historical memory, action selection.",
    "slSmYGc8ee.pdf.json": "self-attention, weight distributions, variance, network states, deep learning, neural circuit connectivity, low-rank structure, high-rank initialization, low-rank initialization, learning regimes, empirical analyses, theoretical analyses, plasticity, catastrophic forgetting, architecture, model convergence, initial connectivity, initial weight structures, neural tangent kernel (NTK), rich learning regime, lazy learning regime, gradient descent, RNN (recurrent neural networks), cross-entropy loss, mean squared error, effective rank, representation alignment, tangent kernel alignment, Gaussian initialization, Eigenstructure, weight change norm, representation similarity matrix (RSM), weight change, task statistics, NTK movement, expectation across tasks, singular values, two-layer feedforward network, task-agnostic settings, kernel alignment, effective learning richness, effective learning laziness, inductive biases, low-rank eigenspectrum, cognitive tasks, random initialization, local connectivity statistics, Dale’s law, network motifs, learning speed, generalization, plasticity-driven transformations, neural data, resource-intensive transformations, dynamical regime, task-specific features, high-dimensional spaces, block-specific variance, experimental-driven connectivity, effective connectivity rank, evolutionary factors.",
    "cmcD05NPKa.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, greatest common divisor (GCD), training distributions, Transformers, large language models (LLMs), addition, multiplication, fine-tuning, scratchpad, chain-of-thought, algorithmic prompting, eigen-decomposition, polynomial roots, explainability, model predictions, products of divisors, rational arithmetic, number theory, log-uniform distribution, cross-entropy, base representation, sequence-to-sequence transformers, Adam optimizer, learning rate, attention heads, uniform sampling, stratified test set, model accuracy, training epochs, integer divisibility, sieve algorithm, mathematical tasks, Neural Arithmetic Logical Units, modular arithmetic, overfitting, grokking, LSTM, GRU, numerical algorithms, computational biology, high energy physics, experimental settings, model architecture, performance evaluation, empirical observations, theoretical accuracy, dataset balancing, log-uniform operands, learning curves.",
    "d6tUsZeVs7.pdf.json": "Energy-based models, EBMs, Optimal Transport, OT, Entropy-regularized OT, energy potentials, unnormalized likelihood functions, WGAN-based, loss function, pre-trained StyleGAN, AFHQ, image domains, model convergence, generative modelling, dual OT problem, KL divergence, weak dual objective, generalization bounds, Monte-Carlo sampling, Langevin dynamics, stochastic gradients, energy function, Gibbs-Boltzmann distribution, Wasserstein distance, gradient descent, energy-guided approach, 1-Wasserstein distance, 2-Wasserstein distance, stochastic dual maximization, Schrödinger bridge problem, KL (µ∥µθ), P(X), Q(Y), Rademacher complexity, discrete OT, continuous OT, Gaussian-to-Gaussian, unpaired data-to-data translation, toy datasets, image-to-image translation, latent space mapping, conditional plans, training stability, empirical samples, MLP, CNN, style transfer, machine learning, latent distribution, generative models.",
    "ekz1hN5QNh.pdf.json": "hyperbolic spaces, hyperbolic neural networks, HNNs, HCNN, convolutional neural network (CNN), Lorentz model, batch normalization, multinomial logistic regression, Poincaré ball, feature representations, hierarchical structures, representation learning, image data, hierarchical class relations, image classification, image generation, latent embeddings, hyperbolic geometry, numerical stability, statistical measures, hyperbolicity, Riemannian batch normalization, Euclidean models, tree-like structures, geometric prior, factorization, multi-class classification, dimensionality reduction, adversarial robustness, distance metric, Fréchet mean, Fréchet variance, latent hyperbolic distribution, recognition, segmentation, reconstruction, metric learning, deep neural networks, optimal representation, hybrid architectures, training procedure, hyperbolic model, representation capacity, standard vision tasks, empirical performance, related work, graph embedding tasks, feature space, generative models, transferable knowledge, visual representations.",
    "PdaPky8MUn.pdf.json": "self-supervised pretraining, long-range dependencies, state space models, Transformers, Long Range Arena, random initialization, pretraining, denoising objectives, downstream task data, vanilla Transformers, S4, PathX-256 task, structured parameterizations, model convergence, mean absolute performance, effective self pretraining (SPT), accuracy improvement, diagonal linear RNNs, causal sequence modeling, masked sequence modeling, HiPPO theory, convolution kernels, performance estimation, evaluation practices, empirical gains, SPT Transformers, efficient architectures, neural network architectures, sequence classification, image classification, Speech Commands, sCIFAR dataset, BIDMC tasks, multimodal tasks",
    "kklwv4c4dI.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, distributed optimization, saddle point problems, machine learning, smooth unconstrained objectives, composite optimization, non-smooth regularization, Federated Dual Extrapolation, FeDualEx, Bregman divergence, convergence, communication complexity, adversarial robustness, generative adversarial networks, matrix games, multi-agent reinforcement learning, distributed saddle point optimization, global model, aggregation, averaging, sparse solution, dual averaging, mirror descent, stochastic composite saddle point optimization, deterministic composite saddle point optimization, generalized Bregman divergence, generalized proximal operator, convex optimization, primal dual algorithm, curse of primal averaging, composite convex optimization, LASSO, sparse regression, dual extrapolation, proximal operator, optimization process, structured solution, universal adversarial training, logistic regression, MNIST, CIFAR-10, empirical evaluation, convergence rate, communication rounds, local updates, smoothness constant, random samples.",
    "c56TWtYp0W.pdf.json": "self-attention, hyperparameter tuning, spatiotemporal structure, transformers, group tokens, group embedding (GE), Group-Aware transFormer (GAFormer), multivariate time series (MTS), position embeddings, inductive bias, temporal dynamics, temporal structure, channelwise structure, multi-head self-attention, learnable parameters, softmax function, temporal group embedding (TGE), spatial group embedding (SGE), univariate time-series, multivariate datasets, neural decoding, Brain-computer interfaces (BCIs), temporal period, channel-wise, instance-specific, classification, regression, Adam optimizer, state-of-the-art, interpretability, latent structure, dataset, group-wise interactions, dimension reduction layer, constrained learning, neural population activities, motor cortex, temporal semantics, temporal grouping structure, classification accuracy, regression tasks, explicit Runge-Kutta method, many-body systems, gradient descent, Gaussian process, transformer encoder, Hyperparameter optimization, batch size, backbones, temporal patch, channel dynamics, instance-specific group embeddings, position embedding techniques, spatiotemporal hierarchy, neural activity representation, robust representations",
    "vKViCoKGcB.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, data attribution, diffusion models, attribution method, model outputs, DDPMs, CIFAR-10, CelebA, Stable Diffusion, LoRA, counterfactual evaluation, linear datamodeling score, ablation studies, empirical performance, model convergence, loss function, GANs, VAEs, computational scalability, non-convex settings, D-TRAK, TRAK, gradient descent, log-likelihood estimation, Fisher information matrix, U-Net architecture, AdamW optimizer, dropout rate, cosine annealing learning rate schedule, empirical training objective, de-noising diffusion probabilistic models, latent diffusion models, randomness of training mechanism, Shapley values, leave-one-out influences, influence functions, weighted variational bound, empirical Influence, datamodel, reconstruction error, test samples, inference phase, image generation, data curation, poisoning attacks, model behavior debugging, generative tasks, performance adversarial attacks, loss estimation, model parameters, empirical advantages, variational bound, optimization objective.",
    "nZP10evtkV.pdf.json": "optimal transport, adversarial patch attacks, image classification, black-box transfer attack, feature distribution, Wasserstein loss, transferable patches, Neural Networks, Digital Experiments, Convolutional Neural Networks, Transformer models, feature space, adversarial examples, Kullback-Leibler divergence, Generative Adversarial Network, GAP, LaVAN, PS-GAN, TnT method, physical world experiments, batch size, decision boundaries, empirical robustification methods, attack transferability, multimodel transferability, EoT (Expectation over Transformations), adversarially trained models, Sliced-Wasserstein distance, patch applicator operator, optimization artifacts, model convergence, hybrid experiments, robustness evaluation, digitalization effects, targeted class, adversarial noise, model families, training recipes, effective transferability, physical attacks, total variation loss, feature representations, local gradient smoothing, multi-layer objectives, representation space, class distributions.",
    "eepoE7iLpL.pdf.json": "self-attention, probabilistic perspective, invariant sufficient statistic, compound selection, utility function, set function, permutation invariance, information aggregation module, empirical log likelihood, variational distribution, conditional distribution, DeepSets, sufficient statistic, adequate statistic, optimal subset, neural subset selection, INSET, hierarchical symmetry, symmetry group, model structure, energy-based model, Jaccard coefficient, product recommendation, set anomaly detection, PDBBind, BindingDB, Bernoulli distributions, optimization criteria, Maximal Invariant, regularization, variational inference, supervised data, machine learning, set-valued outputs, compound database, feature vectors, Mini-Batch Consistency, encoder-decoder architecture, neural networks, probabilistic modeling, statistical inference, interaction between components, hierarchical representations, image-related tasks, information-sharing, Convolutional Neural Networks, set-pooling layers, model convergence, loss function, performance evaluation, dimensionality reduction.",
    "wkbeqr5XhC.pdf.json": "LUM-ViT, deep learning, pre-acquisition modulation, hyperspectral data, under-sampling mask, kernel-level weight binarization, three-stage fine-tuning, ImageNet, DMD (Digital Micromirror Device), compressive sensing, hyperspectral imaging (HSI), convolutional neural networks (CNN), vision transformers (ViT), learnable mask, dimensionality reduction, feature extraction, accuracy loss, training phase, real-world performance, spectral channels, patch-embedding layer, optical modulation, Gumbel-Softmax, mean squared error (MSE), dynamic mask strategies, multi-spectral data, optical hardware, binary decision mask, spatial light modulator, single-pixel imaging, machine learning, prior information, DMD operation, Hadamard product, classification loss, model convergence, robustness, network architectures.",
    "ZWzUA9zeAg.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, data augmentation, pre-trained text-to-image diffusion models, image-to-image transformations, few-shot image classification, weed recognition, DA-Fusion, generative models, Foundation Models, synthetic images, Stable Diffusion, latent variable models, thermodynamic diffusion, image classification tasks, classifier free guidance, Textual Inversion, photo-realistic images, inversion graphics, semantic segmentation, loss function, gradient descent, few-shot learning, real-world datasets, resolution, sample quality, robustness, image transformations, generative data augmentation, visual recognition, internet-scale data, data-centric strategies, model-centric strategies, image editing, augmentation strategy, class-agnostic values, pseudo-prompts, randomization technique, diffusion process, neural networks, Gaussian transitions, training classifiers, crop and segment, hyperparameters, generative models, synthetic data, few-shot setting, ImageNet, Caltech101, Flowers102, FGVC Aircraft, Stanford Cars, COCO, PASCAL VOC, LAION-5B, random sampling, attention weights, Markov chain, loss function minimization, visually coherent, color and geometry transformations, Real Guidance, prompt construction, classifier performance, generative modeling, data leakage prevention, visual appearance, Deep Neural Networks, image features, augmentation robustness, representation learning, data imbalance, model training, model fine-tuning, image appearance modifications, learning rate, classification accuracy, safety checker, harmful content prevention, ethical considerations, latent space, environmental variation, cross-validated accuracy.",
    "VXak3CZZGC.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, out-of-distribution (OOD) generalization, domain-invariant representations, hyperspherical space, intra-class variation, inter-class separation, prototypical learning objective, OOD generalization error, maximum likelihood estimation, Wasserstein distance, total variation, Hellinger distance, stochastic gradient descent, ResNet-18, ResNet-50, CIFAR-10, CIFAR-10-C, PACS, Office-Home, VLCS, minimum risk, learning algorithm, class prototype, embedding geometry, van Mises-Fisher distribution, classification accuracy, empirical performance, multi-source domain generalization, Sinkhorn divergence, contrastive learning, vMF distribution, domain adaptation, invariant risk regularizer, risk bound.",
    "x7d1qXEn1e.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, image denoisers, inverse problems, image restoration network, deep neural networks, prior models, state-of-the-art restoration models, convergence, stationary point, global functional, imaging inverse problems, ill-posed, prior modeling, computational imaging, biomedical imaging, computer vision, deep learning, ML-based priors, super-resolution, deblurring, inpainting, microscopy, medical imaging, model-based deep learning (MBDL), plug-and-play priors (PnP), regularization by denoising (RED), deep unfolding (DU), compressed sensing using generative models (CSGM), deep equilibrium models (DEQ), minimum mean-squared error (MMSE), log-likelihood estimation, Deep Restoration Priors (DRP), Gaussian density, weighted Euclidean seminorm, least-squares data-fidelity term, total variation (TV) regularizer, proximal-gradient method, convex functions, optimization problem, data fidelity, regularization parameter, spectral convergence, PSNR, SwinIR, deep image super-resolution models, DRUNet, GAN, CNN, statistical interpretations, score matching, diffusion models.",
    "RemfXx7ebP.pdf.json": "RNA design, tertiary structure, secondary structure, primary sequence, hierarchical data-efficient representation learning, contrastive learning, benchmark dataset, structural modeling, loss function, RNA tertiary structure prediction, RNA secondary structure prediction, natural RNA sequences, data-driven RNA design, structural complexity, structural dependency, RDesign, self-attention, sequence length distribution, graph representation, message-passing neural networks (MPNNs), recovery metric, Macro-F1 score, tertiary structure modeling, structural similarity, cluster-level representation learning, sample-level representation learning, hyperspherical space, probabilistic models, RNAfold, Mfold, UNAFold, RNAStructure, RNAInverse, RNASSD, INFO-RNA, NUPACK, antaRNA, aRNAque, eM2dRNAs, MCTS-RNA, confidence-aware sample-level representation learning, molecular dynamics, Protein Data Bank (PDB), GNN (Graph Neural Networks), Gaussian noise, RMSD (Root Mean Square Deviation), TM-score, energy minimization, thermodynamic parameters, structural constraints, nucleotide bases, non-coding RNA, ribonucleotides, nucleotides, biophysical modeling, computational modeling, graph-based representation, gentle constraints, spatial distances, dihedral angles.",
    "b66P1u0k15.pdf.json": "technical concepts, dynamic re-balancing, deep long-tailed recognition (DLTR), representation learning, Pareto optimal solutions, multi-objective optimization (MOO), multi-task learning (MTL), variability collapse loss, worst-case optimization, gradient conflicts, logit adjustment, sampling strategy, loss re-weighting, input perturbation, cost-sensitive loss, re-sampling, re-weighting, re-margining, Balanced Softmax, LDAM-DRW, MiSLAS, GCL, cRT + Mixup, M2m, state-of-the-art performance, gradient similarity, optimization conflicts, classification task, imbalanced dataset, model architecture modifications, shared feature extraction, task-specific optimization, cosine similarity, Hessian spectrum analysis, performance improvements, empirical analysis, theoretical complexity, statistical analysis, optimization trajectory, gradient similarity analysis, code availability.",
    "cZttUMTiPL.pdf.json": "stable probability distribution, neural networks, local linearization, total variation distance, ReLU, Gaussian distribution, Cauchy distribution, input uncertainties, output uncertainties, uncertainty propagation, moment matching, Probabilistic Neural Network (PNN), Monte Carlo (MC), distributed loss function, predictive uncertainty, model uncertainty, data uncertainty, total variation (TV), Wasserstein distance, marginal moment matching, deterministic variational inference approximation (DVIA), pairwise distribution loss, selective prediction, UCI dataset, calibration, cross-entropy, affine transformations, Jacobian, softmax function, Gumbel noise, empirical risk, Dirichlet outputs, risk-coverage, pairwise probabilities, uncertainty quantification",
    "VJvbOSXRUq.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, GNN, explainability methods, empirical evaluations, counterfactual reasoning, stability, non-convex loss surfaces, benchmarking study, perturbation-based explainability methods, Pareto-optimal methods, GCN, gradient-based, instance-level explainers, model-level explanations, sensitivity scores, importance scores, causal contribution, Shapley value, GNNExplainer, PGExplainer, TAGExplainer, RCExplainer, CF-GNNExplainer, GLG-Explainer, GCFExplainer, GEM, clear, noise, stochasticity, message passing GNNs, L1 norm, inference subgraph, Granger causality, variational factors, topological constraints, factual reasoning, adjacency matrix, generative modeling, IND, transductive, inductive, GAT, GIN, GraphSAGE, sufficiency, Jaccard similarity, noise volume, robustness, feasibility, explanation size, counterfactual explainers, mutual information, embedding explainer, domain-specific considerations, quantum entanglement, collective networks, dataset names, RCEXPLAINER, CF2, GNN architecture, graphical models, summarization techniques, loss function, accuracy, statistical performance, data constraints, stakeholder implications, practical scenarios, model convergence, codebase, grounds for explanation, dataset characteristics, outcomes, feature noise, sparsity, graph classification, node classification, deep-learning models, explainability research, topological noise, validation frameworks, decision-making processes.",
    "UMOlFJzLfL.pdf.json": "stochastic gradient descent, SGD, linear stability, stationary point, sharpness, generalization error, overparameterized neural networks, hyperparameters, loss function, loss Hessian, mean-squared error, cross-entropy loss, implicit regularization, parameter tuples, gradient descent, coherent measure, empirical performance, additively decomposable loss function, eigenvalue, Frobenius norm, predictive proxy, coherence measure, geometry of the Hessian, instability, convergence, divergence, model weights, hessian coherence measure, optimization algorithm, batch size, learning rate, low stable rank, Edge-of-Stability phenomenon, stability analysis, sharpness-aware methods, Gaussian noise, neural networks, empirical validation, theoretical framework.",
    "nBCuRzjqK7.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, contrastive learning, enhanced decomposition architecture, global autocorrelation, positive and negative pairs, self-supervised manner, long-term forecasting performance, temporal convolution networks, temporal locality, temporal globality, AutoCon, sliding window approach, time-series data, decomposition-based models, linear model, recurrent neural networks (RNNs), DLinear, CNN-based models, representation learning, dynamic time warping (DTW), short-term branch, long-term branch, normalization, denormalization, input sequence, output sequence, model architecture, long-term variations, statistical process theory, autoregressive integrated moving average (ARIMA), time series decomposition, multivariate forecasting, attention mechanism, gradient descent, loss function, cosine similarity, mini-batch, time-lag, temporal alignment.",
    "g7ohDlTITL.pdf.json": "Riemannian Flow Matching, RFM, normalizing flows, manifolds, generative modeling, high dimensions, simulation-free, vector field, premetric, geodesic distance, continuous normalizing flows, CNFs, Flow Matching, implicit vector field, probability density path, Riemannian divergence, tangent space, divergence computation, conditional flow, ordinary differential equation (ODE), spectral decompositions, manifold datasets, maximum likelihood, non-Euclidean spaces, Riemannian diffusion models, spectral distances, eigenfunctions, Laplace-Beltrami operator, diffusion distance, biharmonic distance, training objectives, optimization, high-dimensional torus, triangular meshes, non-trivial curvature, probability paths, Conditional Flow Matching, Riemannian Conditional Flow Matching, metrics, machine learning, Euclidean space.",
    "m52uU0dVbH.pdf.json": "vertical federated learning (VFL), federated learning (FL), adversarial attacks, adversarial examples (AEs), attack strategies, online optimization, adversarial example generation (AEG), corruption pattern selection (CPS), multi-armed bandit (MAB), Thompson sampling with Empirical maximum reward (E-TS), expected maximum reward, gradient estimation, natural evolution strategy (NES), perturbation optimization, attack success rate (ASR), inner problem, outer problem, regret bound, query-based methods, transfer-based methods, zeroth-order optimization (ZOO), distributed learning, model convergence, computational constraints, sensor data, collaborative inference, loss function, cross-entropy, margin loss, benchmark datasets, black-box setting.",
    "farT6XXntP.pdf.json": "\"Generative Large Language Models\", \"NLP tasks\", \"translation task\", \"fine-tuning approach\", \"monolingual data\", \"parallel data\", \"Advanced Language Model-based trAnslator\", \"ALMA\", \"LLaMA-2\", \"BLEU\", \"COMET\", \"WMT’21\", \"WMT’22\", \"decoder-only models\", \"GPT\", \"PaLM\", \"OPT\", \"BLOOM\", \"NLLB\", \"GPT-3.5\", \"GPT-4\", \"XGLM\", \"data quality\", \"catastrophic forgetting\", \"Causal Language Modeling (CLM)\", \"hyperparameter settings\", \"LoRA\", \"Fine-Tuning\", \"Transformer\", \"language pairs\", \"zero-shot evaluation\", \"translation performance\", \"training recipe\", \"translation generation\", \"multilingual linguistic knowledge\", \"language modeling\", \"source sentence\", \"target sentence\", \"log-likelihood loss\", \"zero-shot performance\", \"high-quality datasets\", \"human-written datasets\", \"WMT test data\", \"Flores-200\", \"training paradigms\", \"parameter size\", \"token prediction\", \"machine translation\", \"encoder-decoder frameworks\", \"many-to-many multilingual translation\", \"sacreBLEU\", \"translationese\", \"LMs\", \"backbone models\", \"fine-tuning stages\", \"empirical evaluations\", \"cross-lingual capabilities\".",
    "x1ptaXpOYa.pdf.json": "ADoPD, document page decomposition, data-driven document taxonomy, large-scale dataset, document image understanding, human-in-the-loop, entity segmentation, text detection, tagging, captioning, zero-shot capabilities, CLIP, LLM, OCR, data diversity, out-of-distribution detection, K-means algorithm, hybrid data annotation, pretrained models, visual elements, text bounding boxes, annotation categories, MSCOCO, RVL-CDIP, ImageNet, DocLayNet, DiT, Faster R-CNN, Deformable-DETR, Cascade Mask-RCNN, CropFormer, Mask2Former, Transformer, CNN, pretraining models, task-driven models, EntitySeg, prompt-based methods, multimodal information, segmentation performance, document understanding, hierarchical structuring, algorithmic biases.",
    "x5txICnnjC.pdf.json": "gradient descent, synaptic plasticity, mirror descent, Euclidean geometry, non-Euclidean distances, log-normal weight distributions, Kullback-Leibler divergence, natural gradient descent, Bregman divergence, dual space, synaptic weight distributions, loss function, implicit bias, synaptic geometry, Gaussian distribution, negative entropy, synaptic weights, neural network model, learning algorithms, dimensional weights, power propagation, bandit algorithms, deep networks, finite root, convex optimization, stochastic processes.",
    "j511LaqEeP.pdf.json": "split conformal prediction, uncertainty sets, black-box neural models, data exchangeability, non-exchangeable data, conformal methods, statistical guarantees, F1-score, false negative rate, nonexchangeable conformal risk control, monotone loss function, distribution drift, time series, coverage, conformal risk control, empirical risk, expected value, total variation distance, calibration data, exchangeable data distribution, non-conformity score function, Kullback-Leibler divergence, maximum entropy principle, λ-insensitive absolute loss, open-domain question answering, Natural Questions dataset, multilabel classification, synthetic data, electricity usage monitoring, deep learning, sentence-transformer model, multivariate Gaussian distribution, logistic regression, least squares regression, changes in coefficients, machine learning systems, prediction sets, optimal λ, λ̂, coverage guarantees, calibration set, data points, logistic regression models, semantic search, distributions, neural model predictions, model performance, calibration variables, prediction intervals.",
    "fGAIgO75dG.pdf.json": "directed acyclic graph (DAG), linear structural equation model (SEM), differentiable optimization, acyclicity, combinatorial acyclicity constraint, penalty parameter retuning, homoscedasticity, sparsity-aware learning, CoLiDE (Concomitant Linear DAG Estimation), regression-based criterion, gradient computation, closed-form estimation, heteroscedastic scenarios, causal relationships, causal discovery, Markov equivalence, continuous relaxation, likelihood-based methods, ordinary least squares (LS), homoscedastic linear Gaussian models, ℓ1-norm penalty, smoothed concomitant lasso, noise level estimation, convex score function, DAG learning, robustness, structural Hamming distance (SHD), noise distributions, Gaussian noise, Exponential noise, Laplace noise, noise variance, maximum likelihood, convergence properties, Birkhoff polytope, SoftSort operator, Gumbel-Sinkhorn approximation, discrete optimization methods, score-based methods, greedy search strategies, independence tests, causal paths, True Positive Rate (TPR), False Discovery Rate (FDR).",
    "vESNKdEMGp.pdf.json": "large language models (LLMs), jailbreak problem, multilingual jailbreak challenges, unintentional scenario, intentional scenario, malicious instructions, safety mechanisms, low-resource languages, high-resource languages, ChatGPT, GPT-4, SELF-DEFENSE framework, multilingual training data, safety fine-tuning, model evaluation, redteaming, content filtering, reinforcement learning from human feedback (RLHF), multilingualism, MultiJail dataset, unsafe content, malicious attacks, adaptive adversary, language processing tasks, safety training, unsafe responses, machine translation, malicious instruction, Llama, Vicuna, SeaLLM, algorithm, multilingual safety capabilities, safety alignment, ethical deployment, adversarial inputs.",
    "S5aUhpuyap.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, recurrent circuit model, structured priors, latent variables, probabilistic inference, sensory information, contextual information, task-specific posteriors, diffusion models, dendritic nonlinearities, denoising, stochastic somatic integration, dynamical system, prior distribution, posterior distribution, Gaussian noise, sampling dynamics, probabilistic representation, neural mechanisms, neural correlates, sampling-based probabilistic computation, recurrent network, multimodal posteriors, neural representation, Bayesian inference, probabilistic population code, high-dimensional distributions, Markov Chain Monte Carlo, generative models, variational autoencoders (VAEs), normalizing flows, generative adversarial networks (GANs), KL divergence, model convergence, non-linear manifolds, likelihood functions, hyperparameter γ, feedforward sub-circuit, neural signatures, contextual priors, attention mechanisms, autoregressive models, MNIST dataset, visual perception, latent variable modeling.",
    "oKn9c6ytLx.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, generative AI, autonomous agents, natural language commands, realistic environment, benchmark tasks, functional correctness, long-horizon tasks, reasoning before acting, large language models, WebArena, online shopping, content management systems, collaborative software development, social forum discussions, task diversity, task execution, artificial intelligence, environment reproduction, natural language intent, action sequences, data collection, evaluation criteria, accessibility tree, compound action space, user profiles, reward function, intent analysis, information-seeking tasks, site navigation, content configuration, interactive decision-making agents, hierarchical planning, failure recovery, programmatically check, intent collection, few-shot learning, GPT-4, PALM-2, LLM, Docker containers, gym-API, unachievable tasks, response accuracy, action history, document object model (DOM), flexible configuration, user manuals, semantic equivalence, proactivity in task solving, knowledge resources, open-source libraries, evaluation methods, robustness of agents, task-specific benchmarks.",
    "RthOl4jHw5.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, continuous robot evolution, policy transfer, Meta-Evolve, evolutionary robot sequences, robot evolution tree, inter-robot imitation, statistical matching methods, transition dynamics, kinematic tree topology, optimization, manipulation policy, agile locomotion policy, expert policy, Markov Decision Process (MDP), evolution parameters, policy rollouts, Lp vector norm, p-Steiner tree, robot morphology, robot parametric space, geometric median, imitation learning, robot dynamics, transfer learning, Hand Manipulation Suite (HMS), Non-Parametric Policy Gradient (NPG), REvolveR, HERD, robot grippers, simulation cost, sample efficiency, training iterations, policy optimization, exploration cost, heuristic approach, evolutionary paths, target robots, source robot, kinematic behavior, robot hardware difference, policies, simulation epochs, task completion, parallel training, ablation studies, morphological changes, optimization objective, learning efficiency, evolving robots.",
    "OqTMUPuLuC.pdf.json": "self-attention, hyperparameter tuning, dataset bias, overfitting, uninterpretability, knowledge-driven, interactive environment, driver agent, memory component, large language models (LLMs), DiLu, reasoning module, reflection module, decision-making, common-sense knowledge, reinforcement learning, generalization, memory module, Chain-of-Thought (CoT), Highway-env, real-world datasets, few-shot experiences, scene descriptions, reasoning processes, AGI, ICL, instruction following, reasoning with chain-of-thought, GPT-3, PaLM, LLaMA, GPT-4, GRAD, inference, error correction, autonomous vehicles, robotic manipulation, multi-modal understanding, lifelong skill learning",
    "guRNebwZBb.pdf.json": "prefix language model (prefixLM), causal language model (causalLM), in-context learning (ICL), transformer models, gradient descent, linear regression, online gradient descent, multi-layer LSA-transformers, convergence behavior, stationary points, least square solution, empirical experiments, softmax attention, self-attention, Linear Self-Attention (LSA), auto-regressive attention, MMLU, BBH, T5, GPT-3, PaLM2, PaLI-X, synthetic tasks, empirical evidence, performance comparison, weight vector, convergence properties, attention masks.",
    "oEF7qExD9F.pdf.json": "self-attention, RNN, parallel training, low-cost inference, state-of-the-art, LMUFormer, Legendre Memory Units, convolutional patch embedding, convolutional channel mixer, spiking neural network, LSTM, GRU, SOTA, FLOPs, Speech Commands V2, psMNIST, sequence learning, long-range dependencies, rate coding, direct coding, Leaky Integrate-and-Fire, memory state vector, activation functions, non-linear activation layers, Gaussian Error Linear Units, Rectified Linear Units, fast Fourier transform, cross-domain learning, sequential data processing, sequence length reduction, mathematical properties of Legendre polynomials, Long Range Arena, benchmark, SpikeGPT, document retrieval, image classification, attention mechanisms, classification accuracy.",
    "RZBy8oHTz4.pdf.json": "self-supervised learning, contrastive learning, spectral contrastive loss, positive pairs, negative pairs, zero-mean regularization, orthogonality, representations, covariance regularization, empirical successes, positive-pair graph, spectral decomposition, adjacency matrix, sample-contrastive method, dimension-contrastive property, Barlow Twins loss, VICReg loss, unsupervised domain adaptation (UDA), supervised learning, Neural Collapse, label noise, noise transition matrix, linear classifier, dataset, feature dimension, augmentations, Stochastic Block Model, class-mean features, contrastive pretraining, regularization strength, Dirichlet energy, posterior distribution, maximally distant, equiangular frame, representation dimensionality, probability distribution, polynomial function.",
    "yLClGs770I.pdf.json": "MAmmoTH, large language models, LLMs, MathInstruct, instruction tuning dataset, chain-of-thought, CoT, program-of-thought, PoT, mathematical reasoning, hybrid rationales, open-source models, mathematical reasoning datasets, accuracy gain, MATH, competition-level dataset, WizardMath, GPT-4, CoT result, diverse problem coverage, model convergence, dataset-specific fine-tuning, rejection sampling fine-tuning, RFT, Galactica, MINERVA, GSM8K, MMLU-Math, AQuA, Python interpreter, sympy, numpy, hybrid instruction tuning, Llama, Code Llama, accuracy, generalization, neural networks, multi-hop reasoning, quantitative reasoning, zero-shot learning, in-domain datasets, out-of-domain datasets, hybrid decoding strategy.",
    "OeH6Fdhv7q.pdf.json": "Mesh Handle Predictor, Shape-aware Motion Diffusion, TapMo, skeleton-free 3D characters, shape deformation-aware features, diffusion model, auto-animation methods, SMPL model, weakly-supervised training, motion generation models, motion representation, text-guided motion, mesh deformation feature, parameterized model, GCN (Graph Convolutional Network), Linear Blend Skinning, motion synthesis, character geometry, skinning weights, semantic parts, global control, local translations, rotations, motion adaptations, Mesh-specific adaptation, motion priors, LSTM (Long Short-Term Memory), GAN (Generative Adversarial Network), dimensionality reduction, Euclidean Distance Function, adversarial loss, multi-token strategies, Transformer Decoder, ARAP-Loss (As-Rigid-As-Possible Loss), Handle-FID, animation quality, geometry quality, motion representations, qualitative experiments, quantitative experiments.",
    "NltzxpG0nz.pdf.json": "large language models (LLMs), self-driven capability, Steve-Eye, multimodal model, visual encoder, multimodal feedback, open-world instruction pairs, multimodal perception, foundational knowledge base, skill prediction, skill planning, environmental visual captioning (ENV-VC), foundational knowledge question answering (FK-QA), skill prediction and planning (SPP), text input/output (I/O), prompt engineering, Minecraft, MineDojo, ChatGPT, two-stage instruction tuning, negative log-likelihood objective, visual tokenizer, VQ-GAN, CLIP, RL (reinforcement learning), self-supervised techniques, Generative Agents, large multimodal models (LMMs), planning errors, skill graph, skill search algorithm, real-time situational awareness, executable policies, agent performance, qualitative example.",
    "ezscMer8L0.pdf.json": "self-attention, zero-shot generalization, Conv-LoRA, parameter-efficient fine-tuning (PEFT), Low-Rank Adaptation (LoRA), ViT encoder, multi-class semantic segmentation, image segmentation, SAM (Segment Anything Model), image-related inductive biases, vision-specific inductive biases, promptable model, masks, binary mask prediction, high-level image semantics, segmentation knowledge, dynamic selection, Mixture-of-Experts (MoE), lightweight convolution, convolutional parameters, convolution layers, multi-scale local priors, visual prompt tuning (VPT), Scale and Shift Feature Modulation (SSF), FCN (Fully Convolutional Networks), U-Net, Deeplab, PSPNet, DANet, SANet, EMA, PVT (Pyramid Vision Transformer), Swin Transformer, CvT (Convolutional Vision Transformer), CoaT, LeViT, Segformer, PVT v2, image features, high-level semantic information, structure loss, random horizontal flip, Adam optimizer, masks of various scales, segmentation tasks, natural images, medical images, agriculture, remote sensing, camouflaged object segmentation, shadow detection, polyp segmentation, skin lesion segmentation, leaf disease segmentation, road segmentation, Trans10K-v1, Trans10K-v2, image encoder, mask decoder, classification branch, layer normalization, feed-forward networks (FFN), fine-tuning, calibration loss, attention mechanisms, composite loss, data augmentation, learning rate, optimizer, hyperparameter tuning.",
    "yeeVBMDAwy.pdf.json": "Variance-enlarged Poisson Learning (VPL), V-Laplace, V-Poisson, Variance-enlarged Graph Poisson Networks (V-GPN), graph-based semi-supervised learning, degenerate solutions, labeled data, unlabeled data, label functions, label propagation, regularization term, Poisson equation, graph neural networks, graph Laplacian, ℓp distance, optimization problem, soft label constraints, empirical optimization, class overlap, random walks, edge weights, labeled points, discrete case, variational case, model convergence, machine learning, semi-supervised learning (SSL), theoretical exploration, node classification, attention mechanism, variance term, bias reduction, convexity, weighted p-Poisson equation, machine learning algorithms, classification accuracy, label rates, dataset names (MNIST, FashionMNIST, CIFAR-10, Cora, CiteSeer).",
    "efFmBWioSc.pdf.json": "self-attention, reinforcement learning, offline training, vision-language foundation models, multimodal agent, WebGUM, HTML comprehension, grounded multimodal perception, multi-step reasoning, instruction-following, T5, ViT, LLM, pre-trained models, large-scale dataset, MiniWoB++, PaLM-540B, DOM, GNN, LSTM, Flan-T5, temporal perception, local perception, dataset scaling, model scaling, action prediction, instruction-tuned, task planning, code generation, visual context, interactive decision making, behavior cloning, strong positive transfer, generalization, dataset construction.",
    "viftsX50Rt.pdf.json": "g-GRFs, random walk, weighted adjacency matrix, graph kernels, subquadratic time complexity, modulation function, neural network, pointwise estimation, graph ordinary differential equations, node clustering, kernel regression, Gram matrix, kernel trick, imaging kernel, Euclidean dot products, kernel function, inner product, Hilbert space, kernel evaluations, random features, Monte-Carlo approach, Euclidean kernels, Gaussian kernel, softmax kernel, angular kernel, Laplacian kernel, diffusion kernel, kernel learning, empirical Rademacher complexity, spectral radius, fixed graph kernels, d-regularised Laplacian kernel, neural modulation function, kernel-based machine learning, implicit kernel learning, time evolution, community detection, unweighted adjacency matrix, robust theoretical analysis, efficient approximation, low-rank decomposition, random walkers, computational bottleneck, dimensionality reduction, higher-quality approximation.",
    "XhYWgjqCrV.pdf.json": "MogaNet, ConvNets, multi-order game-theoretic interaction, discriminative visual representation, global-range feature interaction, self-attention, representation bottleneck, adversarial attacks, hierarchical layouts, context aggregation, locality perception, feature extraction, model convergence, kernel size, channel-wise redundancy, Feature Decomposition (FD), Multi-order Gated Aggregation (Moga), Depth-wise Convolution (DWConv), Siamese networks, loss function, Bayesian inference, normalization layer, visual recognition, ImageNet-1K, COCO, ADE20K, 2D human pose estimation, 3D human pose estimation, generative adversarial networks (GAN), spatial mixing block, channel mixing block, hyperparameter tuning, computational overhead, Moga Block, top-1 accuracy, FLOPs, parameters, semantic segmentation, Model Generalizability, effective convolutions, performance trade-offs, parameter efficiency, channel aggregation, Deep Neural Networks (DNNs), batch normalization (BN), Visual Transformers (ViTs), gradient descent, model architectures.",
    "ijK5hyxs0n.pdf.json": "self-attention, metanetworks, Graph Metanetworks (GMNs), graph neural networks (GNNs), neural architectures, parameter graphs, Directed Acyclic Graphs (DAGs), Multilayer Perceptron (MLP), Convolutional Neural Networks (CNNs), multi-head attention layers, normalization layers, residual connections, equivariance, permutation symmetries, Neural DAG Automorphisms, message-passing, expressive power, weight-sharing, CIFAR-10 dataset, DeepSets, Vision Transformers, R-squared, Kendall τ, parameter permutation symmetries, statNN, NP-NFN, 1D CNNs, 2D CNNs, GroupNorm, BatchNorm, Transformers, group-equivariant architectures, neural graph automorphisms, computational graphs.",
    "aOnUe8ah7j.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, panoptic symbol spotting, computer-aided design (CAD), graphic primitives, point cloud segmentation, point transformer, mask2former, attention with connection module (ACM), contrastive connection learning (CCL), KNN interpolation, GAT-CADNet, FloorPlanCAD dataset, vector graphics (VG), symbol recognition, symbol spotting, point cloud analysis, geometric primitives, InfoNCE loss, semantic segmentation, instance segmentation, panoptic segmentation, Mask2Former, graph convolutions networks (GCN), vision transformer (ViT), bilinear interpolation, distance measurement, temperature in contrastive learning, segmentation quality (SQ), recognition quality (RQ), true positive (TP), false positive (FP), false negative (FN), intersection over union (IoU)",
    "NY3wMJuaLf.pdf.json": "federated learning, FL, data heterogeneity, model divergence, performance, FedCOG, complementary data generation, knowledge-distillation-based model training, model correction, Secure Aggregation, plug-and-play, hyperparameter, task-driven loss, Kullback–Leibler divergence, cross-entropy loss, Jensen-Shannon divergence, multi-label dataset, FLAIR, Gaussian noise, Dirichlet distribution, NIID-1, NIID-2, FedAvg, FedProx, SCAFFOLD, MOON, FedSAM, FedDecorr, FedAvgM, FedExP, model consistency, local model training, global model, consensus knowledge, data augmentation, model adjustment, model aggregation, local datasets, gradient correction, update momentum, client drift, data diversity, performance gain, model difference, generalization, personalization, soft labels, hard labels, optimization problem, empirical analysis, extensive experiments, training paradigm.",
    "eNoiRal5xi.pdf.json": "self-attention, hyperparameter tuning, domain generalization, Sharpness-Aware Minimization (SAM), Unknown Domain Inconsistency Minimization (UDIM), loss landscape, generalization, empirical crafting, domain shift, deep learning, PAC-Bayes theory, population risk, model convergence, overfitting, perturbation methods, local parameter region, invariant loss surface, multi-class classification, f-divergence, statistical significance, gradient descent, variance-based optimization, gradient variance, optimization, model parameters, classification loss, domain discrepancy, consistent generalization, sharpness-based methods, flat minima, CIFAR-10-C, PACS, OfficeHome, DomainNet, Leave-One-Out Domain Generalization (LOODG), Single Source Domain Generalization (SDG), loss function, gradient norm, cross-domain inconsistency score, model adaptation, parameter perturbation, empirical validation, upper bound, data perturbation, Hessian matrix, perturbation vector, sampling bias.",
    "ZEZ0CPmoSI.pdf.json": "compressed gradient descent, CGD, matrix-smooth, non-convex objectives, empirical evidence, optimization problems, stochastic gradient descent, SGD, matrix stepsize, layer-wise compression, matrix smoothness, convex optimization, communication complexity, random matrix, sketches, sparser estimator, block-diagonal smoothness, distributed optimization, neural networks, learning problems, convergence properties, hyper-parameters, deterministic algorithms, communication burden, variance reduction, deterministic convergence, fixed matrix stepsize, asymptotic analysis, smoothness matrix, layer-wise structure, gradient norm, Hessian, expected communication complexity, stochastic estimator, optimization techniques, layer-wise normalization, compression guarantees, empirical risk minimization, convex constraints, random sampling, distributed gradient descent.",
    "bshfchPM9H.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, visual question answering, Rapper, Reinforced Rationale-Prompted Paradigm, knowledge distillation, large language models, Reinforcement Learning from NLE Feedback, visual reasoning, interpretability, attention mechanisms, gradient-based activations, Natural Language Explanation, visual-language tasks, VL-NLE, implausibility, hallucination, pre-training, architecture design, model convergence, loss function, VQA-X, e-SNLI-VE, UNITER, Oscar, GPT2, NLX-GPT, S3C, CIDEr, SPICE, LLaMA-65B, RefCLIPScore, mPLUG-Owl, Proximal Policy Optimization, LLM, visual facts, rationale generator, reasoning module, VQA accuracy, quantitative experiments, qualitative experiments.",
    "NjNfLdxr3A.pdf.json": "LoRA, Vector-based Random Matrix Adaptation, VeRA, low-rank matrices, scaling vectors, memory efficiency, finetuning, General Language Understanding Evaluation, GLUE, natural language understanding, natural language generation, E2E, instruction tuning, GPT-4, GPT-3, Llama, Llama2, Alpaca dataset, Vision Transformer, ViT, CIFAR100, Food101, Flowers102, RESISC45, ImageNet-21k, hyperparameter tuning, performance evaluation, parameter-efficient, parameter count, gradient descent, random matrices, random projections, model efficiency, Kaiming initialization, parameter reduction, model adaptation, frozen matrices, neural networks, self-attention, MLP, ablation study, intrinsic dimensionality, task-switching, LayerNorm, bitfit, Adapter tuning, AdaLoRA, low intrinsic dimensionality, optimizer state maintenance.",
    "rxlF2Zv8x0.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, protein engineering, fitness landscape, Tikunov regularization, Gibbs sampling, Graph-based Smoothing (GGS), Model-based optimization, Green Fluorescent Protein (GFP), Adeno-Associated Virus (AAV), MCMC, discrete energy-based models, neural network, directed evolution, protein fitness, machine learning (ML), energy function, Hamming distance, k-nearest neighbor (kNN), Total Variation (TV), graph signal processing, smoothness, Gaussian noise, Boltzmann distribution, residue location, energetic model (EBM), GWG, reinforcement learning, generative models, model-based adaptive sampling (CbAS), Bayesian optimization (BO-qei), dropout, hyperparameter, regularization, experimental noise, median fitness, diversity, novelty, Mean-Squared Error (MSE), sequence-to-fitness mapping, optimization technique, spectral graph theory.",
    "TjGJFkU3xL.pdf.json": "Proximal causal learning, causal effect estimation, unobserved confounders, doubly robust estimator, continuous treatments, delta function, kernel-based estimator, oracle form, influence function, mean square error (MSE), synthetic datasets, Proximal Causal Learning (PCL), causal inference, treatment bridge function, outcome bridge function, Proximal Inverse Probability Weighting (PIPW), Proximal Doubly Robust (PDR) estimator, regression-based models, generalized propensity score, entropy balance-based methods, non-compliance, policy function, propensity score, smooth approximation, computational burden, nuisance function, kernel function, Dirac delta function, Epanechnikov kernel, Gaussian kernels, Generative Adversarial Networks (GANs), conditional normalizing flows (CNFs), Average Causal Effect (ACE), residual mean squared error (RMSE), Rademacher complexity, minimax optimization, sampling distribution, statistical bias, treatment effect, causality, two-stage penalized regression, optimization problem.",
    "eT6oLkm1cm.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, adversarial training (AT), robust overfitting, Annealing Self-Distillation Rectification (ADR), deep neural network (DNN), calibration ability, model consistency, soft labels, consistency regularization, gradient descent, label smoothing (LS), knowledge distillation (KD), Mean Teacher (MT), PGD adversary, projected gradient descent (PGD), loss function, softmax temperature, exponential moving average (EMA), Weight Average (WA), Adversarial Weight Perturbation (AWP), CIFAR-10, CIFAR-100, TinyImageNet-200, ResNet-18, robustness, accuracy, overconfident predictions, adversarial attacks, distribution shift, output entropy, interpolation factor, cosine annealing, JS divergence, noise-aware labels, data distribution, adversarial region, attack budget, temperature scaling, Jacobian-based techniques.",
    "SBj2Qdhgew.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, federated learning (FL), group fairness, global fairness, local fairness, data heterogeneity, partial information decomposition (PID), Unique Disparity, Redundant Disparity, Masked Disparity, Accuracy-Global-Local Fairness Optimality Problem (AGLFOP), convex optimization, statistical parity, mutual information, conditional mutual information, gradient descent, loss function, model convergence, dataset, ADULT dataset, statistical parity gap, interaction information, ensemble learning, FedAvg algorithm, dependency modeling, client fairness, machine learning models, sensitivity analysis, bias mitigation, fairness metrics, information theory, entropy, numerical example, classification error, Pareto frontiers, discrete variable distributions, client distribution.",
    "VTF8yNQM66.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, SWE-bench, language models, software engineering, codebase, model convergence, loss function, evaluation framework, state-of-the-art, software engineering problems, GitHub issues, pull requests, SWE-Llama, BM25, CodeLlama, LMs, training dataset, task instances, programming benchmarks, model predictions, execution environments, complex reasoning, patch generation, neural networks, unit tests, model performance, patch files, fail-to-pass tests, pass-to-pass tests, cross-context code editing, continuous updates, retrieval-based approach, oracle retrieval, metric evaluation, context distribution shifts, Cyclomatic complexity, Halstead complexity, model evaluation, software engineering tools",
    "pA8Q5WiEMg.pdf.json": "self-attention, hyperparameter tuning, online meta learning, Online-Within-Online (OWO), non-convex setting, regret bounds, initialization, step size, task similarity, averaged regret, piecewise Lipschitz functions, generalization bounds, statistical meta learning, Exponentially Weighted Aggregation (EWA), PAC-Bayes generalization error bound, optimization algorithms, transfer risk bound, task-averaged regret, Follow-The-Leader (FTL), Follow-The-Regularized-Leader (FTRL), logarithmic regret, statistical multi-task learning, KL-divergence, distribution assumption, multi-task generalization error, online-to-batch arguments",
    "tuzTN0eIO5.pdf.json": "zero pipeline bubbles, synchronous training, pipeline parallelism, scheduling strategy, backward computation, gradient, pipeline schedules, distributed training, memory limit, algorithm, model configuration, throughput, GPipe, asynchronous PP, one-forward-one-backward (1F1B), tensor parallelism (TP), data parallelism (DP), ZeRO, memory efficient schedule, zero bubble schedule, bubble rate, heuristic algorithm, optimization, transformer architecture, attention heads, memory usage, interconnection, empirical evaluations, compute nodes, NVIDIA A100, eight GPUs, communication bandwidth, automatic pipeline scheduling, mixed strategies, activation memory, peak memory consumption, communication time (Tcomm), numerical robustness, optimization semantics, gradient norm clipping, dynamic scheduling, empirical measurements, mixed precision settings, microbatches, system performances, algorithmic efficiency, empirical evidence.",
    "dTlKCQuuxP.pdf.json": "macro motion analysis, motion analysis, dynamic scene representation, Phase-based neural polynomial Gabor fields (Phase-PGF), dynamic neural representations, low-dimensional representation, spatial domain, frequency domain, neural radiance fields, deformation fields, flow fields, D-NeRF, motion loop detection, motion factorization, motion smoothing, motion magnification, motion separation, wavelet-based neural fields, Gabor basis, periodic motion, motion components, macro motions, motion intensity adjustment, polynomial neural fields (PNF), high-fidelity rendering, adversarial training, patch-based sampling, perceptual loss, Gabor functions, latent feature map, temporal phases, sampling points, volume rendering, cross-correlation, Fréchet Inception Score, neural latent decoder, multi-view videos, 2D videos, 3D dynamic scenes, hybrid representations, inductive bias, temporal manipulation, high-resolution images, spatial and frequency filters, dynamic signals",
    "dl0u4ODCuW.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, retrosynthesis, stochastic processes, retro-fallback, successful synthesis probability (SSP), search algorithm, AND/OR graph, bidirectional graph, buyability, feasibility function, backward reaction model, MCTS, retro*, Gaussian processes, Bayesian neural networks, response surface, chemical reactions, molecular synthesis, reaction prediction model, dynamic programming, heuristics, computational complexity, empirical evaluation, random variables, single-step retrosynthesis, template classifier, generative models, drug discovery, optimization algorithm, reaction model, synthetic accessibility (SA) score, deep learning, cheminformatics.",
    "rzBskAEmoc.pdf.json": "self-attention, multiple instance learning, whole slide images, ContextAware Multiple Instance Learning, neighbor-constrained attention, cancer diagnostic, tumor subtyping, non-small cell lung cancer, TCGA-NSCLC, lymph node metastasis, CAMELYON16, CAMELYON17, weakly supervised learning, diagnostic machines, classification accuracy, WSI classification, feature extraction, transformer-based architectures, bio-topological constraints, convolutional layers, attention mechanisms, attention scores, feature similarity scores, Nystromformer, cross-entropy loss, SimCLR, normalized temperature-scaled cross entropy, deep learning, attention-based pooling, graph-based representation, slide-level prediction, Dice score, specificity, hyperparameter tuning, model interpretability, classification layer, feature aggregator, softmax function, area under the curve, feature representations, local details, global contexts, spatial dispersion, statistical models, feature embeddings, cross-attention, tile-level labels, attention map, predictive accuracy, augmentation techniques, classification task, tile predictions, cancerous regions, contextual information, contrastive learning, self-supervised learning, transformer layer, softmax operation, feature representation, sliding window technique, relational dependencies.",
    "c9xsaASm9L.pdf.json": "correlation mode decomposition (CMD), training dynamics, parameter space, ResNets, Transformers, test set generalization, stochastic gradient descent (SGD), low-dimensional sub-space hypothesis, eigenvectors, training efficiency, dimensionality reduction, data-driven approach, training dynamics modeling, online CMD, embedded CMD, loss landscapes, federated learning, compactly modeled dynamics, communication overhead, modes, trajectory correlation matrix, dynamical analysis techniques, Dynamic Mode Decomposition (DMD), affine transformation, convexity, nonlinearity, empirical studies, random projections, Quasi-Newton training scheme, model convergence, loss function, ablation studies, stability evaluations, benchmark models, architecture design, performance analysis.",
    "v7ZPwoHU1j.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, K-means clustering, semidefinite programming (SDP), K-means optimization problem, statistical optimality guarantees, nonnegative matrix factorization (NMF), Burer–Monteiro factorization, Euclidean space, cost function, NP hard, approximation algorithms, Lloyd’s algorithm, spectral clustering, Gaussian mixture model, membership matrix, positivity constraints, positive semidefinite (psd) matrices, primal-dual gradient descent, linear convergence, projected gradient descent, mixed-integer linear program (MILP), information-theoretic limit, exact recovery, mis-clustering error, augmentation parameter, augmented Lagrangian method (ALM).",
    "WPZ2yPag4K.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, factuality, hallucinations, reinforcement learning from human feedback (RLHF), maximum likelihood, Direct Preference Optimization (DPO), large language models (LLMs), model confidence, preference ranking, error rate, training datasets, FactScore, FacTool, natural language inference, Llama-2, GPT-3.5, confidence scores, atomic claims, external knowledge base, truthfulness estimation, preference pairs, automated fact-checking, retrieval-free approach, objective function, KL-divergence, proximal policy optimization (PPO), Bradley-Terry model, fact-checking, preference learning, loss function",
    "smy4DsUbBo.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, graph neural networks (GNNs), finite element modeling, structure-property relationships, fourth-order tensors, SE(3) equivariance, thermodynamic law, conservation of energy, error metrics, predictive performance, architected material design, positive semi-definite (PSD), Euclidean Equivariant Message Passing Neural Networks (MPNNs), stiffness tensor, strain energy, constitutive law, machine learning methods, message passing, orthotropic symmetry, 3D anisotropic stiffness, VAE model, tensor field networks, numerical methods, eigenvalue decomposition, data augmentation, rotational symmetries, crystal graph convolutions (CGC), particle dynamics, dynamic data augmentation.",
    "anzIzGZuLi.pdf.json": "self-attention, hyperparameter tuning, tabular data prediction, TP-BERTa, relative magnitude tokenization, intra-feature attention, deep neural networks (DNNs), language models (LMs), classification tasks, regression tasks, Gradient Boosted Decision Trees (GBDTs), translatable knowledge transfer, TransTab, XTab, FT-Transformer, RoBERTa, magnitude tokens, triplet loss, feature discretization, C4.5 Discretization algorithm, numerical feature handling, automatic model training, AUC (Area Under Curve), multi-head self-attention (MHSA), semantics of feature names, binary classification, categorical features, decision tree, supervised loss, numerical encoding strategy, training paradigm, feature type distribution, evaluation metrics.",
    "PczQtTsTIX.pdf.json": "CrossQ, sample efficiency, deep reinforcement learning, update-to-data (UTD) ratio, gradient update steps, computational cost, Batch Normalization, state-of-the-art, soft actor-critic (SAC), Randomized Ensembled Double Q-Learning (REDQ), Dropout Q functions (DroQ), continuous control tasks, actor-critic algorithm, maximum entropy reinforcement learning, Q function, policy π, Markov Decision Process (MDP), policy delay, temporal difference (TD) learning, gradient-based optimization, Q value estimation bias, wider critic layers, empirical investigations, model-free off-policy reinforcement learning, learning performance, algorithmic design complexity, reinforcement learning (RL), layer normalization (LayerNorm), mixed moments, Batch Renormalization (BRN), synthetic data generation, action-value function, state space, action space, reward function, discount factor, critic networks, exploration-exploitation trade-off, wallclock time, local minima, ablation studies.",
    "ZDGKPbF0VQ.pdf.json": "Advantage-Leftover Lunch RL (A-LOL), Reinforcement Learning with Human Feedback (RLHF), Language Model (LM), offline policy gradient algorithms, sequence-level classifiers, human-designed scoring functions, positive advantage, sample-efficient, Proximal Policy Optimization (PPO), hyperparameters, training instability, mode collapse, negative log-likelihood loss (NLL), weighted Behavior Cloning, reward-based methods, importance weight, reward functions, Commonsense Reasoning, R-LOL, DPO, PRO, cross entropy loss, multi-head attention, loss function, training data advantage, negative advantage, sequence-to-sequence task, training stability, language generation tasks, helpfulness, safety, human preference labels, validation set, Reddit response generation, quality improvements, diversity measures, generative responses.",
    "a745RnSFLT.pdf.json": "zero-shot learning, prompted vision-language models, performance, overfitting, PAC-Bayes bounds, generalization bounds, ImageNet, handcrafted prompts, greedy search, model selection, prompt engineering, deep learning, datadependent bounds, KL divergence, pretrained foundation models, CLIP, ALIGN, weak supervision, task specification, text embeddings, image embeddings, empirical risk minimization, prompt length, discrete prompting methods, empirical success, generalization error, statistical guarantees, training data, language model, discrete hypothesis space, risk, complexity measure, learning algorithm, VC-dimension, margin, norm-based bounds, loss function, prompt search, structural risk minimization, prior distribution, posterior probability, token embeddings, language encoder, image encoder, cosine similarity, classification tasks, zero-shot image classification, text-to-image generation, supervised learning, data contamination, generalization ability, small data regime, linear probe, gradient-based methods, soft prompts, natural language processing, tokenization, feature extractors, random labels, evaluation datasets, performance guarantees.",
    "ViNe1fjGME.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, deep graph clustering, representation learning, unsupervised scenarios, temporal graphs, static graphs, computational consumption, Temporal Graph Clustering, TGC, clustering-oriented, adjacency matrix, dynamic information, node clustering, K-means algorithm, temporal information mining, clustering module, historical neighbor sequence, node assignment distribution, graph reconstruction, clustering loss, temporal loss, conditional interaction intensity, negative sampling, Student's t-distribution, KL divergence, cosine similarity, batch processing, memory overflow problem, temporal relationship, dynamic interactions, dataset limitation, interaction sequence, adjacency relationships, temporal conditional intensity, transferability, HTNE, GAE, DFCN, SDCN, TGN, TREND.",
    "YCPDFfmkFr.pdf.json": "convex quadratic programming, optimization layers, primal-dual augmented Lagrangian, differentiability, closest feasible solutions, Jacobian, implicit differentiation, conservative Jacobian, numerical robustness, learning tasks, QP solutions, model convergence, differentiating through optimization, optimization problems, training layers, feasibility, learning models, predictive performance, optimization techniques, neural network architectures, open-source software, QPLayer, mixed integer linear programming (MILP), parameterization, optimal shift, learning frameworks, gradient descent, optimization procedures, automatic differentiation, strong assumptions, learning capabilities, architecture design, C++ software, primal feasibility, classical ℓ2 sense, batch training, MSE loss, learning rate, constraint matrix, computational efficiency, performance evaluation, model training, existing QP layers.",
    "usrChqw6yK.pdf.json": "zero-shot capability, vision language models, open-vocabulary object detection, detector training, region embeddings, categorical labels, fine-grained text description, Descriptor-Enhanced Open Vocabulary Detector (DVDet), conditional context prompts, hierarchical textual descriptors, region-text alignment, large language models (LLMs), interactive knowledge repository, box-level annotations, text-image alignment, ViLD, CLIP, image encoder, text encoder, coarse and category-level alignment, descriptor-level alignment, Conditional Context visual Prompt (CCP), feature-level visual prompt, hierarchical update mechanism, descriptor merging, semantic selection strategy, open vocabulary benchmarks, COCO, LVIS, mean Average Precision (mAP), Faster R-CNN, ResNet50, CenterNet2, visual prompt learning, prompt-based methods, multi-object detection, descriptor hierarchy, fine-grained descriptors, PASCAL VOC, contextual background information, iterative interaction, knowledge distillation, alignment abilities",
    "OI3RoHoWAN.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, simulation data, scene-level diversity, task-level generalization, large language models (LLMs), GENSIM, goal-directed generation, exploratory generation, GPT4, multitask policy training, sim-to-real adaptation, robotic policies, architecture design, language model finetuning, task library, visuomotor policies, task description, Ravens benchmark, code generation, programming synthesis, domain randomization, policy learning, task and motion planning, physical grounding, retrieval augmented generation (RAG), affordance predictions, language-conditioned behavior cloning, task dataset, program synthesis, generative models, manipulation tasks, task statistics, language models in robotics, generative task creation, enhanced adaptability, expert demonstrations, task progression, benchmarking LLMs, multi-task training, reduced overfitting, demonstration trajectory, zero-shot transfer, task achievability, model convergence, robotic simulations, construct validity, coprimary coding systems, simulation pretraining, generalization capabilities",
    "dcjtMYkpXx.pdf.json": "RLHF, overoptimization, gold reward model, proxy reward model, ensemble-based optimization, conservative optimization, worst-case optimization (WCO), uncertainty-weighted optimization (UWO), best-of-n sampling (BoN), proximal policy optimization (PPO), KL penalty, label noise, human feedback, reward models, language models, synthetic human feedback, model fine-tuning, performance improvement, ensemble methods, hyperparameters, scaling laws, Alpaca dataset, Pythia models, model convergence, sampling methods, enhancement techniques, instruction-following capabilities, robustness, reward approximation, empirical success, preference labels, data scaling, policy optimization, mean optimization, variance, training dataset size.",
    "Tuh4nZVb0g.pdf.json": "Time-Series, LLM-for-TS, TS-for-LLM, TEST, TS embedding, prompt tuning, Contrastive Learning, instance-wise contrast, feature-wise contrast, text-prototype-aligned contrast, large language models, tokenization, soft prompts, model-centric design, data-centric representation, downstream tasks, TS classification, forecasting, representation tasks, supervised fine-tuning, SFT, gradient descent, LSTM, CNN, RNN, transformer, pattern machine, embeddings, similarity-based embedding, prototype selection, instance discrimination, temporal-level contrast, clustering-level contrast, TCN, GELU, dilated convolution, statistical models, machine learning models, few-shot learning, multivariate time series, univariate time series, SOTA TS models, SVM, Bayes theorem.",
    "NxoFmGgWC9.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, generative pre-trained models, visual robot manipulation, GR-1, language-conditioned visual robot manipulation, large-scale video generative pre-training, end-to-end learning, robot data finetuning, CALVIN benchmark, video prediction, multimodal learning, action prediction, long-horizon tasks, generalization capabilities, multi-task learning, causal transformer, language input, visual input, robot state input, contrastive learning, decision transformer (DT), pre-training, video generative pre-training, Ego4D dataset, smooth-L1 loss, binary cross entropy (BCE) loss, model predictive control, zero-shot unseen scene generalization, unseen language generalization, robotics data, CLIP, ViT, pre-trained models, LLMs, RT-2, GATO, VIMA, VIPER, SpawnNet, GAN, CNN, LSTM, sequential decision making, task domain, teacher-student model, random sampling, masked image modeling.",
    "abL5LJNZ49.pdf.json": "state changes, procedure planning, instructional videos, structured state space, causal relations, sequence modeling, visual state observations, language state descriptions, commonsense knowledge, chain-of-thought prompting, cross-modal contrastive learning, large language models (LLMs), SCHEMA, mid-state prediction, step prediction, autoregressive Transformers, policy learning, probabilistic modeling, diffusion models, weak supervision, StepFormer, video procedural captioning, Action-modifying steps, temporal relations, visual feature extractor, classification of steps, masked token prediction, contrastive loss, mid-state learning, visual-language alignment, Ground-truth steps, state-modifying actions, benchmark datasets, CrossTask, COIN, NIV, state description generation, success rate, mean accuracy, mean intersection over union (mIoU), video annotations, reinforcement learning, generative adversarial framework, event information extraction.",
    "mMaQvkMzDi.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, ResNet, BERT, ImageNet, MNIST, architecture design, model convergence, loss function, LSTM, CNN, SVM, GAN, thermodynamic equilibrium, quantum entanglement, protein folding, Large Language Models (LLMs), Large Multimodal Models (LMMs), Flamingo model, VQA accuracy, instruction tuning, RLHF, in-context learning (ICL), Multitask-ICL, Chain-of-Hindsight-ICL, Self-Correcting-ICL, object hallucinations, answer abstention, compositionality, explainability, instruction following, OpenAI, IDEFICS, Open Flamingo (OF), CLIP-ViT, VQA, CIDEr, TDIUC, CREPE, SugarCREPE, VQA-X, LlaVA, multimodal instruction datasets.",
    "uvFhCUPjtI.pdf.json": "Evolving Graph Fourier Transform, EFT, spectral transform, temporal graphs, evolving graph spectra, optimization, Laplacian, pseudo-spectrum, spectral GNNs, inverse transform, discrete Fourier Transform, DFT, Graph Fourier Transform, GFT, eigendecomposition, Eigenvalue Decomposition, joint graph, computationally efficient, spatio-temporal graph-neural network, Kronecker product, signal variation, variational characterization, frequency domain, spectral methods, Chebyshev polynomial basis, node classification, link prediction, dynamic graphs, temporal characteristics, spectral energy, compactness, filtering module, global interactions, noise signals, dynamic graph signal, sequential recommendation, temporal representation learning, large-scale datasets, state-of-the-art performance, temporal domain frequency components, architectural design, loss function, model convergence, deep learning models, temporal aspect, temporal edge, dynamic graph structures, memory updater, vanishing gradient, temporal signal, frequency components, polynomial filter, convolution matrix, signal reconstruction, structural noise, variations in signals, performance metrics, experimental validation, theoretical foundations.",
    "dN4vpVTvWX.pdf.json": "TUVF, Texture UV Radiance Fields, 3D content creation, mixed reality, digital twins, 3D shape modeling, realistic textures, GANs, single-view images, texture representation, canonical UV sphere, Canonical Surface Auto-encoder, self-supervised, surface rendering, volumetric rendering, texture mapping network, RGB image, point-based radiance field, adversarial loss, Chamfer Distance, generative adversarial networks, variational autoencoders, density fields, Patch-based Discriminator, Frechet Inception Distance, Kernel Inception Distance, 3D reconstruction, neural radiance fields, NeRF, CIPS-UV, style embedding, texture synthesis, texture transfer, texture editing, diverse textures, image manipulation, CIPS generator, equirectangular projection, point clouds, implicit function, training objectives, deep learning.",
    "dnqPvUjyRI.pdf.json": "self-training, pseudo labeling, confirmation bias, high-quality pseudo labels, fast convergence, task versatility, Semi-supervised Reward framework, SemiReward, reward scores, pseudo-label selection, generator model, subsampling strategy, classification tasks, regression tasks, SSL benchmarks, empirical studies, consistency regularization, label similarity, cosine similarity, one-hot encoding, soft one-hot encoding, pseudo-label generation, pseudo-label selection, supervised loss, unsupervised loss, rewarder network, MLP, cross-attention module, Ce, Adam optimizer, Mean Teacher, Pseudo Label, FlexMatch, Free/SoftMatch, confidence-based thresholding, CRMatch, consistency loss, gradient descent, model fS, model fT, ImageNet, CIFAR-100, UrbanSound8k, AG News, UDA, coupon.",
    "OF5x1dzWSS.pdf.json": "adversarial training, instance reweighted adversarial training, importance weights, distributionally robust optimization, KL-divergence, theoretical convergence guarantee, doubly robust instance reweighted AT framework, minimax optimization problem, average robust performance, hyperparameter tuning, geometric interpretations, model capacity, robust performance, adversarial attacks, PGD attacks, standard classification datasets, Bilevel optimization, compositional bilevel optimization, heuristic techniques, log-barrier penalty method, implicit function theorem, stochastic gradient descent, CIFAR10, GTSRB, STL10, SVHN, Gaussian noise, weak data points, robust accuracy, loss function, model convergence, neural networks, instance reweighting, data augmentation, robust overfitting, image recognition datasets, robust performance metrics, adversarially robust networks.",
    "iAW2EQXfwb.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, ensemble reinforcement learning, negatively correlated sub-policies, dynamic selector policy, policy regularisation, Markov decision process, state-of-the-art, level generation benchmark, reward functions, Pareto front, online level generation, experience-driven procedural content generation, reinforcement learning, EDRL, diversity, action space, trajectory distribution, stochastic policy, multimodal ensemble policy, Gaussian mixture models, negative correlation regularisation, 2-Wasserstein distance, policy iteration, policy gradient, deep reinforcement learning, cumulative reward, diversity of generated levels, asynchronous off-policy training framework, soft-actor critic, Qϱ-function, ensemble size, regularisation coefficient, Mario level generation, novelty search, quality-diversity search, population-based RL, exploration, reward evaluation, generative adversarial networks, reinforcement learning algorithms.",
    "okYdj8Ysru.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, Deep Neural Networks (DNNs), Riemannian normalization, Riemannian Batch Normalization (RBN), Symmetric Positive Definite (SPD) manifolds, Lie groups, Euclidean geometry, Riemannian geometry, Batch Normalization (BN), weighted Fréchet mean (WFM), Fréchet mean, Riemannian mean, Riemannian variance, Affine-Invariant Metric (AIM), Log-Euclidean Metric (LEM), LogCholesky Metric (LCM), Karcher flow algorithm, SPD neural networks, Special Orthogonal (SO) groups, Special Euclidean (SE) groups, Riemannian metric, group operation, tangent space, population statistics, maximum likelihood estimator (MLE), convexity constraint, probabilistic density function (PDF), geodesic distance, pullback metric, left-invariant metric, matrix Lie groups, deformation concept, normalization layers, internal covariate shift, Cholesky decomposition, momentum, empirical experiments, LieBN, deep learning, domain adaptation, normalization techniques, experimental evaluation, statistical moments.",
    "YPIA7bgd5y.pdf.json": "self-attention, in-context learning (ICL), Large Language Models (LLMs), gradient descent, Bayesian inference, few-shot ICL, zero-shot predictions, accuracy, log likelihood, entropy, probabilistic metrics, input-label relationship, pre-training preference, conventional learning algorithm, label randomization, label relationships, authorship identification, LLaMa-2, Falcon, SST-2, AG News, MRPC, RTE, WNLI, MQP, Hate Speech, Financial Phrasebank, task formulations, model size, input token limit, probabilistic learning, prediction preferences, example demonstrations, model parameters, instruction-tuned models, label-independent learning, label-dependent learning, context size, training dynamics, prediction certainty, prompt-based alignment.",
    "c0MyyXyGfn.pdf.json": "reinforcement learning, scalar reward functions, continuous space, lexicographic multi-objective reinforcement learning, prioritized subtasks, soft Q-decomposition, PSQD, value decomposition, zero-shot composition, state-action spaces, offline learning, action indifference space, scalarized reward function, Markov decision process, ε-lexicographic MORL, Q-decomposition, maximum entropy reinforcement learning, soft Q-learning, subtask transformation, KL-divergence, DNN, high-dimensional control, robot control tasks, action selection, conditional distribution, subtask priorities, monolithic policy, empirical approximation, temporal-difference loss, adaptive learning, iterative learning, training data, heuristic representation, state-based priority, ε-optimal actions, subtask Q-functions, exploration framework, priority constraints.",
    "WvFoJccpo8.pdf.json": "large language models, LLMs, quantization-aware low-rank adaptation, QA-LoRA, group-wise operators, low-rank adaptation, LoRA, parameter-efficient fine-tuning, PEFT, quantization, low-bit integers, fine-tuning, memory usage, computational burden, LLaMA, LLaMA2, INT4, zero-shot, few-shot, MMLU benchmark, scaling factors, zero parameters, quantization errors, post-training quantization, PTQ, adaptive quantization, dynamic quantization, weight matrix, scaling and zero factors, quantized model, inference speed, computational efficiency, language understanding, stochastic rounding, learned rounding, switchback layers, mixed-precision inference, QLoRA, NF4, FP16, FP32, hyperparameters, training time, Alpaca, FLAN v2, mass multitask language understanding, parameter distribution, scaling and zero parameters, dimensionality reduction, pre-trained weights, low-rank matrices, fast inference, CUDA, loss function, gradient descent",
    "jOm5p3q7c7.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, Markov decision process (MDP), sample complexity, upper bound, lower bound, cardinality, mixing time, error tolerance, optimal policy, average reward MDP (AMDP), uniformly ergodic, Cauchy convergence, reinforcement learning (RL), mixing, mixing time parameter (tmix), stationary Markov deterministic policy, Poisson's equation, empirical MDP model, dynamic programming, Q-learning, variance-reduced Q-learning, discounted MDP (DMDP), span semi-norm, transition kernel, average reward, optimal average reward, minorization time, discounted value function, Bellman equation, empirical transition matrix, stochastic mirror descent (SMD), sample size, performance measure, algorithm design, analytical techniques, algorithmic complexity, statistical representation, empirical achievements, numerical experiments, asymptotic analysis, state space Markov chains, model-based algorithms, model-free learning, policy learning, finite state action spaces, transition matrix, upper complexity bound, lower complexity bound, worst-case analysis.",
    "kUuKFW7DIF.pdf.json": "self-supervised learning, multi-resolution information, speech self-supervised representation learning, hierarchical Transformer architecture, HuBERT-style masked prediction objectives, inference efficiency, LibriSpeech, Speech Universal PERformance Benchmark, Multilingual SUPERB, speaker verification, speech enhancement, voice conversion, Automatic Speech Recognition, short-term Fourier transform, convolutional feature extractor, Mel filter banks, frame-wise processing, K-means algorithm, masked unit prediction, multi-resolution HuBERT, MR-HuBERT, hidden representations, upsampling module, downsampling module, training configuration, hyperparameter, Word Error Rate, Phone Error Rate, Character Error Rate, accuracy, BLEU, Short-Time Objective Intelligibility, Perceptual Evaluation of Speech Quality, Scale-Invariant Signal-to-Distortion Ratio improvement, MACs, transformer layers, generative models, contrastive models, predictive models, embedding, recurrent neural network, convolutional neural network.",
    "sLkj91HIZU.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, mixture models, transformers, neural sequence models, generative process, decision-theoretic optimal procedure, data-driven exponential weights, mean-squared error, sample-efficient, robust, federated learning, crowdsourcing, recommendation systems, linear regressions, subpopulations, i.i.d data, linear functions, Bayes-optimal error, linear regression, deep learning architecture, gradient descent, mixed linear regression, empirical observations, polynomial functions, multi-layer perceptron (MLP), stochastic gradient descent, noise level, covariate distribution, hypercontractivity, spectral norm, weight scaling, weight shifting, softmax, Gaussian error linear unit (GeLU), decision-theoretic optimal method, in-context learning, large language models (LLM), empirical performance, posterior mean, expectation-maximization (EM), ridge regression, mixture distribution",
    "ZULjcYLWKe.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, offline reinforcement learning (RL), robustness, state observation perturbations, sensor errors, adversarial attacks, Diffusion Model-Based Predictor (DMBP), conditional diffusion models, state-based RL tasks, error accumulation, non-Markovian training objective, sum entropy, denoised states, multiple-layer perceptron (MLP), diffusion-based generative models, trajectory generators, behavior cloners, Markov Decision Processes (MDP), state-action-reward transitions, reinforcement learning policy, Q-value, temporal difference (TD) method, actor-critic framework, policy regularization, conservative Q estimation, Gaussian noise, stochasticity, robustness against perturbations, uncertain observations, adversarial training, state-adversarial Markov decision process (SA-MDP), Batch Constrained deep Q-learning (BCQ), Conservative Q-Learning (CQL), TD3 with Behavior Cloning (TD3+BC), Diffusion Q-Learning (Diffusion QL), Robust Offline Reinforcement Learning (RORL), noise prediction, data inpainting, U-net, uncertainty in actions, uncertainty in transitions and rewards.",
    "h7DGnWGeos.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, retrosynthetic planning, synthetic routes, chemical experiments, Active Retrosynthetic Planning (ARP), actor-critic framework, reaction quality, route quality evaluation, single-step reaction prediction, multi-step planning, yield, toxicity, material costs, work-up difficulty, Markov decision process (MDP), reward function, query cost, action space, state space, reinforcement learning, TD3 algorithm, cumulative product, normalized route quality, reinforcement learning-based, neural-based A*-like algorithm, building block materials, SMILES sequences, graph-based search policy, uncertainty of stochastic retrosynthetic planning, query rate, success rate, expert dataset, USPTO-50k benchmark dataset, training procedure, Monte-Carlo tree search (MCTS), UCT (Upper Confidence Bound applied to Trees), method in Guo et al. (2020), transition function, reaction quality annotations, template-based methods, template-free methods, semi-template-based methods, reaction candidates, embedding, feature extraction, trajectory collection, query decisions, a prior knowledge, Retro*, GRASP, eMolecules, and-size reaction candidate pool.",
    "qae04YACHs.pdf.json": "self-attention, Transformer-Modulated Diffusion Model (TMDM), conditional diffusion generative process, multivariate time series (MTS) forecasting, uncertainty estimation, historical time series data, covariate-dependence, transformer-based methods, probabilistic forecasting, prediction interval coverage probability (PICP), quantile interval coverage error (QICE), conditional mean, Kullback–Leibler (KL) divergence, denoising diffusion probabilistic model (DDPM), Gaussian distribution, distribution forecasting, score matching, Langevin dynamics, hybrid optimization, sequence dependencies, time series characteristics, latent variable, neural networks, Continuous Ranked Probability Score (CRPS), Error metrics, Bayesian framework, time series information extraction, noise model, forward and reverse processes, diffusion probabilistic models, empirical evaluation, model integration, plug-and-play framework, state-of-the-art (SOTA), training stability, transformer structures, evaluation metrics.",
    "Th6NyL07na.pdf.json": "self-attention, large language models (LLMs), hallucinations, pretraining, decoding strategy, next-token distribution, logits, transformer layers, Decoding by Contrasting Layers (DoLa), truthfulness, maximum likelihood language modeling, KL divergence, mass-seeking behavior, linguistic knowledge, knowledge neurons, pretrained BERT, autoregressive LM, contrastive decoding, Jensen-Shannon Divergence (JSD), LLaMA models, multiple choices tasks, TruthfulQA, FACTOR, open-ended generation tasks, chain-of-thought reasoning, StrategyQA, GSM8K, GPT-4, embedding layer, transformer layers, affine layer, next-word prediction, dynamic layer selection, premature layer, mature layer, factual knowledge, adaptive plausibility constraint (APC), repetition penalty, model interpretability, contrastive decoding approach, context-aware decoding (CAD), Autocontrastive Decoding (ACD)",
    "MIEnYtlGyv.pdf.json": "E(3)-equivariant, autoregressive model, 3D molecular geometries, molecular fragments, G-SchNet, G-SphereNet, message-passing, rotationally invariant features, spherical harmonic signals, QM9 dataset, generative models, chemically valid configurations, stable molecular structures, quantum mechanics, angular frequency, spherical harmonics, orthonormal basis, E(3)-invariant, E(3)-equivariant, focus distribution, target species distribution, target position distribution, permutation-equivariant, permutation-invariant, multi-layer perceptron (MLP), spherical coordinates, radial component, bond lengths, bispectrum, Maximum Mean Discrepancy (MMD), valid completion rate (VCR), inference speed",
    "vXxardq6db.pdf.json": "sliceGPT, model compression, transformer networks, computational invariance, post-training sparsification, embedding dimension, large language models, parameter pruning, weight matrices, inference time, speedup, zero-shot tasks, multi-head self-attention, feed-forward network, LayerNorm, RMSNorm, distillation, tensor decomposition, low-rank factorization, orthogonal matrix, principal component analysis, structured sparsity, unstructured pruning, quantization, transformer architecture, loss function, perplexity, foundation model, autoregressive models, weight removal, Optimal Brain Surgeon (OBS), Optimal Brain Compression (OBC), GPTQ, SparseGPT, embeddings, attention block, Feed Forward Network (FFN), perplexity score, calibration dataset, HuggingFace Transformers, PyTorch, WikiText-2, Quadro RTX6000, A100 GPUs, LLAMA-2, OPT, generative models, data movement, computational complexity, task performance, eigenvectors, signal matrix, PCA, eigenvalues.",
    "fJNnerz6iH.pdf.json": "hypernetworks, image generation, multi-task learning, training instability, convergence, hyperparameter choices, magnitude proportionality, optimization, Magnitude Invariant Parametrizations (MIP), Bayesian optimization, generative models, amortized model learning, continual learning, meta-learning, gradient clipping, gradient variance, stochastic gradient descent, architecture design, parameter initialization, exploding gradients, vanishing gradients, weight normalization, adaptive optimization, stochastic gradients, Adam, SGD, LeakyReLU, Fourier features, residual forms, MNIST, ResNet, OxfordFlowers-102, OASIS dataset, U-Net, parameter uncertainty, model calibration, hyperparameter initialization, training dynamics, normalization layers, optimization problem, hypernetwork architecture, feature normalization, Kaiming initialization, Glorot initialization, activation function, empirical analysis.",
    "qxLVaYbsSI.pdf.json": "technical concepts, federated semi-supervised learning, FSSL, gradient conflicts, Twin-sight, mutual guidance, neighbourhood-preserving constraint, objective functions, supervised model, unsupervised model, label deficiency, semi-supervised learning, decentralized learning, pseudo-labeling, teacher-student models, cross-entropy loss, instance discrimination, model convergence, client drift, data heterogeneity, FedAvg, FedProx, SCAFFOLD, semi-supervised federated learning, SemiFL, consistency training, MixMatch, FixMatch, RemixMatch, self-supervised learning, contrastive methods, SimCLR, InfoNCE loss, dataset partitioning, CIFAR-10, SVHN, Fashion-MNIST, CIFAR-100, hyper-parameters, data distribution, memory overhead, communication overhead, dual-model paradigm, manifold perspective, Twin-sight loss, gradient conflict, experimental settings.",
    "ZZTkLDRmkg.pdf.json": "elliptic partial differential equations, PDEs, time-independent PDEs, fluid dynamics, plasma physics, solid mechanics, neural operators, Boundary-Embedded Neural Operators, BENO, architecture, complex geometries, inhomogeneous boundary values, Green’s function, Graph Neural Networks, GNNs, Transformer encoder, latent vector, message passing layer, model convergence, loss function, finite element methods, FEM, finite difference methods, FDM, boundary value problems, Laplace equation, Poisson equation, interior source term, boundary values, dual-branch network, message passing neural network, BE-MPNN, mean squared error, MSE, relative L2 error, MAE, graph construction, Delaunay triangulation, mesh graph, K-nearest nodes, boundary shape, boundary condition, inhomogeneous boundary condition, geometric center, attention mechanism, self-attention, multi-head self-attention, edge features, embeddings, temporal distribution shift, FNO, Fourier neural operator, GKN, GNN-PDE, MP-PDE, physics-informed neural networks, PINNs, operator kernel, group theory.",
    "eAKmQPe3m1.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, T2I, PIXART-α, Transformer, diffusion model, image generation, Imagen, SDXL, Midjourney, pixel dependency, text-image alignment, image aesthetic quality, cross-attention, Diffusion Transformer, DiT, Vision-Language model, auto-labeling, LAION, SAM, JourneyDB, DALL·E 2, Stable Diffusion, RAPHAEL, FID, T2I-CompBench, Fréchet Inception Distance, generative models, concept density, training cost, CO2 emissions, image synthesis, aesthetic data, reparameterization, training strategy decomposition, latent features, multi-aspect augmentation, AdamW optimizer, LLaVA, GAN, DDPM, Latent Diffusion Model.",
    "nFMS6wF2xq.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, conditional diffusion models, cross-modal context, text-visual relationships, forward process, reverse process, diffusion models, CONTEXTDIFF, DDPMs, DDIMs, spatio-temporal attention, text-to-image generation, text-to-video editing, CLIP latent space, noise schedule, Gaussian distribution, Denoising Diffusion Probabilistic Models, Denoising Diffusion Implicit Models, variational bound, negative log likelihood, KL divergence, relational network, cross attention, multimodal data, encoder, attention layers, prompt decomposition, empirical results, context-aware trajectory adapter.",
    "TYXtXLYHpR.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, TIMEVIEW, eXplainable Artificial Intelligence (XAI), Transparent models, Linear Regression, Logistic Regression, Generalized Additive Models (GAMs), Decision Trees, trajectory comprehension, bi-level transparency, time series forecasting, static features, motifs, compositions, B-Spline basis functions, saliency methods, feature importance, counterfactual reasoning, black-box models, L2 regularization, model visualization, regression model, classification model, dynamic motifs",
    "LNLjU5C5dK.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, reinforcement learning from human feedback (RLHF), supervised fine-tuning (SFT), imitation learning, FIGA, large language models (LLMs), alignment dataset, loss function, response-level quality signal, fine-grained quality signals, behavior cloning, reward collapse, proximal policy optimization (PPO), reward model (RM), alignment without reinforcement learning (RL), rejection-sampling, quantile ranking, token-level weights, Levenshtein distance, alignment learning, aligned language models, Alpaca, LLaMA, DeBERTa, ChatGPT, SPA dataset, SFT performance, hyperparameters selection, bad cases, good behaviors, bad behaviors, downstream tasks, version of reinforcement learning (RL), training instability, quality signals, knowledge utilization, human alignment, open-ended generation, dynamic programming algorithm, edit distance, fine-tuning method, response revision, preference datasets, instruction dataset.",
    "fh8EYKFKns.pdf.json": "self-supervised learning, reinforcement learning from human feedback (RLHF), artificial general intelligence (AGI), reward hacking, situational awareness, capability misgeneralization, goal misgeneralization, internally-represented goals, broadly-scoped goals, power-seeking behavior, human feedback, alignment problem, reward function, model-free reinforcement learning, neural networks, deep learning, gradient calculations, emergent behaviors, empirical analysis, large language models, deep reinforcement learning, reward misspecification, deceptive alignment, instrumental convergence, training distribution, distributional shift, multi-task learning, sample efficiency, cross-task generalization, multi-step reasoning, large foundation model, ChatGPT, InstructGPT, zero-shot learning, advanced planning, architecture design, empirical findings, theoretical findings, feedback mechanisms.",
    "WjRPZsfeBO.pdf.json": "Variational Autoencoders, Wasserstein Autoencoders, feature representations, latent space, statistical guarantees, model efficiency, Generative Adversarial Networks (GANs), conditional VAE (CVAE), InfoVAE, β-VAE, VQ-VAE, image generation, text generation, speech synthesis, drug discovery, DALL-E model, multi-modal distributions, adversarial losses, Wasserstein distance, optimal transport, data distributions, low-dimensional structure, high-dimensional feature space, expected excess risk, intrinsic dimension, Principal Component Analysis (PCA), excess risk analysis, M-estimation, Vapnik-Chervonenkis (VC) dimension, f-WAEs, encoding guarantees, decoding guarantees, Total Variation (TV) distance, empirical measure, Maximum Mean Discrepancy (MMD), neural networks, Lipschitz-smoothness, error analysis, encoder, generator, reconstruction cost, regularizers, statistical properties, hyperparameter tuning, gradient descent, optimization error, learning theory, intrinsic dimensionality, empirical success, latent codes, reconstruction issues, empirical objective, dissimilarity measures.",
    "xhEN0kJh4q.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, fitness landscape, model-based optimization, sparse training set, separation in design space, protein engineering, directed evolution, model-guided searching schemes, sequence-fitness relationship, Bayesian optimization, generative models, VAE, Property-Prioritized Generative Variational Auto-Encoder (PPGVAE), Gaussian processes, Gaussian mixture model (GMM), noise reduction, reward-weighted regression (RWR), cross entropy method (CEM-PI), property oracle, iterative process, maximum likelihood estimation (MLE), functional proteins, training samples, optimization success, latent space, i.i.d., variance, temperature, hyperparameter, physics-informed neural networks (PINN), discrete design spaces, continuous design spaces, AAV dataset, semi-synthetic protein datasets, fitness measurements, improvements, sampling budget, deep valleys, exploration, exploitation, significant bottleneck.",
    "d94x0gWTUX.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, reinforcement learning from human feedback (RLHF), reward modeling (RM), Themis, dynamic reasoning, tool engagement, task-specific tool engagement, reasoning traces, interpretive capacity, scoring reliability, Gopher 280B, TruthfulQA, preference ranking, tool-related RM datasets, external environments, external tools, toolbank, tool invocation, multi-tools, tool utilization, Chain-of-Thought, ReAct, auto-regressive training, pair-wise ranking loss, auto-regressive language modeling loss, GPT-4, Vicuna-7B, Burt-Large, TARA, human preference evaluation, tool-invoked processes generation, mathematical problem-solving, model convergence, loss function, sigma function, reward generation process, prompt, observation, rationale, tool-agent, reasoning architecture, model performance, truthfulness, factuality, scaling trends, hyper-parameter tuning, multi-agent interactions, tool-oriented learning, tool-augmented learning, external knowledge, knowledge-based queries, comprehensive dataset, performance metrics, human labelers.",
    "hkSjjs4o5d.pdf.json": "differentially private algorithms, well-clustered graphs, subgraph, high inner conductance, low outer conductance, spectral clustering, (ε,δ)-DP algorithm, ground truth clusters, cluster recovery, k-partition, misclassification ratio, non-private algorithms, statistical analysis, privacy-preserving, differential privacy, stochastic block models, cluster structure, semi-definite program (SDP), strong convexity, Laplacian matrix, adjacency matrix, graph clustering, k-means, inner conductance, outer conductance, Cheeger inequality, spectral embedding, normalized indicator vectors, Gaussian mechanism, ℓ1-sensitivity, ℓ2-sensitivity, experimental evaluation, SBM (stochastic block model), gradient descent, empirical trade-off, approximation algorithms, statistical stability, spectral methods, algorithm utility, clustering objectives, random response, error analysis, polynomial-time algorithms, optimization problem, unweighted graphs, sensitivity analysis, Gram matrix, eigenvalues, eigendecomposition.",
    "PXNrncg2DF.pdf.json": "Open-world entity segmentation, Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), Segment Anything Model (SAM), self-supervision, self-exploration, self-instruction, self-correction, pseudolabels, visual feature clustering, Mask2Former, teacher-student mutual-learning, DINO, ViT-B/8 architecture, agglomerative clustering, hierarchical segmentation, semi-supervised learning, confidence scores, dynamic threshold, average recall (AR), segmentation masks, SA-1B dataset, MS-COCO, LVIS, ADE20K, EntitySeg, FreeSOLO, CutLER, protein folding, self-supervised visual representation learning, noise reduction, image segmentation, multi-granularity analysis",
    "sBQwvucduK.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, 3D control, street view generation, Bird’s-Eye View (BEV), geometry control, object shapes, occlusion patterns, road surface elevations, perception data synthesis, 3D object detection, MAGICDRIVE, 3D geometry controls, camera poses, road maps, 3D bounding boxes, textual descriptions, cross-view attention module, high-fidelity synthesis, BEV segmentation, deep learning models, synthetic data, generative models, object detection, semantic segmentation, 3D environment, BEV map segmentation, multi-camera street-view images, realism, controllability, training dynamics, data augmentation, BEVGen, BEVControl, ControlNet, Text-to-Image (T2I), latent diffusion models (LDM), Variable Length Input, Fourier embedding, Classifier-Free Guidance (CFG), scene annotations, multi-perspective camera views, embedding techniques, Gaussian noise distribution, visual transformations, multi-camera view consistency, MVDiffusion, depth perception, image generation, Fréchet Inception Distance (FID), nuScenes dataset, object localization, Classifier-free Guidance (CFG), 3D spatial controls, UNet architecture.",
    "X6tNkN6ate.pdf.json": "Denoising diffusion models, conditional generation, density modeling, mutual information, conditional mutual information, information decomposition, high-dimensional space, pixel-wise information decomposition, pointwise mutual information, model convergence, Minimum Mean Square Error (MMSE), Log Likelihood Ratio (LLR), Gaussian noise, generative models, attentional mechanisms, compositional understanding, ARO benchmark, Stable Diffusion, image-text matching, diffusion processes, information-theoretic estimator, neural networks, Prompt Intervention, Contrastive Learning, pixel-level CMI, importance sampling, image segmentation, Feature Pyramid Networks (FPN), selective editing, latent space diffusion models, Diffuse Attention Attribution Map (DAAM), information flow, gradient sensitivity, noise regression, high-fidelity recovery, thresholding effects, AI perspectives, object localization, model interpretability.",
    "u3dX2CEIZb.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, Mixture-of-Experts (MoE), differentiable physics, optimization, PDE-constrained optimization, loss function, physical dynamics, conservation laws, numerical surrogates, implicit function theorem, neural PDE solver, physics-informed neural networks (PINNs), hard constraints, soft constraints, differential operator, partial differential equations (PDEs), non-linear least squares solver, Levenberg-Marquardt, neural networks (NNs), training stability, model convergence, dynamics, turbulent Navier-Stokes, diffusion-sorption, data efficiency, computational costs, memory costs, spatiotemporal discretizations, inductive bias, empirical evidence, computational graph, auto-differentiation, gradient descent, suite of basis functions, boundary conditions, characteristic layers, inverse design.",
    "rR03qFesqk.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, position encoding, functional relative position encoding, FIRE, length generalization, Transformer, Absolute Positional Encoding (APE), Rotary Positional Encoding (RoPE), Relative Positional Encoding (RPE), T5’s RPE, Alibi, Kerple, Sandwich, C4 language modeling, long text benchmarks, MLP (Multilayer Perceptron), log transformation, progressive interpolation, causal attention, training length, input sequence lengths, output sequence lengths, perplexity, fine-tuning, GLUE, SuperGLUE, NarrativeQA, empirical study, generalization performance",
    "QibPzdVrRu.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient flow, binary classification, two-layer ReLU network, well-separated data, numerical experiments, MNIST dataset, loss function, alignment phase, low-rank bias, implicit bias, neural dynamics, random initialization, training dataset, neuron alignment, gradient descent, covariance matrix, nuclear norm, Frobenius norm, directional dynamics, training loss, exponential loss, logistic loss, differential inclusion, spectral norm, Caratheodory solution, indicator function, identity matrix, normal distribution, converging dynamics, trapping regions, convergence analysis, fitting phase, sparsity-inducing biases, Clark sub-differentials, two-layer networks, ReLU activation function, error bound, covariance structure, training data misclassification, maximum norm, polynomial time, statistical measures, empirical performance, generalization performance, first-order methods, structured networks, matrix factorization models, high-dimensional data, eigenvalue analysis, neural network architectures, separation condition, Gaussian distribution, maximal projections, singular values, overparametrization, activation pattern, training regime",
    "TqL2xBwXP3.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, large language models (LLMs), GeoLLM, spatial information, population density, economic livelihoods, geospatial knowledge, OpenStreetMap, Pearson's r, sample efficiency, robustness, model convergence, Llama 2, RoBERTa, GPT-3.5, gradient descent, fine-tuning, pretraining dataset, nightlight, k-NN, XGBoost, prompt tuning, classification, regression, overfitting, deep learning, model predictive power, feature extraction, distance, direction, auxiliary map data, socioeconomic indicators, WorldPop, US Census Bureau (USCB), Zillow, DHS SustainBench, mean income, asset wealth, LLM performance, multimodal data, temporal data, QLoRA, embeddings, spatial covariates.",
    "u3dHl287oB.pdf.json": "self-attention, catastrophic forgetting, continual learning, overparameterization, task similarity, linear regression, model convergence, permutation task benchmarks, gradient descent, Euclidean distances, dimensionality of transformed subspace (DOTS), expected forgetting, analytical expression, worst-case forgetting, orthogonal transformation, synthetic data, stochastic gradient descent (SGD), hyperparameter tuning, neural network architectures, empirical evaluation, feature representations, deterministic tasks, random orthogonal operator, analysis on permutation datasets, statistical risk, training error, neural tangent kernel (NTK), batch normalization, empirical risk minimization, interpolation threshold, two-task learning, representation similarity, gradient-based learning, multi-task learning, statistical models, model accuracy.",
    "QyFm3D3Tzi.pdf.json": "spatio-temporal modeling, generative pre-training, GPD, few-shot learning, urban knowledge transfer, feature extraction, generative diffusion model, Transformer-based denoising diffusion model, state-of-the-art baselines, traffic speed prediction, crowd flow prediction, transfer learning, coarse-grained methods, fine-grained methods, knowledge transfer, similarity calculation, multi-task learning, pre-trained models, Natural Language Processing, prompting techniques, hypernetwork, diffusion probabilistic models, Generative Adversarial Networks, spatio-temporal graph, node features, adjacency matrix, parameters optimization, parameter generation, model parameters, conditioning strategies, self-attention, spatio-temporal prediction models, STGCN, GWN, MLP-based model, historical signals, mean absolute error, root mean squared error, Gaussian noise, denoising process, parameter tokenization, spatial prompt, temporal prompt, inductive bias, cross-modal conditioning, urban computing applications, knowledge graph, spatiotemporal characteristics.",
    "VkWbxFrCC8.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, COMpression with Bayesian Implicit NEural Representations (COMBINER), Implicit Neural Representation (INR), Robust and Enhanced COMBINER (RECOMBINER), variational Gaussian mean-field Bayesian neural network, β-evidence lower bound (β-ELBO), Kullback-Leibler divergence, relative entropy coding (REC), SIREN activations, factorized Gaussian variational posterior, variational Bayesian neural networks (BNN), Fourier embeddings, learned positional encodings, hierarchical Bayesian model, patch-INRs, image, audio, video, protein structure data, CIFAR-10, Kodak, LibriSpeech, UCF-101, depth-limited global-bound A˚ coding, stochastic gradient descent, linear reparameterization, rate-distortion performance, loss function, model convergence, encoding time complexity.",
    "hAYHmV1gM8.pdf.json": "Federated learning, data privacy, collaborative training, decentralized clients, non-i.i.d data, performance degradation, convergence, multi-domain FL, skewed label distribution, Federated learning Without normalizations (FedWon), batch normalization (BN), scaled weight standardization, model convergence, convolution layers, Federated Averaging (FedAvg), cross-silo FL, cross-device FL, domain generalization, deep neural networks (DNN), convolutional neural network (CNN), datasets, Digits-Five, Office-Caltech-10, DomainNet, ResNet, AlexNet, stochastic gradient descent (SGD), general global model, state-of-the-art methods, adaptive gradient clipping (AGC), model aggregation, optimization landscapes, feature distributions, parameterized convolution, WSConv, empirical evaluation, medical diagnosis.",
    "qTlcbLSm4p.pdf.json": "diffusion models, image synthesis, high-resolution generation, discrete cosine transformation, Signal-to-Noise Ratio, Relay Diffusion Model, blurring diffusion, block noise, FID, CelebA-HQ, sFID, ImageNet, ADM, LDM, DiT, UNet, training efficiency, noise schedule, isotropic Gaussian noise, linear schedule, cosine schedule, variational lower bound, denoising diffusion probabilistic models, Gaussian transitions, forward diffusion process, Markov chain, backward process, stochastic differential equations, denoising diffusion implicit models, EDM framework, training objective, cascaded diffusion model, fDM, text-to-image generation, Inverse Heat Dissipation Model, heat dissipation, thermodynamic process, partial differential equations, patch-wise blurring, frequency spectrum analysis, low-frequency information, high-resolution images, hyperparameter tuning, conditioning augmentation, SNR, sampling steps, stochastic sampler, SDE, ODE, sampling algorithms, number of function evaluations, classifier-free guidance",
    "juE0rWGCJW.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, Graph Neural Network (GNN), EX-Graph, Ethereum, X, OpenSea, Non-Fungible Tokens (NFTs), Ethereum Name Service (ENS), wash trading, matching links, Area Under the Curve (AUC), DeepWalk, Node2Vec, GGNN, GCN, GAT, GraphSage, APPNP, TAGCN, Cluster-GCN, DAGNN, GATv2, R-GCN, blockchain, on-chain, off-chain, statistical analysis, feature extraction, principal component analysis (PCA), temporal information, node features, synthetic features, Ethereum transaction records, edge set, node set, data imbalance, user privacy, X accounts, off-chain features, financial crimes, phishing scams, detection, classification, significant role, temporal consistency, social media activity, network representation learning, empirical methods, machine learning.",
    "LJWizuuBUy.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, empirical Fisher Information Matrix, neural network pruning, SWAP, Entropic Wasserstein regression (EWR), optimal transport problem, neighborhood interpolation, noise mitigation, state-of-the-art (SoTA), MobileNetV1, loss function, model complexity, resource-constrained environments, computational overhead, GPT-4, Fisher information matrix (FIM), mixed integer quadratic programming (MIQP), sparse linear regression (LR), Taylor Expansion, gradient descent, covariance information, adversarial attacks, federated learning (FL), gradient averaging, Wasserstein distance, sliced probability divergence, entropic regularization, optimal transport (OT) problem, 2-Wasserstein distance, empirical distributions, stochastic gradient descent (SGD), iterative hard thresholding (IHT), robustness, block diagonal Hessian, convergence, accuracy improvement, empirical formulations, numerical results, noise reduction, model interpretability.",
    "xAqcJ9XoTf.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, Laplacian eigenvectors, eigendecomposition, message-passing GNNs, Stable and Expressive Positional Encodings (SPE), expressive power, basis invariant functions, eigenvectors, eigenvalues, permutation equivariance, stability, graph neural networks (GNNs), Graph Transformers, generalized positional encodings, eigenvalue decomposition, L2 norm, Frobenius norm, adjacency matrix, normalized Laplacian, deep sets, graph regression, domain generalization, 1-Wasserstein distance, SignNet, BasisNet, ZINC, Alchemy, DrugOOD, molecular property prediction.",
    "tPEwSYPtAC.pdf.json": "sharpness, OOD generalization, robustness, flat minima, empirical risk minimization (ERM), ridge regression, deep learning, optimization, overparameterized neural networks, in-distribution (ID) generalization, out-of-distribution (OOD) distance, VC-dimension, Rademacher Complexity, PAC-Bayes, loss function, sharpness-based OOD generalization bound, algorithmic robustness, distributional shift, Hessian matrix, loss landscape, K non-overlapping subspaces, robustness constant, convex loss function, relative flatness, expectation, statistical distance, worst-case error, distribution measures, error gap, training set, optimization-based methods, Euclidean norm, domain adaptation, causal framework, transfer transductive learning, uniform bound, total variation distance, dimension reduction, regularization parameter.",
    "PEuDO2EiDr.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, Recurrent Time-Frequency Separation Network (RTFS-Net), Short-Time Fourier Transform (STFT), cocktail party problem, Audio-only Speech Separation (AOSS), Audio-visual Speech Separation (AVSS), Time-domain (T-domain), Time-Frequency domain (TF-domain), Conv-TasNet, DualPathRNN, LSTM, transformer, Cross-dimensional Attention Fusion (CAF), Spectral Source Separation (S3), multi-layered RNN, inference speed, mask separation, masking, grad-CAM, spectral-temporal information, SI-SNRi, SDRi, PESQ, gradient descent, model convergence, model hyperparameters, deep learning, neural networks, computational complexity, dimensionality reduction, training time, acoustic features, data compression, multi-modal information, TAR units, Visualvoice, TFPSNet, TF-GridNet, CTCNet, LRS2, LRS3, VoxCeleb2, WSJ0-2mix, MACs, parameters, attention mechanism, signal-processing, speaker separation, acoustic dimensions, audio encoder, video encoder.",
    "b3Cu426njo.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, meta-learning, task-invariant prior, probability density functions, recurrent neural networks, proximal gradient descent, few-shot learning, multi-block neural network, empirical risk minimization, algorithm unrolling, piecewise linear functions, optimization-based meta-learning, model-agnostic meta-learning (MAML), iterative optimizer, maximum a posteriori (MAP), negative log-likelihood, negative log-prior, Gaussian prior, Bayesian formulations, control points, regularizer, deep learning, convolutional neural networks (CNN), recurrent neural networks (RNN), PLF, softmax, gradient descent, stochastic gradient descent (SGD), task-specific model, interpretability, regularization, learned prior, proximal operator, parameterization, sparse signals, expectation-maximization, empirical performance analysis, optimization viewpoint, cumulative prior knowledge, loss function, inferential statistics, inductive bias, task-specific estimate.",
    "wHBfxhZu1u.pdf.json": "Rotary Position Embeddings (RoPE), YaRN, context window, transformer-based language models, LLaMA, positional information, in-context learning (ICL), absolute sinusoidal position encoding, learnable absolute position encoding, relative positional encoding, T5 Relative Bias, XPos, ALiBi, Position Interpolation (PI), NTK-aware interpolation, Dynamic NTK interpolation, NTK-by-parts interpolation, temperature scaling, attention mechanism, perplexity, token position, sequence length, hidden layer, attention weights, gradient descent, loss function, AdamW, scale factor, wavelength, embeddings, fine-tuning, rotary position embedding, neural tangent kernel (NTK) theory, open-source models, Flash Attention 2, autoregressive generation, long-range abilities.",
    "YItWKZci78.pdf.json": "mean-field Langevin dynamics, minimax optimization, probability distributions, symmetric updates, convergence, mean-field Langevin averaged gradient (MFL-AG), average-iterate convergence, mixed Nash equilibrium, time and particle discretization, propagation of chaos, mean-field Langevin anchored best response (MFL-ABR), best response dynamics, zero-sum Markov games, McKean-Vlasov stochastic process, Wasserstein gradient flow, entropy-regularized convex functional, Gaussian noise, gradient descent ascent, coupled distribution-dependent stochastic processes, KL regularization, Kakutani fixed-point theorem, Nikaido-Isoda error (NI error), log-Sobolev inequality, dual averaging methods, functional derivatives, Lipschitz continuous, mean-field best response (MF-BR), continuous-time convergence, bilinear problems, regularized value functions, Q-functions, empirical distributions, discrete time step, particle approximations, kernel density plots, Wasserstein distances, optimization error, linear last-iterate convergence, uniform-in-time control, propagation of chaos result, nonconvex-nonconcave potentials.",
    "RNGUbTYSjk.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, min-max optimization, generative adversarial networks, GANs, convex-concave, nonconvex-nonconcave, convergence guarantee, weak Minty variational inequality, weak MVI, extragradient algorithm, EG+, Lipschitz continuity, saddle point, gradient descent ascent, optimal improvement, loss function, cyclic behaviors, robust learning, sharpness-aware minimization, variational inequalities, saddle gradient operator, exploration steps, adaptive exploration, projection onto hyperplane, stochastic extragradient, best-iterate convergence, last-iterate convergence, parameter selection, convergence results, adaptive stepsize, adaptiveEG+, bilinear, PolarGame, Forsaken, n-step extragradient, min-max problems.",
    "MnMWa94t12.pdf.json": "Dynamic Scene Transformer (DyST), neural scene representation, latent decomposition, monocular video, synthetic dataset, Dynamic Shapenet Objects (DySO), generative modelling, 3D visual scenes, novel view synthesis (NVS), Neural Radiance Fields (NeRF), camera pose, scene dynamics, latent representations, Neural Scene Flow Fields, Spacetime Neural Irradiance Fields, Depth Estimation, Pose Estimator, Structure-from-Motion (SfM), Camera Estimator, latent control latent, contrastiveness metric, principal component analysis (PCA), motion freezing, video-to-video motion transfer, LIDAR, 3D representation learning, multi-view video, video dataset, Something-Something v2 dataset (SSv2), model architecture, empirical study, optimization steps, downstream applications, latent control swap, image generation quality, training scheme.",
    "MSe8YFbhUE.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, visual reinforcement learning, sample efficiency, asymptotic performance, random seeds, motorically inactive exploration, neuronal activity, policy networks, dormant ratio, model-free algorithm, exploration-exploitation trade-offs, DeepMind Control Suite, MetaWorld, Adroit, DrM, DrQ-v2, ALIX, TACO, actor-critic, Partially Observable Markov Decision Process (POMDP), state space, observation space, action space, state transition kernel, reward function, discount factor, optimal policy, dormant neurons, expressivity loss, perturbation reset method, exploration scheduler, expectile regression, locomotion control, dexterous hand manipulation, sparse rewards, high-dimensional action spaces, intrinsic motivation-oriented exploration, uncertainty-oriented exploration.",
    "MCNqgUFTHI.pdf.json": "self-attention, dialogue policy planning, large language models (LLMs), prompting schemes, reinforcement learning, human-annotated data, dynamic interaction data, self-play simulation, dialogue agent, PPDPP, supervised fine-tuning, prompt-based policy planning, zero-shot prompting, few-shot prompting, corpus-based learning, dialogue act prediction, Markov Decision Process (MDP), RoBERTa, AI feedback, goal-oriented feedback, policy planning capability, emotional support dialogues, negotiation dialogues, tutoring dialogues, Sale-to-List Ratio (SL%), Average Turn (AT), success rate at turn t (SR@t), conversational recommendation, learnable plug-ins, dialogue systems, interaction setting, emotional intensity, plug-and-play dialogue policy planner, empirical evaluation, reinforcement learning from AI feedback (RLAIF), scalar rewards, interaction history, turn-level response quality measurements, multi-turn interactions, dialog-level interactive evaluation, conversational goals.",
    "nZP6NgD3QY.pdf.json": "Multi-task learning (MTL), task arithmetic, Adaptive Model Merging (AdaMerging), merging coefficients, task vector, entropy minimization, model generalization, task-wise AdaMerging, layer-wise AdaMerging, prediction loss, multi-task model merging, pre-trained models, ViT, BERT, Fisher Merging, RegMean, Ties-Merging, L2 distance, precision, Spearman correlation coefficient, Shannon entropy, model parameters, knowledge transfer, computational cost, architectural methods, optimization-based methods, gradient dominance, grid search, combinatorial optimization search, unseen downstream tasks, data distribution shifts, robustness, task vectors based methods, model merging performance, knowledge sharing, high computation cost, task-specific features, general features, multi-task setup, automatic differentiation.",
    "N23A4ybMJr.pdf.json": "self-attention, vision transformers, hierarchical architectures, fast attention, approximate attention, local interactions, global interactions, high-resolution inputs, relative positional embedding, rotary embeddings, semantic segmentation, Win-Win, optical flow, Spring benchmark, mean intersection-over-union, EPE, structured subset, multi-head self-attention, convolutional heads, RoPE positional embeddings, quadratic complexity, attention sparsification, local attention mechanisms, sub-quadratic attention, tokens, masking, training strategies, random windows, temperature hyper-parameter, spatial reduction, transformers, monocular tasks, binocular tasks, high-resolution outputs, DPT, ViT-Det, SAM, gradient checkpointing, optical flow estimation, FlyingChairs, FlyingThings, TartanAir, CroCo-Flow, GMFlow+, image-level tasks, token selection, window sampling, inference time, cropping strategy.",
    "NkmJotfL42.pdf.json": "generalization bound, uniform tightness, population loss, population risk, training set, learned hypothesis, margin bounds, stability bounds, learning algorithm, empirical risk, complexity measure, overparameterized setting, random initialization, neural networks, VC dimension, Rademacher bounds, spectral norm, Frobenius norm, path-norm, Fisher-Rao norm, PAC-Bayes-flatness, sharpness-flatness measures, ERM algorithm, estimability, hypothesis class, 0-1 loss, deep learning, implicit regularization, SGD, generalization measures, empirical distribution, realizable distributions, algorithm-dependent estimability, estimability trade-off, hypothesis classes, linear functions, parity functions, multiclass classification, diverse distributions, natural images, MNIST, CIFAR, Jiang et al. (2020), Hardt et al. (2016), Nagarajan & Kolter (2019c), Bartlett & Long (2021), Abbe & Sandon (2020).",
    "mw1PWNSWZP.pdf.json": "COMMITPACK, instruction tuning, large language models (LLMs), Git commits, StarCoder, performance improvement, HumanEval, HUMANEVALPACK, OCTOCODER, OCTOGEEX, COMMITPACKFT, code synthesis, code repair, code explanation, code instruction, xP3x, Self-Instruct, OASST, programming languages, data filtering, pass@1, natural language tasks, pretraining, model usability, code generation, bug fixing, pass@k, natural language instructions, generalization abilities, quality filters, programming languages generalization, closed-source models, commercial use, training data, instruction datasets, benchmarking, model evaluation, neural language models, synthetic data, code editing, execution-based metric, task-specific methods, natural language processing (NLP).",
    "sGVmr7KHfn.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, universal domain adaptation, target private category, concept shift, Memory-Assisted Sub-Prototype Mining, MemSPM, Unsupervised Domain Adaptation, UDA, Domain Consensus Clustering, domain-invariant feature learning, sub-prototypes, semantic ambiguity, intra-class structure, classifier, visual encoder, task-oriented embedding, attention-based addressing, cosine similarity, cycle-consistent alignment, classification accuracy, unknown classes, known classes, H-score, CLIP, ViT-B/16, Office-31, Office-Home, VisDA, Domain-Net, PCA, SVM, MSE loss, latent space, embedding, data labels, cross-entropy loss, domain alignment loss, regularization, adaptive threshold, attention weights, sub-class, feature representation, similarity metric, softmax operation.",
    "lGUyAuuTYZ.pdf.json": "BINARY NEURAL NETWORKS (BNN), Spiking Neural Networks (SNN), sparse binary activation neural networks (BANN), Hoyer regularizer, Hoyer threshold layer, gradient descent, activation sparsity, low-power vision tasks, state-of-the-art (SOTA), binary weights, bi-polar values, uni-polar binary activation neural networks, custom non-linear functions, floating point operations (FLOPs), energy efficiency, accuracy-FLOPs trade-off, cross-entropy loss, non-binary operations, object detection, VGG16, ResNet, CIFAR10, ImageNet, Adam optimizer, stochastic gradient descent (SGD), training framework, hyperparameter, Kaiming uniform initialization, loss function, soft reset mechanism, membrane potential, parameter count, spike-element-wise (SEW) architecture, dynamic down-scale threshold, time steps, computational efficiency, hardware acceleration, quantization-aware training, activation map, threshold parameter, noise mitigation, sign quantization function, low-latency SNNs, energy-efficient DNNs, portability, training from scratch, data representation, memory footprint, factor of sparsity, LIF model, surrogate gradient, practical implementation, training dynamics.",
    "lajn1iROCu.pdf.json": "Reinforcement Learning, RL, scalable training, distributed system, ReaLly Scalable RL (SRL), large-scale training, parallelized training, trajectories, dataflow abstraction, training throughput, learning performance, task handlers, actor worker, policy worker, trainer worker, Actor, Policy, Trainer, Proximal Policy Optimization (PPO), environment simulation, policy inference, gradient descent, CPU, GPU, multi-agent, intrinsic goals, black-box environment programs, inference streams, sample streams, parameter server, resource allocation, optimization techniques, sample generation, environment ring, trainer pre-fetching, customized algorithms, Deep Q-Network (DQN), reinforcement signals, empirical benchmarks, high-throughput data transmission, tunable configuration, asynchronous communication, smart environments, sample efficiency, academic libraries, runtime scheduling, Algorithmic Complexity, OpenAI, DeepMind, AlphaStar, OpenAI Five, hide-and-seek environment, computational efficiency, Novelty-based learning, exploration-exploitation, multi-node training, cluster computing.",
    "w7BwaDHppp.pdf.json": "neural radiance fields, NeRFs, novelty views, unbounded scene, bounded region, mapping functions, geometric understanding, stereographic projection, p-norm distance, ray parameterization, volumetric density, novel view synthesis, MLP-based models, voxel grids, NeRF++ , Instant-NGP (iNGP), TensoRF, 3D query points, deterministic functions, inverse distance, mapping function, adapted mapping function, spatial representation capacity, contract mapping, manifold shape, scene geometry, Euclidean distance, angular ray parameterization, uniform sampling, distance metrics, point cloud, RANSAC framework, model convergence, loss function, ray origin, rendering spaces, memory issues, projection function, disparity distance, log-metric distance, Kalman filter, multi-sphere images, balance spherical grids, space subdivision, sampling strategies, non-linear sampling, adaptive intervals, state-of-the-art results, neural rendering, background representation, MLP layers, depth values, total error, projective mapping, batch size, learning rates, volumetric representations, area-preserving mapping, scene optimization, cloudy estimations, convex shapes, neural rendering methods, pixel color synthesis, manifold representation, sequence of samples, learning capacity, geometric-aware mapping, high-dimensional projection, multi-scale rendering, predictability of p-values, iterative refinement, rendering quality, artifact presence.",
    "tsE5HLYtYg.pdf.json": "Reinforcement Learning, Safe Reinforcement Learning, SafeRL, zero-cost performance, world model, SafeDreamer, Lagrangian-based methods, Dreamer framework, Safety-Gymnasium benchmark, Constrained Markov decision process (CMDP), dynamics models, online safety-reward planning (OSRP), Constrained Cross-Entropy Method (CCEM), Augmented Lagrangian, PID Lagrangian, model-based RL, MPD (Camacho et al., 2007), CCEM (Wen & Topcu, 2018), LAMBDA, policy search algorithm, safety criteria, model inaccuracies, sample efficiency, planning processes, reward maximization, cost functions, reward return, cost return, safe actor training, stochastic backpropagation, Recurrent State Space Model (RSSM), variational auto-encoding, latent rollouts, stop-gradient operator, expected upper bound (Minmax penalty), temporal optimization, Bayesian principles, safety-reward balance, real-world applications, optimization objectives, error balancing, safety-aware world model.",
    "xUzWmFdglP.pdf.json": "privacy amplification, differential privacy, DP-SGD, machine learning, state-of-the-art, DP-FTRL, matrix mechanism, correlated noise, independent noise, MMCC, conditional composition theorem, privacy-utility trade-offs, empirical utility, moments accountant, privacy loss distributions, mixture of Gaussians, Gaussian mechanism, binary tree, binary-tree-DP-FTRL, stochastic gradient descent, encoder matrix, decoder matrix, amplification, zero-out adjacency, posterior outputs, sampling probability, noise standard deviation, sensitivity, bounded sensitivity, adaptively chosen, uniform sampling, shuffling, crowding effects, empirical improvements, DP mechanisms, non-adaptive mechanisms, adaptive mechanisms, privacy guarantees, DP algorithms, privacy budget, minimizing error, portfolio analysis, research direction, optimal utility, convex set, differentiable functions.",
    "iCNOK45Csv.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, dataset distillation, backdoor attacks, kernel methods, trigger pattern generation, optimization-based trigger design, distilled datasets, deep neural networks, computational efficiency, performance-matching dataset distillation, parameter-preserving, distribution-preserving, clean test accuracy (CTA), attack success rate (ASR), reproducing kernel Hilbert space (RKHS), neural tangent kernel (NTK), bi-level optimization, conflict loss, projection loss, generalization gap, simple-trigger, relax-trigger, KIP (Kernel Inducing Points), CIFAR-10, GTSRB, SCAn, AC, SS, Strip, ABL, NAD, STRIP, FP, Hadamard product, empirical risk, conflict loss, regularization term, trace operator, dataset D, dataset D̃, kernel matrix, linear combination, kernel k(·, ·).",
    "kBTzlxM2J1.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, knowledge graphs, KG completion, node classification, question answering, recommendation, NEURAL-LP, DRUM, soundness, completeness, Datalog, rule extraction, Feed-Forward Networks, Convolutional Neural Networks, Graph Neural Networks, model predictions, loss function, empirical performance, sound rules, faithful rule sets, multipath conjunction, counting, invariance principles, model expressivity, MMDRUM, SMDRUM, GRU, empirical evidence, model convergence, model parameters, confidence score, causal inference, tensor factorization, matrix multiplication, Laplace approximation, statistical guarantees",
    "cuAxSHcsSX.pdf.json": "cross-silo federated linear contextual bandit, LCB, differential privacy, privacy protection, regret bound, communication cost, algorithmic framework, federated LCB algorithm, privacy protocols, silo-level local differential privacy, shuffle model of differential privacy, privacy amplification, tree-based mechanism, hidden vector, multi-armed bandits, local differential privacy, Federated DP, generic algorithmic and analytical framework, privacy noise, group pseudo-regret, central server, context information, pseudo-regret, Gaussian noise, local randomizer, binary tree structure, vector sum mechanism, privacy guarantee, amplification lemma, noisy p-sums, bias vector, covariance matrix, feature vector, logarithmic cost, privacy leakage, communication schedule, optimal regret, privacy constraints, regret performance, Flexibility of privacy protocols, user-sensitive information, privacy budget, client-level differential privacy, privacy characters, context-action pair",
    "juuyW8B8ig.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, gradient descent, log-likelihood estimation, Bayes theorem, Text-to-Image (T2I), Visual Question Answering (VQA), language-informed visual concept representation, concept encoders, concept embeddings, visual concepts, linguistic structures, Textual Inversion, generative model, disentangled concept embeddings, compositionality, vision-language models, synthetic images, fine-grained visual nuances, common concept structures, test-time finetuning, large pre-trained vision-language models, image generation, object classification, scene graphs, symbolic programs, Vision-Language Models (VLMs), GAN, diffusion-based T2I models, attention maps, continuous concept embeddings, BLIP-2, CLIP, multilayer perceptron, linear layer, average pooling, LeakyReLU, personalizing methods, optimization, conditional diffusion model, semantic annotations.",
    "qODvxQ8TXW.pdf.json": "Learning Rate Rewinding, Iterative Magnitude Pruning, lottery tickets, deep overparameterized neural networks, overparametrization, Learning Rate Rewinding (LRR), Iterative Magnitude Pruning (IMP), Weight Rewinding (WR), Lottery Ticket Hypothesis (LTH), gradient flow dynamics, mask learning, parameter optimization, neural network sparsification, gradient descent, closed form solutions, ReLU activation, random sparse masks, sign switches, parameter signs, benchmark datasets, CIFAR10, CIFAR100, Tiny ImageNet, ResNet18, ResNet50, training dynamics, statistical properties, optimization advantages, empirical existence proof.",
    "g1S72T3FGc.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, active learning, stream-based active learning, pool-based active learning, bandit-based approaches, K-class classification, exploitation network, exploration network, regret upper bounds, non-parametric setting, cumulative regret, label budget, loss function, neural network approximations, empirical performance, theoretical performance guarantee, end-to-end embedding, CoreSet, BADGE, DynamicAL, ALBL, NEURONAL-S, NEURONAL-P, Neural Tangent Kernel (NTK), 0-1 loss, Bounded loss, exploration strategy, performance guarantee, uncertainty sampling, diversity sampling, model convergence, decision-maker, instance selection, test accuracy, computational cost, K arms, input dimension, output dimension, effective dimension, training error, minimax active learning rate, VC dimension, contextual bandits, error-growth rate, hyper-parameters, universal approximation theorem, gradient descent, ablation study.",
    "NYN1b8GRGS.pdf.json": "self-training, image matching, in-the-wild images, GIM, zero-shot evaluation benchmark, robust fitting, model generalization, architecture design, dataset scalability, image matching architecture, domain-specific datasets, complementary matching methods, data augmentation, cross-domain generalization, super-resolution matching, robust matching, RootSIFT, SuperGlue, LoFTR, DKM, SfM, MVS, RGBD scans, MegaDepth, ScanNet, data diversity, video data, pose error, 6-DoF localization, homography estimation, two-view matching, visual localization, hierarchical metric, visual correspondence, video frame correspondence, label propagation, video training efficiency, essential matrix estimation, gradient descent, machine learning models, foundation models, temporal information in videos, point cloud registration, 3D reconstruction, machine vision applications, image-text pairs, RANSAC, dataset names.",
    "kUveo5k1GF.pdf.json": "equilibrium propagation, backpropagation of error algorithm, weight symmetry, infinitesimal equilibrium perturbations, unbiased gradient estimates, generalized EP, Cauchy integral, Jacobian, homeostatic objective, energy-based model, differentiable Hopfield network, neural oscillations, feed-forward networks, recurrent backpropagation, deep equilibrium models, teaching amplitude, stochastic gradient descent, Fashion MNIST, CIFAR-10, ImageNet, loss function, Jacobian asymmetry, functional symmetry, Frobenius norm, Hutchinson trace estimator, predictive coding networks, automatic differentiation, nudge parameter, gradient estimate, neuronal error vector, complex-differentiable non-symmetric networks, gradient computation, neural networks",
    "cMPm8YFXZe.pdf.json": "Alternating Denoising Diffusion Process (ADDP), vector-quantized (VQ), recognition tasks, generation tasks, representation learning, pixels, VQ tokens, evidence lower bound (ELBO), Masked Image Modeling (MIM), deep generative models, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), autoregressive models, diffusion models, high-fidelity images, ImageNet, COCO, ADE20k, classification, segmentation, image synthesis, spatially sensitive information, model performance, transfer performance, generative pre-training, generative modeling, masking strategy, token-to-pixel decoding, pixel-to-token generation, Dirac delta distribution, KL divergence, linear probing, learnable encoder-decoder network, token predictor, reliable tokens, unreliable tokens, sampling, weighted sum, VQGAN, ViT-VQGAN, MAE, BEiT, image classification, object detection, semantic segmentation.",
    "pAsQSWlDUf.pdf.json": "self-supervised learning, contrastive learning, SoftCLT, instance-wise contrastive loss, temporal contrastive loss, soft assignments, distance metrics, dynamic time warping, DTW, InfoNCE loss, UCR datasets, UEA datasets, complexity, embedding function, nonlinear embedding, representation learning, semi-supervised learning, transfer learning, anomaly detection, deep learning, classification, temporal relationships, hierarchical loss, soft temporal contrastive learning, soft instance-wise contrastive learning, data augmentation, statistical significance, performance evaluation, hyperparameter tuning, SOTA performance, representation vector, t-SNE, pretext task, masking, supervised fine-tuning, cold-start setting.",
    "Spp2i1hKwV.pdf.json": "in-context learning, selective annotation, influence-driven method, large language models, annotation costs, directed graph, diffusion process, greedy algorithm, maximum marginal gain, data diversity, data representativeness, theoretical support, prompt retrieval, few-shot learning, supervised fine-tuning, extensive annotated examples, unlabeled data pool, cosine similarity, influence maximization, independent-cascade diffusion model, submodular condition, influence function, annotation budget, selected subset, Sentence-BERT, machine learning, annotation process, LLMs, benchmarks, classification, commonsense reasoning, dialogue, text generation, GPT-J 6B, GPT-Neo, GPT-3.5-Turbo, Vote-k, random selection, out-of-distribution tasks, empirical evaluations, Auto-IDEAL",
    "yxKZGQLzOP.pdf.json": "self-play, pragmatic inference, program synthesis, programming by example (PBE), user-provided examples, disambiguate, regular expressions, neural networks, cooperative game, computational pragmatic inference, counterfactual reasoning, informative training examples, ambiguity, consistency relation, consistency matrix, literal listener, literal speaker, pragmatic listener, Rational Speech Acts (RSA) framework, Bayesian inference, maximum likelihood training, program-specification pairs, HFT (human fine-tuned), ByT5-small, supervised learning, natural language processing (NLP), template-based sampling, iterative data generation, human-provided specifications, user interactions, training data, model selection, top-p sampling, communication games, data wrangling, spreadsheet formulas, empirical evaluation.",
    "dyG2oLJYyX.pdf.json": "Neural Architecture Search (NAS), Conditional Neural Architecture Generation (NAG), DiffusionNAG, directed graphs, graph diffusion model, architecture design, parameterized predictors, Transferable NAS, Bayesian Optimization (BO), MobileNetV3 (MBv3), ImageNet 1K, stochastic differential equation (SDE), Gaussian normal distribution, score network, directed acyclic graphs (DAG), Bayes' theorem, architecture-accuracy pairs, predictor-guided generation, clean accuracy, robust accuracy, adversarial attack, APGD, positional embedding, score function, generative model, transfer learning, architecture mutation, operational type matrix, adjacency matrix, collection of architectures, random architecture sampling, acquisition function, accuracy prediction, robust performance, task-optimal architectures, computational flow, gradient-based methods, evolutionary algorithms, reinforcement learning, architectural representation, neural architectures, model-guidance mechanism, acquisition optimization strategy, architecture candidate selection, and graph-based generative models.",
    "rnHNDihrIT.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, policy diversity, offline reinforcement learning (RL), high-quality policies, batched data, intrinsic diversity, Stylized Offline RL (SORL), Expectation-Maximization (EM) algorithm, policy learning, trajectory clustering, policy diversification, advantage-weighted style learning, quality-diversity optimization, stochastic optimization, grid-shooting game, Atari games, Markov Decision Process (MDP), action distribution, multi-modality, KL divergence, imitation learning, offline skill discovery, reinforcement learning (RL), EM-based style clustering, task performance, multi-modal datasets, MAPElites, quality-diversity optimization problem, generative model, log-likelihood, posterior distribution, latent variable model, advantage function, value function, action value function, advantage-weighted regression, Lagrangian optimization, complexity metrics, behavior cloning, DKL, online multiplayer game, basketball video game, Dunk City Dynasty, trajectory distribution, diverse behaviors, policy quality, diverse opponents, consistent data quality.",
    "YcW8i9VCf5.pdf.json": "Causal Bayesian Optimization, Adversarial Causal Bayesian Optimization, CBO-MW, structural causal model, non-stationarity, regret bounds, optimistic counterfactual reward estimates, causal graph, classical online learning strategy, combinatorial interventions, submodular rewards, structural equation model, soft intervention model, Gaussian Process models, multiplicative weights, regret guarantee, bounded variance property, Lipschitz continuity, maximum information gain, empirical evidence, statistical models, calibrated uncertainty models, D-CBO-MW, shared mobility system, intervention targets, Bayesian optimization, multi-agent environments, adverse interventions, reward variable, demand patterns, sample efficiency, exploration-exploitation balance.",
    "MloaGA6WwX.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, model-parameter estimation, optimization problems, TADRED, TAsk-DRiven Experimental Design, imaging, data density, image channels, channel-subset, prespecified size, task performance, loss function, gradient descent, magnetic resonance imaging, hyperspectral imaging, supervised feature selection, Fisher matrix, Bayesian experimental design, model-free tasks, reconstruction, neural network, iterative subsampling, feature scoring, high-dimensional optimization, control variables, performance metrics, clinical settings, parameter mapping, acquisition scheme, Rician noise, signal-to-noise ratio, quantitative imaging, empirical acquisition scheme, design optimization, task optimization, biophysical models, image resolution, spectral filters, experimental design optimization, dual selection/task network, feature importance scoring, sample-independent scores, data normalization, data preprocessing, training epochs, computational cost.",
    "ba84RDHFnz.pdf.json": "self-supervised image representation learning, Masked Autoencoding (MAE), masked region autoencoding, architecture design, one-to-many mapping, region-aware representation, interactive segmentation, Natural Language Processing (NLP), autogressive language model, Generative Pre-training Transformer (GPT), masked language model, BERT, reconstructive pre-training, complex context, masked patches, image recognition, asymmetrical design, pixel encoder, object detection, segmentation, loss function, pixel regression, region maps, binary region map, cross-attention layer, region queries, binary classification, mean Average Precision (AP), Intersection-over-Union (mIoU), Region-of-Interest (RoI), R-CNN, Siamese learning, image-computable regions, Segment Anything Model (SAM), clustering-based regions, Felzenszwalb-Huttenlocher (FH) algorithm, Region-Aware Masked Autoencoding (R-MAE), visual backbone, hyper-parameter tuning, training epochs, region reconstruction, Region Autoencoding (RAE), vision transformers (ViTs), length variant, denoising autoencoders, object-centric, permutation equivariance, data augmentation, input image size, state-of-the-art performance, long-tailed object detection, sample efficiency, generative models.",
    "RRayv1ZPN3.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, TAIL (Task-specific Adapters for Imitation Learning), parameter-efficient fine-tuning (PEFT), Bottleneck Adapters, P-Tuning, Low-Rank Adaptation (LoRA), continual adaptation, adapter modules, decision-making models, catastrophic forgetting, adaptation plasticity, transformer backbone, CLIP image encoder, FiLM layers, prefix tuning, parallel integration, sequential integration, transformer adapter modules, experience replay (ER), elastic weight consolidation (EWC), PackNet, gradient descent, loss function, behavioral cloning loss, transformer decoder, internet-scale datasets, RoboAdapter, pretrained models, LIBERO benchmark, adaptation techniques, policy head, GPT2, task-specific weights, learning objectives, model convergence",
    "eJHnSg783t.pdf.json": "DIFFTACTILE, differentiable tactile simulation, robotic manipulation, dense tactile feedback, physics-based contact modeling, Finite Element Method (FEM), multi-material simulator, soft body model, penalty-based contact model, gradient-based optimization, sim-to-real gap, tactile-assisted grasping, optical response, pixel-based neural module, contact dynamics, object shape estimation, grasp stability prediction, Moving Least Square Material Point Method (MLS-MPM), Position-Based Dynamics (PBD), static friction, dynamic friction, contact force distribution, contact surface deformation, hyper-elastic materials, Neo-Hookean model, Lamé parameters, soft elastomer, surface normals, pixel-wise mean squared error (MSE), differentiation, robotic perception tasks, tactile sensors, surface following, cable straightening, case opening, object reposing, visual images, marker motions, deformation gradient, isotropic invariants, simulated image, generative models, bidirectional reflectance distribution function (BRDF), soft body dynamics, rigorous contact modeling, gradient-based trajectory optimization, SAC (Soft Actor-Critic), PPO (Proximal Policy Optimization).",
    "dGH4kHFKFj.pdf.json": "GenCorres, unsupervised joint shape matching, shape matching, mesh generator, unorganized deformable shapes, geometric structures, local rigidity, local conformality, shape correspondence, initialization, deformation priors, implicit generator, template-based deformation, Chamfer distance, state-of-the-art JSM techniques, data-driven, pairwise matching, joint shape matching (JSM), shape collections, shape generator, neural shape generators, local geometric structures, cycle-consistency, non-convex optimization, cycle-consistency constraint, point-based generators, geometric regularization, ARAP (As-Rigid-As-Possible), ACAP (As-Conformal-As-Possible), implicit shape representation, variational auto-encoder (VAE), optimization problem, corresponding vertices, shape morphing, correspondence quality, latent space, latent code, shape quality, generative models, high-fidelity correspondence computation, implicit representations, 3D-CODED, deformation transfer, template mesh, eigenvalues, geometry processing, reconstruction errors, implicit neural representations, graph-based methods, DAUST, SMAL, evaluation protocols, mesh-based generators.",
    "nJnky5K944.pdf.json": "self-attention, softmax function, hardmax function, Boltzmann operator, low-rank weight matrices, universal approximator, contextual mapping, degrees of freedom, parameter efficiency, finite samples, sequence id, tokenwise (rmin, rmax, δ)-separation, attention mechanism, single-head self-attention, multi-head self-attention, continuous permutation equivariant functions, gradient descent, universal approximation theorem, CoNLL-2003 dataset, feed-forward neural network, transformer architecture, NLP, BERT, GPT, CNN, GNN, token embeddings, negative result, ranking, model convergence, empirical evidence, computational complexity",
    "Tr0lPx9woF.pdf.json": "Relative Importance and Activations, RIA, Channel Permutation, post-training pruning, LLMs, N:M sparsity, unstructured pruning, SparseGPT, Wanda, model quantization, network sparsity, Hessian Matrix, OBD, Adaptive Pruning, Iterative Adaprune, BERT, empirical experiments, model size, computation, weight pruning, N:M constraint, plug-and-play, zero-shot tasks, Matrix Permutation, linear sum assignment, Hungarian algorithm, performance degradation, weight matrix, channel corruption, inference acceleration, Sparse Matrix-Matrix Multiplication, Spearman's Rank Correlation Coefficient, activation outliers.",
    "iPWxqnt2ke.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, policy gradient methods, training efficiency, structured optimization, supervised learning, reinforcement learning, gradient subspaces, gradient descent, stochastic gradient descent, deep reinforcement learning, actor-critic methods, Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), loss function, curvature, Hessian matrix, eigenvectors, eigenvalues, mini-batch training, empirical evaluation, Markov decision processes (MDPs), value function, action value function, advantage function, low-dimensional subspaces, parameter-space exploration, second-order optimization, gradient projection, principal component analysis (PCA), differential privacy, adversarial training, function approximators, evolutionary strategies (ES), off-policy learning, on-policy methods, mini-batch approximations, extended replay buffer, parameter-space directions, parameter tuning, gradient projection error, training distribution shifts.",
    "t3vnnLeajU.pdf.json": "DA-CLIP, degradation-aware vision-language model, CLIP, cross-attention, image restoration, image content embedding, degradation embedding, zero-shot predictions, low-level vision, mixed degradation dataset, synthetic captions, U-Net architecture, high-quality image restoration, unified image restoration, vision-language models, image denoising, deraining, dehazing, deblurring, contrastive learning, prompt learning, blind image restoration (BIR), GAN, stable diffusion, IR-SDE, perceptual evaluation, Fréchet inception distance (FID), Learned Perceptual Image Patch Similarity (LPIPS), maximum likelihood loss, prompt module, degradation types, low-quality images, feature embeddings, multi-task framework, high-fidelity image reconstruction, zero-initialised connections, optimization objective, cosine similarity, image-text-degradation pairs, large-scale pretrained models.",
    "Q3YaCghZNt.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, automated program verification, program properties, abstract reasoning, derivation rules, soundness, LLMs (Large Language Models), program synthesis, code repair, program invariants, LEMUR, hybrid approach, deductive steps, proof system, assumptions, stability, invariant generation, formal methods, oracle calls, proof goals, k-induction, bounded model checking, verification tools, ESBMC, UAUTOMIZER, predicate abstraction, model verification, syntax transformation, loop invariants, automated reasoners, program verification calculus, sound automated verification procedure, program execution, reachability property, Boolean predicate, program line, logical implication, stability condition.",
    "uqxBTcWRnj.pdf.json": "Transitional Dictionary Learning, TDL, symbolic knowledge, visual parts, implicit relations, game-theoretic diffusion model, Expectation Maximization, EM algorithm, online prototype clustering, clustering information gain, heuristic shape score, abstract compositional visual object datasets, symbol grounding, transfer learning, unsupervised learning, neural networks, distributed representations, symbolic representations, transitional representation, logical statements, predicate logic, neural logic variables, optimization problem, model convergence, loss function, clustering metrics, neural-symbolic learning, Energy-Based Models, Bayesian Program Learning, contrastive learning, unsupervised segmentation, spatial attention, co-part segmentation, non-negative matrix factorization, reconstruction error, overlapping penalty, clustering loss, reinforcement learning, shape score, human evaluation, interpretability, compositionality, meaningful representations, data labeling service, similarity measures.",
    "tmsqb6WpLz.pdf.json": "self-attention, hyperparameter tuning, zero-shot performance, domain-specific corpus, language models, finetuning, instruction-following LLMs, ChatGPT, controlled-variable text, topic modeling, style transfer, factual knowledge, modeling probabilities, causal language modeling, LLaMA, AdamW optimizer, learning rate, general corpus, domain corpus, catastrophic forgetting, continual language learning, dataset debiasing, learning dynamics, learning rate decay, full-finetuning, low-rank finetuning, perplexity, PubMed, C4, Pile of Law, Amazon reviews, cross-entropy loss, language model, topic bias, style bias, knowledge learning, human judging, automatic data generation, generative model, likelihood ratio, topic and style priors, domain adaptation, qualitative observations, instructional data, knowledge distillation, surface learning, language modeling, general abilities, biomedical knowledge, MMLU, medical knowledge, Bayesian inference, probability distributions",
    "NvbeD9Ttkx.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, second-order information, FOSI, meta-algorithm, optimization process, quadratic functions, first-order optimizer, second-order optimizer, convergence, computational efficiency, gradient descent, Newton's method, Hessian, effective condition number, deep neural networks (DNNs), stochastic optimization, hybrid optimizers, inverse preconditioner, Lanczos algorithm, extreme eigenvalues, eigenvectors, loss function, learning rate, momentum, K-FAC, Adam, Heavy-Ball, L-BFGS, stochastic gradient descent, curvature information, diagonal matrix, spectral analysis, approximation error, matrix-vector product, Hadamard product, numerical instability, error amplification, open-source implementation, model convergence, wall time, dataset, empirical evaluation.",
    "PcxQgtHGj2.pdf.json": "offline deep reinforcement learning, pre-training, Decision Transformer, language corpus, downstream performance, synthetic IID data, one-step Markov chain, Conservative Q-Learning, CQL, Multi-Layer Perceptron, MLP, D4RL Gym locomotion datasets, Natural Language Processing, NLP, Computer Vision, CV, Markov Decision Process, MDP, autoregressive next-state prediction, transformer-based architectures, trajectory modeling, return-to-go, transition matrix, generative models, hyperparameter tuning, forward dynamics prediction, Independent and Identically Distributed, IID, performance improvement, loss function, synthetic data schemes, optimization process, pre-training updates, computational efficiency, centroid prediction.",
    "SdeAPV1irk.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, randomized smoothing, robustness certification, deep neural networks (DNNs), adversarial attacks, statistical sampling, smoothed model, certification guarantees, incremental robustness certification, IRS, certified accuracy, input perturbations, robustness, computational cost, DNN deployment, approximation techniques, quantization, pruning, network behavior, certified radius, disparity, upper bound, probability, lower confidence bound, Gaussian noise, confidence parameter, Clopper-Pearson lemma, binomial confidence interval, ImageNet, CIFAR10, ResNet-20, ResNet-110, ResNet-50, float16, bfloat16, int8 quantization, dynamic per-channel quantization, l1 unstructured pruning, average certified radius, speedup, Monte Carlo estimation, approximate model, Gaussian samples, DNN approximations, incremental program verification, Randomized Smoothing.",
    "ag3o2T51Ht.pdf.json": "text-to-image generative models, concept erasure methods, photo-realistic images, algorithmic toolkit, AI safety, Stable Diffusion (SD), DALL-E 2, LAION-400M, NSFW filters, post hoc concept erasure, inference guidance, fine-tuning, Erased Stable Diffusion (ESD), Selective Amnesia, Forget-me-not, Ablating Concepts, Unified Concept Editing (UCE), Negative Prompt (NP), Safe Latent Diffusion (SLD), Concept Inversion (CI), machine unlearning, denoising diffusion models, latent diffusion models (LDM), Textual Inversion (TI), Gaussian noise vector, conditional input, objective function, classifier-free guidance, generative models, input filtering, image manipulation, artistic style, object removal, identity concepts, NSFW content, CLIP, gradient descent, hyperparameter tuning.",
    "qiduMcw3CU.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, curse of dimensionality, skill primitives, temporal logic specifications, regular language, reinforcement learning, sample efficiency, compositionality, spatial composition, temporal composition, Boolean operators, reward machines (RM), finite state machines (FSM), skill machines (SM), Markov Decision Process (MDP), value function, expected return, optimal action-value function, Q-learning, goal-oriented value functions (WVFs), linear temporal logic (LTL), discount factor, task primitives, constraints, skill compositions, skill transfer, empirical results, continuous control environment, high-dimensional video game, task specification, skill compositions, optimal policy, agent interaction, semantic meaningful behaviours.",
    "qPFsIbF3V6.pdf.json": "self-attention, hyperparameter tuning, learned transpilation, automated symbolic program translation, probabilistic neural language models, neurosymbolic approach, assembly code, GUESS & SKETCH, cross-ISA software management, instruction set architectures (ISAs), semantic equivalence, generative language models, conditional generative language model, Transformer architecture, attention, sketching, program synthesis, correctness specification, deterministic measurable output, counterexample guided inductive synthesis, assembly-to-assembly transpilation, ARMv8, RISC-V, reduced instruction set architectures (ISAs), Cross-compilation, symbolic solver, Rosette, Z3 SMT solver, fine-tuned models, large language models, Few-shot learning, automated transpilers, LLVM intermediate representation, tree-to-tree neural networks, denoising auto-encoding, back-translation, long-tail logical reasoning, performance evaluation, memory management issues.",
    "RvfPnOkPV4.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, WHAT’S IN MY BIG DATA (WIMBD), data contamination, language models (LMs), large text corpora, machine learning (ML), datasets, C4, The Pile, RedPajama, benchmark contamination, Winograd Schema Challenge, GLUE, SuperGLUE, Elasticsearch, map-reduce, token distribution, duplicates, toxic language, personally identifiable information (PII), data statistics, data quality, community-relevant measurements, cross-corpora analysis, unique n-grams, n-grams, document length distribution, count functionality, toxic language classifier, demographic sentiment, data documentation, decontaminating datasets, heuristic methods, deep learning, generative models, AI systems, text-to-image models, quantum entanglement, thermodynamic equilibrium",
    "nHkMm0ywWm.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, causal discovery, latent variables, identification theorem, high-order statistics, causal graph, linear system, LiNGAM, statistical independence, conditional independence, non-Gaussianity, Greedy Equivalence Search, Generalized Independent Noise, causal relationships, measurement models, hierarchical models, Tetrad condition, pure children, atomic units, d-separation, structural equation model, directed acyclic graph, PO-LiNGAM, structure identifiability, surrogate variables, measurement surrogate variables, overlapping variables, causal strength, latent genetic factors, synthetic data, real-world data, correct ordering rate, error rate, F1-score, latent confounders, mixture oracles, conditional independence constraints, overcomplete ICA, measurement assumption, latent hierarchical structure, empirical validation, causal ordering, statistical estimation methods.",
    "YOKnEkIuoi.pdf.json": "self-attention, hyperparameter tuning, variance schedule, probabilistic conditioning, diffusion models, generative models, inverse problems, model convergence, loss function, Conditional Variational Diffusion Model (CVDM), Deep networks, L1 norm, L2 norm, likelihood-based method, Denoising Diffusion Probabilistic Models (DPMs), Conditional Denoising Diffusion Probabilistic Models (CDDPMs), Gaussian diffusion process, continuous-time diffusion loss, noise prediction model, Monte Carlo estimator, Evidence Lower Bound (ELBO), multi-scale structural similarity index measure (MS-SSIM), Mean Absolute Error (MAE), peak signal-to-noise ratio (PSNR), image super-resolution, BioSR dataset, quantitative phase imaging, Transport of Intensity Equation (TIE), convolutional neural networks (CNNs), generative adversarial networks (GANs), regularization term, stochastic differential equation, optimization algorithm, noise embedding, latent variable, variance-preserving condition, Hadamard product, dual-variable schedule, separation-of-variables strategy, spectral domain analysis, phase retrieval, probability distribution, high-resolution image recovery.",
    "qNrJJZAKI3.pdf.json": "Harvard-FairSeg, fairness learning, medical segmentation, segmentation performance equity, equity-scaled segmentation performance metric, fair error-bound scaling (FEBS), loss function, sensitive attributes, Dice coefficient, intersection over union (IoU), Segment Anything Model (SAM), SAMed, TransUNet, artificial intelligence, machine learning, demographics, medical imaging, clinical task, classification tasks, group fairness, individual fairness, fairness metrics, data inequalities, image classification, glaucoma, prompt encoder, mask decoder, nonlinear transformations, adversarially fair representations (ADV), group distributionally robust optimization (GroupDRO), performance discrepancies, algorithmic bias, demographics disparity, pixel-wise annotation, deep learning models, data quality, fairness metrics design, demographics-sensitive attributes, scanning laser ophthalmoscopy (SLO) fundus images, demographic equity, model parameters, segmentation map, Bruch’s membrane opening, internal limiting membrane, training errors, systematic disparities, benchmarking framework, model convergence, medical AI applications.",
    "OUkZXbbwQr.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, reinforcement learning, justifiable policy, debate-based reward model, zero-sum debate game, policy convergence, supporting evidence, accountable decision-making, agent verification, human-in-the-loop evaluations, Markov Decision Processes (MDP), action-value function, Q-learning, double-deep Q-network, dueling architecture, prior probability, argumentation strategies, judge model, preference dataset, SOFA score, MIMIC-III, observational reward, cross-entropy loss, feature attribution, SHAP, leaky-relu activation, parametric relu, preference comparison, adversarial agents, defense strategies, human bias, empirical evaluation, action space, contextualized extensive form game, utility function, reward sparsification, reward decomposition, empirical training, argumentative policies, data-driven approaches.",
    "qCUWVT0Ayy.pdf.json": "LayoutNUWA, Code Instruct Tuning (CIT), Code Initialization (CI), Code Completion (CC), Code Rendering (CR), large language models (LLMs), adaptive quantization, k-Means algorithm, generative adversarial networks (GANs), variational autoencoders (VAE), LayoutGAN, LayoutVAE, LayoutGAN++, NDN, READ, self-attention, Frechet Inception Distance (FID), Maximum Interaction over Union (mIoU), alignment, overlap, datasets, RICO, PubLayNet, Magazine, autoregressive methods, diffusion models, conditional layout generation, numeric tuple optimization, semantic information, numeric values, HTML, layout generation, low-resource datasets, domain-specific, domain-agnostic.",
    "pAoqRlTBtY.pdf.json": "Causal Modelling Agent (CMA), Large Language Models (LLMs), Deep Structural Causal Models (DSCMs), causal discovery, causal relationships, biomarkers, Directed Acyclic Graphs (DAGs), generative process, causal graph, hypothesis generation, model fitting, post-processing, hypothesis amendment, interventional queries, counterfactual reasoning, multi-modal data, log-likelihood, familywise error rate (FWER), synthetic variables, Markov equivalence, retrieval augmented generation (RAG), chain graph models, probabilistic generative modelling, data likelihood, Neuroimaging Initiative (ADNI), Alzheimer’s Disease (AD), counterfactual inference, protein aggregation, tau pathology, cross-sectional studies, graphical independence, observational distribution, TREM2 gene, sTREM2 protein, heterogeneous variables, multi-modal datasets, Bayesian Network, computational graph, inference engine.",
    "gzT61ziSCu.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, higher-order functions, functional analysis, automatic differentiation, JAX, primitive operators, functional gradients, linearization rules, transposition rules, forward mode automatic differentiation, reverse mode automatic differentiation, Jacobian vector product (JVP) rules, vector-Jacobian product (VJP), Fréchet derivative, calculus of variations, least action principle, variational method, functional differentiation, density functional theory (DFT), exchange-correlation functionals, integral functionals, kernel function, Fourier neural operator, Lagrangian mechanics, semi-local functionals, non-local operators, functional derivatives, automatic functional differentiation (AutoFD), functional optimization, variational problem, Euler-Lagrange equation, chain rule, numerical integration, Gaussian quadrature, pointwise transformation, elasticity theory, tensor transformation, composition operator, nabla operator, linearize operator, integrate operator, adjoint operator, numerical grid.",
    "vE5MyzpP92.pdf.json": "deep metric learning, DML, image retrieval, threshold inconsistency, false accept rate, FAR, false reject rate, FRR, Operating-Point-Inconsistency-Score, OPIS, accuracy-threshold consistency Pareto frontier, Threshold-Consistent Margin, TCM, contrastive loss, pairwise loss, proxy-based losses, Smooth-AP loss, Recall@k Surrogate loss, ProxyAnchor, ProxyNCA, SoftTriple, ArcFace, iNaturalist-2018, Stanford Online Product, CUB-200-2011, Cars-196, distance threshold, calibration-aware training, Expected Calibration Error, ECE, Maximum Calibration Error, MCE, Adaptive ECE, calibration threshold, One-Threshold-for-All, OTA, posthoc calibration, Platt calibration, isotonic regression, temperature scaling, conformal prediction, utility score, cosine similarity, hard mining strategy, representation structures, training time complexity, kernel-density estimation, recall@k, accuracy, SOTA methods, high-accuracy regime, low-accuracy regime, feature compression, hyperparameter tuning.",
    "MEGQGNUfPx.pdf.json": "self-attention, adversarial training (AT), robust overfitting, forgetting, Forget to Mitigate Overfitting (FOMO), deep neural networks (DNNs), adversarial attacks, benchmark datasets, model generalization, model convergence, loss function, AutoAttacks, double descent, K-class dataset, ℓp-robustness, adversarial robustness, min-max optimization problem, Ladv, cross-entropy (CE) loss, memory retention, forgetting mechanism, long-term memory, consistency loss, stable model, active forgetting, interleaved training, regularization, sparsity rate, PreAct-ResNet18, WideResNet-3410, PGD-20 attack, robustness, CIFAR-10 dataset, empirical results, knowledge distillation, stochastic weight averaging (SWA), adversarial examples, DNN robustness, Gaussian noise, flatter minima, consistency regularization loss, adversarial weight perturbation (AWP), temporal ensembling (TE), minimum loss landscape.",
    "dHng2O0Jjr.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, instruction tuning, large language models, LLMs, ToolLLM, ToolBench, API collection, RESTful APIs, instruction generation, solution path annotation, depth-first search-based decision tree, DFSDT, automatic evaluator, ToolEval, LLaMA, ToolLLaMA, neural API retriever, APIBench, CoT, ReACT, GPT-4, ChatGPT, SOTA, decision-making strategy, training data, generalization performance, instruction-following ability, pass rate, win rate, embedding similarity, contrastive learning, NDCG, multimodal functionalities, API retriever, decision retraction, tree-of-thought reasoning",
    "pETSfWMUzy.pdf.json": "self-evaluation, rewind mechanism, self-boosting, Rewindable Auto-regressive INference (RAIN), alignment methods, RLHF, RLAIF, RRHF, RAFT, DPO, supervised finetuning (SFT), reward modeling (RM), reinforcement learning (RL), Proximal Policy Optimization (PPO), superficial alignment hypothesis, reject sampling, self-alignment, generative modeling, evaluation outcomes, token set, search process, harmful responses, helpfulness rate, truthfulness, LLaMA, LLaMA 30B, LLaMA-2-chat 13B, Anthropic’s Helpful and Harmless (HH) dataset, TruthfulQA dataset, IMDB dataset, GPT-4, sampling, embedding, cosine similarity, parameter updates, fixed LLMs, auto-regressive inference, empirical findings, human preference data, unaligned LLMs, computational resources, advanced models, exploration encouragement.",
    "kIP0duasBb.pdf.json": "zero-shot generalization, test time adaptation (TTA), CLIP, reinforcement learning with CLIP feedback (RLCF), entropy minimization, prompt tuning, contrastive loss, vision-language models (VLMs), cosine similarity, zero-shot classification, zero-shot image retrieval, zero-shot image captioning, CLIPScore, task-specific sampling strategies, learnable prefix tokens, model outputs, feedback mechanism, empirical results, model convergence, data augmentors, classification task, retrieval task, perfect reward model, feedback resource, domain gap, flexible framework, incremental learning, momentum buffer, temperature scaling, cross-domain image captioning, fully test-time adaptation, image encoder tuning, text encoder tuning, knowledge distillation (KD), reinforcement learning with human feedback (RLHF), proximal policy optimization (PPO), self-supervised auxiliary tasks, contrastive learning, dynamic learning, embedding space, language supervision, reward function, average CLIPScore, image captioning models, beam search, episodic TTA, weight decay, AdamW optimizer, evaluation metrics, empirical validation.",
    "tOzCcDdH9O.pdf.json": "Matryoshka Diffusion, high-resolution image synthesis, video synthesis, diffusion process, NestedUNet architecture, computational challenges, progressive training schedule, zero-shot generalization, class-conditioned image generation, text-to-image applications, text-to-video applications, multiresolution loss, denoising objective, latent variable models, signal-to-noise ratio, backward model, neural network, hierarchical generation, auto-encoder, latent diffusion models (LDMs), GANs, hyperparameters, Fréchet Inception Distance (FID), CLIP scores, CC12M dataset, ImageNet, WebVid-10M, gradient descent, self-attention layers, downsampled latent space, model convergence, optimization, progressive compression, training efficiency, generative models, Augmented space, multi-scale computation sharing, smooth transition, parameter sharing, temporal dynamics, image-text pairs.",
    "ktG8Tun1Cy.pdf.json": "Score Distillation Sampling (SDS), Classifier-Free Guidance (CFG), Classifier Score Distillation (CSD), Neural Radiance Fields (NeRF), diffusion models, likelihood-based generative model, conditional score function, Mean Squared Error (MSE), probability density distillation, Gaussian transition kernels, deep learning, 3D content creation, text-to-3D generation, implicit classification model, CLIP loss, negative classifier score, variational lower bound, high-density data regions, extreme photo-realistic appearances, texture synthesis, text-driven 3D editing, multi-view-consistent diffusion models, latent variables, Markov Chain, pre-trained 2D diffusion models, self-attention, gradient descent, shape generation, shape editing, texture field, diffusion guidance, high-quality generation results, scalable 3D generation, dual-objective Classifier Score Distillation, annealed negative classifier score optimization, 3D representations, implicit noise-aware classifiers, adaptive negative classifier scores.",
    "wfzXa8e783.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, fine-tuning, Stable Diffusion, LyCORIS, Lora, LoRa, LoHa, LoKr, GLoRA, (IA)3, DreamBooth, Textual Inversion, text-to-image synthesis, image generation, latent diffusion model, LAION 5-billion dataset, concept fidelity, text-image alignment, image diversity, preservation, base model style, loss function, metrics, probabilistic generative models, denoising operations, model customization, hyperparameters, cross-attention control, ControlNet, matrix factorization, federated learning, Hadamard product, Kronecker product, evaluating fine-tuned models, prompt categories, fidelity measures, controllability, diversity, base model preservation, image quality, data balancing, dimensionality, training epochs, learning rate, algorithm evaluation, performance discrepancy, dataset influence, evaluation metrics, fine-tuning algorithms.",
    "qoHeuRAcSl.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, mode families, motion constraints, manipulation planning, robot manipulation, explanation-based learning, structured policies, MMLP, 2D continuous-space, robotic manipulation, counterfactual perturbations, classification accuracy, action abstractions, grounding, affordances, behavior cloning, Gaussian noises, multi-layer perceptrons, motion constraints, kinematic constraints, trajectory following controller, loss function, mode-specific policies, trajectory segmentation, pseudo-attractor, potential field, task execution, action commands, state-action pairs, performance drop, perturbation, task success labels",
    "p4eG8rCa0b.pdf.json": "depth disentanglement training (DDT), soft guidance, COMPOSE AND CONQUER (CNC), text-conditional diffusion models, synthetic image triplets, 3D object placement, global stylistic semantics, segmentation maps, depth maps, bounding boxes, inpainting masks, latent text-conditional DM (LDM), Stable Diffusion (SD), UNet, ResNet, Transformer, CLIP image embeddings, generative latent variable models, cross-attention, concept bleeding, COCO-Stuff, Pick-a-Pic, variational lower bound, FID, Inception Score, CLIPScore, MAE, SSIM, English, synthetic images, relative depth associations, image embeddings, spatial grounding information, perceptual similarity, structural similarity",
    "WOiOzHG2zD.pdf.json": "Noisy Text Fields (NTFs), TextField3D, conditional 3D generative model, 3D representation, open-vocabulary generation, V-L pre-trained knowledge, multi-modal discrimination, text-3D discriminator, text-2.5D discriminator, NTFGen module, NTFBind module, 3D latent code, view-invariant image latent code, differentiation, 3D data, text prompts, generative latent codes, cosine distance, text consistency, geometry generation, texture generation, styleGAN, DreamFields, DreamFusion, Point-E, Shap-E, synthetic 3D data, Objaverse, ShapeNet, CLIP, BERT, T5, FID, CLIP-score, GAN, diffusion models, NeRF, Score Distillation Sampling (SDS), Variational Score Distillation (VSD), encoder-decoder architecture, hidden representations, audio-visual correspondence, performance benchmarks, 3D generative adversarial network (GAN), 3D consistency problems.",
    "ShjMHfmPs0.pdf.json": "self-attention, hyperparameter tuning, zero-shot learning, autophagous loops, generative AI, synthetic data, Model Autophagy Disorder (MAD), fixed real training data, fresh real data, data quality, data diversity, generative image models, StyleGAN2, Stable Diffusion, ChatGPT, LAION-5B dataset, Wasserstein GAN (WGAN), Normalizing Flow, Gaussian mixture models, Fréchet inception distance (FID), precision, recall, sampling bias, generative models, diffusion model, gradient descent, empirical analysis, deep learning models, classifier-free diffusion guidance factor, zero-shot learning, artifacts, model convergence, loss function, model sampling biases, architectural fingerprint."
}